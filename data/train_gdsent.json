[{"qid": "2bc9c4f9c19c167187f2", "term": "Genghis Khan", "description": "founder and first Great Khan of the Mongol Empire", "question": "Are more people today related to Genghis Khan than Julius Caesar?", "answer": true, "facts": ["Julius Caesar had three children.", "Genghis Khan had sixteen children.", "Modern geneticists have determined that  out of every 200 men today has DNA that can be traced to Genghis Khan."], "decomposition": ["How many kids did Julius Caesar have?", "How many kids did Genghis Khan have?", "Is #2 greater than #1?"], "evidence": [[[["Caesarion-2", "Julia (daughter of Caesar)-1"]], [["Alakhai Bekhi-1", "Tolui-1"], "no_evidence"], ["operation"]], [[["Julius Caesar-75"]], [["Genghis Khan-17"]], ["operation"]], [[["Gaius Julius Caesar-7"]], [["Genghis Khan-15"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["", "Julia (c. 76 BC \u2013 54 BC) was the daughter of Roman dictator Julius Caesar by his first or second wife Cornelia, and his only child from his marriages."], ["", "Tolui (c.1191\u20131232) was the fourth son of Genghis Khan by his chief khatun B\u00f6rte."]]}, {"qid": "03caf265939fab701dee", "term": "The Police", "description": "English rock band", "question": "Could the members of The Police perform lawful arrests?", "answer": false, "facts": ["The members of The Police were musicians, not law enforcement officers.", "Only law enforcement officers can perform lawful arrests."], "decomposition": ["Who can perform lawful arrests?", "Are members of The Police also #1?"], "evidence": [[[["Arrest-2"]], [["Citizen's arrest-2", "The Police-1"], "operation"]], [[["Arrest-7", "Law enforcement officer-13"]], [["Law enforcement officer-13", "The Police-8"]]], [[["Lawful interception-28"]], ["operation"]]], "golden_sentence": [["Police and various other officers have powers of arrest."], ["", ""]]}, {"qid": "aadc8000bfcb987d6b9d", "term": "Depression (mood)", "description": "state of low mood and fatigue", "question": "Would a Monoamine Oxidase candy bar cheer up a depressed friend?", "answer": false, "facts": ["Depression is caused by low levels of serotonin, dopamine and norepinephrine.", "Monoamine Oxidase breaks down neurotransmitters and lowers levels of serotonin, dopamine and norepinephrine."], "decomposition": ["Depression is caused by low levels of what chemicals?", "Monoamine Oxidase has an effect on what chemicals?", "Of the chemicals listed in both #1 and #2, does Monoamine Oxidase raise their levels?"], "evidence": [[[["Monoamine oxidase-8"]], [["Monoamine oxidase-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Depression (mood)-13"]], [["Monoamine oxidase-8"]], [["Serotonin-36"], "operation"]], [[["Biology of depression-1"]], [["Monoamine oxidase-7"]], ["operation"]]], "golden_sentence": [["For example, unusually high or low levels of MAOs in the body have been associated with schizophrenia, depression, attention deficit disorder, substance abuse, migraines, and irregular sexual maturation."], [""]]}, {"qid": "d1a138ecfa13ee277ab4", "term": "Grey seal", "description": "species of seal", "question": "Would a dog respond to bell before Grey seal?", "answer": true, "facts": ["Grey seals have no ear flaps and their ears canals are filled with wax.", "Grey seals hear better underwater when their ears open like a valve.", "Dogs have sensitive ears that can hear as far as a quarter of a mile away."], "decomposition": ["How sensitive is a grey seal's hearing on land?", "How sensitive is a dog's hearing on land?", "Is #2 better than #1?"], "evidence": [[[["Pinniped-24"]], [["Hearing range-11", "Hertz-5"]], ["operation"]], [[["Grey seal-1"], "no_evidence"], [["Dog-54"], "no_evidence"], ["no_evidence", "operation"]], [[["Grey seal-1"], "no_evidence"], [["Dog anatomy-114"]], ["operation"]]], "golden_sentence": [["500 to 32,000\u00a0Hz in the northern fur seal, compared to 20 to 20,000\u00a0Hz in humans), their airborne hearing sensitivity is weaker overall."], ["The hearing ability of a dog is dependent on breed and age, though the range of hearing is usually around 67\u00a0Hz to 45\u00a0kHz.", ""]]}, {"qid": "f945d8a4274bb3805989", "term": "Pound sterling", "description": "Official currency of the United Kingdom and other territories", "question": "Is a pound sterling valuable?", "answer": false, "facts": ["A pound sterling is fiat money.", "Fiat money is backed by government decree and has no intrinsic value.", "One pound sterling is worth about 1.24 US dollars by May of 2020."], "decomposition": ["What is the value of the Pound Sterling based on?", "Is #1 the material used in making it?"], "evidence": [[[["Pound sterling-16"]], [["Pound sterling-16"]]], [[["Pound sterling-1", "Pound sterling-12"]], [["Pound sterling-71"]]], [[["Pound sterling-16"]], [["One pound (British coin)-3"], "operation"]]], "golden_sentence": [[""], ["At various times, the pound sterling was commodity money or bank notes backed by silver or gold, but it is currently fiat money, with its value determined only by its continued acceptance in the national and international economy."]]}, {"qid": "4b72e0ddaff371e921aa", "term": "Shrimp", "description": "Decapod crustaceans", "question": "Is shrimp scampi definitely free of plastic?", "answer": false, "facts": ["Shrimp scampi is a dish made with shrimp.", "Shrimp have been found to contain microplastics.", "Microplastics are plastic material."], "decomposition": ["What protein is Shrimp scampi made out of?", "What have #1 been found to contain?", "Are #2 free from plastic?"], "evidence": [[[["Scampi-1"]], [["Plastic pollution-31", "Plastic pollution-48"], "no_evidence"], ["no_evidence", "operation"]], [[["Fish-92", "Scampi-2"]], [["Microplastics-12"]], [["Microplastics-1"]]], [[["Scampi-8"]], ["no_evidence"], ["no_evidence"]]], "golden_sentence": [[""], ["", ""]]}, {"qid": "93e3995669f121e630ef", "term": "Rede Globo", "description": "Brazilian commercial television network", "question": "Do the anchors on Rede Globo speak Chinese?", "answer": false, "facts": ["Rede Globo is a Brazilian television network.", "The official language of Brazil is Portuguese."], "decomposition": ["What country broadcasts Rede Globo?", "What is the official language of #1?", "Is #2 Chinese?"], "evidence": [[[["Rede Globo-1"]], [["Brazil-1"]], ["operation"]], [[["Rede Globo-1"]], [["Brazil-1"]], ["operation"]], [[["Rede Globo-1"]], [["Portuguese language-1"]], ["operation"]]], "golden_sentence": [["Rede Globo (Portuguese:\u00a0[\u02c8\u0281ed\u0292i \u02c8\u0261lobu], Globe Network), or simply Globo, is a Brazilian free-to-air television network, launched by media proprietor Roberto Marinho on 26 April 1965."], ["It is the largest country to have Portuguese as an official language and the only one in the Americas; it is also one of the most multicultural and ethnically diverse nations, due to over a century of mass immigration from around the world."]]}, {"qid": "8bc8ea4aa7a9ff69b37f", "term": "Wonder Woman (2017 film)", "description": "American superhero film directed by Patty Jenkins", "question": "Is a Boeing 737 cost covered by Wonder Woman (2017 film) box office receipts?", "answer": true, "facts": ["The average cost of a US Boeing 737 plane is 1.6 million dollars.", "Wonder Woman (2017 film) grossed over 800 million dollars at the box office."], "decomposition": ["How much does a Boeing 737 cost?", "How much did the 2017 movie Wonder Woman gross?", "Is #2 greater than #1?"], "evidence": [[["no_evidence"], [["Wonder Woman (2017 film)-3"]], ["no_evidence", "operation"]], [["no_evidence"], [["Wonder Woman (2017 film)-3"]], ["operation"]], [[["Boeing 737-13"], "no_evidence"], [["Wonder Woman (2017 film)-31"]], ["operation"]]], "golden_sentence": [["It grossed over $821\u00a0million worldwide, making it the tenth highest-grossing film of 2017 as well as the highest-grossing film by a solo female director."]]}, {"qid": "288b94dd2ffbaacbb22f", "term": "Casio", "description": "Japanese electronics company", "question": "Can you buy Casio products at Petco?", "answer": false, "facts": ["Casio is a manufacturer of consumer electronics and watches", "Petco is a chain store that sells pet supplies like food, bowls, litter, toys, cages and grooming equipment"], "decomposition": ["What kind of products does Casio manufacture?", "What kind of products does Petco sell?", "Does #1 overlap with #2?"], "evidence": [[[["Casio-1"]], [["Petco-1"]], ["operation"]], [[["Casio-1"]], [["Petco-1"]], ["operation"]], [[["Casio-1"]], [["Petco-1"]], ["operation"]]], "golden_sentence": [["Its products include calculators, mobile phones, digital cameras, electronic musical instruments, and analogue and digital watches."], ["Petco sells pet products and services, as well as certain types of live animals."]]}, {"qid": "9956c07e69bfa6f23e51", "term": "Space Race", "description": "Competition between the USSR and the USA to explore space", "question": "Did the Space Race use relay batons?", "answer": false, "facts": ["The Space Race was a competition between the USA and USSR regarding spaceflight and exploration", "Relay batons are used in relay races", "Relay races are athletic track and field events"], "decomposition": ["What was the Space Race?", "What are relay batons used for?", "Is #1 the same as #2?"], "evidence": [[[["Space Race-1"]], [["Relay race-11"]], [["Relay race-11", "Space Race-1"], "operation"]], [[["Space Race-1"]], [["Relay race-1"]], ["operation"]], [[["Space Race-1"]], [["Relay race-1"]], ["operation"]]], "golden_sentence": [["The Space Race was a 20th-century competition between two Cold War rivals, the Soviet Union (USSR) and the United States (US), to achieve firsts in spaceflight capability."], [""], ["", ""]]}, {"qid": "02b706b9b1b051fc7d5b", "term": "Deciduous", "description": "Trees or shrubs that lose their leaves seasonally", "question": "Are Christmas trees dissimilar to deciduous trees?", "answer": true, "facts": ["Christmas trees are usually pine trees.", "Pine trees keep their needles all year round."], "decomposition": ["Which kind of trees are commonly used as Christmas trees?", "Are #1 dissimilar to deciduous trees?"], "evidence": [[[["Christmas tree-1"]], [["Deciduous-1"], "operation"]], [[["Christmas tree-56"]], [["Fir-1"], "operation"]], [[["Christmas tree-1"]], [["Deciduous-1"]]]], "golden_sentence": [["A Christmas tree is a decorated tree, usually an evergreen conifer, such as a spruce, pine or fir, or an artificial tree of similar appearance, associated with the celebration of Christmas, hence its name, originating in Northern Europe."], [""]]}, {"qid": "9db098aea8bab57bb47d", "term": "Biochemistry", "description": "study of chemical processes in living organisms", "question": "Does Biochemistry study gluons?", "answer": false, "facts": ["Biochemistry studies role, function, and structure of biomolecules.", "Gluon, the so-called messenger particle of the strong nuclear force, which binds sub-atomic particles known as quarks within the protons and neutrons of stable matter as well as within heavier, short-lived particles created at high energies.", "biomolecules are comprised of atoms. "], "decomposition": ["What are gluons?", "What things are studied in biochemistry?", "Is #1 included in #2?"], "evidence": [[[["Gluon-1"]], [["Biochemistry-1"]], ["operation"]], [[["Gluon-1"]], [["Biochemistry-2"], "no_evidence"], ["operation"]], [[["Gluon-1"]], [["Biochemistry-1", "Biochemistry-4"]], ["operation"]]], "golden_sentence": [["A gluon (/\u02c8\u0261lu\u02d0\u0252n/) is an elementary particle that acts as the exchange particle (or gauge boson) for the strong force between quarks."], ["Biochemistry, sometimes called biological chemistry, is the study of chemical processes within and relating to living organisms."]]}, {"qid": "12ace6f9fcd0ce3f7b64", "term": "Kingdom of Hungary", "description": "former Central European monarchy (1000\u20131946)", "question": "Did land owners elect their rulers in the Kingdom of Hungary?", "answer": false, "facts": ["The Kingdom of Hungary was a monarchy.", "Monarchies do not allow citizens to elect their own rulers."], "decomposition": ["Which kind of government ruled over the Kingdom of Hungary?", "Does #1 allow citizens to elect their own rulers?"], "evidence": [[[["Kingdom of Hungary-1"]], [["Monarchy-1"], "operation"]], [[["Kingdom of Hungary-1"]], [["Kingdom of Hungary-1"]]], [[["Kingdom of Hungary-1"]], [["Monarchy-16"], "operation"]]], "golden_sentence": [["The Principality of Hungary emerged as a Christian kingdom upon the coronation of the first king Stephen I at Esztergom around the year 1000; his family (the \u00c1rp\u00e1d dynasty) led the monarchy for 300\u00a0years."], [""]]}, {"qid": "fcd77236180fa2cb4e69", "term": "Nancy Pelosi", "description": "52nd speaker of the United States House of Representatives", "question": "Would Nancy Pelosi publicly denounce abortion?", "answer": false, "facts": ["Abortion is a topic that divides the Republican Party and the Democratic Party.", "Nancy Pelosi is a Democrat and appeals to that base.", "The Democratic base is largely pro-choice with abortion. "], "decomposition": ["Which US political party does Nancy Pelosi identify with?", "Do most people who identify with #1 oppose abortion?"], "evidence": [[[["Nancy Pelosi-1"]], [["Democratic Party (United States)-3"], "operation"]], [[["Nancy Pelosi-1"]], [["Democratic Party (United States)-3"], "no_evidence"]], [[["Speaker of the United States House of Representatives-3"]], [["Democratic Party (United States)-70"]]]], "golden_sentence": [[""], [""]]}, {"qid": "bcd0ea8e008bc77fc9a0", "term": "Dragon Ball", "description": "Japanese media franchise", "question": "Does Dragon Ball shows and movies fall short of Friday 13th number of projects?", "answer": true, "facts": ["Dragon Ball has 6 TV series, 3 TV specials, and 2 direct to video spinoffs as of 2020.", "Friday the 13th has 12 movies in the franchise and 1 TV series as of 2020."], "decomposition": ["How many Dragon Ball series, TV specials and other features have been released?", "How many Friday the 13th franchise films and television series have been released?", "Is #2 greater than #1?"], "evidence": [[[["Dragon Ball-2"]], [["Friday the 13th (franchise)-1"]], ["operation"]], [[["Dragon Ball-2"]], [["Friday the 13th (franchise)-1", "Friday the 13th (franchise)-2"]], ["operation"]], [[["Dragon Ball-28", "Dragon Ball-33"], "no_evidence"], [["Friday the 13th (franchise)-3"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Additionally, the studio has developed 20 animated feature films and three television specials, as well as two anime sequel series titled Dragon Ball GT (1996\u20131997) and Dragon Ball Super (2015\u20132018)."], ["Friday the 13th is an American horror franchise that comprises twelve slasher films, a television series, novels, comic books, video games, and tie\u2011in merchandise."]]}, {"qid": "8157923564915cd3ac47", "term": "Amnesia", "description": "Cognitive disorder where the memory is disturbed or lost", "question": "Would a student of the class of 2017 have amnesia about 9/11?", "answer": true, "facts": ["Childhood amnesia is common, with most adults not remembering their lives before 2 or 3 years of age", "9/11 occurred in 2001", "Students graduating high-school in 2017 would have been born in 1999"], "decomposition": ["What year did 9/11 occur?", "In what year would students graduating high-school in 2017 have been born in?", "What age is childhood amnesia most common in?", "If someone was born in #2, how old would they have been in #1?", "Does #3 overlap with #4?"], "evidence": [[[["September 11 attacks-1"]], [["Yara Shahidi-1"], "no_evidence"], [["Childhood amnesia-1"]], ["operation"], ["operation"]], [[["September 11 attacks-1"]], [["Secondary school-1"], "no_evidence"], [["Childhood amnesia-1"]], ["no_evidence", "operation"], ["no_evidence", "operation"]], [[["Inside 9/11-1"]], [["Twelfth grade-54"]], [["Childhood amnesia-3"]], [["Infant-2"]], [["Childhood amnesia-3", "Infant-2"], "operation"]]], "golden_sentence": [["The September 11 attacks (also referred to as 9/11) were a series of four coordinated terrorist attacks by the Islamic terrorist group al-Qaeda against the United States on the morning of Tuesday, September 11, 2001."], [""], ["Childhood amnesia, also called infantile amnesia, is the inability of adults to retrieve episodic memories (memories of situations or events) before the age of two to four years, as well as the period before the age of ten of which adults retain fewer memories than might otherwise be expected given the passage of time."]]}, {"qid": "ef3e52eca39670061e16", "term": "Psychic", "description": "person who claims to use extrasensory perception to identify information hidden from the normal senses", "question": "Would a psychic who admits to hot reading be trustworthy?", "answer": false, "facts": ["Hot reading is a technique used by people presenting themselves as psychics to acquire information about a subject prior to the psychic session.", "Hot reading is considered deception in the psychic community."], "decomposition": ["What do people pretend to be in order to successfully carry out hot reading?", "Do the 'real' #1 consider hot reading to be genuine?"], "evidence": [[[["Hot reading-1", "Hot reading-2"]], ["operation"]], [[["Hot reading-1"]], [["Hot reading-1", "Hot reading-2"]]], [[["Hot reading-1"]], [["Psychic-1"], "no_evidence", "operation"]]], "golden_sentence": [["", ""]]}, {"qid": "6b11357d483a2abd47ad", "term": "Comma", "description": "Punctuation mark", "question": "Is average number of peas in a pod enough commas for a billion?", "answer": true, "facts": ["The average number of peas in a pod is 6 or 7.", "A billion is a number that has three commas in it."], "decomposition": ["How many peas are in the average pod?", "How many commas are needed for a billion?", "Is #1 at least equal to #2?"], "evidence": [[[["Pea-1"], "no_evidence"], [["Billion-2"]], ["no_evidence", "operation"]], [[["Pea-1"], "no_evidence"], [["Billion-2"]], ["operation"]], [[["Pea-1"]], [["1,000,000,000-1"]], ["operation"]]], "golden_sentence": [["Each pod contains several peas, which can be green or yellow."], ["This is one thousand times larger than the short scale billion, and equivalent to the short scale trillion."]]}, {"qid": "8d4cc62ee044a42c62dc", "term": "Model (person)", "description": "person employed to display, advertise and promote products, or to serve as a visual aid", "question": "Does actress Leila George lack the height to be a model?", "answer": false, "facts": ["Actress Leila George, the daughter of Vincent D'onofrio, is 5'9\" tall.", "Model Cindy Crawford is 5'9\" tall.", "Model Agyness Deyn is 5'8\" tall.", "Model Sara Sampaio is 5'8\" tall."], "decomposition": ["How tall is Leila George?", "How tall is Cindy Crawford? ", "What is the height of model Sara Sampaio?", "Is #1 shorter than both #2 and #3?"], "evidence": [[[["Leila George-2"], "no_evidence"], [["Cindy Crawford-9"]], [["Sara Sampaio-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Leila George-1"], "no_evidence"], [["Cindy Crawford-9"]], [["Sara Sampaio-1"], "operation"], ["operation"]], [["no_evidence"], [["Cindy Crawford-9"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["Crawford is 5\u00a0feet 9\u00a0inches (175\u00a0cm) tall with brown hair and eyes."], [""]]}, {"qid": "45cb43c9ef8bf4c5662b", "term": "Dustin Hoffman", "description": "American actor and director", "question": "Is Dustin Hoffman one of the B'nei Yisrael?", "answer": true, "facts": ["Dustin Hoffman was raised in a Jewish family.", "In modern Hebrew, b'nei yisrael (\"children of Israel\") can denote the Jewish people at any time in history."], "decomposition": ["What does B'nei Yisrael refer to?", "What religion was Dustin Hoffman family as he was growing up?", "Is #2 the same as #1?"], "evidence": [[[["Israelites-11"]], [["Dustin Hoffman-7", "Dustin Hoffman-8"]], ["operation"]], [[["Israelites-8"]], [["Dustin Hoffman-7"]], ["operation"]], [[["Indian Jews in Israel-7"]], [["Dustin Hoffman-7"]], ["operation"]]], "golden_sentence": [["In modern Hebrew, b'nei yisrael (\"children of Israel\") can denote the Jewish people at any time in history; it is typically used to emphasize Jewish ethnic identity."], ["Hoffman is Jewish, from an Ashkenazi Jewish family of immigrants from Kiev, Russian Empire, and Ia\u0219i, Romania (the family's surname was spelled \"Goikhman\" in the Russian Empire).", ""]]}, {"qid": "e67af86f240ce2f24b84", "term": "McDonald's", "description": "American fast food restaurant chain", "question": "If you were on a diet, would you have to skip lunch at McDonald's?", "answer": false, "facts": ["McDonald's offers low calorie brunch options like parfaits and egg white sandwiches. ", "McDonald's offers low calorie lunch options including basic hamburgers and salads."], "decomposition": ["What is characteristic of food eaten by someone on a diet?", "Are lunch options characterized by #1 unavailable at McDonald's?"], "evidence": [[[["Dieting-1"]], [["McDonald's-2"], "operation"]], [[["Healthy diet-2"]], [["McDonald's-2"]]], [[["Dieting-21"], "no_evidence"], [["McDonald's-2"], "operation"]]], "golden_sentence": [["Dieting is the practice of eating food in a regulated and supervised fashion to decrease, maintain, or increase body weight, or to prevent and treat diseases, such as diabetes and obesity."], [""]]}, {"qid": "18ec6b0a1ecc89a49e38", "term": "The Powerpuff Girls", "description": "American animated television series", "question": "Could the Powerpuff Girls hypothetically attend the Camden Military Academy?", "answer": false, "facts": ["The Powerpuff Girls are kindergarten aged girls.", "Camden Military Academy is a private, all-male, military boarding school located in Camden, South Carolina.", "Camden Military Academy accepts male students in grades 7 through 12."], "decomposition": ["What gender are the Powerpuff Girls?", "What gender is allowed to attend the Camden Military Academy?", "Is #1 the same as #2?"], "evidence": [[[["The Powerpuff Girls-1"]], [["Camden Military Academy-1"]], [["Camden Military Academy-1", "The Powerpuff Girls-1"]]], [[["The Powerpuff Girls-1"]], [["Camden Military Academy-1"]], ["operation"]], [[["The Powerpuff Girls-1"]], [["Camden Military Academy-1"]], ["operation"]]], "golden_sentence": [["It centers on Blossom, Bubbles, and Buttercup, three kindergarten-aged girls with superpowers."], ["Camden Military Academy (CMA) is a private, all-male, military boarding school located in Camden, South Carolina, United States."], ["", ""]]}, {"qid": "51d679c2511fa35fc05f", "term": "CT scan", "description": "medical imaging procedure which uses X-rays to produce cross-sectional images", "question": "Would an uninsured person be more likely than an insured person to decline a CT scan?", "answer": true, "facts": ["Without insurance, a CT scan can cost up to $5,000.", "Most insurance companies will cover or reimburse the cost of a CT scan."], "decomposition": ["Typically how much does it cost to get a CT scan without insurance?", "On average, how much does it cost to get a CT scan with insurance?", "Is #2 less than #1?"], "evidence": [[[["Full-body CT scan-12"]], [["Full-body CT scan-12"]], ["operation"]], [[["CT scan-53"], "no_evidence"], [["CT scan-53"], "no_evidence"], ["operation"]], [[["CT scan-53"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Possibly high cost: At a cost of US$600 to $3000, full-body scans are expensive, and are rarely covered by insurance."], ["Possibly high cost: At a cost of US$600 to $3000, full-body scans are expensive, and are rarely covered by insurance."]]}, {"qid": "9054329906964851c6d7", "term": "Donatello", "description": "Italian painter and sculptor", "question": "Was the Donatello crucifix identified in 2020 life size?", "answer": false, "facts": ["The crucifix discovered in the church of Sant\u2019Angelo depicts an adult man.", "The crucifix discovered in the church of Sant\u2019Angelo is 89 cm high.", "The crucifix discovered in the church of Sant'Angelo was identified as being a work of Donatello.", "The average height of an adult man has been at least 150 cm in historical times."], "decomposition": ["The crucifix sculpted by Donatello and identified in 2020 is a depiction of what?", "What is the average height of #1?", "What is the average height of a real, living, #1?", "Is #2 equal to #3?"], "evidence": [[[["Donatello-17"]], ["no_evidence"], [["Instrument of Jesus' crucifixion-22"]], ["operation"]], [[["Crucifix-1", "Donatello-17"]], ["no_evidence"], [["Crucifixion of Jesus-65"], "no_evidence"], ["no_evidence", "operation"]], [[["Donatello-17"]], [["Crucifix-1"], "no_evidence"], [["Short stature-2"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "91a392355a23a601fc93", "term": "Disneyland Paris", "description": "Theme park resort in France owned by The Walt Disney Company", "question": "Is Disneyland Paris the largest Disney resort?", "answer": false, "facts": ["Disneyland Paris contains two parks, several hotels, and a shopping district.", "By comparison, Walt Disney World in Florida contains four parks, two waterparks, a shopping district, and many hotels.", "Disney World is bigger than Disneyland Paris Resort and Disneyland California Resort combined."], "decomposition": ["How big is Disneyland Paris in square miles?", "How big is Walt Disney World in square miles?", "Is #1 larger than #2?"], "evidence": [[[["Disneyland Paris-40"]], [["Walt Disney World-1"]], ["operation"]], [[["Disneyland Paris-6"]], [["Walt Disney World-1"]], ["operation"]], [[["Disneyland Paris-40"]], [["Walt Disney World-1"]], ["operation"]]], "golden_sentence": [["The park is approximately 4,800 acres (1,942\u00a0ha), and is divided into two main parks that each hold separate attraction areas within them."], ["The property covers nearly 25,000 acres (39\u00a0sq\u00a0mi; 101\u00a0km2), of which only half has been used."]]}, {"qid": "2feda7a77fc2d3f2fb2e", "term": "Keyboard layout", "description": "any specific mechanical, visual, or functional arrangement of the keys of a keyboard or typewriter", "question": "Could someone with fine motor control issues benefit from an altered keyboard layout?", "answer": true, "facts": ["Fine motor control involves making small, precise movements like painting or typing. ", "The standard keyboard layout is designed to be used by someone without any motor control issues.", "There are modified keyboards for multiple types of disability."], "decomposition": ["What types of keyboard layouts exist?", "Among #1, which keyboard layouts are optimized for disabilities?", "Are any of #2 better for those with limited fine motor control?"], "evidence": [[[["Keyboard layout-170", "Keyboard layout-43", "Keyboard layout-52"]], [["Keyboard layout-170"]], [["Keyboard layout-170"], "no_evidence"]], [[["Keyboard layout-33"]], [["Keyboard layout-170"]], [["Keyboard layout-170"]]], [[["Computer keyboard-27"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["", "Although there are a large number of keyboard layouts used for languages written with Latin-script alphabets, most of these layouts are quite similar.", ""], [""], ["Its advantage is that people with disabilities who cannot press two keys at the same time will still be able to use it to type in Hangul."]]}, {"qid": "20287f4a7c88c5ab5dfe", "term": "Adam Sandler", "description": "American actor, comedian, screenwriter, and producer", "question": "Would Adam Sandler get a reference to Cole Spouse and a scuba man doll?", "answer": true, "facts": ["Adam Sandler and Cole Sprouse starred together in \"Big Daddy\".", "A prop used throughout \"Big Daddy\" was a scuba diving action figure that Cole's character called 'Scuba Steve'."], "decomposition": ["What did Adam Sandler and Cole Sprouse star together in?", "Was a scuba man doll used as a prop by Cole in #1?"], "evidence": [[[["Big Daddy (1999 film)-1"]], ["no_evidence"]], [[["Big Daddy (1999 film)-1"]], ["no_evidence", "operation"]], [[["Big Daddy (1999 film)-1"]], ["no_evidence"]]], "golden_sentence": [["Big Daddy is a 1999 American comedy film directed by Dennis Dugan and starring Adam Sandler, Joey Lauren Adams, Jon Stewart, Rob Schneider, Cole Sprouse, Dylan Sprouse, and Leslie Mann."]]}, {"qid": "6b64db440d84d202ab0c", "term": "Common warthog", "description": "Wild member of the pig family", "question": "Is there a warthog on Broadway?", "answer": true, "facts": ["Disney's The Lion King is a popular Broadway musical.", "One of the characters is named Pumbaa.", "Pumbaa is a warthog."], "decomposition": ["Which animals did Disney movie 'Lion King' feature?", "Is a warthog included in #1?", "Was Lion King adapted for a Broadway musical?", "Considering #2 and #3, would the warthog appear on Broadway?"], "evidence": [[[["The Lion King-10"]], [["The Lion King-10"]], [["The Lion King-24"]], [["The Lion King-10", "The Lion King-24"], "operation"]], [[["The Lion King-10"]], ["operation"], [["The Lion King (musical)-1"]], ["operation"]], [[["The Lion King-10"]], ["operation"], [["The Lion King (musical)-2"]], ["operation"]]], "golden_sentence": [[""], ["Simba collapses in a desert and is rescued by Timon and Pumbaa, a meerkat and warthog, who are fellow outcasts and chase away the vultures."], [""], ["", ""]]}, {"qid": "6d4bf491231a478e4822", "term": "Ice", "description": "water frozen into the solid state", "question": "Does Disney have an ice princess?", "answer": true, "facts": ["In 2013, Disney released Frozen.", "Frozen features Elsa, a princess with magical ice powers."], "decomposition": ["What are some popular Disney characters?", "Is any of #1 an ice princess?"], "evidence": [[[["Disney Princess-1"], "no_evidence"], [["Elsa (Frozen)-2"]]], [[["Elsa (Frozen)-2"]], ["operation"]], [[["Disney Princess-33"]], [["Disney Princess-33", "Elsa (Frozen)-3"], "no_evidence"]]], "golden_sentence": [[""], [""]]}, {"qid": "dc33be5b6e8b1bc0333a", "term": "Biochemistry", "description": "study of chemical processes in living organisms", "question": "Would a geographer use biochemistry in their work?", "answer": false, "facts": ["A geographer is a scientist whose area of study is geography, the study of Earth's natural environment and human society.", "Biochemistry is the study of chemical processes within and relating to living organisms."], "decomposition": ["What is the area of study of a geographer?", "What is the area of study of Biochemistry", "Is any of #1 in #2?"], "evidence": [[[["Geography-22"]], [["Biochemistry-1"]], ["operation"]], [[["Geographer-1"]], [["Biochemistry-1"]], ["operation"]], [[["Geographer-1"]], [["Biochemistry-1", "Biochemistry-4"]], ["operation"]]], "golden_sentence": [[""], ["Biochemistry, sometimes called biological chemistry, is the study of chemical processes within and relating to living organisms."]]}, {"qid": "6613219dca94a3f583ef", "term": "Compact disc", "description": "Optical disc for storage and playback of digital audio", "question": "Could George Washington's own speeches have been recorded live to a compact disc?", "answer": false, "facts": ["George Washington died in 1799.", "CDs weren't invented until 1982."], "decomposition": ["When did George Washington die?", "When were compact discs introduced?", "Is #1 after #2?"], "evidence": [[[["George Washington-1"]], [["Compact disc-1"]], ["operation"]], [[["George Washington-121"]], [["Compact disc-7"]], ["operation"]], [[["George Washington-1"]], [["Compact disc-1"]], ["operation"]]], "golden_sentence": [["George Washington (February 22, 1732 \u2013 December 14, 1799) was an American political leader, military general, statesman, and founding father who served as the first president of the United States from 1789 to 1797."], ["Compact disc (CD) is a digital optical disc data storage format that was co-developed by Philips and Sony and released in 1982."]]}, {"qid": "a0763e14dbe3b24b8cd7", "term": "Sulfur", "description": "Chemical element with atomic number 16", "question": "Would food made with black salt smell of sulfur?", "answer": true, "facts": ["Black Salt has is a kiln-fired rock salt that contains sulfur.", "When black salt is cooked with, it smells similar to rotten eggs. ", "Rotten eggs smell like sulfur."], "decomposition": ["What does black salt smell like when cooked?", "Does #1 smell similar to sulfur?"], "evidence": [[[["Kala namak-8"]], [["Kala namak-8"]]], [[["Kala namak-1"]], [["Kala namak-1"]]], [[["Sea salt-9"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], ["Sodium chloride provides kala namak with its salty taste, iron sulfide provides its dark violet hue, and all the sulfur compounds give kala namak its slight savory taste as well as a highly distinctive smell, with hydrogen sulfide being the most prominent contributor to the smell."]]}, {"qid": "2b5d632ce884eaa27740", "term": "Spirit (rover)", "description": "NASA Mars rover", "question": "Would a broadcast from Spirit make the news in 2020?", "answer": true, "facts": ["In 2010, the Spirit rover sent what is recorded as its final transmission.", "In 2019, another rover made the news with its viral \"final words\""], "decomposition": ["When did the martian rover 'Spirit' send its last transmission?", "Which rover sent another 'final message' in 2019?", "Is #1 before 2020 and did #2 make the news?"], "evidence": [[[["Spirit (rover)-1"]], [["Opportunity (rover)-1", "Opportunity (rover)-15"], "no_evidence"], ["no_evidence", "operation"]], [[["Spirit (rover)-1"]], [["Opportunity (rover)-1"]], [["Opportunity (rover)-1", "Spirit (rover)-1"], "operation"]], [[["Mars Exploration Rover-5"]], [["Opportunity (rover)-15"]], [["Opportunity (rover)-16"], "operation"]]], "golden_sentence": [["The rover became stuck in a \"sand trap\" in late 2009 at an angle that hampered recharging of its batteries; its last communication with Earth was sent on March 22, 2010."], ["Opportunity, also known as MER-B (Mars Exploration Rover \u2013 B) or MER-1, and nicknamed \"Oppy\", is a robotic rover that was active on Mars from 2004 until the middle of 2018.", ""]]}, {"qid": "93122e20f464486f1098", "term": "Frankenstein", "description": "1818 novel by Mary Shelley", "question": "Could Robert Wadlow hypothetically see Frankenstein's monster's bald spot from above?", "answer": true, "facts": ["The monster in Mary Shelley's novel, Frankenstein, was said to be 8 feet tall.", "Robert Wadlow was the world's tallest man.", "Robert Wadlow was 8 feet 11.1 inches tall."], "decomposition": ["How tall is Frankenstein?", "How tall is Robert Wadlow?", "Is #2 greater than #1?"], "evidence": [[[["Frankenstein-8"]], [["Robert Wadlow-2"]], ["operation"]], [[["Frankenstein-8"]], [["Robert Wadlow-2"]], ["operation"]], [[["Frankenstein-8"], "no_evidence"], [["Robert Wadlow-2"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Eventually, he undertakes the creation of a humanoid, but due to the difficulty in replicating the minute parts of the human body, Victor makes the Creature tall, about 8 feet (2.4\u00a0m) in height and proportionally large."], ["Wadlow reached 8\u00a0ft 11.1\u00a0in (2.72\u00a0m) in height and weighed 439\u00a0lb (199\u00a0kg) at his death at age 22."]]}, {"qid": "705b0f54ded3671f851a", "term": "Tibia", "description": "larger of the two bones of the leg below the knee for vertebrates", "question": "Is the tibia necessary to win the Stanley Cup?", "answer": true, "facts": ["The Stanley Cup is the championship trophy of the National Hockey League", "Ice hockey is a game played by individuals wearing ice skates to move around a frozen playing field", "The tibia is a leg bone", "Legs are required in order to use ice skates"], "decomposition": ["Which achievement leads to the award of the Stanley Cup?", "Which sport does #1 involve?", "Which body parts are actively involved in playing #2", "Which part of the body is the tibia found in?", "Is #4 included in #3?"], "evidence": [[[["Stanley Cup-1"]], [["Ice hockey-1"]], [["Ice hockey-55"]], [["Tibia-1"]], ["operation"]], [[["Stanley Cup-1"]], [["Ice hockey-1"]], [["Ice skate-1"]], [["Tibia-1"]], ["operation"]], [[["Stanley Cup-1"]], [["Stanley Cup-1"]], [["Ice hockey-43"], "no_evidence"], [["Tibia-1"]], ["operation"]]], "golden_sentence": [["The trophy was commissioned in 1892 as the Dominion Hockey Challenge Cup and is named after Lord Stanley of Preston, the Governor General of Canada, who donated it as an award to Canada's top-ranking amateur ice hockey club."], ["Ice hockey is a contact team sport played on ice, usually in a rink, in which two teams of skaters use their sticks to shoot a vulcanized rubber puck into their opponent's net to score goals."], ["This includes a helmet with either a visor or a full face mask, shoulder pads, elbow pads, mouth guard, protective gloves, heavily padded shorts (also known as hockey pants) or a girdle, athletic cup (also known as a jock, for males; and jill, for females), shin pads, skates, and (optionally) a neck protector."], ["The tibia is found on the medial side of the leg next to the fibula and closer to the median plane or centre-line."]]}, {"qid": "6902fd44c42e6067fed1", "term": "Ethics", "description": "branch of philosophy that systematizes, defends, and recommends concepts of right and wrong conduct", "question": "Would an ethics professor teach a class on Cezanne?", "answer": false, "facts": ["Cezanne was an Impressionist painter", "Aesthetics is the branch of philosophy that deals with the arts"], "decomposition": ["What was Cezanne known for?", "What branch of philosophy would deal with #1?", "Is #2 the same as ethics? "], "evidence": [[[["Paul C\u00e9zanne-1"]], [["Paul C\u00e9zanne-33"], "no_evidence"], [["Ethics-1"], "operation"]], [[["Paul C\u00e9zanne-1"]], [["Aesthetics-1"]], [["Ethics-1"], "operation"]], [[["Paul C\u00e9zanne-1"]], [["Paul C\u00e9zanne-33"]], ["operation"]]], "golden_sentence": [["Paul C\u00e9zanne (/se\u026a\u02c8z\u00e6n/ say-ZAN, also UK: /s\u026a\u02c8z\u00e6n/ siz-AN, US: /se\u026a\u02c8z\u0251\u02d0n/ say-ZAHN, French:\u00a0[p\u0254l sezan]; 19 January 1839 \u2013 22 October 1906) was a French artist and Post-Impressionist painter whose work laid the foundations of the transition from the 19th-century conception of artistic endeavor to a new and radically different world of art in the 20th century."], [""], [""]]}, {"qid": "c32d6c247da9be7099b4", "term": "Rice pudding", "description": "Dish made from rice mixed with water or milk", "question": "If you add water to rice pudding is it horchata?", "answer": false, "facts": ["Horchata is a drink made from soaking dry rice in water for hours, then pulverizing and straining the mixture and adding spices and sweetener.", "Rice pudding includes ingredients like eggs, and whole grains of cooked rice. "], "decomposition": ["What ingredients are in horchata?", "What ingredients are in rice pudding?", "If you add water to #2, is it the same as #1?"], "evidence": [[[["Horchata-1"]], [["Rice pudding-1"]], ["operation"]], [[["Horchata-9"]], [["Rice pudding-4"]], ["operation"]], [[["Horchata-1"]], [["Rice pudding-1"]], ["operation"]]], "golden_sentence": [["In Spain it is made with soaked, ground, and sweetened tiger nuts, but in Mexico and other parts of the Americas the base is white rice."], ["Rice pudding is a dish made from rice mixed with water or milk and other ingredients such as cinnamon and raisins."]]}, {"qid": "b2b094303f7899b65336", "term": "Nickel", "description": "Chemical element with atomic number 28", "question": "If your skin was turning the color of a zombie, could it be because of nickel?", "answer": true, "facts": ["Zombies are often depicted as green in pallor. ", "Nickel in jewelry often turns skin a greenish color."], "decomposition": ["What color skin are zombies typically depicted with?", "Does Nickel turn a person's skin #1?"], "evidence": [[[["Zombie-3"]], [["Nickel allergy-12"]]], [["no_evidence"], [["Glass coloring and color marking-3"], "no_evidence"]], [[["Zombie-3"], "no_evidence"], [["Pallor mortis-1"], "no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "ce4cb971198309c93006", "term": "Dolce & Gabbana", "description": "Italian fashion house", "question": "Would a Dolce & Gabbana suit wearer be shunned by their Amish cousins?", "answer": true, "facts": ["Dolce & Gabbana is an Italian luxury fashion design company.", "The Amish, who value plain clothes, frown upon buttons and have banned velcro and zippers", "The Two Tone Dolce & Gabbana suit has several buttons.", "The Amish cease interactions with sinners by avoiding, or shunning them."], "decomposition": ["What type of clothing do the Amish prefer?", "What happens if an Amish person wears clothes going against #1?", "What clothing pieces are Dolce & Gabbana known for?", "If Amish cousins wore #3, would #2 happen to them?"], "evidence": [[[["Amish-27"]], [["Excommunication-39"]], [["Dolce & Gabbana-1"]], ["operation"]], [[["Plain dress-3"]], [["Amish-6"]], [["Dolce & Gabbana-1"]], ["operation"]], [[["Amish-1"]], [["Amish-6"]], [["Dolce & Gabbana-1", "Dolce & Gabbana-32"]], [["Amish-6"], "operation"]]], "golden_sentence": [["The Amish are known for their plain attire."], ["If the errant member persists without repentance and rejects even the admonition of the congregation, that person is excommunicated or excluded from church membership."], [""]]}, {"qid": "36cdce4bdbdaf630113c", "term": "Illuminati", "description": "A name given to several groups, both real and fictitious", "question": "Is the Illuminati card game still popular?", "answer": false, "facts": ["The original version of the game was released in 1982.", "A collectible card game version was released in 1995 but only had one set.", "The most recent edition of the base game was published in 2007."], "decomposition": ["When was the last Illuminati card game published?", "Was #1 with the last few years?"], "evidence": [[[["Illuminati (game)-1"], "no_evidence"], ["no_evidence"]], [[["Illuminati (game)-2", "Illuminati (game)-4"], "no_evidence"], ["no_evidence", "operation"]], [[["Illuminati (game)-13"]], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "a985a23635dd9c3852d3", "term": "Strawberry", "description": "edible fruit", "question": "Can a strawberry get worms similar to dogs?", "answer": true, "facts": ["Strawberry can suffer from black root rot and nematodes.", "Dogs can suffer from a variety of worms including roundworms that lay eggs on them.", "Nematodes are parasites that are also called roundworms and ascarids.", "Nematodes are parasites that feed off of strawberry plants."], "decomposition": ["What types of worms can strawberries become infected with?", "What types of worms can dogs become infected with?", "Are any of #1 present in #2?"], "evidence": [[[["Ditylenchus dipsaci-6"], "no_evidence"], [["Dog-18"]], ["no_evidence", "operation"]], [[["Strawberry-26"]], [["Worm-7"]], [["Worm-7"]]], [["no_evidence"], [["Dog-18"]], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["Dogs are also susceptible to parasites such as fleas, ticks, mites, hookworms, tapeworms, roundworms, and heartworms (roundworm species that lives in the heart of dogs)."]]}, {"qid": "2f59da1520d3c5f9f83a", "term": "Menthol", "description": "chemical compound", "question": "Is Menthol associated with Thanksgiving?", "answer": false, "facts": ["Menthol is the main component of peppermint oil and is responsible for the noticeable cooling sensation. ", "During Thanksgiving, turkey, potatoes, gravy, and pie are common dishes. None of which have menthol in it."], "decomposition": ["What are some common dishes served during Thanksgiving?", "Does any of #1 contain menthol?"], "evidence": [[[["Thanksgiving (United States)-1"]], ["operation"]], [[["Thanksgiving-6"]], [["Menthol-1"]]], [[["Thanksgiving dinner-10", "Thanksgiving dinner-16"]], [["Menthol-23", "Thanksgiving dinner-10", "Thanksgiving dinner-16"]]]], "golden_sentence": [["The dinner traditionally consists of foods and dishes indigenous to the Americas, namely turkey, potatoes (usually mashed), squash, corn (maize), green beans, cranberries (typically in sauce form), and pumpkin pie."]]}, {"qid": "0a87b4e592e3963796bb", "term": "Eggplant", "description": "plant species Solanum melongena", "question": "Would someone in Mumbai refer to Solanum melongena as an eggplant?", "answer": false, "facts": ["Mumbia is a city in India.", "India is a country located in South Asia.", "In South Asia the Solanum melongena plant is referred to as a brinjal."], "decomposition": ["In what country Mumbai located?", "In what region is #1 located?", "What is Solanum melongena referred to as in #2?", "Is #3 the word \"eggplant\"?"], "evidence": [[[["Mumbai-1"]], [["India-1"]], [["Eggplant-1"]], ["operation"]], [[["Mumbai-3"]], [["India-1"]], [["Eggplant-16"]], [["Eggplant-16"]]], [[["Mumbai-1"]], [["India-1"]], [["Eggplant-1"]], ["operation"]]], "golden_sentence": [["Mumbai lies on the Konkan coast on the west coast of India and has a deep natural harbour."], ["India (Hindi: Bh\u0101rat), officially the Republic of India (Hindi: Bh\u0101rat Ga\u1e47ar\u0101jya), is a country in South Asia."], [""]]}, {"qid": "c78c5bd6fbff96fff7aa", "term": "Lie", "description": "intentionally false statement to a person or group made by another person or group who knows it is not wholly the truth", "question": "Is it okay to lie after taking an oath in a court of law?", "answer": false, "facts": ["In a court of law, lying under oath is considered perjury. ", "Perjury is considered a crime."], "decomposition": ["When you lie in court, what is that considered?", "Is #1 legal?"], "evidence": [[[["Perjury-1"]], [["Perjury-2"]]], [[["Perjury-1"]], ["operation"]], [[["Perjury-1"]], [["Perjury-2"]]]], "golden_sentence": [["For example, it is not perjury to lie about one's age except if age is a fact material to influencing the legal result, such as eligibility for old age retirement benefits or whether a person was of an age to have legal capacity."], [""]]}, {"qid": "6debbf95bba7b1fcf852", "term": "Art", "description": "Creative work to evoke emotional response", "question": "Is art prioritized in the US education system?", "answer": false, "facts": ["Art classes are often the first to be cut during times of low school funds. ", "Between 1999 and 2009, schools offering visual arts curriculum decreased by 7%. "], "decomposition": ["When funding is low in schools, what subjects are typically cut first?", "What types of classes are #1 classified as?", "Is art not classified as #2?"], "evidence": [[[["North Andover High School-14"], "no_evidence"], ["operation"], ["operation"]], [[["Art education in the United States-2"]], [["The arts-3"]], ["operation"]], [[["Public school funding in the United States-9"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Proponents of the cuts argue that both band and drama are extra-curricular activities and thus the user fees are justified (athletes also pay user fees) and that core subjects such as English and Mathematics are required fields of study for aspiring college students and need to be given priority."]]}, {"qid": "2dff5bccacfab5c64ec6", "term": "Soup", "description": "primarily liquid food", "question": "While on a liquid diet, are there some types of soup you cannot eat?", "answer": true, "facts": ["Italian wedding soup has large chunks including meatballs and pasta which require chewing.", "Chicken Noodle soup has chunks of chicken and large noodles in it that require chewing."], "decomposition": ["Are there any soups that contain substantially solid portions?"], "evidence": [[[["Gumbo-1", "Menudo (soup)-1"], "no_evidence"]], [[["Soup-1", "Stew-1"], "operation"]], [[["Chicken soup-1"]]]], "golden_sentence": [["", ""]]}, {"qid": "a59e21160e115f8ae435", "term": "Twelve-tone technique", "description": "method of musical composition devised by Arnold Sch\u00f6nberg to ensure that all 12 notes of the chromatic scale are equally often, so that the music avoids being in a key", "question": "Would it be impossible to use an Iwato scale for a twelve-tone technique composition?", "answer": true, "facts": ["The Iwato scale has 5 pitch cases.", "The chromatic scale has 12 pitch cases."], "decomposition": ["How many pitch cases are used for the Iwato scale?", "Is #1 not equal to the number of pitch cases needed for the twelve-tone technique?"], "evidence": [[[["Iwato scale-1"]], [["Twelve-tone technique-1"], "operation"]], [[["Iwato scale-1"]], ["no_evidence", "operation"]], [[["Iwato scale-1"]], [["Twelve-tone technique-1"], "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "8703c535b9454f6d0d0a", "term": "Gulf of Finland", "description": "arm of the Baltic Sea", "question": "Would the Titanic be well preserved at the bottom of the Gulf of Finland?", "answer": true, "facts": ["The bottom of the gulf is one of the world's largest ship cemeteries. Because of the low salinity and cold waters, and no shipworms, the ships are relatively well preserved.", "RMS Titanic was a British passenger ship."], "decomposition": ["Are ships well-preserved in the Gulf of Finland?", "Was the RMS Titanic a ship?", "Is both #1 and #2 \"yes\""], "evidence": [[[["Gulf of Finland-28"]], [["RMS Titanic-1"]], ["operation"]], [[["Gulf of Finland-28"]], [["RMS Titanic-1"]], ["operation"]], [[["Gulf of Finland-28"]], [["RMS Titanic-1"]], ["operation"]]], "golden_sentence": [["Because of the low salinity and cold waters, and no shipworms, the ships are relatively well preserved."], ["RMS Titanic was a British passenger liner operated by the White Star Line that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after striking an iceberg during her maiden voyage from Southampton to New York City."]]}, {"qid": "9cbfaad8a60a5b69f255", "term": "Hepatitis", "description": "inflammation of the liver tissue", "question": "Could a dandelion suffer from hepatitis?", "answer": false, "facts": ["Only creatures that contain a liver can suffer from hepatitis.", "The liver is an organ only found in vertebrates.", "Vertebrates exist in the kingdom Animalia.", "Dandelions are plants in the kingdom Plantae."], "decomposition": ["Hepatitis is the inflammation of what?", "In which kingdom is #1 found?", "In what kingdom are dandelions found?", "Is #3 the same as #2?"], "evidence": [[[["Hepatitis-1"]], [["Animal-1", "Animal-2", "Liver-1", "Vertebrate-1"]], [["Plant-1", "Taraxacum-1"]], ["operation"]], [[["Hepatitis-1"]], [["Liver-1", "Vertebrate-1"]], [["Taraxacum-1"]], ["operation"]], [[["Hepatitis-4"]], [["Liver-98"]], [["Chondrilla (plant)-3"]], ["operation"]]], "golden_sentence": [["Hepatitis is inflammation of the liver tissue."], ["Animals (also referred to as metazoa) are multicellular eukaryotic organisms that form the biological kingdom Animalia.", "Most living animal species are in the Bilateria, a clade whose members have a bilaterally symmetric body plan.", "", ""], ["Plants are mainly multicellular, predominantly photosynthetic eukaryotes of the kingdom Plantae.", ""]]}, {"qid": "ae0df71d989593b1a244", "term": "Week", "description": "unit of time", "question": "Would a week be enough time to watch every episode of Ugly Betty?", "answer": true, "facts": ["There are 168 hours in a week.", "The entire Ugly Betty series is 85 hours long."], "decomposition": ["How many hours are in a week?", "How long is the entire Ugly Betty series?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Week-9"]], [["Ugly Betty-10"]], [["Ugly Betty-10"], "operation"]], [["no_evidence"], [["Ugly Betty-16"], "no_evidence"], ["operation"]], [[["Week-9"], "operation"], ["no_evidence"], ["no_evidence"]]], "golden_sentence": [["1 week = 7 days = 168 hours = 10,080 minutes = 604,800 seconds."], [""], [""]]}, {"qid": "7a8b8bb51ceffba950c8", "term": "Bitcoin", "description": "decentralized cryptocurrency", "question": "Was the Louisiana Purchase made with bitcoin?", "answer": false, "facts": ["Bitcoin was launched as a currency in 2009.", "The Louisiana Purchase was in 1803."], "decomposition": ["When was Bitcoin launched?", "When did the Louisiana Purchase take place?", "Is #1 prior to #2?"], "evidence": [[[["Bitcoin-2"]], [["Louisiana Purchase-1"]], ["operation"]], [[["Bitcoin-2"]], [["Louisiana Purchase-1"]], ["operation"]], [[["Bitcoin-4"]], [["Louisiana Purchase-1"]], ["operation"]]], "golden_sentence": [["Bitcoin was invented in 2008 by an unknown person or group of people using the name Satoshi Nakamoto and started in 2009 when its source code was released as open-source software."], ["The Louisiana Purchase (French: Vente de la Louisiane 'Sale of Louisiana') was the acquisition of the territory of Louisiana by the United States from France in 1803."]]}, {"qid": "93a1522140926b9b26dc", "term": "Serfdom", "description": "status of peasants under feudalism", "question": "Did Japanese serfdom have higher status than English counterpart?", "answer": true, "facts": ["Serfs in Medieval England were peasants that were indentured servants to their lords.", "Serfs were often harshly treated and had little legal redress against the actions of their lords.", "Japanese serfs were farmers and fishermen.", "Japanese believed that serfs produced food, which was depended on by all classes, therefore, they worked harder."], "decomposition": ["How did English lords treat their serfs?", "What did the Japanese recognize serfs as?", "Is #2 higher in importance than #1?"], "evidence": [[[["Serfdom-2"]], [["Serfdom-5"]], ["operation"]], [[["Serfdom-2"]], [["Manorialism-17"]], ["operation"]], [[["Serfdom-2"], "no_evidence"], [["Sh\u014den-8", "Sh\u014den-9"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["As with slaves, serfs could be bought, sold, or traded, with some limitations: they generally could be sold only together with land (with the exception of the kholops in Russia and villeins in gross in England who could be traded like regular slaves), could be abused with no rights over their own bodies, could not leave the land they were bound to, and could marry only with their lord's permission."], [""]]}, {"qid": "9cdaf15e24e108132239", "term": "Papaya", "description": "species of plant, use Q12330939 for the papaya (the fruit)", "question": "Is the best tasting part of the papaya in the center?", "answer": false, "facts": ["The center of a papaya contains a cluster of seeds.", "Papaya seeds, while edible, are often discarded for the soft flesh that surrounds them."], "decomposition": ["What is usually located in the center of papayas?", "Is #1 usually consumed by people when eating papayas?"], "evidence": [[[["Avocado-26"], "no_evidence"], [["Fruit pit carving-1"], "no_evidence", "operation"]], [[["Papaya-3"]], [["Papaya-30"]]], [[["Papaya-3"], "no_evidence"], [["Papaya-21"], "operation"]]], "golden_sentence": [["If there is no change by this time, the avocado pit is discarded."], ["However, the art is now facing extinction because few black olive trees are planted in China and there are very few people interested in learning this art."]]}, {"qid": "53d1cebefbc79d01767b", "term": "Lil Wayne", "description": "American rapper, record executive and actor from Louisiana", "question": "Could Lil Wayne's children ride in a Chevrolet Corvette ZR1 together?", "answer": false, "facts": ["Lil Wayne has four children.", "A Chevrolet Corvette ZR1 has 2 seats."], "decomposition": ["How many people can a Chevrolet Corvette ZR1 seat at a time?", "How many children does Lil Wayne have?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Chevrolet Corvette-1"]], ["no_evidence"], ["operation"]], [[["Chevrolet Corvette-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Chevrolet Corvette-1"]], ["no_evidence"], ["operation"]]], "golden_sentence": [["The Chevrolet Corvette, colloquially known as the Vette, is a two-door, two-passenger sports car manufactured and marketed by Chevrolet across more than 60 years of production and eight design generations."]]}, {"qid": "b633fba1a27353dca600", "term": "Dancing with the Stars", "description": "several international television series based on the format of the British TV series Strictly Come Dancing", "question": "Is double duty an incorrect phrase for host of Dancing With The Stars?", "answer": false, "facts": ["Double duty refers to having more than one job at the same time.", "The host of Dancing WIth The Stars is Tom Bergeron.", "Tom Bergeron is the host of America's Funniest Home Videos."], "decomposition": ["Who is the host of TV series 'Dancing WIth The Stars'?", "Who hosts America's Funniest Home Videos?", "Do #1 and #2 being the same fail to meet the definition of double duty?"], "evidence": [[[["Dancing with the Stars (American TV series)-1"]], [["America's Funniest Home Videos-16"]], ["operation"]], [[["Dancing with the Stars-18"]], [["America's Funniest Home Videos-23"]], ["operation"]], [[["Dancing with the Stars-18"]], [["America's Funniest Home Videos-16"]], [["Double Duty-9"], "operation"]]], "golden_sentence": [["The show is hosted by Tom Bergeron, alongside Erin Andrews, who became co-host in season eighteen."], [""]]}, {"qid": "5be68ad876e646b6e5ce", "term": "Ocelot", "description": "Small wild cat", "question": "Could an ocelot subsist on a single bee hummingbird per day?", "answer": false, "facts": ["An ocelot requires 600\u2013800 grams (21\u201328 oz) of food every day to satisfy its energy requirements.", "Bee hummingbirds are the smallest living birds, with an average weight of around 2-3 grams."], "decomposition": ["How much food does an ocelot need to live per day?", "How much does a bee hummingbird weigh?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Ocelot-25"], "no_evidence"], [["Bee hummingbird-2"]], ["operation"]], [[["Ocelot-25"]], [["Bee hummingbird-2"]], ["operation"]], [[["Ocelot-26"], "no_evidence"], [["Bee hummingbird-2"]], ["operation"]]], "golden_sentence": [["An ocelot requires 600\u2013800\u00a0g (21\u201328\u00a0oz) of food every day to satisfy its energy requirements."], ["Females weigh 2.6\u00a0g (0.092\u00a0oz) and are 6.1\u00a0cm (2.4\u00a0in) long, and are slightly larger than males, with an average weight of 1.95\u00a0g (0.069\u00a0oz) and length of 5.5\u00a0cm (2.2\u00a0in)."]]}, {"qid": "4d4073f700eaf4926385", "term": "Ukrainian Greek Catholic Church", "description": "Byzantine Rite Eastern Catholic Church", "question": "Does Ukrainian Greek Catholic Church recognize Alexander Nevsky as a saint?", "answer": false, "facts": ["Alexander Nevsky was a Prince of Novgorod that fought against German and Swiss Invaders.", "The Russian Orthodox Church named Alexander Nevsky.a saint in 1547.", "The Russian Orthodox Church is a member of the Eastern Orthodox Church and has their own list of saints.", "The Catholic Church and the Eastern Orthodox Church have been in a state of official schism since the East\u2013West Schism of 1054.", "The Ukrainian Greek Catholic Church is a branch of the Catholic Church."], "decomposition": ["In which religion is Alexander Nevsky considered a saint?", "What religion is the Ukrainian Greek Catholic Church a part of?", "Is #1 and #2 the same?"], "evidence": [[[["Alexander Nevsky-2"]], [["Ukrainian Greek Catholic Church-1"]], [["Russian Orthodox Church-73"], "operation"]], [[["Alexander Nevsky-2"]], [["Ukrainian Greek Catholic Church-1"]], ["operation"]], [[["Alexander Nevsky-2"]], [["Ukrainian Greek Catholic Church-31"]], ["operation"]]], "golden_sentence": [["Metropolite Macarius canonized Alexander Nevsky as a saint of the Russian Orthodox Church in 1547."], ["'Ukrainian Greek-Catholic Church'; Latin: Ecclesia Graeco-Catholica Ucrainae) is a Byzantine Rite Eastern Catholic Church in full communion with the Pope and the worldwide Catholic Church."], [""]]}, {"qid": "a585f3922f952943a387", "term": "James Bond", "description": "Media franchise about a British spy", "question": "Was the original James Bond actor born near the Washington Monument?", "answer": false, "facts": ["The original James Bond actor was Sean Connery.", "Sean Connery was born in Scotland.", "The Washington Monument is located in Washington, D.C.", "Washington, D.C. and Scotland are nearly 3,500 miles apart."], "decomposition": ["Who originally played James Bond?", "Where was #1 born?", "Where is the Washington Monument located?", "What is the distance between #2 and #3?", "Is #4 a short enough of a distance to be considered \"close\"?"], "evidence": [[[["Portrayal of James Bond in film-3"]], [["Barry Nelson-2"]], [["Washington Monument-1"]], ["no_evidence"], ["no_evidence"]], [[["Portrayal of James Bond in film-8"]], [["Sean Connery-4"]], [["Washington Monument-1"]], ["no_evidence"], ["no_evidence"]], [[["James Bond-28"]], [["Sean Connery-4"]], [["Washington Monument-1"]], [["Atlantic Ocean-1"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["After considering \"refined\" English actors such as Cary Grant and David Niven, the producers cast Sean Connery as Bond in the film."], ["Nelson was born in San Francisco, California, the son of Norwegian immigrants, Betsy (n\u00e9e Christophersen) and Trygve Nielsen His year of birth has been subject to some debate, but his 1943 Army enlistment record and his 1993 voter registration records list 1917 as the year of his birth."], ["The Washington Monument is an obelisk on the National Mall in Washington, D.C., built to commemorate George Washington, once commander-in-chief of the Continental Army (1775\u20131784), in the American Revolutionary War and the first President of the United States (1789\u20131797)."]]}, {"qid": "4caf60f6a5d8230aae85", "term": "J. P. Morgan", "description": "American financier, banker, philanthropist and art collector", "question": "Did J. P. Morgan have healthy lungs?", "answer": false, "facts": ["J. P. Morgan smoked dozens of cigars per day.", "Smoking tobacco damages the lungs and increases the risk of lung cancer significantly."], "decomposition": ["What are the causes of unhealthy lungs?", "Did J. P. Morgan engage in any part of #1?"], "evidence": [[[["Lung-3"]], [["J. P. Morgan-29"]]], [[["Smoking-5"]], ["operation"]], [[["Rheumatoid lung disease-6"]], [["J. P. Morgan-4"], "operation"]]], "golden_sentence": [["A number of occupational lung diseases can be caused by substances such as coal dust, asbestos fibres, and crystalline silica dust."], [""]]}, {"qid": "ef8a239619bb9fc742d8", "term": "Firefighter", "description": "rescuer trained to extinguish hazardous fires", "question": "Would Firefighters be included in a September 11th memorial?", "answer": true, "facts": ["September 11th is remembered as a day of mourning for the lives lost during a terrorist attack in NYC.", "Firefighters were among the first responders to the crisis, and many died. "], "decomposition": ["Who gets remembered on September 11th?", "Were firefighters among #1?"], "evidence": [[[["9/11 Tribute Museum-1"]], [["9/11 Tribute Museum-7"], "operation"]], [[["September 11 attacks-2"]], ["operation"]], [[["September 11 attacks-118", "September 11 attacks-63"]], ["operation"]]], "golden_sentence": [["The 9/11 Tribute Museum, formerly known as the 9/11 Tribute Center and Tribute WTC, shares the personal stories of family members who lost loved ones, survivors, rescue and recovery workers, volunteers and Lower Manhattan residents with those who want to learn about the September 11 attacks."], [""]]}, {"qid": "24100172ec340cbf23e2", "term": "Andrew Johnson", "description": "17th president of the United States", "question": "Does Andrew Johnson's presidential number exceed Elagabalus's Emperor number?", "answer": false, "facts": ["Andrew Johnson was the 17th president of the United States.", "Elagabalus was the 25th Roman Emperor."], "decomposition": ["What number president was Andrew Johnson?", "What number emperor  was Elagabalus?", "Is #1 greater than #2?"], "evidence": [[[["Andrew Johnson-1"]], [["Elagabalus-1"]], ["operation"]], [[["Andrew Johnson-1"]], [["Elagabalus-1"], "no_evidence"], ["operation"]], [[["Andrew Johnson-1"]], [["Elagabalus-1"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Andrew Johnson (December 29, 1808\u00a0\u2013 July 31, 1875) was the 17th president of the United States, serving from 1865 to 1869."], ["Elagabalus (/\u02cc\u025bl\u0259\u02c8\u0261\u00e6b\u0259l\u0259s/ EL-\u0259-GAB-\u0259-l\u0259s), also known as Heliogabalus (/\u02cchi\u02d0li\u0259-, -lio\u028a-/ HEE-lee-\u0259-, -\u2060lee-oh-; Latin: Marcus Aurelius Antoninus Augustus; c. 204 \u2013 11 March 222), was Roman emperor from 218 to 222."]]}, {"qid": "53aaaae99d25fa7b946b", "term": "Chinese calendar", "description": "Lunisolar calendar from China", "question": "Are any animals in Chinese calendar Chordata?", "answer": true, "facts": ["The chinese zodiac based on the Chinese calendar has a number of animals including dogs and pigs.", "Chordata is a scientific classification of an animals phylum.", "The phylum of pigs is Chordata."], "decomposition": ["What animals are on the Chinese calendar?", "Which animals in #1 have a notochord and dorsal neural tube?", "Which animals in #2 have pharyngeal slits and an endostyle at some stage of development?", "Which animals in #3 have a post-anal tail?", "Is there at least one animal listed in #4?"], "evidence": [[[["Chinese astrology-10", "Chordate-1"]], [["Tiger-27"], "no_evidence"], [["Tiger-27"], "no_evidence"], [["Tiger-27"], "no_evidence"], ["operation"]], [[["Chinese zodiac-5"]], [["Chordate-1"], "no_evidence"], [["Chordate-2"], "no_evidence"], [["Chordate-6"], "no_evidence"], ["operation"]], [[["Chinese zodiac-5"]], [["Chinese zodiac-5", "Notochord-1"]], [["Chinese zodiac-5", "Pharyngeal slit-1"]], [["Chinese zodiac-5", "Chordate-1"]], ["operation"]]], "golden_sentence": [["They are in order as follows: the Rat, Ox, Tiger, Rabbit, Dragon, Snake, Horse, Goat, Monkey, Rooster, Dog, and Pig.", ""], [""], [""], [""]]}, {"qid": "e0e284b873700b715698", "term": "Yeti", "description": "Folkloric ape-like creature from Asia", "question": "Would a hypothetical Yeti be towered over by Andre the Giant?", "answer": true, "facts": ["The mythical Yeti is said to be between 200 and 400 pounds.", "The mythical Yeti is said to be around 6 feet tall.", "Andre the Giant was an imposing 7'4\" and 529 pounds."], "decomposition": ["How tall was Andre the Giant?", "How tall are Yeti thought to be?", "Is #2 greater than #1?"], "evidence": [[[["Andr\u00e9 the Giant-2"]], [["Chuchuna-3"]], ["operation"]], [[["Andr\u00e9 the Giant-2"]], [["Expedition Everest-12", "Yeti-8"], "no_evidence"], ["no_evidence", "operation"]], [[["Andre the Giant Has a Posse-2"]], [["Yeti-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["Roussimoff stood at over seven feet tall, which was a result of gigantism caused by excess growth hormone, and later resulted in acromegaly."], ["It is described as being roughly six to seven feet tall."]]}, {"qid": "b06f0eb16fe78e6a057f", "term": "The Little Prince", "description": "Novella by Antoine de Saint-Exup\u00e9ry", "question": "Was The Little Prince's titular character allergic to flowers?", "answer": false, "facts": ["The Little Prince tends to and cares for a rose.", "The Little Prince falls in love with a rose. "], "decomposition": ["Who is the titular character of The Little Prince?", "Does #1 avoid interacting with flowers?"], "evidence": [[[["The Little Prince-1", "The Little Prince-5"]], [["The Little Prince-11"], "operation"]], [[["The Little Prince-1"]], [["Rose-1", "The Little Prince-11"]]], [[["The Little Prince-5"]], [["The Little Prince-16", "The Little Prince-20"], "operation"]]], "golden_sentence": [["", ""], ["The prince says he nourished the rose and attended her, making a screen or glass globe to protect her from the cold wind, watering her, and keeping off the caterpillars."]]}, {"qid": "b6a5655300d16667bf62", "term": "University of Pittsburgh", "description": "American state-related research university located in Pittsburgh, Pennsylvania", "question": "Did Millard Fillmore help to establish the University of Pittsburgh?", "answer": false, "facts": ["The University of Pittsburgh was established in 1787.", "Millard Fillmore was born in 1800."], "decomposition": ["When was the University of Pittsburgh established?", "When was Millard Fillmore born?", "Is #2 before #1?"], "evidence": [[[["University of Pittsburgh-1"]], [["Millard Fillmore-1"]], ["operation"]], [[["University of Pittsburgh-5"]], [["Millard Fillmore-5"]], ["operation"]], [[["University of Pittsburgh-1"]], [["Millard Fillmore-1"]], ["operation"]]], "golden_sentence": [["Pitt was founded as the Pittsburgh Academy in 1787 on the edge of the American frontier."], ["Millard Fillmore (January 7, 1800 \u2013 March 8, 1874) was the 13th president of the United States (1850\u20131853), the last to be a member of the Whig Party while in the White House."]]}, {"qid": "97de2e3114d98bc503da", "term": "Northern fur seal", "description": "The largest fur seal in the northern hemisphere", "question": "Does Northern fur seal make good pet for six year old?", "answer": false, "facts": ["An average six year old weighs 45 pounds.", "An adult Northern fur seal can weigh up to 120 pounds.", "Northern fur seals have sharp canine teeth for tearing.", "Northern fur seals live near rivers to feed off of fish populations."], "decomposition": ["What environment do Northern fur seals thrive in?", "Do six-years olds have safe access to #1?"], "evidence": [[[["Northern fur seal-9"]], ["no_evidence"]], [[["Northern fur seal-9"]], ["operation"]], [[["Northern fur seal-9"]], ["no_evidence"]]], "golden_sentence": [[""]]}, {"qid": "b923b0639ec816a029f3", "term": "Pear", "description": "genus of plants", "question": "Would a pear sink in water?", "answer": false, "facts": ["The density of a raw pear is about 0.59 g/cm^3.", "The density of water is about 1 g/cm^3.", "Objects only sink if they are denser than the surrounding fluid."], "decomposition": ["What is the density of a pear?", "What is the density of water?", "Is #1 greater than #2?"], "evidence": [[[["Density-12", "Pear-8"]], [["Density-12", "Density-5"]], ["operation"]], [["no_evidence"], [["Water-7"]], ["no_evidence", "operation"]], [["no_evidence"], [["Properties of water-14"]], ["operation"]]], "golden_sentence": [["", ""], ["The density at all points of a homogeneous object equals its total mass divided by its total volume.", "the ratio of the density of the material to that of a standard material, usually water."]]}, {"qid": "a5ec8f0c7508a2e73c27", "term": "Noah's Ark", "description": "the vessel in the Genesis flood narrative", "question": "WIll Noah's Ark hypothetically sail through flooded Lincoln Tunnel?", "answer": false, "facts": ["Scholars have determined Noah's Ark to be 75 feet wide.", "Each lane of the Lincoln Tunnel is 21 feet wide."], "decomposition": ["What is the width of the Lincoln tunnel?", "What is the width of the Noah's ark?", "Is #1 greater than #2?"], "evidence": [[[["Lincoln Tunnel-5"]], [["Cubit-1", "Noah's Ark-3"]], ["operation"]], [[["Lincoln Tunnel-5"]], [["Cubit-13", "Noah's Ark-3"]], ["operation"]], [[["Lincoln Tunnel-5"]], [["Noah's Ark-3"], "no_evidence"], ["operation"]]], "golden_sentence": [["There is a width limit of 8\u00a0feet 6\u00a0inches (2.59\u00a0m) for vehicles entering the tunnel."], ["These definitions typically ranged between 444 and 529.2\u00a0mm (17.48 and 20.83\u00a0in), with an ancient Roman cubit being as long as 120\u00a0cm (47\u00a0in).", "Accordingly, Noah's instructions are given to him by God (Genesis 6:14\u201316): the ark is to be 300 cubits long, 50 cubits wide, and 30 cubits high."]]}, {"qid": "55ee7cf86554a4738829", "term": "Haiku", "description": "very short form of Japanese poetry", "question": "Can you write a whole Haiku in a single tweet?", "answer": true, "facts": ["A Haiku is a Japanese poetry in three phrases.", "The average Haiku is composed of 60 to 70 characters.", "A tweet is a short message sent on Twitter.", "The character limit of a single tweet on Twitter is 140."], "decomposition": ["How many characters can be expected to be in an average haiku?", "What is the current character limit of a single tweet?", "Is #1 reasonably less than #2?"], "evidence": [[[["Haiku-2"], "no_evidence"], [["Twitter-1"]], ["operation"]], [[["Haiku-2"]], [["Twitter-1"]], ["no_evidence", "operation"]], [[["Haiku-2"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["This is often represented by the juxtaposition of two images or ideas and a kireji (\"cutting word\") between them, a kind of verbal punctuation mark which signals the moment of separation and colours the manner in which the juxtaposed elements are related."], ["Tweets were originally restricted to 140 characters, but was doubled to 280 for non-Asian languages in November 2017."]]}, {"qid": "dff078089049f3956096", "term": "Saltwater crocodile", "description": "species of reptile", "question": "Would you take a photo of a Saltwater crocodile in Memphis?", "answer": false, "facts": ["The saltwater crocodile is native to saltwater habitats and brackish wetlands from India's east coast across Southeast Asia and the Sundaic region to northern Australia and Micronesia.", "Memphis is a city in the United States."], "decomposition": ["Where can saltwater crocodiles be found?", "Is Memphis located in any of #1?"], "evidence": [[[["Saltwater crocodile-1"]], ["operation"]], [[["Saltwater crocodile-1"]], [["Memphis, Tennessee-1"]]], [[["Saltwater crocodile-20"]], ["operation"]]], "golden_sentence": [["The saltwater crocodile (Crocodylus porosus) is a crocodilian native to saltwater habitats and brackish wetlands from India's east coast across Southeast Asia and the Sundaic region to northern Australia and Micronesia."]]}, {"qid": "c5209b3594b4e36252b1", "term": "Kangaroo", "description": "\u0441ommon name of family of marsupials", "question": "Could Scooby Doo fit in a kangaroo pouch?", "answer": false, "facts": ["Scooby Doo is a fictional cartoon Great Dane.", "Great Danes can be 30-34 inches in height.", "Kangaroo babies can fit in their mother's pouch until they're 10 weeks of age.", "A 10 week old kangaroo is much smaller than a Great Dane."], "decomposition": ["What type of creature was Scooby-Doo?", "How large are #1?", "What resides in a kangaroo pouch?", "How large are #3?", "Is #2 approximately equal to #4?"], "evidence": [[[["Scooby-Doo (character)-1"]], [["Great Dane-3"]], [["Pouch (marsupial)-2"]], ["no_evidence"], ["operation"]], [[["Scooby-Doo-1"]], [["Great Dane-10"]], [["Kangaroo-34"]], [["Red kangaroo-13"]], ["operation"]], [[["Scooby-Doo-1"]], [["Great Dane-10"]], [["Marsupial-26"]], [["Red kangaroo-13"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], ["cm (44.0\u00a0in) from paw to shoulder."], ["Kangaroos and wallabies allow their young to live in the pouch well after they are physically capable of leaving, often keeping two different joeys in the pouch, one tiny and one fully developed."]]}, {"qid": "c03b3a8b7278d6533e14", "term": "Manta ray", "description": "genus of fishes", "question": "Do manta rays live in water above the safe temperature for cold food storage?", "answer": true, "facts": ["For cold foods, the food safe temperature is 40 degrees Fahrenheit and below.", "Manta rays prefer water temperatures above 68 \u00b0F (20 \u00b0C)."], "decomposition": ["What temperature should cold food be stored at?", "What kind of water do manta rays live in?", "What is the normal temperature of #2?", "Is #3 higher than #1?"], "evidence": [[[["Food storage-8"]], [["Manta ray-2"]], [["Manta ray-21"]], ["operation"]], [[["Refrigeration-23"], "no_evidence"], [["Manta ray-2"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Food storage-4", "Food storage-8"]], [["Manta ray-21"]], [["Manta ray-21"]], ["operation"]]], "golden_sentence": [["Foods held at temperatures above 40 \u00b0F for more than 2 hours should not be consumed."], ["Mantas are found in warm temperate, subtropical and tropical waters."], ["They prefer water temperatures above 68\u00a0\u00b0F (20\u00a0\u00b0C) and M. alfredi is predominantly found in tropical areas."]]}, {"qid": "4a8e46a783130e624a75", "term": "Kayak", "description": "small boat propelled with a double-bladed paddle", "question": "Is the kayak a traditional boat in New Zealand?", "answer": false, "facts": ["Kayaks were developed by native peoples to hunt in northern waters of the Arctic Ocean, North Atlantic, Bering Sea and North Pacific. ", "New Zealand is in the Southern Hemisphere.", "The native Maori people of New Zealand arrived there in canoes."], "decomposition": ["What cultures invented the kayak?", "What cultures are native to New Zealand?", "Is there overlap between #1 and #2?"], "evidence": [[[["Kayak-5"]], [["New Zealand-7"]], ["operation"]], [[["Kayak-5"]], [["Culture of New Zealand-1"]], ["operation"]], [[["Kayak-4"]], [["M\u0101ori people-1"]], ["operation"]]], "golden_sentence": [["Kayaks (Inuktitut: qajaq (\u1583\u152d\u1585 Inuktitut pronunciation:\u00a0[q\u0251\u02c8j\u0251q]), Yup'ik: qayaq (from qai- \"surface; top\"), Aleut: Iqyax) were originally developed by the Inuit, Yup'ik, and Aleut."], ["At some point a group of M\u0101ori migrated to R\u0113kohu, now known as the Chatham Islands, where they developed their distinct Moriori culture."]]}, {"qid": "5ff5e5f7d9ea2f8096cf", "term": "Subway (restaurant)", "description": "American fast food chain", "question": "Has the Subway restaurant franchise had any connections with child abusers?", "answer": true, "facts": ["Subway hired Jared Fogle as a spokesman for their sandwich shops.", "Jared Fogle was convicted for having sex with minors and for possessing child pornography. "], "decomposition": ["Was Jared Fogle a spokesman for Subway?", "Is Jared Fogle a sexual abuser of children?", "Are #1 and #2 the same?"], "evidence": [[[["Jared Fogle-2"]], [["Jared Fogle-40"]], ["operation"]], [[["Jared Fogle-2"]], [["Jared Fogle-15"]], ["operation"]], [[["Jared Fogle-1"]], [["Jared Fogle-3"]], ["operation"]]], "golden_sentence": [["Subway hired Fogle as a spokesperson in 2000, and he was featured in much of the company's advertising from 2000 to 2015; he appeared in Subway commercials, was parodied in an episode of South Park, and had appearances in the Sharknado film series, among others."], [""]]}, {"qid": "6ea0690630ae64813043", "term": "Presidency of Richard Nixon", "description": "American cabinet", "question": "Would the high school class of 2010 have lived through the Presidency of Richard Nixon?", "answer": false, "facts": ["People in the high school class of 2010 were born between 1991 and 1993.", "Richard Nixon was President of the United States until 1974."], "decomposition": ["When was Richard Nixon president of the US until?", "What year range would the high school class of 2010 be born in?", "Is #1 in #2?"], "evidence": [[[["Richard Nixon-1"]], [["Secondary education in the United States-36"], "no_evidence"], ["operation"]], [[["Richard Nixon-1"]], [["Secondary education-1"], "no_evidence"], ["operation"]], [[["Richard Nixon-46"]], ["no_evidence"], ["operation"]]], "golden_sentence": [["Richard Milhous Nixon (January 9, 1913 \u2013 April 22, 1994) was the 37th president of the United States, serving from 1969 until 1974."], [""]]}, {"qid": "8508b81f3e002ea9eb33", "term": "Deacon", "description": "ministry in the Christian Church", "question": "Would a Deacon be likely to be a fan of the podcast 'God Awful Movies'?", "answer": false, "facts": ["God Awful Movies is a podcast in which people review and mock religious films. ", "The hosts of God Awful Movies take a disrespectful approach to their film critique."], "decomposition": ["What is the main topic of God Awful Movies?", "What is God Awful Movies position on #1?", "What are deacon's positions on #1?", "Are #2 and #3 the same or similar?"], "evidence": [[["no_evidence"], ["no_evidence"], [["Deacon-1"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], ["no_evidence"], [["Deacon-1"], "no_evidence"], ["operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence"], ["no_evidence"]]], "golden_sentence": [[""]]}, {"qid": "7984476fb6d7b9150f68", "term": "Giant squid", "description": "Deep-ocean dwelling squid in the family Architeuthidae", "question": "Could a giant squid fit aboard the deck of the titanic?", "answer": true, "facts": ["Decks on the Titanic were as long as 500ft.", "Giant Squid grow to be around 59ft in length."], "decomposition": ["What is the length of a giant squid?", "What was the length of a deck on the Titanic?", "Is #1 less than #2?"], "evidence": [[[["Giant squid-1"]], [["RMS Titanic-11"]], ["operation"]], [[["Giant squid-1"]], [["First-class facilities of the RMS Titanic-12"]], ["operation"]], [[["Giant squid-1"]], [["RMS Titanic-11"]], ["operation"]]], "golden_sentence": [["Giant squid can grow to a tremendous size, offering an example of deep-sea gigantism: recent estimates put the maximum size at 12\u00a0m (39\u00a0ft) or 13\u00a0m (43\u00a0ft) for females and 10\u00a0m (33\u00a0ft) for males from the posterior fins to the tip of the two long tentacles (longer than the colossal squid at an estimated 9\u201310\u00a0m (30\u201333\u00a0ft), but lighter, one of the largest living organisms)."], ["Titanic was 882\u00a0feet 9\u00a0inches (269.06\u00a0m) long with a maximum breadth of 92\u00a0feet 6\u00a0inches (28.19\u00a0m)."]]}, {"qid": "2ccbc66d2195641accf4", "term": "Groundhog Day", "description": "Traditional method of weather prediction", "question": "Is Antarctica a good location for Groundhog Day?", "answer": false, "facts": ["Groundhog Day relies on a groundhog seeing their shadow.", "Antarctica has an irregular sun pattern and some days have no sun rise or 24 hour sunlight.", "Antarctica has temperatures can range from -10C to -60C.", "Groundhogs live in forests or woodlands with plenty of sunlight."], "decomposition": ["What does a groundhog have to see in order for a prediction to be made on Groundhog Day?", "Which particular light source is responsible for casting #1?", "How is the #2 pattern like on Antarctica?", "Is #3 is irregular?"], "evidence": [[[["Groundhog Day-1"]], [["Shadow-1"]], [["Antarctica-44"]], [["Antarctica-44"]]], [[["Groundhog Day-1"]], [["Shadow-16"]], [["Antarctica-44"]], [["Midnight sun-3"], "operation"]], [[["Groundhog Day-1"]], [["Sunlight-1"]], [["Antarctica-44", "Antarctica-46"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["It derives from the Pennsylvania Dutch superstition that if a groundhog emerging from its burrow on this day sees its shadow due to clear weather, it will retreat to its den and winter will persist for six more weeks; but if it does not see its shadow because of cloudiness, spring will arrive early."], [""], [""], [""]]}, {"qid": "ecfc14afb69cd0ab0232", "term": "The Powerpuff Girls", "description": "American animated television series", "question": "Can the Powerpuff Girls form a complete tag team wrestling match?", "answer": false, "facts": ["A tag team wrestling match is contested between at least two teams of at least two wrestlers each", "There are only three people in the Powerpuff Girls"], "decomposition": ["What is the minimum number of people that can participate in a tag team match in professional wrestling?", "The Powerpuff girls are how many in number?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Tag team-7"]], [["The Powerpuff Girls-1"]], ["operation"]], [[["Professional wrestling match types-4"]], [["The Powerpuff Girls-1"]], ["operation"]], [[["Tag team-7"]], [["The Powerpuff Girls-1"]], ["operation"]]], "golden_sentence": [["The basic tag team match has two teams of two wrestlers facing off against each other."], ["The Powerpuff Girls is an American superhero animated television series created by animator Craig McCracken for Cartoon Network and the fifth of the network's Cartoon Cartoons brand."]]}, {"qid": "5f6f5f71393b30c62c6a", "term": "White blood cell", "description": "type of cells of the immunological system", "question": "Will someone die without white blood cells?", "answer": true, "facts": ["White blood cells protect people against disease.", "Disease kills people."], "decomposition": ["What function do white blood cells serve in the body?", "Can a human live without #1?"], "evidence": [[[["White blood cell-1"]], [["White blood cell-15"], "no_evidence", "operation"]], [[["Blood cell-7"]], [["Blood cell-9"]]], [[["Innate immune system-11"]], ["operation"]]], "golden_sentence": [["White blood cells (WBCs), also called leukocytes or leucocytes, are the cells of the immune system that are involved in protecting the body against both infectious disease and foreign invaders."], [""]]}, {"qid": "1e4cfd7f0b9bfe500325", "term": "Boat", "description": "vessel for transport by water", "question": "Does rock star Keith Richards play a captain of a boat in a movie?", "answer": true, "facts": ["Keith Richards has a cameo appearance in two of the Pirates of the Caribbean movies.", "He plays Captain Teague, the elderly father of famous pirate Captain Jack Sparrow.", "In At World's End, he is the member of the council of Pirate Lords who is responsible for keeping the Pirate Code, and there is a brief shot of him and his crew aboard their ship during the sequence where the pirates are raising their banners in preparation to fight."], "decomposition": ["What role did Keith Richards play in the Pirates of the Caribbean movies?", "Can #1 be considered a captain of a boat?"], "evidence": [[[["Keith Richards-47"]], [["Captain-1"], "operation"]], [[["Keith Richards-47"]], [["Captain-1"]]], [[["Keith Richards-47"]], ["operation"]]], "golden_sentence": [["Richards made a cameo appearance as Captain Teague, the father of Captain Jack Sparrow (played by Johnny Depp), in Pirates of the Caribbean: At World's End, released in May 2007, and won the Best Celebrity Cameo award at the 2007 Spike Horror Awards for the role."], [""]]}, {"qid": "ce27f46d1e98be50cae9", "term": "Eleventh grade", "description": "educational year", "question": "Would an eleventh-grader be eligible for Medicare?", "answer": false, "facts": ["Students in the 11th grade are typically between 16-17 years old.", "The age requirement for most Medicare recipients is 65 or older."], "decomposition": ["What ages are people in eleventh grade?", "What ages are most medicare recipients?", "Is there an overlap between #1 and #2?"], "evidence": [[[["Eleventh grade-1"]], [["Medicare (United States)-1"]], ["operation"]], [[["Eleventh grade-1"], "operation"], [["Medicare Advantage-26"], "operation"], ["no_evidence"]], [[["Eleventh grade-1"]], [["Medicare (United States)-1"]], ["operation"]]], "golden_sentence": [["Students are typically 16\u201317 years of age, depending on the country and the students' birthdays."], [""]]}, {"qid": "6e77be5a0b69140d26e0", "term": "Panth\u00e9on", "description": "mausoleum in Paris", "question": "Is there a full Neptunian orbit between the first two burials of women in the Panth\u00e9on?", "answer": false, "facts": ["In 1907, Sophie Berthelot is the first woman to be interred in the Panth\u00e9on ", "In 1995, Marie Curie is the second woman to be interred there", "Neptune takes 165 years to go around the sun"], "decomposition": ["In what year was the first woman buried in the Panth\u00e9on?", "In what year was the second woman buried in the Panth\u00e9on?", "How many years are between #1 and #2?", "How many years does it take for Neptune to orbit the Sun?", "Is #4 less than or equal to #3?"], "evidence": [[[["Panth\u00e9on-34"]], [["Panth\u00e9on-34"]], ["operation"], [["Neptune-1"]], ["operation"]], [[["Panth\u00e9on-34"]], [["Panth\u00e9on-34"]], ["operation"], [["Neptune-1"]], ["operation"]], [[["Panth\u00e9on-34"]], [["Panth\u00e9on-34"]], ["operation"], [["Neptune-1"]], ["operation"]]], "golden_sentence": [["Marie Curie was interred in 1995, the first woman interred on merit."], ["Simone Veil was interred in 2018, and her husband Antoine Veil was interred alongside her so not to be separated."], ["Neptune orbits the Sun once every 164.8\u00a0years at an average distance of 30.1\u00a0AU (4.5\u00a0billion\u00a0km; 2.8\u00a0billion\u00a0mi)."]]}, {"qid": "9d80ea5dfcf2d0e09526", "term": "Boat", "description": "vessel for transport by water", "question": "Will Oasis cruise boat traverse the Lincoln Tunnel?", "answer": false, "facts": ["The Lincoln Tunnel has a maximum height clearance of 13 feet.", "The height of the Oasis cruise boat is 236 feet."], "decomposition": ["What is the maximum height clearance of the Lincoln Tunnel?", "How tall is the Oasis cruise ship?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Lincoln Tunnel-5"]], [["Oasis-class cruise ship-2"]], ["operation"]], [[["Lincoln Tunnel-5"]], [["Oasis of the Seas-6"]], ["operation"]], [[["Lincoln Tunnel-5"]], [["Oasis-class cruise ship-2", "Oasis-class cruise ship-3"]], ["operation"]]], "golden_sentence": [["Each tube provides a 21.5-foot-wide (6.6\u00a0m) roadway with two lanes and 13 feet (4.0\u00a0m) of vertical clearance."], ["Oasis of the Seas is also 8.5 metres (28\u00a0ft) wider, and with a gross tonnage of 225,282, is around 70,000 tonnes larger."]]}, {"qid": "9c4decb692b44f6eb5a8", "term": "Motor vehicle", "description": "self-propelled wheeled vehicle", "question": "Could Oscar Wilde have operated a motor vehicle?", "answer": true, "facts": ["Motor vehicles were in use by the 1890s", "Oscar Wilde lived until 1900"], "decomposition": ["When were cars first used?", "When did Oscar Wilde pass away?", "Did #2 happen after #1?"], "evidence": [[[["Car-2"]], [["Oscar Wilde-1"]], ["operation"]], [[["Car-14"]], [["Oscar Wilde-1"]], ["operation"]], [[["Car-2"]], [["Oscar Wilde-1"]], ["operation"]]], "golden_sentence": [["Cars came into global use during the 20th century, and developed economies depend on them."], ["Oscar Fingal O'Flahertie Wills Wilde (16 October 1854\u00a0\u2013 30 November 1900) was an Irish poet and playwright."]]}, {"qid": "bbf6f10857cc76304e17", "term": "Tokyo Tower", "description": "observation tower", "question": "Will Tokyo Tower be repainted only once during President Trump's first term?", "answer": true, "facts": ["Tokyo Tower is repainted every five years ", "The last repainting began in 2018", "Trump's first presidential term is from 2017 to 2021"], "decomposition": ["How long (in years) is President Trump's first term?", "How often (interval in years) is the Tokyo Tower repainted?", "Is #2 divided by #1 less than two?"], "evidence": [[[["Term of office-11"], "no_evidence"], [["Tokyo Tower-10"]], ["operation"]], [[["President of the United States-4"]], [["Tokyo Tower-10"]], ["operation"]], [[["President of the United States-5"]], [["Tokyo Tower-3"]], ["operation"]]], "golden_sentence": [["In the United States, the president of the United States is elected indirectly through the United States Electoral College to a four-year term, with a term limit of two terms (totaling eight years) or a maximum of ten years if the president acted as president for two years or less in a term where another was elected as president, imposed by the Twenty-second Amendment to the United States Constitution, ratified in 1951."], ["Every five years, the tower is repainted in a process that takes about a year to complete."]]}, {"qid": "0c542f47b9ff3d598c09", "term": "Supreme Court of the United States", "description": "Highest court in the United States", "question": "Has a neanderthal ever served on the Supreme Court of the United States?", "answer": false, "facts": ["The Supreme Court was established in 1789.", "Neanderthals are primitive humans that lived 40,000 years ago."], "decomposition": ["How long ago did Neanderthals live?", "How long ago was the Supreme Court of the United States formed?", "Is #2 greater than #1?"], "evidence": [[[["Neanderthal-1"]], [["Supreme Court of the United States-2"]], ["operation"]], [[["Neanderthal-1"]], [["Supreme Court of the United States-2"]], ["operation"]], [[["Neanderthal-1"]], [["Supreme Court of the United States-8"]], ["operation"]]], "golden_sentence": [["Neanderthals (/ni\u02c8\u00e6nd\u0259rt\u0251\u02d0l, ne\u026a-, -\u03b8\u0254\u02d0l/, also Neandertals or Neandert(h)alers, Homo neanderthalensis or Homo sapiens neanderthalensis) are an extinct species or subspecies of archaic humans who lived in Eurasia until about 40,000 years ago (40 kya [thousand years ago])."], [""]]}, {"qid": "69114020c21b6cde4b58", "term": "Clouded leopard", "description": "species of mammal found from the Himalayan foothills through mainland Southeast Asia into China", "question": "Can Clouded leopards chase down many Pronghorn antelopes?", "answer": false, "facts": ["The top speed of a Clouded leopard is 40 MPH.", "The top speed of a Pronghorn antelope is 61 MPH."], "decomposition": ["What is the top speed for a Clouded leopard ?", "What is the top speed for a Pronghorn antelope ?", "Is #1 greater then or equal to #2?"], "evidence": [[[["Clouded leopard-31"], "no_evidence"], [["Pronghorn-12"]], ["operation"]], [["no_evidence"], [["Pronghorn-12"]], ["operation"]], [[["Leopard-4"], "no_evidence"], [["Pronghorn-12"]], ["operation"]]], "golden_sentence": [[""], ["The top speed is very hard to measure accurately and varies between individuals; it can run 35\u00a0mph for 4\u00a0mi (56\u00a0km/h for 6\u00a0km), 42\u00a0mph for 1\u00a0mi (67\u00a0km/h for 1.6\u00a0km), and 55\u00a0mph for 0.5\u00a0mi (88.5\u00a0km/h for 0.8\u00a0km)."]]}, {"qid": "944d93b54deb583bff9e", "term": "Wednesday", "description": "Day of the week", "question": "Does the anatomy of a camel lend itself to jokes on Wednesdays?", "answer": true, "facts": ["Wednesday is often referred to as 'hump day' as a joke.", "Camels are known for having a significant hump. "], "decomposition": ["As a joke, what is Wednesday otherwise known as?", "What are camels known for having?", "Is there overlap between #1 and #2?"], "evidence": [[[["Wednesday-25"]], [["Camel-1"]], ["operation"]], [[["Wednesday-25"]], [["Camel-1"]], ["operation"]], [[["Wednesday-25"]], [["Camel-1"]], ["operation"]]], "golden_sentence": [["Wednesday is sometimes informally referred to as \"hump day\" in North America, a reference to the fact that Wednesday is the middle day\u2014or \"hump\"\u2014of a typical work week."], ["Camels are working animals especially suited to their desert habitat and are a vital means of transport for passengers and cargo."]]}, {"qid": "d76a600d54a8c718adc4", "term": "Goofy", "description": "Disney cartoon character", "question": "Can voice actors for Goofy and Bugs Bunny each get one stripe from American flag?", "answer": true, "facts": ["The American flag has 13 stripes on it.", "Since the role originated in 1932, six people have voiced the character of Goofy.", "Since 1940, seven people have voiced the character of Bugs Bunny."], "decomposition": ["How many stripes does the American flag have?", "How many people have been the voice of Goofy?", "How many people have been the voice of Bugs Bunny?", "What is #2 plus #3?", "Is #1 equal to or greater than #4?"], "evidence": [[[["Flag of the United States-1"]], [["Bill Farmer-1", "Hal Smith (actor)-16", "Pinto Colvig-1", "Stuart Buchanan-1", "Tony Pope-2"], "no_evidence"], [["Bugs Bunny-26", "Bugs Bunny-41"]], ["operation"], ["operation"]], [[["Flag of the United States-1"]], [["Goofy-43"]], [["Billy West-1", "Eric Bauza-1", "Greg Burson-2", "Jeff Bergman-1", "Joe Alaskey-2", "Mel Blanc-1", "Sam Vincent (voice actor)-1"]], ["operation"], ["operation"]], [[["Flag of the United States-1"]], [["Goofy-43"]], [["Bugs Bunny-41"], "no_evidence"], ["no_evidence", "operation"], ["no_evidence", "operation"]]], "golden_sentence": [["The 50 stars on the flag represent the 50 states of the United States of America, and the 13 stripes represent the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S. Nicknames for the flag include the Stars and Stripes, Old Glory, and the Star-Spangled Banner."], ["", "", "", "", "He was also the voice of Goofy for 11 years garnering 17 Gold and Platinum records."], ["", ""]]}, {"qid": "d721bf92e287cc10818f", "term": "Clark Gable", "description": "American actor", "question": "Did Clark Gable appear in any movies scored by John Williams?", "answer": false, "facts": ["Clark Gable died in 1960.", "John Williams scored his first movie in 1961."], "decomposition": ["When did Clark Gable die?", "When did John Williams begin creating movie scores?", "Is #2 before #1?"], "evidence": [[[["Clark Gable-1"]], [["John Williams-13"]], ["operation"]], [[["Clark Gable-1"]], [["John Williams-11"]], ["operation"]], [[["Clark Gable-1"]], [["John Williams-14"]], ["operation"]]], "golden_sentence": [["William Clark Gable (February 1, 1901 \u2013 November 16, 1960) was an American film actor, often referred to as \"The King of Hollywood\"."], ["Williams's first film composition was for the 1958 B movie Daddy-O, and his first screen credit came two years later in Because They're Young."]]}, {"qid": "c327953f6fc128f53152", "term": "Linus Torvalds", "description": "Creator and lead developer of Linux kernel", "question": "Is Linus Torvalds' wife unable to physically defend herself?", "answer": false, "facts": ["Linus Torvalds is married to Tove Torvalds.", "Tove Torvalds is a six-time Finnish national karate champion.", "Karate is now predominantly a striking art using punching, kicking, knee strikes, elbow strikes and open-hand techniques such as knife-hands, spear-hands and palm-heel strikes."], "decomposition": ["Who is Linus Torvalds' wife?", "What is #1 well known for?", "Can #2 not be used as a form of self defense?"], "evidence": [[[["Linus Torvalds-21"], "no_evidence"], ["no_evidence"], ["no_evidence"]], [[["Linus Torvalds-20"]], [["Linus Torvalds-20"]], [["Karate-22"]]], [[["Linus Torvalds-20"]], [["Linus Torvalds-20"]], [["Karate-1"], "operation"]]], "golden_sentence": [[""]]}, {"qid": "8bd0842a5b3757f3f0e0", "term": "Eric Clapton", "description": "English musician, singer, songwriter, and guitarist", "question": "Did Eric Clapton have similar taste in women to one of the Beatles?", "answer": true, "facts": ["The Beatles consisted of John Lennon, Paul McCartney, George Harrison, and Ringo Starr.", "George Harrison was married to Pattie Boyd from 1966-1977.", "Eric Clapton married Pattie Boyd in 1979."], "decomposition": ["Who are the spouses Eric Clapton has had?", "Who are the spouses the members of the Beatles have had?", "Is #1 listed in #2?"], "evidence": [[[["Eric Clapton-78"]], [["George Harrison-4"]], ["operation"]], [[["Eric Clapton-78"]], [["George Harrison-4"]], ["operation"]], [[["Eric Clapton-78"]], [["Pattie Boyd-1"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "a9123ebe197992d60c18", "term": "Eddie Murphy", "description": "American stand-up comedian and actor", "question": "Did Eddie Murphy's father see his first stand up show?", "answer": false, "facts": ["Eddie Murphy's father died when Eddie Murphy was 8 years old.", "Eddie Murphy's stand up career began when he was 15 years old."], "decomposition": ["How old was Eddie Murphy when he released his first stand up show?", "How old was Eddie Murphy when his father died?", "Is #2 greater than #1?"], "evidence": [[[["Eddie Murphy-9"], "no_evidence"], [["Eddie Murphy-7"]], [["Eddie Murphy-1"]]], [[["Eddie Murphy-9"]], [["Eddie Murphy-7"]], ["operation"]], [[["Eddie Murphy-8"]], [["Eddie Murphy-7"]], ["operation"]]], "golden_sentence": [["Eddie Murphy was his first album, released in 1982."], ["My mother and father broke up when I was three, and he died when I was eight, so I have very dim memories (\u2026) He was a victim of the Murphy charm (laughs)."], [""]]}, {"qid": "91b06fbef3fbf79f8940", "term": "Silverfish", "description": "species of insect", "question": "Could a silverfish reach the top of the Empire State Building?", "answer": false, "facts": ["Silverfish cannot fly.", "Animals that cannot fly can only access objects at or near ground level without mechanical assistance.", "The top of the Empire State Building is \t1,454 ft high."], "decomposition": ["How high is the Empire State Building?", "What class of animals do silverfish belong to?", "Can #2 typically get to heights of #1 without assistance?"], "evidence": [[[["Empire State Building-1"]], [["Silverfish-1"]], [["Silverfish-1"]]], [[["Empire State Building-1"]], [["Silverfish-1"]], ["operation"]], [[["Empire State Building-1"]], [["Silverfish-1"]], ["operation"]]], "golden_sentence": [["The Empire State Building is a 102-story Art Deco skyscraper in Midtown Manhattan in New York City."], ["A silverfish (Lepisma saccharina) is a small, primitive, wingless insect in the order Zygentoma (formerly Thysanura)."], [""]]}, {"qid": "0ef256b2599a8c8079c4", "term": "Law & Order", "description": "original television series (1990-2010)", "question": "Are there winged statuettes in the home of the creator of Law & Order?", "answer": true, "facts": ["Law & Order was created by Dick Wolf", "Dick Wolf won an Emmy in 2007 ", "Dick Wolf won an Emmy in 1997", "The Emmy statuette is of a winged woman holding an atom"], "decomposition": ["What award has a trophy that is a winged statuette?", "Who is the creator of Law & Order?", "Has #2 ever won #1?"], "evidence": [[[["Emmy Award-9"]], [["Dick Wolf-1"]], [["Dick Wolf-1"]]], [[["Emmy Award-2"]], [["Law & Order-1"]], [["Dick Wolf-1"]]], [[["Emmy Award-2"]], [["Law & Order (franchise)-1"]], [["Dick Wolf-1"], "operation"]]], "golden_sentence": [["The Emmy statuette, depicting a winged woman holding an atom, was designed by television engineer Louis McManus, who used his wife as the model."], ["Richard Anthony Wolf (born December 20, 1946) is an American television producer, best known as the creator and executive producer of the Law & Order franchise."], [""]]}, {"qid": "e292a112c101f6ef0506", "term": "Islamophobia", "description": "Fear, hatred of, or prejudice against the Islamic religion or Muslims generally,", "question": "Was Donald Trump the target of Islamophobia?", "answer": false, "facts": ["Islamophobia targets Muslims", "Donald Trump is a Presbyterian, a denomination of Christianity"], "decomposition": ["Islamophobia is the fear of which set of people?", "Does Donald Trump identify as #1?"], "evidence": [[[["Islamophobia-1"]], [["Donald Trump-11"]]], [[["Islamophobia-1"]], [["Donald Trump-11"]]], [[["Islamophobia-1"]], [["Donald Trump-9"]]]], "golden_sentence": [["Islamophobia is the fear, hatred of, or prejudice against the Islamic religion or Muslims generally, especially when seen as a geopolitical force or the source of terrorism."], [""]]}, {"qid": "247d868160a2401fcf78", "term": "Golden Gate Bridge", "description": "suspension bridge on the San Francisco Bay", "question": "Would baker's dozen of side by side Mac Trucks jam up Golden Gate Bridge?", "answer": true, "facts": ["The width of the Golden Gate Bridge is 90 feet. ", "A baker's dozen includes 13 items.", "The width of a Mac truck is around 8 feet."], "decomposition": ["How wide is a Mac truck?", "How many items are in a baker's dozen?", "What is #1 multiplied by #2?", "How wide is the Golden Gate Bridge?", "Is #3 greater than or equal to #4?"], "evidence": [[[["Mack Trucks-1"], "no_evidence"], [["Dozen-7"]], ["no_evidence", "operation"], [["Golden Gate Bridge-34"]], ["no_evidence", "operation"]], [[["Mack model EH trucks-6"]], [["Dozen-8"]], ["operation"], [["Golden Gate Bridge-36"], "no_evidence"], ["operation"]], [[["Mack Granite-2"]], [["Dozen-7"]], ["operation"], [["Golden Gate Bridge-25"]], ["operation"]]], "golden_sentence": [[""], ["The broadest use of baker's dozen today is simply a group of thirteen objects (often baked goods)."], [""]]}, {"qid": "7593403a64c95bce1d5f", "term": "Brussels sprout", "description": "vegetable", "question": "Could someone mistake the smell of your brussels sprouts for a fart?", "answer": true, "facts": ["Brussels Sprouts are cruciferous vegetables.", "Cruciferous vegetables have a sulfur containing chemical called glucosinolate in them", "When you cook brussels sprouts, their smell intensifies. "], "decomposition": ["What kind of vegetable are brussels sprouts?", "What chemical is found inside #1?", "What happens to #2 when you cook them?", "Does #3 smell like farts?"], "evidence": [[[["Brussels sprout-1"]], [["Brussels sprout-13"]], [["Brussels sprout-13", "Brussels sprout-16"]], [["Glucosinolate-1"], "no_evidence"]], [[["Brussels sprout-2"]], [["Brussels sprout-13"]], [["Brussels sprout-16"]], [["Flatulence-17"]]], [[["Brussels sprout-1"]], [["Brussels sprout-13", "Cabbage-39"]], [["Cabbage-39"]], [["Hydrogen sulfide-2"], "operation"]]], "golden_sentence": [["The Brussels sprout is a member of the Gemmifera Group of cabbages (Brassica oleracea), grown for its edible buds."], ["Although boiling reduces the level of sulforaphane, steaming, microwave cooking, and stir frying do not cause a significant loss."], ["", ""], [""]]}, {"qid": "47b00daf62692980d146", "term": "Guitarist", "description": "person who plays the guitar", "question": "Do guitarist's have fingers that can handle pain better than average?", "answer": true, "facts": ["Guitarists typically have calloused fingertips. ", "Callouses are formed of layers of dead skin and usually lack sensation."], "decomposition": ["What typically forms on a Guitarists' finger?", "Does #1 usually cause a lack of sensation?"], "evidence": [[[["Callus-3"]], [["Callus-12"], "no_evidence", "operation"]], [[["Callus-3"]], ["no_evidence", "operation"]], [[["Callus-3"]], [["Callus-13", "Callus-6"]]]], "golden_sentence": [["Normally, a callus will form on any part of the skin exposed to excess friction over a long period of time."], ["When it is usually not desirable to form a callus, minimizing rubbing and pressure will prevent callus formation."]]}, {"qid": "efaf1bbbe63e1bcc91ee", "term": "1965", "description": "Year", "question": "Were there under 150,000 American troops in Vietnam in 1965?", "answer": true, "facts": ["In 1965 the president announced an intention to increase the amount of troops to 125,000", "There were only 75,000 prior to 1965"], "decomposition": ["How many American troops were in Vietnam in 1965?", "Is #1 less than 150,000?"], "evidence": [[[["Vietnam War-58"]], ["operation"]], [[["Vietnam War-56", "Vietnam War-58"], "no_evidence"], ["operation"]], [[["1965 in the Vietnam War-96"]], ["operation"]]], "golden_sentence": [["On 8 March 1965, 3,500 U.S. Marines were landed near Da Nang, South Vietnam."]]}, {"qid": "d454e76bd74f12787b22", "term": "Maize", "description": "Cereal grain", "question": "Did Native American tribes teach Spaniards how to cultivate maize?", "answer": true, "facts": ["In 1492, Spanish settlers brought Maize back to Europe from America.", "Native Americans cultivated and bred the first maize from wild grasses."], "decomposition": ["Who cultivated the maize that Spaniards took to Europe from America in 1492?", "Were #1 Native Americans?"], "evidence": [[[["Maize-11"]], ["no_evidence"]], [[["Maize-10"]], [["Mapuche-1"], "operation"]], [[["History of the Caribbean-6", "Maize-11", "Maize-13", "Maize-4"], "no_evidence"], ["operation"]]], "golden_sentence": [["After the arrival of Europeans in 1492, Spanish settlers consumed maize and explorers and traders carried it back to Europe and introduced it to other countries."]]}, {"qid": "239c172a5fd8a0e41f1b", "term": "Oscar Wilde", "description": "19th-century Irish poet, playwright and aesthete", "question": "Has Oscar Wilde's most famous character ever been in an Eva Green project?", "answer": true, "facts": ["Oscar Wilde is most famous for his book The Picture of Dorian Gray.", "Dorian Gray is a beautiful socialite that is the main character of The Picture of Dorian Gray.", "Acclaimed actress, Eva Green has starred in numerous films and TV shows including Penny Dreadful.", "Dorian Gray flirts with Vanessa Ives in the Penny Dreadful episode, Seance.", "Vanessa Ives is played by Eva Green."], "decomposition": ["What is Oscar Wilde's most famous book?", "Who is the main character of #1?", "What episode of Penny Dreadful was #2 in?", "Is one of the characters in #3 played by Eva Green?"], "evidence": [[[["Oscar Wilde-1"]], [["The Picture of Dorian Gray-4"]], [["Penny Dreadful (TV series)-2"]], [["Eva Green-4"]]], [[["Oscar Wilde-1"]], [["The Picture of Dorian Gray-4"]], [["Penny Dreadful (TV series)-2"], "no_evidence"], [["Penny Dreadful (TV series)-3"]]], [[["Oscar Wilde-1"]], [["The Picture of Dorian Gray-4"]], [["Penny Dreadful (TV series)-2"], "no_evidence"], [["Reeve Carney-1", "Vanessa Ives-1"], "no_evidence"]]], "golden_sentence": [["He is best remembered for his epigrams and plays, his novel The Picture of Dorian Gray, and the circumstances of his criminal conviction for \"gross indecency\", imprisonment, and early death at age 46."], ["Dorian Gray is the subject of a full-length portrait in oil by Basil Hallward, an artist impressed and infatuated by Dorian's beauty; he believes that Dorian's beauty is responsible for the new mood in his art as a painter."], [""], [""]]}, {"qid": "c517eb25934267b2f636", "term": "Disgust", "description": "Basic emotion", "question": "Do frogs feel disgust?", "answer": true, "facts": ["Disgust is a basic emotion.", "Frogs make sounds that express their emotions."], "decomposition": ["What is disgust?", "Do frogs express #1?"], "evidence": [[[["Disgust-1"]], [["Frog-42"], "no_evidence"]], [[["Disgust-1"]], [["Frog-2"], "no_evidence", "operation"]], [[["Disgust-47"], "operation"], ["no_evidence"]]], "golden_sentence": [["Disgust (Middle French desgouster, from des- dis- + goust taste, from Latin gustus; akin to Latin gustare to taste) is an emotional response of rejection or revulsion to something potentially contagious or something considered offensive, distasteful, or unpleasant."], ["Frogs have ten pairs of cranial nerves which pass information from the outside directly to the brain, and ten pairs of spinal nerves which pass information from the extremities to the brain through the spinal cord."]]}, {"qid": "70dac6d1251273f88a9c", "term": "Monogamy", "description": "Relationship form where each individual has only one partner during their lifetime or at any one time", "question": "Did either Kublai Khan or his grandfather practice monogamy?", "answer": false, "facts": ["Kublai Khan was married multiple times and was said by some to have thousands of concubines.", "Kublai Khans grandfather was Genghis Khan.", "Genghis Khan had six wives and was said to have over 500 concubines."], "decomposition": ["How many times was Kublai Khan married?", "Who was Kublai Khan's grandfather?", "How many times wives did #2 have?", "Is #1 equal to 1 and is #3 equal 1?"], "evidence": [[[["Kublai Khan-73"]], [["Kublai Khan-5"]], [["Genghis Khan-22"]], ["operation"]], [[["Kublai Khan-74"]], [["Kublai Khan-2"]], [["Genghis Khan-18"]], ["operation"]], [[["Kublai Khan-73"]], [["Kublai Khan-5"]], [["Genghis Khan-15"]], ["operation"]]], "golden_sentence": [["citation needed] In the 13th century, Marco Polo recorded that Kublai had four wives and a great number of concubines."], ["As his grandfather Genghis Khan advised, Sorghaghtani chose a Buddhist Tangut woman as her son's nurse, whom Kublai later honored highly."], [""]]}, {"qid": "047877f551ae87fcd556", "term": "Parent", "description": "father or mother", "question": "Does a person need to be a parent to become a grandparent?", "answer": true, "facts": ["Parents care for their children.", "When the children grow up and have kids of their own, the parents become grandparents to those kids.", "A person who is not a parent has no kids, therefore nobody to produce grandchildren for them."], "decomposition": ["What must a person have in order to be known as a grandparent?", "What would the parents of #1 be to the person?", "Must one be a parent to have #2?"], "evidence": [[[["Grandparent-1"]], [["Grandparent-1"]], [["Grandparent-1"]]], [[["Parent-7"], "no_evidence"], [["Parent-1"]], ["operation"]], [[["Grandparent-1"]], [["Child-2"]], [["Parent-1"], "operation"]]], "golden_sentence": [["Every sexually-reproducing living organism who is not a genetic chimera has a maximum of four genetic grandparents, eight genetic great-grandparents, sixteen genetic great-great-grandparents, thirty-two genetic great-great-great-grandparents, etc."], ["Grandparents are the parents of a person's father or mother \u2013 paternal or maternal."], ["[citation needed] It is not known for certain what spurred this increase in longevity but largely results in the improved medical technology and living standard, but it is generally believed that a key consequence of three generations being alive together was the preservation of information which could otherwise have been lost; an example of this important information might have been where to find water in times of drought."]]}, {"qid": "22b351288645a62d7f99", "term": "Mental disorder", "description": "Distressing thought or behavior pattern", "question": "Did Van Gogh suffer from a mental disorder?", "answer": true, "facts": ["Mental disorders can be characterized by psychotic episodes and delusions", "Van Gogh suffered from psychotic episodes and delusions"], "decomposition": ["What are mental disorders characterized as?", "What issues did Van Gogh suffer from?", "Is #1 the same as #2?"], "evidence": [[[["Mental disorder-40"]], [["Vincent van Gogh-3"]], ["operation"]], [[["Mental disorder-1"]], [["Vincent van Gogh-3"]], ["operation"]], [[["Causes of mental disorders-58"], "operation"], [["Van Gogh syndrome-4"], "no_evidence"], ["no_evidence"]]], "golden_sentence": [["It is also the case that, while often being characterized in purely negative terms, some mental traits or states labeled as disorders can also involve above-average creativity, non-conformity, goal-striving, meticulousness, or empathy."], ["Van Gogh suffered from psychotic episodes and delusions and though he worried about his mental stability, he often neglected his physical health, did not eat properly and drank heavily."]]}, {"qid": "513414cc1cb9485d9e1d", "term": "C-SPAN", "description": "American pay television network", "question": "Is the span in C-SPAN named after Alan Greenspan?", "answer": false, "facts": ["Alan Greenspan was chairman of the Federal Reserve from 1987 to 2006.", "CSPAN is a cable news network that was created in 1979.", "Alan Greenspan was relatively unknown to the world at large in 1979."], "decomposition": ["When was C-SPAN created?", "When was Alan Greenspan well-known?", "Is #1 contained within #2?"], "evidence": [[[["C-SPAN-1"]], [["Alan Greenspan-1"]], [["Alan Greenspan-1", "C-SPAN-1"]]], [[["Cable television-2"]], [["Alan Greenspan-1"]], ["operation"]], [[["C-SPAN-1"]], [["Alan Greenspan-1", "Alan Greenspan-7"]], ["operation"]]], "golden_sentence": [["Cable-Satellite Public Affairs Network (C-SPAN; /\u02c8si\u02d0\u02ccsp\u00e6n/) is an American cable and satellite television network that was created in 1979 by the cable television industry as a nonprofit public service."], ["Alan Greenspan KBE (/\u02c8\u00e6l\u0259n \u02c8\u0261ri\u02d0nsp\u00e6n/; born March 6, 1926) is an American economist who served as Chair of the Federal Reserve of the United States from 1987 to 2006."], ["", ""]]}, {"qid": "af0b75c58e29cae8d566", "term": "Eminem", "description": "American rapper and actor", "question": "Would Eminem perform well at the International Mathematical Olympiad?", "answer": false, "facts": ["Eminem dropped out of Lincoln High School at age 17 and disliked math and social studies.", "Mathematics competitions or mathematical olympiads are competitive events where participants sit a mathematics test.", "The content on the International Mathematical Olympiad ranges from extremely difficult algebra and pre-calculus problems to problems on branches of mathematics not conventionally covered at school and often not at university level either."], "decomposition": ["What levels of mathematics are covered in the International Mathematical Olympiad?", "What levels of mathematics is Eminem competent in?", "Does #2 meet the minimum level in #1?"], "evidence": [[[["International Mathematical Olympiad-2"]], [["Eminem-8"]], [["Eminem-8", "International Mathematical Olympiad-2"]]], [[["International Mathematical Olympiad-2"]], [["Eminem-8"]], [["Eminem-8"]]], [[["International Mathematical Olympiad-1", "International Mathematical Olympiad-2"]], [["Eminem-8"]], ["operation"]]], "golden_sentence": [["The content ranges from extremely difficult algebra and pre-calculus problems to problems on branches of mathematics not conventionally covered at school and often not at university level either, such as projective and complex geometry, functional equations, combinatorics, and well-grounded number theory, of which extensive knowledge of theorems is required."], [""], ["", ""]]}, {"qid": "744c987d7dd7c14425dd", "term": "Guitar Hero", "description": "video game series", "question": "Is Guitar Hero Beatles inappropriate for a US third grader?", "answer": false, "facts": ["The average age of a US third grader is 8.", "Guitar Hero is recommended for ages 7 and up.", "The Beatles were a British rock band with a plethora of radio friendly hits."], "decomposition": ["How old is the average US third grader?", "What is the recommended age to play Guitar Hero?", "Is #1 higher than #2?"], "evidence": [[[["Third grade-1"]], ["no_evidence"], ["operation"]], [[["Third grade-1"]], ["no_evidence"], ["operation"]], [[["Third grade-1"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Students are usually 8\u20139 years old, depending on when their birthday occurs."]]}, {"qid": "36d2fcaebf5fb04d1397", "term": "Society", "description": "Social group involved in persistent social interaction", "question": "In American society, will a bachelor's degree often include a leap year?", "answer": true, "facts": ["Leap years occur every four years.", "In American society, a bachelor's degree takes about four years."], "decomposition": ["Leap years occur after how many years' interval?", "How many years does an average bachelor's degree take in the US?", "Is #2 divided by #1 greater than or equal to one?"], "evidence": [[[["Leap year-16"]], [["Bachelor's degree-1"]], ["operation"]], [[["Leap year-6"]], [["Bachelor's degree-37"]], ["operation"]], [[["Leap year-2"]], [["Bachelor's degree-1", "Bachelor's degree-37"]], ["operation"]]], "golden_sentence": [["This day is added to the calendar in leap years as a corrective measure, because the Earth does not orbit the sun in precisely 365 days."], ["A bachelor's degree (from Middle Latin baccalaureus) or baccalaureate (from Modern Latin baccalaureatus) is an undergraduate academic degree awarded by colleges and universities upon completion of a course of study lasting three to seven years (depending on institution and academic discipline)."]]}, {"qid": "678cd91b0e940994a15f", "term": "Ariana Grande", "description": "American singer, songwriter, and actress", "question": "At a presentation about post traumatic stress disorder, would Ariana Grande be a topic of relevance?", "answer": true, "facts": ["Ariana Grande was performing in Manchester in 2017 when explosives were set off in an act of terrorism.", "Ariana Grande has spoken openly about her trauma experience and her PTSD regarding the Manchester Bombing."], "decomposition": ["What happened during Ariana Grande's performance in Manchester in 2017?", "What types of events cause post traumatic stress disorder?", "Is there any overlap between #1 and #2?"], "evidence": [[[["Ariana Grande-22"]], [["Posttraumatic stress disorder-1"]], ["operation"]], [[["Manchester Arena bombing-4"]], [["Posttraumatic stress disorder-1"]], ["operation"]], [[["Ariana Grande-22"]], [["Traumatic stress-7"]], [["Traumatic stress-7"]]]], "golden_sentence": [["On May 22, 2017, her concert at the Manchester Arena was the target of a suicide bombing - a shrapnel-laden homemade bomb detonated by an Islamic extremist as people were leaving the arena."], ["Posttraumatic stress disorder (PTSD) is a mental disorder that can develop after a person is exposed to a traumatic event, such as sexual assault, warfare, traffic collisions, child abuse, or other threats on a person's life."]]}, {"qid": "cea9c6f0012a627dbe8a", "term": "Banana", "description": "edible fruit", "question": "Were plants crucial for The King of Rock'n Roll's snack with bananas?", "answer": true, "facts": ["Elvis Presley is known as The King of Rock'n Roll.", "Elvis Presley loved to eat peanut butter and bananas.", "Bananas come from banana plants.", "Peanut butter comes from peanuts, which come from peanut plants."], "decomposition": ["Who is commonly referred to as The King of Rock 'n Roll?", "Which snacks was #1 known to take with bananas?", "Are #2 plants products or made from them?"], "evidence": [[[["King of Rock and Roll (disambiguation)-1"]], [["Elvis Presley-86"]], [["Peanut butter, banana and bacon sandwich-1"]]], [[["Elvis Presley-1"]], [["Elvis Presley-86"]], [["Peanut butter-1"], "operation"]], [[["Elvis Presley-1"]], [["Elvis Presley-86"]], [["Peanut butter-1", "Peanut-1"]]]], "golden_sentence": [["The King of Rock'n Roll is a nickname most commonly associated with American rock and roll icon, Elvis Presley."], ["In particular, his love of calorie-laden fried peanut butter, banana, and (sometimes) bacon sandwiches, now known as \"Elvis sandwiches\", came to stand for this aspect of his persona."], [""]]}, {"qid": "bfaf846771d6efbb53c3", "term": "The Atlantic", "description": "Magazine and multi-platform publisher based in Washington, D.C.", "question": "Could you read The Atlantic magazine during the Games of the XXII Olympiad?", "answer": true, "facts": ["The Atlantic magazine, founded in 1857, still publishes as of May 2020.", "The XXII Olympiad was the official name for the 1980 Summer Olympics."], "decomposition": ["When was The Atlantic Magazine founded?", "When was the XXII Olypiad?", "Is #2 after #1?"], "evidence": [[[["The Atlantic-1"]], [["1980 Summer Olympics-1"]], ["operation"]], [[["The Atlantic-1"]], [["1980 Summer Olympics-1"]], ["operation"]], [[["The Atlantic-1"]], [["1980 Summer Olympics-1"]], ["operation"]]], "golden_sentence": [["It was founded in 1857 in Boston, Massachusetts, as The Atlantic Monthly, a literary and cultural commentary magazine that published leading writers' commentary on the abolition of slavery, education, and other major issues in contemporary political affairs."], ["The 1980 Summer Olympics, officially known as the Games of the XXII Olympiad (Russian: \u0418\u0301\u0433\u0440\u044b XXII \u041e\u043b\u0438\u043c\u043f\u0438\u0430\u0301\u0434\u044b, tr."]]}, {"qid": "8a5edfb7385edb776926", "term": "Bob Marley", "description": "Jamaican singer-songwriter", "question": "Can you find Bob Marley's face in most smoke shops?", "answer": true, "facts": ["Bob Marley's face is on the packaging of a popular brand of rolling papers.", "Bob Marley is a popular graphic to print on t-shirts for sale to smokers."], "decomposition": ["Where can one find Bob Marley's face printed on?", "Are any items from #1 commonly found in smoke shops?"], "evidence": [[[["Bob Marley-1"], "no_evidence"], [["Head shop-1", "Head shop-2"], "no_evidence", "operation"]], [[["Marley Natural-2"]], ["operation"]], [[["Bob Marley-48"]], ["operation"]]], "golden_sentence": [[""], ["A head shop is a retail outlet specializing in paraphernalia used for consumption of cannabis and tobacco and items related to cannabis culture and related countercultures.", "Other items offered typically include hashish pipes, \"one hitter\" pipes; pipe screens; bongs (also referred to as water pipes); roach clips (used for smoking the end of a marijuana \"joint\"); vaporizers used for inhaling THC vapor from cannabis; rolling papers; rolling machines; small weighing scales; small ziplock baggies; cannabis grinders; blacklight-responsive posters and blacklights; incense; cigarette lighters; \"stashes\", which include a range of standard consumer products such as clocks, books, tins of cleaning powder, and toilet brushes which have hidden compartments for cannabis and non-camouflaged \"stash boxes\" which are tins or wooden containers for storing marijuana; and legal highs such as whipped-cream chargers (which contain nitrous oxide) and Salvia divinorum (both of which are illegal in some countries and some US states for recreational purposes)."]]}, {"qid": "22922e2b351d2eef9e85", "term": "Elizabeth I of England", "description": "Queen regnant of England and Ireland from 17 November 1558 until 24 March 1603", "question": "Could Elizabeth I of England have seen the play Dido, Queen of Carthage ?", "answer": true, "facts": ["Elizabeth I of England lived from 1533 - 1603.", "Dido, Queen of Carthage is a short play written by the English playwright Christopher Marlowe.", " It was probably written between 1587 and 1593."], "decomposition": ["When was the play Dido, Queen of Carthage written?", "Was Elizabeth I of England alive during the period covered by #1?"], "evidence": [[[["Dido, Queen of Carthage (play)-1"]], [["Elizabeth I of England-1"]]], [[["Dido, Queen of Carthage (play)-1"]], [["Elizabeth I of England-1"]]], [[["Dido, Queen of Carthage (play)-1"]], [["Elizabeth I of England-1"]]]], "golden_sentence": [["It was probably written between 1587 and 1593, and was first published in 1594."], ["Elizabeth I (7 September 1533 \u2013 24 March 1603) was Queen of England and Ireland from 17 November 1558 until her death on 24 March 1603."]]}, {"qid": "6cf3f6d63de7860e2610", "term": "Bipolar disorder", "description": "mental disorder that causes periods of depression and abnormally elevated mood", "question": "Is Britney Spears' breakdown attributed to bipolar disorder?", "answer": true, "facts": ["In 2008 Britney Spears was detained in a psychiatric hospital for bipolar disorder.", "In 2007 Britney Spears shaved her head during a breakdown. "], "decomposition": ["When did Britney Spears have a breakdown?", "What major event happened to Britney Spears the year after #1?", "What was the reason behind #2 happening to Britney Spears?"], "evidence": [[[["Britney Spears-25"]], [["Britney Spears-27"]], ["no_evidence"]], [[["Britney Spears-25"]], [["Britney Spears-24"]], ["no_evidence"]], [[["Britney Spears-25"]], [["Britney Spears-27"]], [["Britney Spears-61"], "no_evidence"]]], "golden_sentence": [[""], [""]]}, {"qid": "58f2a892e2497bb16691", "term": "Chlorine", "description": "Chemical element with atomic number 17", "question": "Is week old chlorine water safe to drink?", "answer": true, "facts": ["Chlorine is a chemical that is the second lightest halogen element.", "Chlorine is toxic and can attack the respiratory system of humans.", "Chlorine is highly soluble in water and will dissolve in around 4 and a half days.", "The Water Quality and Health Council states that chlorination of drinking water protects consumers from diseases caused by waterborne microorganisms."], "decomposition": ["How long does it take for chlorine to dissolve in water?", "Is water with dissolved chlorine safe to drink?", "Is #2 positive and #1 less than a week?"], "evidence": [[[["Water chlorination-5"]], [["History of water supply and sanitation-75"]], ["operation"]], [[["Chlorine-66"], "no_evidence"], [["Hypochlorous acid-3"], "operation"], ["no_evidence", "operation"]], [[["Chlorine-13"], "no_evidence"], [["History of water supply and sanitation-75"]], ["no_evidence", "operation"]]], "golden_sentence": [["The chlorine bubbles out, and in ten to fifteen minutes the water is absolutely safe."], ["The chlorine bubbles out, and in ten to fifteen minutes the water is absolutely safe."]]}, {"qid": "58d541b693e93f54646a", "term": "Foot (unit)", "description": "customary unit of length", "question": "Is the foot part of the metric system?", "answer": false, "facts": ["The metric system measures distance based on the meter unit.", "The foot is part of the American standard system of measurement."], "decomposition": ["Which units of measure are used in the metric system?", "Is foot included in #1?"], "evidence": [[[["Metric system-1", "Metric system-27"]], [["Foot (unit)-1"], "operation"]], [[["Metric system-5"]], ["operation"]], [[["Metric system-28"]], ["operation"]]], "golden_sentence": [["", "The centimetre\u2013gram\u2013second system of units (CGS) was the first coherent metric system, having been developed in the 1860s and promoted by Maxwell and Thomson."], [""]]}, {"qid": "921a367ea09712b5d7a5", "term": "Rede Globo", "description": "Brazilian commercial television network", "question": "Would it be typical for a Rede Globo anchor to say Konnichiwa to the viewers?", "answer": false, "facts": ["Konnichiwa is a greeting in the Japanese language.", "The national language of Brazil is Portuguese."], "decomposition": ["In which country is Rede Globo based?", "What is the official language in #1?", "What language is Konnichiwa?", "Are #2 and #3 the same?"], "evidence": [[[["Rede Globo-1"]], [["Portuguese language-1"]], [["Konnichi wa-1"]], ["operation"]], [[["Rede Globo-1"]], [["Portuguese language-1"]], [["Konnichi wa-1"]], ["operation"]], [[["Rede Globo-1"]], [["Rede Globo-1"]], [["Konnichi wa-1"]], ["operation"]]], "golden_sentence": [["Rede Globo (Portuguese:\u00a0[\u02c8\u0281ed\u0292i \u02c8\u0261lobu], Globe Network), or simply Globo, is a Brazilian free-to-air television network, launched by media proprietor Roberto Marinho on 26 April 1965."], ["A Portuguese-speaking person or nation is referred to as \"Lusophone\" (Lus\u00f3fono)."], ["Konnichi wa (\u3053\u3093\u306b\u3061\u306f or in kanji \u4eca\u65e5\u306f) is a Japanese greeting, typically a mid-day to early evening greeting (10:00 AM to 7:59 PM)."]]}, {"qid": "104db562c76a709eb419", "term": "Hades", "description": "Greek god of the underworld in Greek mythology", "question": "Does Hades appear in a Disney Channel musical movie?", "answer": true, "facts": ["The Descendants Trilogy is a series of musical movies that aired on Disney Channel between 2015 and 2019.", "Hades appears as a supporting character in the third Descendants movie."], "decomposition": ["Which major musical series has been aired on Disney Channel?", "Has Hades been featured in any of #1?"], "evidence": [[[["Hercules (franchise)-1"]], [["Megara (Disney character)-17"]]], [[["Descendants (franchise)-1"]], [["Descendants 3-3"], "operation"]], [[["Hercules (franchise)-17", "Megara (Disney character)-17"]], [["Megara (Disney character)-17"]]]], "golden_sentence": [[""], [""]]}, {"qid": "bab8fdadfb7bd5e93447", "term": "Minor League Baseball", "description": "hierarchy of professional baseball leagues affiliated with Major League Baseball", "question": "Were weather phenomena avoided when naming minor league baseball teams?", "answer": false, "facts": ["Weather phenomena refers to types of weather caused conditions such as cyclones, storms, and tsunamis.", "Minor league baseball teams include the Brooklyn Cyclones and Lake Elsinore Storm."], "decomposition": ["What are some names of weather phenomena?", "What are the name of minor league baseball teams?", "Are any terms in #1 also present in #2?"], "evidence": [[[["Weather-5"], "no_evidence"], [["Omaha Storm Chasers-1"], "no_evidence"], ["operation"]], [[["Glossary of meteorology-1"], "no_evidence"], [["Minor League Baseball-40"], "no_evidence"], ["no_evidence", "operation"]], [[["Thunder-1"]], [["Trenton Thunder-1"]], ["operation"]]], "golden_sentence": [["On Earth, the common weather phenomena include wind, cloud, rain, snow, fog and dust storms."], ["The Omaha Storm Chasers are a Minor League Baseball team of the Pacific Coast League (PCL) and the Triple-A affiliate of the Kansas City Royals."]]}, {"qid": "b0232a817839a4931ed9", "term": "Voyager 2", "description": "Space probe and the second-farthest man-made object from Earth", "question": "Could a Hwasong-15 missile hypothetically reach Voyager 2?", "answer": false, "facts": ["Voyager 2 was a probe that traveled to the interstellar medium of space.", "The interstellar medium is over 12,161,300,000 miles away from earth.", "The Hwasong-15 missile is a North Korean missile with a range of 8,000 miles."], "decomposition": ["How far away from Earth has Voyager 2 traveled?", "What is the range of a Hwasong-15 missile?", "Is #2 greater or equal to #1?"], "evidence": [[[["Voyager 2-3"]], [["Hwasong-15-3"]], ["operation"]], [[["Voyager 2-3"]], [["Hwasong-15-1"]], ["operation"]], [[["Voyager 2-3"]], [["Hwasong-15-3"]], ["operation"]]], "golden_sentence": [["On November 5, 2018, at a distance of 122\u00a0AU (1.83\u00d71010\u00a0km) (about 16:58 light-hours) from the Sun, moving at a velocity of 15.341\u00a0km/s (55,230\u00a0km/h) relative to the Sun, Voyager 2 left the heliosphere, and entered the interstellar medium (ISM), a region of outer space beyond the influence of the Solar System, joining Voyager 1 which had reached the interstellar medium in 2012."], ["Based on its trajectory and distance, the missile would have a range of more than 13,000\u00a0km (8,100 miles) \u2013 more than enough to reach Washington D.C. and the rest of the United States, albeit, according to the Union of Concerned Scientists, probably with a reduced payload."]]}, {"qid": "5cdc2fe1b3f493590e96", "term": "Armadillo", "description": "family of mammals", "question": "Could someone theoretically use an armadillo as a shield?", "answer": true, "facts": ["Armadillos have hard armor made of dermal bone.", "Humans have ended up in the hospital due to bullets ricocheting against an armadillo's shell."], "decomposition": ["What are the basic features of a shield?", "Does any part of the armadillo's body possess any of #1?"], "evidence": [[[["Shield-1"]], [["Armadillo-2"], "operation"]], [[["Shield-2"]], [["Armadillo-2"], "operation"]], [[["Shield-1"]], [["Armadillo-2"]]]], "golden_sentence": [["Shields are used to intercept specific attacks, whether from close-ranged weaponry or projectiles such as arrows, by means of active blocks, as well as to provide passive protection by closing one or more lines of engagement during combat."], ["They have short legs, but can move quite quickly."]]}, {"qid": "7a16e259ec3aacd46d18", "term": "Linus Torvalds", "description": "Creator and lead developer of Linux kernel", "question": "Does Linus Torvalds make money off of DirectX?", "answer": false, "facts": ["DirectX is a proprietary technology owned by Microsoft", "Linus Torvalds is the creator and lead developer for the open-source Linux kernel", "The Linux kernel is used in operating systems that are competitors of Microsoft Windows"], "decomposition": ["Which company owns the DirectX technology?", "Which operating system does #1 develop?", "Linus Torvalds develops which operating system?", "Is #2 the same as #3?"], "evidence": [[[["DirectX-1"]], [["Microsoft Windows-1"]], [["Linus Torvalds-1"]], ["operation"]], [[["DirectX-1"]], [["DirectX-1"]], [["Linus Torvalds-1"]], ["operation"]], [[["DirectX-1"]], [["Microsoft Windows-1"]], [["Linus Torvalds-1"]], ["operation"]]], "golden_sentence": [[""], ["Microsoft Windows, commonly referred to as Windows, is a group of several proprietary graphical operating system families, all of which are developed and marketed by Microsoft."], ["Linus Benedict Torvalds (/\u02c8li\u02d0n\u0259s \u02c8t\u0254\u02d0rv\u0254\u02d0ldz/ LEE-n\u0259s TOR-vawldz, Finland Swedish:\u00a0[\u02c8li\u02d0n\u0289s \u02c8tu\u02d0rv\u0251lds] (listen); born 28 December 1969) is a Finnish-American software engineer who is the creator and, historically, the principal developer of the Linux kernel, which is the kernel for Linux operating systems (distributions) and other operating systems such as Android and Chrome OS."]]}, {"qid": "8a947cc706d82ad5b606", "term": "Mediterranean Sea", "description": "Sea connected to the Atlantic Ocean between Europe, Africa and Asia", "question": "Did a Mediterranean Sea creature kill Steve Irwin?", "answer": true, "facts": ["Steve Irwin was killed by a Stingray animal.", "Batoids are sea ray animals that live in the Mediterranean Sea.", "Batoids and stingrays are related by sharing a scientific class of Chondrichthyes."], "decomposition": ["Which animal killed Steve Irwin?", "Is #1 a sea creature"], "evidence": [[[["Steve Irwin-35"]], [["Stingray-2"]]], [[["Steve Irwin-35"]], [["Broad stingray-4", "Great Barrier Reef-7"]]], [[["Steve Irwin-35"]], [["Stingray-1"]]]], "golden_sentence": [["Irwin died on 4 September 2006 after being pierced in the chest by a stingray barb while filming in Australia's Great Barrier Reef."], [""]]}, {"qid": "8d48ef854781edff3026", "term": "Zika virus", "description": "Species of virus", "question": "Do you need to worry about Zika virus in Antarctica? ", "answer": false, "facts": ["Mosquitoes cannot survive in the climate of Antarctica.", "Zika virus is primarily spread through mosquito bites. "], "decomposition": ["What animal spreads the Zika Virus?", "What is the climate of Antarctica?", "Can #1 survive in #2?"], "evidence": [[[["Zika virus-10"]], [["Antarctica-42"]], [["Antarctica-42", "Mosquito-68"], "operation"]], [[["Zika fever-2"]], [["Antarctica-42"]], [["Mosquito-61"], "operation"]], [[["Aedes-1", "Zika fever-2"]], [["Antarctica-2"]], ["operation"]]], "golden_sentence": [["Zika is primarily spread by the female Aedes aegypti mosquito, which is active mostly in the daytime."], ["Antarctica is the coldest of Earth's continents."], ["", ""]]}, {"qid": "72b2178bc5ab72b57c9a", "term": "Christmas carol", "description": "Song or hymn or carol on the theme of Christmas", "question": "Did the writer of Christmas carol fast during Ramadan? ", "answer": false, "facts": ["The writer of Christmas carol is Charles Dickens, who is a Christian. ", "Christians do not fast during Ramadan. "], "decomposition": ["Which group of people fast during Ramadan?", "Christmas carols are composed by and for which group of people?", "Are #2 and #1 the same?"], "evidence": [[[["Ramadan-1"]], [["Christmas and holiday season-2", "Christmas carol-1"]], ["operation"]], [[["Ramadan-1"]], [["Christmas carol-1", "Christmas-1"]], ["operation"]], [[["Ramadan-1"]], [["Christmas carol-10"]], ["operation"]]], "golden_sentence": [["Ramadan (/\u02ccr\u00e6m\u0259\u02c8d\u0251\u02d0n/, also US: /\u02ccr\u0251\u02d0m-, \u02c8r\u00e6m\u0259d\u0251\u02d0n, \u02c8r\u0251\u02d0m-/, UK: /\u02c8r\u00e6m\u0259d\u00e6n/;) or Ramazan (Arabic: \u0631\u064e\u0645\u064e\u0636\u064e\u0627\u0646\u200e, romanized:\u00a0Rama\u1e0d\u0101n [ra.ma.d\u02e4a\u02d0n]; also spelled Ramzan, Ramadhan, or Ramathan) is the ninth month of the Islamic calendar, observed by Muslims worldwide as a month of fasting (sawm), prayer, reflection and community."], ["", ""]]}, {"qid": "a46d7c79c3cae661a145", "term": "Haiku", "description": "very short form of Japanese poetry", "question": "Are most books written as a Haiku?", "answer": false, "facts": ["Haiku is a very short poem", "Haiku is written with 3 short phrases."], "decomposition": ["What is the format of a haiku?", "Are chapter books written like #1?"], "evidence": [[[["Haiku-2"]], ["operation"]], [[["Haiku-2"]], [["Chapter book-1"]]], [[["Haiku-2"]], ["no_evidence"]]], "golden_sentence": [["Traditional haiku often consist of 17 on (also known as morae though often loosely translated as \"syllables\"), in three phrases of 5, 7, and 5 on, respectively."]]}, {"qid": "405dbaafb8faeb57b1b2", "term": "1976 Summer Olympics", "description": "Games of the XXI Olympiad, held in Montr\u00e9al in 1976", "question": "Does the country that received the most gold medals during the 1976 Olympics still exist?", "answer": false, "facts": ["The Soviet Union received the most gold medals during the 1976 Summer Olympics", "The Soviet Union existed from 1922 to 1991"], "decomposition": ["In the 1976 Summer Olympics, which country received the most gold medals?", "Does #1 still exist as a country?"], "evidence": [[[["1976 Summer Olympics-3"]], [["Soviet Union-1"], "operation"]], [[["1976 Summer Olympics-3"]], [["Soviet Union-1"]]], [[["1976 Summer Olympics medal table-3"]], [["Islam in the Soviet Union-1"]]]], "golden_sentence": [["Twenty-nine countries, mostly African, boycotted the Montreal Games when the International Olympic Committee (IOC) refused to ban New Zealand, after the New Zealand national rugby union team had toured South Africa earlier in 1976 in defiance of the United Nations' calls for a sporting embargo."], [""]]}, {"qid": "934e9728911e2511986b", "term": "Chuck Norris", "description": "American martial artist, actor, film producer and screenwriter", "question": "Will Chuck Norris be a nonagenarian by time next leap year after 2020 happens?", "answer": false, "facts": ["A nonagenarian is a person between 90 and 99 years of age.", "Chuck Norris is 80 years old in 2020.", "The next leap year after 2020 is 2024.", "Chuck Norris will be 84 in 2024."], "decomposition": ["When was Chuck Norris born?", "When is the next leap year after 2020?", "What is the difference between #1 and #2?", "How many years of age makes one a nonagenarian?", "Is #3 greater than or equal to #4?"], "evidence": [[[["Chuck Norris-1"]], [["Determination of the day of the week-14"]], ["operation"], [["Manuel Pinto da Fonseca-9"]], ["operation"]], [[["Chuck Norris-1"]], [["Leap year-6"], "no_evidence"], ["operation"], ["no_evidence"], ["operation"]], [[["Chuck Norris-4"]], [["2024-1"]], ["operation"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Carlos Ray \"Chuck\" Norris (born March 10, 1940) is an American martial artist, actor, film producer and screenwriter."], ["the year 2024 will be a leap year starting on Monday: its first 2 months, except 29 February, will correspond to those of the 2018 calendar year and its subsequent 10 months will correspond to the 2019 calendar year."], ["He was elected as Grand Master in his 60th year, and he lived to be a nonagenarian, ruling the Order for 32 years."]]}, {"qid": "cd6dcccbc720bf6119c8", "term": "Silk", "description": "fine, lustrous, natural fiber produced by the larvae of various silk moths, especially the species Bombyx mori", "question": "Does Bombyx mori have a monopoly over silk production?", "answer": false, "facts": ["A monopoly refers to the exclusive supply of a good.", "The Bombyx mori is a moth famous for its silk production.", "Spiders, beetles, caterpillars, and fleas produce silk.", "Wild silk produced by caterpillars has been used in China, Europe, and South Asia since antiquity."], "decomposition": ["In a monopoly, how many different entities supply goods?", "What insects produce silk?", "How many things are listed in #2?", "Is #3 equal to #1?"], "evidence": [[[["Monopoly-1"]], [["Bombyx mori-1", "Silk-2"]], ["operation"], ["operation"]], [[["Monopoly-1"]], [["Silk-2"]], [["Silk-2"]], ["operation"]], [[["Monopoly-2"]], [["Silk-2"]], ["operation"], ["operation"]]], "golden_sentence": [[""], ["", "Silk is mainly produced by the larvae of insects undergoing complete metamorphosis, but some insects, such as webspinners and raspy crickets, produce silk throughout their lives."]]}, {"qid": "4c4cf39534a5147f0b01", "term": "Cannabis (drug)", "description": "psychoactive drug from the Cannabis plant", "question": "Has cannabis been a big influence in rap music genre?", "answer": true, "facts": ["Rapper Dr. Dre named his 1992 album, The Chronic, a reference to marijuana.", "Cannabis is a flowering plant also known as marijuana.", "Rapper Canibus took his name from cannabis.", "Rapper Snoop Dogg's song OG has a line, \"Rolling up my Mary Jane,\" a reference to marijuana.", "Rap group Bone Thugs N Harmony's Weed Song is a reference to marijuana."], "decomposition": ["What is Rapper Dr. Dre's Album The Chronic a reference to?", "What did Rapper Canibus get his name from?", "Snoop Dogg's line \"Rolling up my Mary Jane\" from the song OG has reference to?", "Are all #1, #2, #3 the same as cannabis?"], "evidence": [[[["The Chronic-1"]], [["Canibus-4", "Cannabis sativa-1"]], [["Mary + Jane-1"]], ["operation"]], [[["The Chronic-1"]], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["The Chronic-1"]], [["Cannabis (drug)-1"]], [["Snoop Dogg-88"], "no_evidence"], ["operation"]]], "golden_sentence": [["The album is named after a slang term for high-grade cannabis, and its cover is an homage to Zig-Zag rolling papers."], ["He began rhyming in the early '90s and by 1992 under the name Canibus Sativa, and formed a duo called T.H.E.M.", "Cannabis sativa is an annual herbaceous flowering plant indigenous to eastern Asia but now of cosmopolitan distribution due to widespread cultivation."], [""]]}, {"qid": "e9b19bc7d518d7b11ec7", "term": "Jews", "description": "Ancient nation and ethnoreligious group from the Levant", "question": "Do Jews believe in any New Testament angels?", "answer": true, "facts": ["The New Testament is a book central to Christianity.", "The New Testament features a number of angels including Michael, and Gabriel.", "The Talmud is the central text of Rabbinic Judaism.", "The Talmud names four angels who would later be known as archangels, surrounding God's throne: Michael, Gabriel, Uriel, and Raphael."], "decomposition": ["What book is the central text of Rabbinic Judaism?", "Does #1 mention any angels?", "Are the angels mentioned in #2 also mentioned in the New testament?"], "evidence": [[[["Rabbinic Judaism-3"]], [["Angels in Judaism-1"]], [["Angels in Judaism-1"]]], [[["Rabbinic Judaism-3", "Talmud-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Hebrew Bible-1"]], [["Book of Daniel-2", "Michael (archangel)-2"]], [["Michael (archangel)-3"], "operation"]]], "golden_sentence": [["Rabbinic Judaism is distinguished by belief in Moses as \"our Rabbi\" and that God revealed the Torah in two parts, as both the Written and the Oral Torah, also known as the Mishnah."], [""], [""]]}, {"qid": "fbb8e270790c9c9b16d1", "term": "Christians", "description": "people who adhere to Christianity", "question": "Do Christians anticipate an existence in Sheol after death?", "answer": false, "facts": ["Sheol appears in the Christian Bible, in the Old Testament.", "Christians do not recognize Sheol as part of their afterlife."], "decomposition": ["Which Testament of the Bible makes reference to Sheol?", "Is #1 the New Testament?", "Is Sheol included in Christians' concept of afterlife as expressed in the New Testament?", "Is #2 or #3 positive?"], "evidence": [[[["Sheol-2"]], ["operation"], [["Heaven-15", "Hell-34"], "no_evidence"], ["operation"]], [[["Sheol-1"]], [["New Testament-1"], "operation"], [["New Testament-11"], "no_evidence"], ["operation"]], [[["Sheol-2"]], [["Sheol-1"]], [["Afterlife-44"]], ["operation"]]], "golden_sentence": [["This is reflected in the New Testament where Hades is both the underworld of the dead and the personification of it."], ["", "The word hell does not appear in the Greek New Testament; instead one of three words is used: the Greek words Tartarus or Hades, or the Hebrew word Gehinnom."]]}, {"qid": "4129702b8c04d31bd007", "term": "Courage", "description": "quality of mind or spirit that enables a person to face difficulty, danger, or pain", "question": "Would an anxious person benefit from receiving courage from the Wizard of Oz?", "answer": false, "facts": ["An anxious person may benefit from medication or therapy.", "The Wizard of Oz cannot give courage to anyone."], "decomposition": ["What would an anxious person benefit from receiving?", "Can the Wizard of Oz provide #1?"], "evidence": [[[["Anxiety disorder-3", "Anxiety disorder-58"]], [["The Wizard of Oz (1939 film)-8"], "operation"]], [[["Anxiety-2", "Panic attack-47"], "no_evidence"], [["The Wonderful Wizard of Oz-10"], "no_evidence", "operation"]], [[["Anxiety-1", "Courage-1"]], [["The Wonderful Wizard of Oz-13"], "operation"]]], "golden_sentence": [["Treatment may include lifestyle changes, counselling, and medications.", "Medications include SSRIs or SNRIs are first line choices for generalized anxiety disorder."], [""]]}, {"qid": "155c80c22c695a49ad5b", "term": "Polyamory", "description": "Practice of or desire for intimate relationships with more than one partner", "question": "Is polyamory allowed in the Catholic Church?", "answer": false, "facts": ["A central tenet of the Catholic Church is a one-to-one match between man and woman.", "The ten commandments claim that \"coveting your neighbors wife\" is a sin."], "decomposition": ["What is Polyamory?", "Is #1 allowed in catholic churches?"], "evidence": [[[["Polyamory-10"]], [["Polygamy-34"]]], [[["Polyamory-1"]], [["Catholic Church-66"], "operation"]], [[["Polyamory-1"]], [["Religion and sexuality-16"]]]], "golden_sentence": [["polyamorists emphasize respect, trust, and honesty for all partners."], ["The Roman Catholic Church condemns polygamy; the Catechism of the Catholic Church lists it in paragraph 2387 under the head \"Other offenses against the dignity of marriage\" and states that it \"is not in accord with the moral law.\""]]}, {"qid": "942ec13cf67a99594e74", "term": "Police officer", "description": "warranted employee of a police force", "question": "Does a Generation Y member satisfy NYPD police officer age requirement?", "answer": true, "facts": ["The NYPD has a minimum age requirement of 21.", "Members of Generation Y were born between 1980 and 1994."], "decomposition": ["How old do you have to be to be an NYPD officer?", "How old are Generation Y members currently?", "Is #2 higher than #1?"], "evidence": [[[["New York City Police Department Auxiliary Police-31"], "no_evidence"], [["Millennials-1"]], ["operation"]], [["no_evidence"], [["Millennials-1"]], ["no_evidence", "operation"]], [["no_evidence"], [["Millennials-1"]], ["no_evidence", "operation"]]], "golden_sentence": [["Undercover Vice Ops: Officers that have not reached the age of 21 years old (18-20 1/2) can be utilized to assist the NYPD Vice Unit Narcotics Division for \"Quality of Life enforcement\" stings which address illegal alcohol, knife, and spray paint sales."], [""]]}, {"qid": "a231ac31e68b1281be3d", "term": "Alice's Adventures in Wonderland", "description": "book by Lewis Carroll", "question": "Could the main character of \"Alice's Adventures in Wonderland\" join a Masonic Lodge?", "answer": false, "facts": ["The main character of \"Alice's Adventures in Wonderland\" is Alice, a young girl.", "Masonic Lodge membership is restricted to men over the age of either 18 or 21, depending on jurisdiction."], "decomposition": ["Who is the main character of \"Alice's Adventures in Wonderland\"?", "Does #1 meet the age and gender requirements for Masonic Lodge membership?"], "evidence": [[[["Alice's Adventures in Wonderland-1"]], [["Masonic lodge-11"], "operation"]], [[["Alice's Adventures in Wonderland-1"]], [["Freemasonry-3"], "no_evidence", "operation"]], [[["Alice's Adventures in Wonderland-1"]], [["Freemasonry-14", "Girl-1"]]]], "golden_sentence": [[""], ["(this is not a universal requirement)."]]}, {"qid": "c5f0202c84eb45db41cb", "term": "Fairy", "description": "mythical being or legendary creature", "question": "Did King James I despise fairy beings?", "answer": true, "facts": ["King James I wrote Daemonologie in which he stated that a fairy was a being that could act as a familiar.", "A familiar was an animal or spirit that conspired with The Devil.", "King James I presided over the execution of Agnes Sampson.", "Agnes Sampson was accused of conspiring with familiars and was burned at the stake."], "decomposition": ["What did King James I claim that fairies could act as in his book 'Daemonologie'", "Which beings did he execute Agnes Sampson for allegedly conspiring with?", "Is #1 the same as #2?"], "evidence": [[[["Daemonologie-8"], "no_evidence"], [["Agnes Sampson-9"], "no_evidence"], ["operation"]], [[["Daemonologie-15"]], [["Agnes Sampson-7"], "no_evidence"], ["operation"]], [[["Daemonologie-6"], "no_evidence"], [["Agnes Sampson-10", "Agnes Sampson-5", "Agnes Sampson-9"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "7dec073f94dadd70a940", "term": "Evander Holyfield", "description": "American boxer", "question": "Did Evander Holyfield compete in an Olympics hosted in the western hemisphere?", "answer": true, "facts": ["Evander Holyfield won a bronze medal during the 1984 Summer Olympics.", "The 1984 Olympics were held in Los Angeles, California.", "California is in the United States, which is located entirely within the western hemisphere."], "decomposition": ["Which Olympic games have been held in the Western Hemisphere?", "Did Evander Holyfield compete in any events listed in #1?"], "evidence": [[[["1984 Summer Olympics-1"], "no_evidence"], [["Evander Holyfield-2"], "operation"]], [[["1984 Summer Olympics-1", "Western Hemisphere-3"], "no_evidence"], [["Evander Holyfield-2"]]], [[["1984 Summer Olympics-1"]], [["Evander Holyfield-2"], "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "b18c3b40dfd498acb07a", "term": "Harry Houdini", "description": "American magician, escapologist, and stunt performer", "question": "Did Harry Houdini's wife make psychics look foolish?", "answer": true, "facts": ["Psychics are people that claim to have special powers to talk to the dead.", "Harry Houdini gave his wife a code word to ask psychics to repeat after his death.", "The wife of Harry Houdini, Wilhelmina Beatrice \"Bess\" Rahner, asked several psychics for the code word and none knew it."], "decomposition": ["What did Harry Houdini give to his wife to test psychics after his death?", "Were psychics unable to provide #1?"], "evidence": [[[["Harry Houdini-60"]], [["Harry Houdini-60"], "operation"]], [[["Harry Houdini-60"]], [["Harry Houdini-60"], "operation"]], [[["Harry Houdini-60"]], [["Bess Houdini-6"], "operation"]]], "golden_sentence": [["Before Houdini died, he and his wife agreed that if Houdini found it possible to communicate after death, he would communicate the message \"Rosabelle believe\", a secret code which they agreed to use."], [""]]}, {"qid": "f18a2187e9e14ff37a00", "term": "Handedness", "description": "Better performance or individual preference for use of a hand", "question": "Does handedness determine how you use American Sign Language?", "answer": true, "facts": ["Your dominant hand typically performs the moving part of a sign in ASL.", "Your dominant hand determines the hand you use to finger spell in ASL."], "decomposition": ["Does the dominant hand perform different functions than the other in ASL?"], "evidence": [[[["American Sign Language-40"], "no_evidence"]], [[["American Sign Language-1", "American Sign Language-29"], "no_evidence", "operation"]], [["no_evidence"]]], "golden_sentence": [[""]]}, {"qid": "9fb5db35d76f02775584", "term": "Onion", "description": "vegetable", "question": "Do onions have a form that resembles the inside of a tree?", "answer": true, "facts": ["When bisected, an onion has rings that extend from the core to the outside.", "Trees are formed of a series of rings that extend from the inside to the outside"], "decomposition": ["What is the structure observed in an onion when it is cut open?", "What is the structure of a tree's cross section?", "Is #1 similar to #2?"], "evidence": [[[["Fried onion-8"]], [["International Tree-Ring Data Bank-1"]], [["International Tree-Ring Data Bank-1"], "operation"]], [[["Onion-13"]], [["Tree-28"]], ["operation"]], [[["Onion-13"]], [["Dendrochronology-7"]], ["operation"]]], "golden_sentence": [[""], [""], [""]]}, {"qid": "4630c3cd1f451016fbac", "term": "Moose", "description": "A genus of mammals belonging to the deer, muntjac, roe deer, reindeer, and moose family of ruminants", "question": "Are moose used for work near the kingdom of Arendelle?", "answer": true, "facts": ["The opening scene of Disney's Frozen shows a group of ice breakers.", "They have moose that carry the heavy ice blocks.", "One of them, Kristoff, becomes separated with his moose Sven.", "When Queen Elsa flees Arendelle and Princess Anna gives chase, she quickly encounters Kristoff."], "decomposition": ["What show is the kingdom of Arendelle from?", "In the opening scene of #1, what are a group of men doing?", "Are moose used to carry #2?"], "evidence": [[[["Arendelle: World of Frozen-1", "Elsa (Frozen)-26"], "no_evidence"], ["no_evidence"], ["no_evidence"]], [[["Frozen (2013 film)-45"]], [["Frozen (2013 film)-44"], "no_evidence"], [["Moose-73"], "operation"]], [[["Frozen (2013 film)-6"]], ["no_evidence"], [["Moose-9"], "no_evidence", "operation"]]], "golden_sentence": [["", ""]]}, {"qid": "88d039672732bdfd5b4a", "term": "Eleventh grade", "description": "educational year", "question": "Is eleventh grade required to get a driver's licence?", "answer": false, "facts": ["Eleventh grade is an educational year in high school.", "Many high schools offer driver's education classes.", "Drivers education classes can be taken outside by other organizationsof high school.", "One must pass a driving test to obtain a drivers license."], "decomposition": ["What criteria must be met to obtain a driver's license in the US?", "Is passing the eleventh grade required to meet #1?"], "evidence": [[[["Driver's licenses in the United States-10"]], ["operation"]], [[["Driver's license-3"]], ["operation"]], [[["Driver's license-3"]], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "e85ef54f8e08d34b8fc4", "term": "Blue", "description": "A primary colour between purple and green", "question": "Do some home remedies result in your skin color turning blue?", "answer": true, "facts": ["Colloidal silver is a popular alternative treatment/home remedy that is used by some people.", "Ingestion of colloidal silver in high amounts can tint the skin blue."], "decomposition": ["What can cause skin color to change?", "Of #1, what changes can be caused by ingestion of something?", "Of #2, what causes skin color to become blue?", "Is #3 used in home remedies?"], "evidence": [[[["Argyria-1", "Argyria-6", "Carrot juice-3", "Drug-induced pigmentation-2"]], [["Carrot juice-3", "Drug-induced pigmentation-2", "Medical uses of silver-21"]], [["Argyria-1"]], [["Argyria-5"], "operation"]], [[["Human skin color-41", "Human skin color-56"], "no_evidence"], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Cyanosis-1"]], [["Methemoglobinemia-2"]], [["Methemoglobinemia-6"]], [["Benzocaine-5"], "no_evidence"]]], "golden_sentence": [["", "", "Like many products high in beta-carotene, it may cause temporary carotenoderma, a benign skin condition resulting in an orange-yellow hue to the skin.", "Drug-induced pigmentation of the skin may occur as a consequence of drug administration, and the mechanism may be postinflammatory hyperpigmentation in some cases, but frequently is related to actual deposition of the offending drug in the skin.The incidence of this change varies, and depends on the type of medication involved."], ["", "", "Localized argyria can occur as a result of topical use of silver-containing creams and solutions, while the ingestion, inhalation, or injection can result in generalized argyria."], [""], [""]]}, {"qid": "da9568adc5863f8dcda2", "term": "Jack Kerouac", "description": "American writer", "question": "Was ethanol beneficial to Jack Kerouac's health?", "answer": false, "facts": ["In 1969, at age 47, Kerouac died from an abdominal hemorrhage caused by a lifetime of heavy drinking of alcohol.", "Ethanol is the main ingredient in alcoholic beverages."], "decomposition": ["What did Jack Kerouac die from?", "Is there ethanol in #1? "], "evidence": [[[["Jack Kerouac-41"]], [["Ethanol-1"], "operation"]], [[["Jack Kerouac-41"]], [["Jack Kerouac-41"], "no_evidence"]], [[["Jack Kerouac-41"]], [["Ethanol-1"]]]], "golden_sentence": [["Kerouac was taken to St. Anthony's Hospital, suffering from an esophageal hemorrhage."], [""]]}, {"qid": "40da97762d4dde2cc3c1", "term": "Doctor Strange", "description": "Superhero appearing in Marvel Comics publications and related media", "question": "Did Doctor Strange creators also make Batman?", "answer": false, "facts": ["Doctor Strange is a superhero created by Steve Ditko and Stan Lee.", "Batman is a DC comics superhero.", "Stan Lee worked for Marvel comics, the competitor of DC comics.", "Steve Ditko worked for DC late in his career and worked on Blue Beetle, the Question, the Creeper, Shade the Changing Man, and Hawk and Dove."], "decomposition": ["Who were the creators of the fictional character 'Doctor Strange'?", "Who were the creators of the fictional character 'Batman'?", "Are #1 the same as #2?"], "evidence": [[[["Doctor Strange-1"]], [["Batman-1"]], ["operation"]], [[["Doctor Strange-1"]], [["Batman-1"]], ["operation"]], [[["Doctor Strange-1"]], [["Batman-1"]], ["operation"]]], "golden_sentence": [["Created by artist Steve Ditko and writer Stan Lee, the character first appeared in Strange Tales #110 (cover-dated July 1963)."], ["The character was created by artist Bob Kane and writer Bill Finger, and first appeared in Detective Comics #27 in 1939."]]}, {"qid": "25c30916ff6b742ee9fe", "term": "Charlemagne", "description": "King of the Franks, King of Italy, and Holy Roman Emperor", "question": "Was Charlemagne's father instrumental in outcome of the Battle of Tours?", "answer": false, "facts": ["Charlemagne's father was Pepin the Short.", "Pepin the Short's father was Charles Martel.", "Charles Martel led an army against the Umayyads at the Battle of Tours.", "Pepin the Short spent his early years being raised by monks."], "decomposition": ["Who was Charlemagne's father?", "Was #1 involved in the Battle of Tours?"], "evidence": [[[["Charlemagne-15"]], [["Battle of Tours-1", "Pepin the Short-1"], "no_evidence", "operation"]], [[["Pepin the Short-5"]], [["Battle of Tours-1"]]], [[["Pepin the Short-5"]], [["Battle of Tours-60"]]]], "golden_sentence": [["Charlemagne was the eldest child of Pepin the Short (714\u00a0\u2013\u00a024 September\u00a0768, reigned from 751) and his wife Bertrada of Laon (720\u00a0\u2013\u00a012 July\u00a0783), daughter of Caribert of Laon."], ["The Battle of Tours (10 October 732), also called the Battle of Poitiers and, by Arab sources, the Battle of the Highway of the Martyrs (Arabic: \u0645\u0639\u0631\u0643\u0629 \u0628\u0644\u0627\u0637 \u0627\u0644\u0634\u0647\u062f\u0627\u0621\u200e, romanized:\u00a0Ma'arakat Bal\u0101\u1e6d ash-Shuhad\u0101'), was an important victory of the Frankish and Burgundian forces under Charles Martel over the raiding parties of the Umayyad Caliphate led by Abdul Rahman Al Ghafiqi, Governor-General of al-Andalus.", ""]]}, {"qid": "0730ab9f6cb042ccae7b", "term": "Ham", "description": "Pork from a leg cut that has been preserved by wet or dry curing, with or without smoking", "question": "Would Janet Jackson avoid a dish with ham?", "answer": true, "facts": ["Janet Jackson follows an Islamic practice. ", "Islamic culture avoids eating pork.", "Ham is made from pork."], "decomposition": ["What is Janet Jackson's religion?", "Which foods are prohibited by #1?", "What type of food is ham?", "Is #3 included in #2?"], "evidence": [[["no_evidence"], ["no_evidence"], [["Ham-1"]], ["no_evidence", "operation"]], [["no_evidence"], ["no_evidence"], [["Ham-1"]], ["no_evidence", "operation"]], [[["Janet Jackson-5"]], ["no_evidence"], [["Ham-3"]], ["no_evidence"]]], "golden_sentence": [["Ham is pork from a leg cut that has been preserved by wet or dry curing, with or without smoking."]]}, {"qid": "78603259c42f582c6983", "term": "Giant panda", "description": "species of mammal", "question": "Can giant pandas sell out a Metallica show?", "answer": false, "facts": ["Metallica concerts are held in large arenas attended by tens of thousands of fans", "The highest estimate for the giant panda population is around 3,000 animals"], "decomposition": ["How many people can the large arenas where Metallica plays hold?", "How many giant pandas are there?", "Is #2 greater than or equal to #1?"], "evidence": [[["no_evidence"], [["Giant panda-49"]], ["no_evidence", "operation"]], [[["Metallica (album)-14"], "no_evidence"], [["Giant panda-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Metallica-13"]], [["Giant panda-49"]], ["operation"]]], "golden_sentence": [["Previous population surveys had used conventional methods to estimate the size of the wild panda population, but using a new method that analyzes DNA from panda droppings, scientists believe the wild population may be as large as 3,000."]]}, {"qid": "37d5db0be9371d99bb51", "term": "Armadillo", "description": "family of mammals", "question": "Would multiple average rulers be necessary to measure the length of a giant armadillo?", "answer": true, "facts": ["The average ruler is 12 inches or 30 centimeters in length.", "The typical length of the giant armadillo is 75\u2013100 cm (30\u201339 in), with the tail adding another 50 cm (20 in)."], "decomposition": ["What length are the best selling rulers on Amazon?", "How long is a typical giant armadillo?", "What is #2 divided by #1?", "Is #3 greater than one?"], "evidence": [[[["Ruler-2"], "no_evidence"], [["Giant armadillo-6"]], ["no_evidence", "operation"], ["no_evidence", "operation"]], [["no_evidence"], [["Giant armadillo-5"]], ["operation"], ["operation"]], [[["Ruler-2"], "no_evidence"], [["Giant armadillo-5"], "no_evidence"], ["operation"], ["operation"]]], "golden_sentence": [["12\u00a0in or 30\u00a0cm in length is useful for a ruler to be kept on a desk to help in drawing."], ["The typical length of the species is 75\u2013100\u00a0cm (30\u201339\u00a0in), with the tail adding another 50\u00a0cm (20\u00a0in)."]]}, {"qid": "f849cb8ab18747284e5a", "term": "Hippopotamus", "description": "A large, mostly herbivorous, semiaquatic mammal native to sub-Saharan Africa", "question": "Can you only see hippopotamus in Africa?", "answer": false, "facts": ["The United States has several zoos featuring hippopotamus.", "In the UK, you can see hippopotamus at the Marwell Zoo."], "decomposition": ["Where are animals kept for recreation/sightseeing?", "Can #1 that has hippopotamus be found only inside Africa?"], "evidence": [[[["Zoo-1"]], [["Hippopotamus-44", "Toledo, Ohio-1"]]], [[["Hippopotamus-5"]], [["Hippopotamus-13"]]], [[["Zoo-1"]], [["Hippopotamus-43"], "operation"]]], "golden_sentence": [["A zoo (short for zoological garden; also called an animal park or menagerie) is a facility in which animals are housed within enclosures, cared for, displayed to the public, and in some cases bred."], ["", ""]]}, {"qid": "5a574862071510d1d620", "term": "Kane (wrestler)", "description": "American professional wrestler, actor, businessman, and politician", "question": "Was Kane (wrestler) banned from WCW  headquarters city?", "answer": false, "facts": ["Kane (wrestler is a professional wrestler most known for his WWE tenure.", "Kane wrestled one match in WCW as Bruiser Mastino.", "WWE main rival WCW was headquartered in Atlanta, Georgia.", "Kane competed in an eight-man tag match at Wrestlemania XXVII in the Georgia Dome.", "The Georgia Dome was a stadium in Atlanta Georgia."], "decomposition": ["Where were the headquarters of the WCW?", "Did Kane never perform in #1?"], "evidence": [[[["World Championship Wrestling-4"]], [["Royal Rumble (2002)-1", "Royal Rumble (2002)-15"], "operation"]], [[["World Championship Wrestling-4"]], [["Kane (wrestler)-1"], "no_evidence", "operation"]], [[["World Championship Wrestling-4"]], [["Royal Rumble (2002)-1", "Royal Rumble (2002)-15"]]]], "golden_sentence": [["Its headquarters were located in Atlanta, Georgia."], ["", ""]]}, {"qid": "e8e26233651e73d7a47d", "term": "Rumi", "description": "13th-century Persian poet", "question": "Was Rumi's work serialized in a magazine?", "answer": false, "facts": ["Rumi was a poet who wrote poetry", "Magazines serialize long-form prose like novels"], "decomposition": ["When was the first magazine ever published?", "When was the poet Rumi active?", "Was #1 before #2?"], "evidence": [[[["Magazine-8"]], [["Rumi-1"]], ["operation"]], [[["The Gentleman's Magazine-1"]], [["Rumi-1"]], ["operation"]], [[["Magazine-8"]], [["Rumi-1"]], ["operation"]]], "golden_sentence": [["The earliest example of magazines was Erbauliche Monaths Unterredungen, a literary and philosophy magazine, which was launched in 1663 in Germany."], ["Jal\u0101l ad-D\u012bn Muhammad R\u016bm\u012b (Persian: \u062c\u0644\u0627\u0644\u200c\u0627\u0644\u062f\u06cc\u0646 \u0645\u062d\u0645\u062f \u0631\u0648\u0645\u06cc\u200e), also known as Jal\u0101l ad-D\u012bn Muhammad Balkh\u012b (\u062c\u0644\u0627\u0644\u200c\u0627\u0644\u062f\u06cc\u0646 \u0645\u062d\u0645\u062f \u0628\u0644\u062e\u0649), Mevl\u00e2n\u00e2/Mawl\u0101n\u0101 (\u0645\u0648\u0644\u0627\u0646\u0627, \"our master\"), Mevlev\u00ee/Mawlaw\u012b (\u0645\u0648\u0644\u0648\u06cc, \"my master\"), and more popularly simply as Rumi (30 September 1207\u00a0\u2013 17 December 1273), was a 13th-century Persian poet, faqih, Islamic scholar, theologian, and Sufi mystic originally from Greater Khorasan."]]}, {"qid": "e5605c72354b4d4b97d3", "term": "Olympia, Washington", "description": "State capital and city in Washington, United States", "question": "Is Olympia, Washington part of \"Ish river country\"?", "answer": true, "facts": ["Poet Robert Sund called the Puget Sound region \"Ish River country\".", "Olympia is in the Puget Sound region."], "decomposition": ["Where is Ish river country? ", "What cities are located in #1?", "Is Olympia included in the list in #2?"], "evidence": [[[["Puget Sound region-1", "Puget Sound region-2"]], [["Washington (state)-79"]], ["operation"]], [[["Puget Sound region-1", "Puget Sound region-2"]], [["Puget Sound region-1"], "no_evidence"], [["Washington (state)-1"], "operation"]], [[["Puget Sound region-2"]], [["Puget Sound-4"]], ["operation"]]], "golden_sentence": [["", "Poet Robert Sund called the Puget Sound region \"Ish River country\", owing to its numerous rivers with names ending in \"ish\", such as the Duwamish, Samish, Sammamish, Skokomish, Skykomish, Snohomish, and the Stillaguamish."], ["There are extensive waterways in the midst of Washington's largest cities, including Seattle, Bellevue, Tacoma and Olympia."]]}, {"qid": "48f71d68c790a0aa9ebf", "term": "Strawberry", "description": "edible fruit", "question": "Would someone with back pain enjoy picking strawberries?", "answer": false, "facts": ["Back pain may be worsened by repeated bending at the waist.", "Strawberries grow very close to the ground."], "decomposition": ["What are some common body postures that can aggravate back pain?", "At which position relative to the ground do strawberries grow?", "What posture would one have to assume to reach #2?", "Is #3 excluded from #1?"], "evidence": [[[["Back pain-43"], "no_evidence"], [["Strawberry-31"], "no_evidence"], ["operation"], ["operation"]], [[["Back pain-20"]], [["Strawberry-31"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Back pain-43"]], [["Strawberry-31"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Typical factors aggravating the back pain of pregnancy include standing, sitting, forward bending, lifting, and walking."], [""]]}, {"qid": "24953f90bfb0bfb87f16", "term": "Prussia", "description": "state in Central Europe between 1525\u20131947", "question": "Was the Euro used in Prussia?", "answer": false, "facts": ["Prussia was formally abolished in 1947.", "The Euro was introduced in 1992."], "decomposition": ["When was Prussia formally abolished?", "When was the Euro introduced?", "Is #2 before #1?"], "evidence": [[[["Prussia-1"]], [["Euro-18"]], ["operation"]], [[["Monarchies in Europe-27"]], [["Euro-23"]], ["operation"]], [[["Prussia-2"]], [["Euro-5"]], ["operation"]]], "golden_sentence": [["It was de facto dissolved by an emergency decree transferring powers of the Prussian government to German Chancellor Franz von Papen in 1932 and de jure by an Allied decree in 1947."], [""]]}, {"qid": "946671156e217235c625", "term": "Eiffel Tower", "description": "Tower located on the Champ de Mars in Paris, France", "question": "Did Eiffel Tower contribute to a war victory?", "answer": true, "facts": ["A radio transmitter located in the Eiffel Tower.", "This transmitter jammed German radio communications.", "This hindrance in German radio communications contributing to the Allied victory at the First Battle of the Marne."], "decomposition": ["What notable events in which Eiffel Tower was of primary importance took place during a war?", "Did any of #1 give a side an advantage during the said war?"], "evidence": [[[["Eiffel Tower-33"]], [["Eiffel Tower-33"]]], [["no_evidence"], ["no_evidence", "operation"]], [[["Eiffel Tower-33"]], [["Eiffel Tower-33"]]]], "golden_sentence": [["In 1914, at the outbreak of World War\u00a0I, a radio transmitter located in the tower jammed German radio communications, seriously hindering their advance on Paris and contributing to the Allied victory at the First Battle of the Marne."], [""]]}, {"qid": "f97288d0e81a18e5c4ee", "term": "Indian Ocean", "description": "The ocean between Africa, Asia, Australia and Antarctica (or the Southern Ocean)", "question": "Has the Indian Ocean garbage patch not completed two full rotations of debris since its discovery?", "answer": true, "facts": ["The Indian Ocean garbage patch was discovered in 2010", "The Indian Ocean garbage patch takes 6 years to complete a circulation "], "decomposition": ["When was the Indian Ocean garbage patch discovered?", "How long does it take for the Indian Ocean garbage patch to complete a rotation?", "How many years has it been since #1?", "Is #3 less than two times #2?"], "evidence": [[[["Indian Ocean garbage patch-1"]], ["no_evidence"], ["operation"], ["no_evidence", "operation"]], [[["Indian Ocean garbage patch-2"]], [["Indian Ocean Gyre-3"]], ["operation"], ["operation"]], [[["Indian Ocean garbage patch-2"]], [["Indian Ocean Gyre-3"]], ["operation"], ["operation"]]], "golden_sentence": [["The Indian Ocean garbage patch, discovered in 2010, is a gyre of marine litter suspended in the upper water column of the central Indian Ocean, specifically the Indian Ocean Gyre, one of the five major oceanic gyres."]]}, {"qid": "5e51ee192fdd0612af1d", "term": "Armageddon", "description": "according to the Book of Revelation, the site of a battle during the end times", "question": "Do some religions look forward to armageddon?", "answer": true, "facts": ["Evangelicals cite that we are living in the beginning of Armageddon and that the rapture will happen soon as a good thing.", "Jehova's Witnesses believe that destroying the present world system and Armageddon is imminent, and that the establishment of God's kingdom over the earth is the only solution for all problems faced by humanity"], "decomposition": ["Where does the concept of Armageddon has its roots?", "#1 is associated with which religion?", "Do adherents of #2 believe in and await the Armageddon?"], "evidence": [[[["Armageddon-5"]], [["Armageddon-4"]], [["Armageddon-4"], "operation"]], [[["Armageddon-1"]], [["New Testament-1"]], [["Armageddon-18", "Jehovah's Witnesses-30"], "operation"]], [[["Armageddon-1"]], [["Book of Revelation-1"]], [["Rapture-40"], "operation"]]], "golden_sentence": [["Armageddon is the symbolic name given to this event based on scripture references regarding divine obliteration of God's enemies."], ["Because of the seemingly highly symbolic and even cryptic language of this one New Testament passage, some Christian scholars conclude that Mount Armageddon must be an idealized location."], ["Because of the seemingly highly symbolic and even cryptic language of this one New Testament passage, some Christian scholars conclude that Mount Armageddon must be an idealized location."]]}, {"qid": "f567d0b3b47e520ca8c5", "term": "War in Vietnam (1945\u201346)", "description": "Prelude to the Indochina Wars", "question": "Could a llama birth twice during War in Vietnam (1945-46)?", "answer": false, "facts": ["The War in Vietnam (1945-46) lasted around 6 months.", "The gestation period for a llama is 11 months."], "decomposition": ["How long did the Vietnam war last?", "How long is llama gestational period?", "What is 2 times #2?", "Is #1 longer than #3?"], "evidence": [[[["Vietnam War-1"]], [["Llama-23"]], ["operation"], ["operation"]], [[["War in Vietnam (1945\u20131946)-3"]], [["Llama-23"]], ["operation"], ["operation"]], [[["Vietnam War-1"]], [["Llama-23"]], ["operation"], ["operation"]]], "golden_sentence": [["The war, considered a Cold War-era proxy war by some, lasted 19 years, with direct U.S. involvement ending in 1973, and included the Laotian Civil War and the Cambodian Civil War, which ended with all three countries becoming communist in 1975."], ["The gestation period of a llama is 11.5 months (350 days)."]]}, {"qid": "7579ebd5b8cfde121d3f", "term": "Rick and Morty", "description": "Animated sitcom", "question": "Is Rick and Morty considered an anime?", "answer": false, "facts": ["Anime is a genre of animation that is hand drawn and is of Japanese origin.", "Rick and Morty is an American animated show."], "decomposition": ["What country does anime come from?", "Rick and Morty is an animated show from which country?", "Do #1 and #2 have the same answer?"], "evidence": [[[["Anime-1"]], [["Rick and Morty-1"]], ["operation"]], [[["Anime-1"]], [["Rick and Morty-1"]], ["operation"]], [[["Anime-10"]], [["Rick and Morty-18"]], ["operation"]]], "golden_sentence": [["Anime (US: /\u02c8\u00e6n\u0259\u02ccme\u026a/, UK: /\u02c8\u00e6n\u026a\u02ccme\u026a/) (Japanese: \u30a2\u30cb\u30e1, Hepburn: anime, [a\u0272ime] (listen), plural: anime) is hand-drawn and computer animation originating from Japan."], [""]]}, {"qid": "e1b942383b45f38a002c", "term": "The Hague", "description": "City and municipality in South Holland, Netherlands", "question": "Does The Hague border multiple bodies of water?", "answer": false, "facts": ["The Hague is in the Netherlands. ", "The Hague is in the Western part of the Netherlands. ", "The Netherlands borders the North Sea to its west. "], "decomposition": ["What country is the Hague located in?", "What bodies of water does #1 border on?", "What part of #1 is the Hague located in?", "How many bodies of water in #2 does #3 border?", "Is #4 greater than 1?"], "evidence": [[[["The Hague-1"]], [["Netherlands-1"]], [["The Hague-1"]], [["The Hague-20"], "operation"], ["operation"]], [[["The Hague-19"]], [["Netherlands-1"]], [["The Hague-20"]], ["operation"], ["operation"]], [[["The Hague-1"]], [["Netherlands-1"]], [["The Hague-1"]], ["operation"], ["operation"]]], "golden_sentence": [["The Hague (/he\u026a\u0261/; Dutch: Den Haag [d\u025bn \u02c8\u0266a\u02d0x] (listen) or 's\u2011Gravenhage [\u02ccsxra\u02d0v\u0259(n)\u02c8\u0266a\u02d0\u0263\u0259] (listen)) is a city on the western coast of the Netherlands on the North Sea and the capital of the province of South Holland."], ["In Europe, it consists of 12 provinces that border Germany to the east, Belgium to the south, and the North Sea to the northwest, with maritime borders in the North Sea with those countries and the United Kingdom."], ["The Hague (/he\u026a\u0261/; Dutch: Den Haag [d\u025bn \u02c8\u0266a\u02d0x] (listen) or 's\u2011Gravenhage [\u02ccsxra\u02d0v\u0259(n)\u02c8\u0266a\u02d0\u0263\u0259] (listen)) is a city on the western coast of the Netherlands on the North Sea and the capital of the province of South Holland."], [""]]}, {"qid": "4f802258689c224665cb", "term": "Chipmunk", "description": "Tribe of mammals (rodent (marmot))", "question": "Could a chipmunk fit 100 chocolate chips in his mouth?", "answer": false, "facts": ["A chipmunk can fit up to two tbsp of food in his mouth.", "There are about 20-25 chocolate chips in a tbsp."], "decomposition": ["What is the carrying capacity of a chipmunks mouth in tbsp.?", "How many chocolate chips are in a tbsp?", "What is 100 divided by #2?", "Is #1 greater than #3?"], "evidence": [[[["Cheek pouch-1"], "no_evidence"], ["no_evidence"], ["operation"], ["operation"]], [[["Cheek pouch-6"], "no_evidence"], [["Chocolate chip-1"], "no_evidence"], ["operation"], ["no_evidence", "operation"]], [[["Cheek pouch-6"], "no_evidence"], [["Chocolate chip-1"], "no_evidence"], ["no_evidence"], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "440b7643d9c4edbdc824", "term": "Anchovy", "description": "Family of fishes", "question": "Would a pescatarian be unable to eat anchovy pizza?", "answer": false, "facts": ["Pescatarians do not eat red meat or chicken but do eat fish.", "Pescatarians have no restrictions with eating cheese."], "decomposition": ["What do pediatricians eat for source of meat?", "Is anchovy not included in #1?"], "evidence": [[[["Pescetarianism-1"]], [["Anchovy-1"]]], [[["Pescetarianism-1"]], [["Anchovy-1", "Seafood-1"]]], [[["Pescetarianism-1"]], [["Anchovy-3"]]]], "golden_sentence": [["Pescetarianism or pescatarianism (/\u02ccp\u025bsk\u0259\u02c8t\u025b\u0259ri\u0259n\u026az\u0259m/) is the practice of adhering to a diet that incorporates seafood as the only source of meat in an otherwise vegetarian diet."], [""]]}, {"qid": "89e58d7843d91dce4486", "term": "Sophist", "description": "Specific kind of teacher in both Ancient Greece and in the Roman Empire", "question": "Would Sophist's have hypothetically made good lawyers?", "answer": true, "facts": ["Sophist's were teachers in ancient Greece that used rhetoric.", "Lawyers must persuade juries that their side of the case is correct.", "Rhetoric is the ancient art of persuasion that was meant to sway audiences in specific situations."], "decomposition": ["What were Sophist's role in Ancient Greece?", "What did #1 use in their position?", "What do lawyers do in their position?", "Would #3 find #2 to be helpful?"], "evidence": [[[["Sophist-1"]], [["Second Sophistic-3"], "no_evidence"], [["Lawyer-7"]], ["operation"]], [[["Sophist-1"]], ["no_evidence"], [["Lawyer-1"]], ["operation"]], [[["Sophist-1", "Sophist-9"]], [["Hellenistic philosophy-3"]], [["Lawyer-1"]], [["Practice of law-1"], "no_evidence"]]], "golden_sentence": [["A sophist (Greek: \u03c3\u03bf\u03c6\u03b9\u03c3\u03c4\u03ae\u03c2, sophistes) was a specific kind of teacher in ancient Greece, in the fifth and fourth centuries BC."], ["They did not teach debate or anything that had to do with politics because rhetoric was restrained due to the imperial government\u2019s rules."], ["Often, lawyers brief a court in writing on the issues in a case before the issues can be orally argued."]]}, {"qid": "f02296f9ff722a042539", "term": "Tonsillitis", "description": "Inflammation of the tonsils", "question": "Can fish get Tonsillitis?", "answer": false, "facts": ["Tonsils are a pair of soft tissue masses located at the rear of the throat", "Tonsillitis is the inflammation of tonsils.", "Fish do not have tonsils.", "Tonsils are only found in mammals. "], "decomposition": ["What does Tonsillitis affect?", "What kinds of animals are #1 found in?", "Are fish #2?"], "evidence": [[[["Tonsillitis-1"]], [["Tonsil-2"]], ["operation"]], [[["Tonsillitis-1"]], [["Tonsil-3"]], ["operation"]], [[["Tonsillitis-1"]], [["Tonsil-3"]], [["Fish-1"], "operation"]]], "golden_sentence": [["Symptoms may include sore throat, fever, enlargement of the tonsils, trouble swallowing, and large lymph nodes around the neck."], [""]]}, {"qid": "83ccd203af0d1dbfb417", "term": "Panth\u00e9on", "description": "mausoleum in Paris", "question": "Will Queen Elizabeth be buried in the Pantheon?", "answer": false, "facts": ["Queen Elizabeth is the reigning monarch of the United Kingdom", "The Pantheon is a resting place for notable French citizens"], "decomposition": ["The Panth\u00e9on is reserved as a mausoleum for citizens of which country?", "Is Queen Elizabeth from #1?"], "evidence": [[[["Panth\u00e9on-2"]], [["Elizabeth II-2"]]], [[["Panth\u00e9on-1"]], [["Elizabeth II-1"], "operation"]], [[["Panth\u00e9on-2"]], [["Elizabeth II-2"], "operation"]]], "golden_sentence": [["By the time the construction was finished, the French Revolution had started, and the National Constituent Assembly voted in 1791 to transform the Church of Saint Genevieve into a mausoleum for the remains of distinguished French citizens, modelled on the Pantheon in Rome which had been used in this way since the 16th century."], [""]]}, {"qid": "8a8bfbc853aeb59252b8", "term": "Boolean algebra", "description": "Algebra involving variables containing only \"true\" and \"false\" (or 1 and 0) as values", "question": "Does coding rely on Boolean algebra characters?", "answer": true, "facts": ["Boolean algebra uses the characters of 1 and 0 to represent true and false.", "Binary code is an essential part of computer coding.", "Binary code consists of the characters 0 and 1 which represents strings of value."], "decomposition": ["What characters does Boolean algebra use?", "What characters does binary code use?", "Are #1 and #2 the same?"], "evidence": [[[["Boolean algebra-1"]], [["Binary code-1"]], ["operation"]], [[["Boolean algebra-1"]], [["Binary code-1"]], ["operation"]], [[["Boolean algebra-1"]], [["Binary code-1"]], ["operation"]]], "golden_sentence": [["In mathematics and mathematical logic, Boolean algebra is the branch of algebra in which the values of the variables are the truth values true and false, usually denoted 1 and 0 respectively."], ["The binary code assigns a pattern of binary digits, also known as bits, to each character, instruction, etc."]]}, {"qid": "c003d69cc25b0f93aea8", "term": "Gandalf", "description": "Fictional character created by J. R. R. Tolkien", "question": "Was Gandalf present at the death of Eomer?", "answer": false, "facts": ["Eomer died in a skirmish with orcs outside Rohan at the beginning of Two Towers.", "Gandalf had been killed by the Balrog at the end of Fellowship of the Ring.", "Gandalf returns with improved powers later on in Two Towers."], "decomposition": ["In which LOTR installment was Gandalf first killed?", "At what point in the LOTR franchise did Eomer die?", "When did Gandalf first reappear after #1?", "Did #2 take place outside of the period between #1 and #3?"], "evidence": [[[["The Lord of the Rings: The Fellowship of the Ring-8"]], [["The Lord of the Rings: The Return of the King-10"], "no_evidence"], [["The Lord of the Rings: The Two Towers-6"]], ["no_evidence", "operation"]], [[["The Lord of the Rings: The Fellowship of the Ring-8"]], [["The Lord of the Rings: The Return of the King-10"]], [["The Lord of the Rings: The Two Towers-2"]], ["operation"]], [[["Gandalf-27"], "no_evidence"], [["\u00c9omer-6"], "no_evidence"], [["Gandalf-28"]], [["Gandalf-31"], "no_evidence", "operation"]]], "golden_sentence": [[""], [""], [""]]}, {"qid": "24277da012562632975c", "term": "Reproduction", "description": "Biological process by which new organisms are generated from one or more parent organisms", "question": "Are those incapable of reproduction incapable of parenthood?", "answer": false, "facts": ["Surrogates are women who will carry a baby to term for a family seeking to adopt.", "Many children are put into the adoption and foster system every year and are always available to adopt, independent of the parents reproductive status."], "decomposition": ["What do surrogate mothers do?", "What purpose do adoption and foster systems serve?", "Do #1 and #2 fail to help couples incapable of reproduction become parents?"], "evidence": [[[["Mother-11"]], [["Adoption-8"]], ["operation"]], [[["Surrogacy-1", "Surrogacy-2"]], [["Adoption-1", "Foster care-1"]], ["operation"]], [[["Surrogacy-1"]], [["Adoption-1"]], ["no_evidence"]]], "golden_sentence": [["A surrogate mother is a woman who bears a child that came from another woman's fertilized ovum on behalf of a couple unable to give birth to children."], [""]]}, {"qid": "4b82bd6ced332e84006e", "term": "Mixed martial arts", "description": "full contact combat sport", "question": "Is Mixed martial arts totally original from Roman Colosseum games?", "answer": false, "facts": ["Mixed Martial arts in the UFC takes place in an enclosed structure called The Octagon.", "The Roman Colosseum games were fought in enclosed arenas where combatants would fight until the last man was standing.", "Mixed martial arts contests are stopped when one of the combatants is incapacitated.", "The Roman Colosseum was performed in front of crowds that numbered in the tens of thousands.", "Over 56,000 people attended UFC 193."], "decomposition": ["What are the major features of UFC's Mixed martial arts?", "What were the major features of Roman Colosseum games?", "Is #1 a complete match with #2?"], "evidence": [[[["Mixed martial arts-1", "Ultimate Fighting Championship-95", "Ultimate Fighting Championship-97", "Ultimate Fighting Championship-99"]], [["Gladiator-1", "Gladiator-37", "Gladiator-40"]], ["operation"]], [[["Ultimate Fighting Championship-1"]], [["Gladiator-1"]], ["operation"]], [[["Mixed martial arts-83"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Mixed martial arts (MMA), sometimes referred to as cage fighting, is a full-contact combat sport based on striking, grappling and ground fighting, made up from various combat sports and martial arts from around the world, the first documented use of the term mixed martial arts was in a review of UFC 1 by television critic Howard Rosenberg in 1993.", "", "", ""], ["", "", ""]]}, {"qid": "cef5fdf4ccbe2e70b2e5", "term": "Herpes simplex virus", "description": "Species of virus", "question": "Can Herpes simplex virus spread on Venus?", "answer": false, "facts": ["Herpes simplex virus is a disease that has the structure of a tiny protein cage.", "Venus is the hottest planet and its temperature can reach 900\u00b0F.", "Proteins lose their structure and break down at temperatures above 105.8\u00b0F."], "decomposition": ["What kind of organism is the Herpes simplex virus?", "What is the maximum temperature that #1 can withstand?", "What is the average temperature on Venus?", "Is #3 less than or equal to #2?"], "evidence": [[[["Herpes simplex virus-1"]], [["Virus-18"], "no_evidence"], [["Venus-2"]], ["operation"]], [[["Herpes simplex virus-1"]], [["Sterilization (microbiology)-14"], "no_evidence"], [["Venus-2"]], ["no_evidence", "operation"]], [[["Herpes simplex virus-1"]], ["no_evidence"], [["Venus-2"]], ["operation"]]], "golden_sentence": [[""], [""], ["Venus is by far the hottest planet in the Solar System, with a mean surface temperature of 735\u00a0K (462\u00a0\u00b0C; 863\u00a0\u00b0F), even though Mercury is closer to the Sun."]]}, {"qid": "8ee80f2e2ad5f472daab", "term": "Bern", "description": "Place in Switzerland", "question": "Is Bern located east of Paris?", "answer": true, "facts": ["Paris is located in France.", "Bern is located in Switzerland.", "Switzerland borders France to the east."], "decomposition": ["What country is Paris located in?", "What country is Bern located in?", "Is #2 located east of #1?"], "evidence": [[[["Administration of Paris-4"]], [["Districts of Switzerland-10"]], [["France-1"]]], [[["Paris-1"]], [["Bern-1"]], ["operation"]], [[["Paris-1"]], [["Bern-1"]], [["Switzerland-1"]]]], "golden_sentence": [["At the 1790 division (during the French Revolution) of France into communes, and again in 1834, Paris was a city only half its modern size, composed of 12 arrondissements, but, in 1860, it annexed bordering communes, some entirely, to create the new administrative map of twenty municipal arrondissements the city still has today."], [""], [""]]}, {"qid": "42a8c711d08cae30fba4", "term": "Eighth Amendment to the United States Constitution", "description": "prohibits cruel and unusual punishment and excessive bail", "question": "Would keelhauling be a fair punishment under the Eighth Amendment?", "answer": false, "facts": ["Keelhauling was a severe punishment whereby the condemned man was dragged beneath the ship\u2019s keel on a rope.", "Keelhauling is considered a form of torture.", "Torture is considered cruel.", "The Eighth Amendment forbids the use of \"cruel and unusual punishment\"."], "decomposition": ["What kind of punishment is keelhauling considered a form of?", "Does the Eighth Amendment allow #1?"], "evidence": [[[["Keelhauling-1"]], [["Eighth Amendment to the United States Constitution-1"], "operation"]], [[["Keelhauling-2"]], [["United States constitutional sentencing law-4"]]], [[["Keelhauling-6"]], [["Eighth Amendment to the United States Constitution-29"]]]], "golden_sentence": [["Keelhauling (Dutch kielhalen; \"to drag along the keel\") is a form of punishment and potential execution once meted out to sailors at sea."], ["The Eighth Amendment (Amendment VIII) of the United States Constitution prohibits the federal government from imposing excessive bail, excessive fines, or cruel and unusual punishments."]]}, {"qid": "51e3a6b5edf8e3427633", "term": "Armadillo", "description": "family of mammals", "question": "Would Franz Ferdinand have survived with armadillo armor?", "answer": false, "facts": ["Franz Ferdinand was the Archduke of Austria that was assassinated in 1914.", "Franz Ferdinand was assasinated with a FN Model 1910 pistol.", "Armadillos have a hard outer shell made of bony plates called osteoderms.", "The armadillos bony plates can withstand some force, but not a bullet."], "decomposition": ["How was Franz Ferdinand killed?", "Can armadillo armor withstand #1?"], "evidence": [[[["Archduke Franz Ferdinand of Austria-25"]], [["Armadillo-2"], "no_evidence", "operation"]], [[["Archduke Franz Ferdinand of Austria-23"]], [["Armadillo-2"]]], [[["Archduke Franz Ferdinand of Austria-25"]], [["Armadillo-2"], "no_evidence"]]], "golden_sentence": [["One bullet pierced Franz Ferdinand's neck while the other pierced Sophie's abdomen."], [""]]}, {"qid": "f6b44795d89994ce848c", "term": "Railroad engineer", "description": "person who operates a train on a railroad or railway", "question": "Can you find a railroad engineer on TNT?", "answer": true, "facts": ["TNT is a cable television network", "Snowpiercer is a show airing on the TNT network", "Snowpiercer involves people living on an active train in a postapocalyptic future", "A railroad engineer is necessary to keep a train running"], "decomposition": ["What movies are about trains or railroads?", "Are any of #1 currently airing on TNT?"], "evidence": [[[["Murder on the Orient Express (2017 film)-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Snowpiercer-1", "Snowpiercer-46"], "no_evidence"], [["Snowpiercer-46"], "operation"]], [[["Planes, Trains and Automobiles-1"]], ["no_evidence"]]], "golden_sentence": [[""]]}, {"qid": "57dc20e0d6e79616752e", "term": "Amazon (company)", "description": "American electronic commerce and cloud computing company", "question": "Could Amazon afford The Mona Lisa?", "answer": true, "facts": ["Amazon is worth over $1 trillion and had a revenue of $232.887 billion in 2018.", "The Mona Lisa had an insurance valuation equivalent to $650 million as of 2018."], "decomposition": ["How much is Amazon worth?", "How much is the Mona Lisa worth?", "Is #1 more than #2?"], "evidence": [[["no_evidence"], [["Mona Lisa-55"]], ["no_evidence", "operation"]], [[["Amazon (company)-64"]], [["Mona Lisa-55"]], ["operation"]], [[["Amazon (company)-65"]], [["Mona Lisa-55"]], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "82e05751e368cb1f3bac", "term": "Martin Luther", "description": "Saxon priest, monk and theologian, seminal figure in Protestant Reformation", "question": "Was Martin Luther same sect as Martin Luther King Jr.?", "answer": false, "facts": ["Martin Luther was a Catholic friar that began the movement of Protestantism after he aired several grievances against the church.", "Martin Luther King Jr. was a Baptist minister.", "Baptists form a major branch of Protestantism.", "Baptists trace their Protestantism to the English Separatist movement of the 1600s.", "Martin Luther lived from 1483-1546."], "decomposition": ["Which religious denomination did Martin Luther belong to for the significant part of his life?", "Which religious denomination did Martin Luther King Jr. identify with for the significant part of his life?", "Is #1 the same as #2?"], "evidence": [[[["Martin Luther-88"]], [["Martin Luther King Jr.-22"]], [["Martin Luther-88"], "operation"]], [[["Martin Luther-1", "Order of Saint Augustine-1"]], [["Calvary Baptist Church (Chester, Pennsylvania)-1", "Martin Luther King Jr.-18"]], ["operation"]], [[["Martin Luther-1"]], [["Martin Luther King Jr.-108"]], ["operation"]]], "golden_sentence": [[""], [""], [""]]}, {"qid": "214e0c9994a099e8cffb", "term": "Jean-Paul Sartre", "description": "French existentialist philosopher, playwright, novelist, screenwriter, political activist, biographer, and literary critic", "question": "Did Sartre write a play about Hell?", "answer": true, "facts": ["In 1944, Sartre released No Exit.", "No Exit is a play about three people mysteriously locked in a room together.", "Late in the play, it is revealed the room is a version of Hell."], "decomposition": ["What is Jean-Paul Sartre's most famous play?", "What is the plot of #1?", "Is Hell a critical element of #2?"], "evidence": [[[["No Exit-1"]], [["No Exit-3"], "no_evidence"], ["operation"]], [[["No Exit-1"]], [["No Exit-3"]], ["operation"]], [[["Jean-Paul Sartre-62"]], [["No Exit-3"]], [["No Exit-3"]]]], "golden_sentence": [["No Exit (French: Huis Clos, pronounced\u00a0[\u0265i klo]) is a 1944 existentialist French play by Jean-Paul Sartre."], [""]]}, {"qid": "7456b93bdb9bc86c830f", "term": "Rowing (sport)", "description": "Sport where individuals or teams row boats by oar", "question": "Can rowing competitions take place indoors?", "answer": false, "facts": ["Rowing is a sport involving propelling boats.", "Boats need a large body of water in order to move.", "There are no indoor facilities big enough to host a pool with enough size for a boating competition."], "decomposition": ["What is the main equipment required for rowing?", "What surface does #1 need in order to move?", "Is there an indoor facility with a big enough amount of #2 to host a competition?"], "evidence": [[[["Rowing-1"]], [["Rowing-1"]], [["Indoor rowing at the 2017 World Games-1"]]], [[["Rowing-1"]], [["Rowing-1"]], [["Olympic-size swimming pool-1"], "no_evidence", "operation"]], [[["Rowing (sport)-20"]], [["Rowing (sport)-20"]], ["no_evidence"]]], "golden_sentence": [["Rowing and paddling are similar but the difference is that rowing requires oars to have a mechanical connection with the boat, while paddles (used for paddling) are hand-held and have no mechanical connection."], [""], [""]]}, {"qid": "5e973b15c54da196c2a2", "term": "Middle Ages", "description": "Period of European history from the 5th to the 15th century", "question": "Were there fifty English kings throughout the Middle Ages?", "answer": false, "facts": ["The Middle Ages was a period of history from 476-1453 AD.", "From 476 to 1453 AD  there were around 36 Kings of England including disputed claimants to the throne."], "decomposition": ["Which span of time is referred to as the Middle Ages?", "How many kings ruled England through #1?", "Is #2 equal to fifty?"], "evidence": [[[["Middle Ages-1"]], ["no_evidence"], ["operation"]], [[["Outline of the Middle Ages-2"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Middle Ages-1"]], [["History of Anglo-Saxon England-35"], "no_evidence"], ["operation"]]], "golden_sentence": [["In the history of Europe, the Middle Ages or Medieval Period lasted from the 5th to the 15th century."]]}, {"qid": "cbd0d8c9a00478dadd25", "term": "Thanksgiving (United States)", "description": "holiday celebrated in the United States on the fourth Thursday in November", "question": "Is Thanksgiving sometimes considered a day of mourning?", "answer": true, "facts": ["The Native American People in the United States were brutalized during the colonization period.", "Native Americans in the US often choose to mourn the genocide of their people on Thanksgiving."], "decomposition": ["When do Native Americans often choose to mourn the genocide of their people?", "Is Thanksgiving included in #1?"], "evidence": [[[["National Day of Mourning (United States protest)-1"]], [["National Day of Mourning (United States protest)-1"], "operation"]], [[["National Day of Mourning (United States protest)-1", "National Day of Mourning (United States protest)-17"]], ["operation"]], [[["National Day of Mourning (United States protest)-17"], "no_evidence"], ["operation"]]], "golden_sentence": [["The National Day of Mourning is an annual protest organized since 1970 by Native Americans of New England on the fourth Thursday of November, the same day as Thanksgiving in the United States."], [""]]}, {"qid": "5be4530ef3ac405360ea", "term": "Sea shanty", "description": "work song sung to accompany labor on board large merchant sailing vessels", "question": "Does Jack Sparrow know any sea shantys?", "answer": true, "facts": ["Jack Sparrow is the main character of the popular 'Pirates of the Caribbean' movie franchise.", "Jack Sparrow is the captain of a pirate ship.", "Jack Sparrow sings many songs while on the sea."], "decomposition": ["Which movie is Jack Sparrow a main character in?", "Which activity is associated with singing of sea shantys?", "As portrayed in #1, is Jack Sparrow in a position to engage in #2?"], "evidence": [[[["Jack Sparrow-1"]], [["Sea shanty-1"]], ["operation"]], [[["Jack Sparrow-1"]], [["Sea shanty-119"]], [["Sea shanty-119"]]], [[["Jack Sparrow-1"]], [["Sea shanty-39", "Sea shanty-4"]], [["Jack Sparrow-1"]]]], "golden_sentence": [["Captain Jack Sparrow is a fictional character and the main protagonist of the Pirates of the Caribbean film series."], ["A sea shanty, chantey, or chanty is a type of work song that was once commonly sung to accompany labor on board large merchant sailing vessels."]]}, {"qid": "92768e1753e69ed088bd", "term": "Honey", "description": "Sweet food made by bees mostly using nectar from flowers", "question": "If someone is a vegan, would they eat honey?", "answer": false, "facts": ["Veganism is a type of diet that excludes all animal products, including those that are byproducts. ", "Honey is considered an animal byproduct. "], "decomposition": ["Do vegans eat animal byproducts?", "Is honey considered an animal byproduct?", "Are the answers to #1 and #2 the same?"], "evidence": [[[["Veganism-1"]], [["Honey-1"]], ["operation"]], [[["Veganism-1"]], [["Honey-1"]], ["operation"]], [[["Veganism-1"]], [["Honey-1"]], ["operation"]]], "golden_sentence": [["Veganism is the practice of abstaining from the use of animal products, particularly in diet, and an associated philosophy that rejects the commodity status of animals."], [""]]}, {"qid": "2daadf85106808e73d88", "term": "Lamborghini", "description": "Italian car manufacturer", "question": "Can Lamborghini's fastest model win a race against a Porsche 911?", "answer": true, "facts": ["Lamborghini's fastest model is the Lamborghini Aventador SVJ Roadster.", "The Lamborghini Aventador SVJ Roadster has a top speed of 217 MPH.", "The Porsche 911 has a top speed of 191 MPH."], "decomposition": ["Which model of Lamborghini is the fastest?", "What is the top speed of #1?", "What is the top speed of a Porsche 911?", "Is #2 greater than #3?"], "evidence": [[[["Lamborghini Veneno-1"]], [["Lamborghini Veneno-7"]], [["Porsche 911-133"]], ["operation"]], [[["Lamborghini Aventador-14"], "no_evidence"], [["Lamborghini Aventador-14"]], [["Porsche 911-129"]], ["operation"]], [[["Fastest Car-1"]], [["Lamborghini Aventador-14"]], [["Porsche 911-94"]], ["operation"]]], "golden_sentence": [["Based on the Lamborghini Aventador, the Veneno was developed to celebrate Lamborghini's 50th anniversary."], ["The car has a braking distance of 30\u00a0m (98.0\u00a0ft) from 97\u20130\u00a0km/h (60\u20130\u00a0mph), and can produce 1.41 G while cornering."], ["The top speed stood at 330\u00a0km/h (205\u00a0mph)."]]}, {"qid": "b94b5f76137914d699a2", "term": "Second Amendment to the United States Constitution", "description": "Part of the Bill of Rights, regarding the right to bear arms", "question": "Was the Second Amendment to the United States Constitution written without consideration for black Americans?", "answer": true, "facts": ["The writers of the Constitutional Amendments did not view black people as legitimate human beings.", "The writers of the Constitutional Amendments believed that slavery benefited black slaves.", "The Constitutional Amendments were written for people that the writers considered human."], "decomposition": ["Who were the writers of the Constitutional Amendments?", "Who was the the Constitutional Amendments written for?", "Did #1 fail to view black Americans as #2?"], "evidence": [[[["Constitution of the United States-63"], "no_evidence"], [["Constitution of the United States-51"], "no_evidence"], [["African Americans-20"], "operation"]], [[["United States Bill of Rights-2"], "no_evidence"], [["Constitution of the United States-132"], "no_evidence"], [["Reconstruction Amendments-2"], "no_evidence", "operation"]], [[["Ratification-18"]], [["Constitution of the United States-3"]], ["no_evidence"]]], "golden_sentence": [[""], ["Article Four outlines the relations among the states and between each state and the federal government."], [""]]}, {"qid": "673aa7970f218678348f", "term": "Gladiator", "description": "combatant who entertained audiences in the Roman Republic and Roman Empire", "question": "Could a Gladiator's weapon crush a diamond?", "answer": false, "facts": ["Gladiators used a sword known as a Gladius.", "The Gladius was a short sword made from various elements of steel.", "Diamond is one the hardest known substances on earth.", "Only diamond can be used to cut another diamond."], "decomposition": ["What material were Gladiator weapons made from?", "Can #1 crush a diamond?"], "evidence": [[[["Gladius-16"]], [["Diamond-1", "Diamond-15"], "no_evidence", "operation"]], [[["Gladiator-36"]], [["Diamond-15"], "no_evidence"]], [[["Gladius-9"]], [["Diamond-15"], "operation"]]], "golden_sentence": [["Blade strength was achieved by welding together strips, in which case the sword had a channel down the center, or by fashioning a single piece of high-carbon steel, rhomboidal in cross-section."], ["", ""]]}, {"qid": "328725018623b356d2fa", "term": "Republic of Korea Navy", "description": "Naval warfare branch of South Korea's military", "question": "Would Republic of Korea Navy dominate Eritrea navy?", "answer": true, "facts": ["The Republic of Korea Navy has 150 ships, 70 aircraft, 70,000 personnel including 29,000 marines .", "The Eritrean Navy has 4 ships and an army of 45,000."], "decomposition": ["How many ships are in the Republic of Korea's navy?", "How many ships are in the Eritrean Navy?", "How many people are in the Republic of Korea's navy?", "How many people are in the Eritrean navy?", "Is #1 greater than #2 and is #3 greater than #4?"], "evidence": [[[["Republic of Korea Navy-1"]], [["Eritrean Navy-4"]], [["Republic of Korea Navy-1"]], [["Eritrean Defence Forces-5"], "no_evidence"], ["operation"]], [[["Republic of Korea Navy-1"]], [["Eritrean Navy-2"], "no_evidence"], [["Republic of Korea Navy-10"]], [["Eritrean Defence Forces-5"], "no_evidence"], ["operation"]], [[["Republic of Korea Navy-81"]], [["Eritrean Navy-1"], "no_evidence"], [["Republic of Korea Navy-1"]], [["Eritrean Defence Forces-5"], "no_evidence"], ["operation"]]], "golden_sentence": [["There are about 150 commissioned ships with the ROK Navy."], [""], ["The ROK Navy has about 70,000 regular personnel including 29,000 Republic of Korea Marines."], [""]]}, {"qid": "c0c2b24870fc12a65581", "term": "Toyota Prius", "description": "Hybrid electric automobile", "question": "Could someone have arrived at Wrestlemania X in a Toyota Prius?", "answer": false, "facts": ["Wrestlemania X took place in 1994", "The Toyota Prius was first manufactured in 1997"], "decomposition": ["When did Wrestlemania X hold?", "When was the Toyota Prius first manufactured?", "Is #2 before #1?"], "evidence": [[[["WrestleMania X-1"]], [["Toyota Prius-1"]], ["operation"]], [[["WrestleMania X-1"]], [["Toyota Prius-1"]], ["operation"]], [[["WrestleMania X-1"]], [["Toyota Prius-1"]], ["operation"]]], "golden_sentence": [["It took place at Madison Square Garden in New York, New York on March 20, 1994."], ["The Toyota Prius (/\u02c8pri\u02d0\u0259s/; Japanese:\u30c8\u30e8\u30bf\u30fb\u30d7\u30ea\u30a6\u30b9, Toyota Puriusu) is a full hybrid electric automobile developed and manufactured by Toyota since 1997."]]}, {"qid": "0c66c1eb80d46ec13dfe", "term": "Guam", "description": "Island territory of the United States of America", "question": "Could the first European visitor to Guam been friends with Queen Victoria?", "answer": false, "facts": ["Portuguese explorer Ferdinand Magellan, while in the service of Spain, was the first European to visit the island.", "Magellan died in 1521.", "Queen Victoria was born in 1819."], "decomposition": ["Who was the first European visitor to Guam?", "When did #1 die?", "When was Queen Victoria born?", "Did #3 come before #2?"], "evidence": [[[["The Boy Who Was-11"], "no_evidence"], ["no_evidence"], [["Queen Victoria-6"], "no_evidence"], ["no_evidence"]], [[["Guam-3"]], [["Ferdinand Magellan-20"]], [["Queen Victoria-4"]], ["operation"]], [[["Guam-3"]], [["Ferdinand Magellan-1"]], [["Queen Victoria-1"]], ["operation"]]], "golden_sentence": [[""], ["The first of these was Princess Charlotte, who was born and died on 27 March 1819, two months before Victoria was born."]]}, {"qid": "0e310d11826723729449", "term": "Adam Sandler", "description": "American actor, comedian, screenwriter, and producer", "question": "Would the average American family find Adam Sandler's home to be too small?", "answer": false, "facts": ["The average American family has about 3 people in it.", "Adam Sandler's home has 14 bedrooms and 7 bathrooms."], "decomposition": ["How many people are in the average American family?", "How big is Adam Sandler's home?", "Would a home the size of #2 be too small for #1 people?"], "evidence": [[[["Nuclear family-1"], "no_evidence"], ["no_evidence"], ["operation"]], [["no_evidence"], [["Adam Sandler-1"], "no_evidence"], ["operation"]], [[["Nuclear family-1"], "no_evidence"], [["Adam Sandler-26"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["It is in contrast to a single-parent family, the larger extended family, and a family with more than two parents."]]}, {"qid": "a28a6ba1629a69f626f5", "term": "Santa Claus", "description": "Folkloric figure, said to deliver gifts to children on Christmas Eve", "question": "Does Santa Claus work during summer?", "answer": false, "facts": ["Christmas is in winter.", "Santa works on Christmas."], "decomposition": ["What holiday does Santa Claus work on?", "Does #1 occur in the summer?"], "evidence": [[[["Santa Claus-1"]], [["Christmas-1", "Summer-2"]]], [[["Santa Claus-1"]], [["Christmas-1"]]], [[["Santa Claus-1"]], [["Christmas-1"], "no_evidence"]]], "golden_sentence": [["Santa Claus, also known as Father Christmas, Saint Nicholas, Saint Nick, Kris Kringle, or simply Santa, is an imaginary figure originating in Western Christian culture who is said to bring gifts to the homes of well-behaved children on the night of Christmas Eve (24 December) or during the early morning hours of Christmas Day (25 December)."], ["", "School textbooks in Ireland follow the cultural norm of summer commencing on 1 May rather than the meteorological definition of 1 June."]]}, {"qid": "26cf87db15b5d86e054c", "term": "Constitution of the United States", "description": "Supreme law of the United States of America", "question": "Is the Hobbit more profitable for proofreader than Constitution of the United States?", "answer": true, "facts": ["Proofreaders typically get paid per the number of words in a document.", "The Constitution of the United States contains around 7,500 words.", "The Hobbit contains 95,356 words."], "decomposition": ["How many words are in the US Constitution?", "What classification is the Hobbit?", "How many words do books in #2 have?", "Is #3 greater than #1?"], "evidence": [[[["Constitution-4"], "no_evidence"], [["Hobbit-2"]], [["Artam\u00e8ne-1"], "no_evidence"], ["operation"]], [[["Constitution of the United States-1", "Constitution of the United States-2"]], [["The Hobbit-1"], "no_evidence"], [["The Hobbit-20"], "no_evidence"], ["no_evidence", "operation"]], [[["State constitution (United States)-2"]], [["Hobbit-7"]], [["The Hobbit-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["The Constitution of Monaco is the shortest written constitution, containing 10 chapters with 97 articles, and a total of 3,814 words."], [""], ["At 1,954,300 words, it is considered one of the longest novels ever published."]]}, {"qid": "dbade87c32442c7f7c7b", "term": "Holy Spirit", "description": "Religious concept with varied meanings", "question": "Is Krishna similar to Holy Spirit?", "answer": true, "facts": ["The Holy Spirit is a Christian concept of a spirit that is an aspect or agent of God that does good in the world.", "Krishna, from Hinduism, is a manifestation of the God Vishnu.", "Krishna brings compassion, tenderness, and love into the world."], "decomposition": ["What are the characteristics of the Christian Holy Spirit?", "What are the characteristics of Krishna?", "Are many characteristics in #2 also found in #1?"], "evidence": [[[["Holy Spirit in Christianity-4"]], [["Krishna-1"]], ["no_evidence"]], [[["Holy Spirit-1"]], [["Krishna-1"]], [["Krishna-1"]]], [[["God in Abrahamic religions-9", "Holy Spirit-1"]], [["Krishna-1"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], ["He is the god of compassion, tenderness, love and is one of the most popular and widely revered among Indian divinities."]]}, {"qid": "e3e974feae4733a52416", "term": "Sacrum", "description": "Triangular-shaped bone at the bottom of the spine", "question": "Do human sacrums have more fused vertebrae than an Alaskan Malamute?", "answer": true, "facts": ["The human sacrum consists of five fused vertebrae.", "An Alaskan Malamute is a large domestic dog breed.", "Dogs have three fused vertebrae attached to their sacrums."], "decomposition": ["How many vertebrae are found in the human sacrum?", "What species of animal is an Alaskan Malamute?", "How many vertebrae are found in a #2's sacrum?", "Is #1 greater than #3?"], "evidence": [[[["Sacrum-1"]], [["Alaskan Malamute-1"]], [["Dog anatomy-54", "Nuchal ligament-10"], "no_evidence"], ["no_evidence", "operation"]], [[["Sacrum-1"]], [["Alaskan Malamute-1"]], [["Sacrum-4"]], ["operation"]], [[["Sacrum-1"]], [["Alaskan Malamute-1"]], [["Sacrum-4"]], ["operation"]]], "golden_sentence": [["The sacrum\u00a0; plural: sacra or sacrums), in human anatomy, is a large, triangular bone at the base of the spine that forms by the fusing of sacral vertebrae S1\u2013S5 between 18 and 30\u00a0years of age."], ["The Alaskan Malamute (/\u02c8m\u00e6l\u0259\u02ccmju\u02d0t/) is a large breed of domestic dog (Canis lupus familiaris) originally bred for their strength and endurance to haul heavy freight as a sled dog."], ["", ""]]}, {"qid": "524d0bc0f72be6cf0950", "term": "Snoop Dogg", "description": "American rapper", "question": "Was Snoop Dogg's debut studio album released on the weekend?", "answer": false, "facts": ["Snoop Dogg's debut studio album was Doggystyle.", "Doggystyle was released on November 23, 1993.", "November 23, 1993 was a Tuesday.", "In the USA, the weekend consists of Saturday and Sunday."], "decomposition": ["What was Snoop Dogg's first studio album?", "When was #1 released?", "What day of the week did #2 occur on?", "What days are considered the weekend?", "Is #3 one of the answers in #4?"], "evidence": [[[["Snoop Dogg-2"]], [["Doggystyle-1"]], ["no_evidence"], [["Workweek and weekend-1"]], ["operation"]], [[["Doggystyle-1"]], [["Doggystyle-1"]], ["no_evidence"], [["Workweek and weekend-1"]], ["no_evidence", "operation"]], [[["Snoop Dogg-2"]], [["Doggystyle-1"]], ["no_evidence"], [["Workweek and weekend-1"]], ["no_evidence", "operation"]]], "golden_sentence": [["Snoop's debut album, Doggystyle, produced by Dr. Dre, was released in 1993 by Death Row Records."], ["It was released on November 23, 1993, by Death Row Records and Interscope Records."], ["In most of the world, the workweek is from Monday to Friday and the weekend is Saturday and Sunday, but other divisions exist: for example, many countries observe a Sunday to Thursday or even Monday to Thursday working week."]]}, {"qid": "e10307b63c63b5405de0", "term": "Sable", "description": "Species of marten", "question": "Are Sable's a good choice of Mustelidae to weigh down a scale?", "answer": false, "facts": ["Mustelidae is the scientific designation for animals that share similarities including polecats, sables, and ferrets.", "Polecats weigh between 2.2 and 3.3 pounds.", "Sable's weigh around 2.4 pounds.", "Ferrets can weigh up to 44 pounds.", "Sable's have sharp teeth and a painful bite and are outlawed in many states."], "decomposition": ["How much does a sable weigh?", "What are the weights of other common members of Mustelidae?", "Is #1 greater than all #2?"], "evidence": [[[["Sable-4"]], [["Mustelidae-2"]], ["operation"]], [[["Sable-4"]], [["Mustelidae-2"], "no_evidence"], ["operation"]], [[["Sable-4"], "operation"], [["Mustelidae-4"], "no_evidence"], ["no_evidence"]]], "golden_sentence": [["Males measure 38\u201356 centimetres (15\u201322\u00a0in) in body length, with a tail measuring 9\u201312 centimetres (3.5\u20134.7\u00a0in), and weigh 880\u20131,800 grams (1.94\u20133.97\u00a0lb)."], [""]]}, {"qid": "ff457df1287e05729812", "term": "Richard III of England", "description": "15th-century King of England", "question": "Was Richard III ruler of Adelaide?", "answer": false, "facts": ["Richard III was King of England and Lord of Ireland from 1483-1485.", "Adelaide is a city in South Australia."], "decomposition": ["When was Richard III ruler of England?", "What country is Adelaide in?", "When was #2 ruled by England?", "Does #1 and #3 overlap?"], "evidence": [[[["Richard III of England-1"]], [["Adelaide-1"]], [["Australia-2"]], ["operation"]], [[["Richard III of England-1"]], [["Adelaide-1"]], [["Australia-11"]], ["operation"]], [[["Richard III of England-1"]], [["Adelaide-1"]], [["Australia-14"]], ["operation"]]], "golden_sentence": [["Richard III (2 October 1452\u00a0\u2013 22 August 1485) was King of England and Lord of Ireland from 1483 until his death in 1485."], ["Adelaide (/\u02c8\u00e6d\u0259le\u026ad/ (listen) AD-\u0259-layd) is the capital city of the state of South Australia, and the fifth-most populous city of Australia."], [""]]}, {"qid": "9bfc9f076e6cb7ffddf7", "term": "Sweet potato", "description": "species of plant", "question": "Do Sweet Potatoes prevent other plants from growing in their place?", "answer": true, "facts": ["When sweet potato plants decompose, they release a chemical that prevents germination in their soil.", "Farmers will work to ensure that all parts of a sweet potato plant are out of the field before trying to grow in it again."], "decomposition": ["What chemical is released when sweet potatoes decompose?", "Where is #1 released into?", "Does #1 prevent other plants from growing in #2?"], "evidence": [[[["Sweet potato-19"], "no_evidence"], [["Sweet potato-19"], "no_evidence"], ["no_evidence"]], [[["Sweet potato-19"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Sweet potato storage-10"], "no_evidence"], ["no_evidence"], ["no_evidence"]]], "golden_sentence": [[""], [""]]}, {"qid": "7a50b0c6d7cdd4989b22", "term": "Spaghetti", "description": "Type of pasta", "question": "Should spaghetti be slick when cooked?", "answer": false, "facts": ["Spaghetti is typically served with a sauce on it.", "When noodles have too smooth a texture, no sauce will stick to them."], "decomposition": ["What is typically served on top of spaghetti?", "Would #1 be able to stick if the spaghetti were slick?"], "evidence": [[[["Spaghetti-2"]], ["operation"]], [[["Spaghetti and meatballs-1"]], ["no_evidence", "operation"]], [[["Tomato sauce-23"]], ["no_evidence", "operation"]]], "golden_sentence": [["A variety of pasta dishes are based on it and it is frequently served with tomato sauce or meat or vegetables."]]}, {"qid": "ecaf24f535cec5bbd37d", "term": "Adrenaline", "description": "hormone, neurotransmitter and medication. Epinephrine is normally produced by both the adrenal glands and certain neurons", "question": "While viewing \"Scary Movie\" is the viewer likely to experience an increase in adrenaline?", "answer": false, "facts": ["Scary Movie is a film that is a comedy take on horror, intended to make viewers laugh but not afraid.", "Adrenaline is produced when a human is frightened or excited."], "decomposition": ["What type of emotion would cause an increase in adrenaline?", "What genre of movie is Scary Movie?", "What emotion do #2 aim to create in the viewer?", "Are #3 and #1 the same?"], "evidence": [[[["Adrenaline-1"]], [["Scary Movie-1"]], [["Parody film-1"], "no_evidence"], ["operation"]], [[["Adrenaline-13"]], [["Scary Movie-1"]], [["Horror film-1"]], ["operation"]], [[["Adrenaline-13"]], [["Parody film-1", "Scary Movie-1"]], [["Comedy film-1"]], ["operation"]]], "golden_sentence": [[""], ["Scary Movie is a 2000 American parody slasher film directed by Keenen Ivory Wayans."], [""]]}, {"qid": "4137f1e9d1bcd92eebcb", "term": "Flour", "description": "powder which is made by grinding cereal grains", "question": "Is All Purpose Flour safe for someone who has celiac disease?", "answer": false, "facts": ["All purpose flour has about 9% gluten in it.", "When someone with Celiac disease eats gluten, their body has an immune response that attacks their small intestine."], "decomposition": ["What do people with celiac disease have to avoid?", "Is #1 absent from all purpose flour?"], "evidence": [[[["Coeliac disease-2"]], [["Flour-26"], "operation"]], [[["Healthy diet-23"]], [["Flour-26", "Healthy diet-23"]]], [[["Coeliac disease-13"]], [["Coeliac disease-13"]]]], "golden_sentence": [[""], ["\"Plain\" refers not only to AP flour's middling gluten content but also to its lack of any added leavening agent (as in self-rising flour)."]]}, {"qid": "619f3456aecb7579979f", "term": "Very Large Telescope", "description": "telescope in the Atacama Desert, Chile", "question": "Is the Very Large Telescope the most productive telescope in the world?", "answer": false, "facts": ["Telescope productivity is measured based on how many scientific papers a telescope generates.", "The Hubble Space Telescope is the most productive telescope in the world. "], "decomposition": ["What are counted when measuring telescope productivity?", "How many occurrences of #1 have there been for the Very Large Telescope?", "How many occurrences of #1 have there been for the Hubble Telescope?", "Is #2 greater than #3?"], "evidence": [[[["Very Large Telescope-3"]], [["Very Large Telescope-16"]], [["Hubble Space Telescope-84"]], ["operation"]], [[["Very Large Telescope-3"]], [["Very Large Telescope-16"]], [["Hubble Space Telescope-84"]], [["Very Large Telescope-3"], "operation"]], [["no_evidence"], [["Very Large Telescope-16"], "no_evidence"], [["Hubble Space Telescope-69"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""], ["Looking at papers several years after their publication, about one-third of all astronomy papers have no citations, while only two percent of papers based on Hubble data have no citations."]]}, {"qid": "a6e6896472f6c2670c30", "term": "Sandal", "description": "Type of footwear with an open upper", "question": "Is it safe to wear sandals in snow?", "answer": false, "facts": ["Sandals have open toes and don't completely cover the feet.", "Snow is very cold and direct exposure to skin can cause hypothermia.", "The feet need to be completely covered to walk through snow safely."], "decomposition": ["What parts of your foot are exposed in sandals?", "What is the temperature of snow?", "Is it safe to have #1 directly exposed to something that is #2?"], "evidence": [[[["Sandal-1"]], [["Winter storm-1"], "no_evidence"], [["Hypothermia-1"], "operation"]], [[["Sandal-1"]], [["Snow-1"], "no_evidence"], [["Frostbite-5"], "no_evidence"]], [[["Sandal-1"]], [["Snow-16"], "no_evidence"], [["Frostbite-1"], "operation"]]], "golden_sentence": [["While the distinction between sandals and other types of footwear can sometimes be blurry (as in the case of huaraches\u2014the woven leather footwear seen in Mexico, and peep-toe pumps), the common understanding is that a sandal leaves all or most of the foot exposed."], ["A winter storm is an event in which varieties of precipitation are formed that only occur at low temperatures, such as snow or sleet, or a rainstorm where ground temperatures are low enough to allow ice to form (i.e."], [""]]}, {"qid": "c1ed44a7ea2ee52410a9", "term": "Cuisine of Hawaii", "description": "Cuisine of Hawaii", "question": "Is the cuisine of Hawaii suitable for a vegan?", "answer": false, "facts": [" Per capita, Hawaiians are the second largest consumers of Spam in the world, right behind Guam.", "Kalua pig is another famous cuisine of Hawaii.", "Fish and seafood are also very common in Hawaii."], "decomposition": ["What are the popular foods in Hawaiian cuisine?", "Which foods will a vegan exclude from their diet?", "Are all of #2 excluded from #1?"], "evidence": [[[["Cuisine of Hawaii-2"]], [["Veganism-1"]], ["operation"]], [[["Cuisine of Hawaii-26"]], [["Veganism-1"]], ["operation"]], [[["Haitian cuisine-15"], "no_evidence"], [["Vegetarian and vegan dog diet-2"], "no_evidence"], ["no_evidence"]]], "golden_sentence": [["This blend of cuisines formed a \"local food\" style unique to Hawaii, resulting in plantation foods like the plate lunch, snacks like Spam musubi, and dishes like the loco moco."], ["Dietary vegans (also known as \"strict vegetarians\") refrain from consuming meat, eggs, dairy products, and any other animal-derived substances."]]}, {"qid": "7273fee808f5fe2d29d4", "term": "Game (hunting)", "description": "animal hunted for sport or for food", "question": "Would a customer be happy if their grocery store meat tasted like game?", "answer": false, "facts": ["\"Gamey\" is a word used to describe meat with a grassier, more wild taste.", "Gaminess in supermarket meat is very unusual.", "Many people find game to be unpleasant in taste."], "decomposition": ["Which kind of meat is referred to as game?", "Are grocery store customers accustomed to #1?"], "evidence": [[[["Game (hunting)-6"]], [["Meat-1"]]], [[["Game (hunting)-1"]], [["Game (hunting)-5"], "no_evidence"]], [[["Game (hunting)-1"]], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "f27d90805e8d49acc7c9", "term": "Royal Air Force", "description": "Aerial warfare service branch of the British Armed Forces", "question": "Is the Royal Air Force ensign on the moon?", "answer": false, "facts": ["The Royal Air Force ensign is the flag of the Royal Air Force", "The Royal Air Force is a branch of the British Armed Forces", "Britain has never landed on the moon"], "decomposition": ["What does the Royal Air Force ensign represent?", "What country is #1 a part of?", "Has #2 ever sent people to the moon?"], "evidence": [[[["Royal Air Force Ensign-3"], "no_evidence"], [["Royal Air Force-4"], "no_evidence"], [["Moon landing-12"], "no_evidence"]], [[["Royal Air Force Ensign-1"]], [["Royal Air Force-1"]], ["operation"]], [[["Royal Air Force Ensign-1"]], [["Royal Air Force-1"]], ["no_evidence"]]], "golden_sentence": [[""], [""], [""]]}, {"qid": "c02a70336d3ed793de53", "term": "Menstruation", "description": "Regular discharge of blood and tissue from the inner lining of the uterus through the vagina", "question": "Are tampons a good 24 hour solution for mentruation?", "answer": false, "facts": ["Tampons are intended for use up to 8 hours at a time. ", "When left in for longer than 8 hours, tampons pose a dangerous risk for a life threatening condition. "], "decomposition": ["How many hours can a tampon be safely used for at a time?", "Is #1 greater than or equal to 24?"], "evidence": [[[["Tampon-11"]], ["operation"]], [[["Tampon-11"]], ["operation"]], [[["Tampon-11"]], ["operation"]]], "golden_sentence": [["Follow package directions for insertion Choose the lowest absorbency needed for one's flow (test of absorbency is approved by FDA) Follow guidelines and directions of tampon usage (located on box's label) Consider using cotton or cloth tampons rather than rayon Change the tampon at least every 6 to 8 hours or more often if needed Alternate usage between tampons and pads Avoid tampon usage overnight or when sleeping Increase awareness of the warning signs of Toxic Shock Syndrome and other tampon-associated health risks (and remove the tampon as soon as a risk factor is noticed) Cases of tampon-connected TSS are very rare in the United Kingdom and United States."]]}, {"qid": "6a0aa197f710fbbf097d", "term": "Hepatitis", "description": "inflammation of the liver tissue", "question": "Can you cure hepatitis with a tonsillectomy?", "answer": false, "facts": ["A tonsillectomy removes the tonsils, glands found in the back of the throat", "Hepatitis is a disease that targets the liver"], "decomposition": ["What organ does hepatitis affect? ", "What organs are removed during a tonsillectomy?", "Is #1 the same as #2?"], "evidence": [[[["Hepatitis-1"]], [["Tonsillectomy-1"]], ["operation"]], [[["Hepatitis-1"]], [["Tonsillectomy-1"]], ["operation"]], [[["Hepatitis-1"]], [["Tonsillectomy-1"]], ["operation"]]], "golden_sentence": [["Hepatitis is inflammation of the liver tissue."], ["Tonsillectomy is a surgical procedure in which both palatine tonsils are fully removed from the back of the throat."]]}, {"qid": "64fa07d5406e59192857", "term": "Adrenaline", "description": "hormone, neurotransmitter and medication. Epinephrine is normally produced by both the adrenal glands and certain neurons", "question": "Can cancer cause excess adrenaline production?", "answer": true, "facts": ["Adrenaline is produced by the adrenal glands.", "Cancer is a disease characterized by the formation of tumors.", "Tumors on the adrenal glands can cause them to over-express."], "decomposition": ["What is cancer cause to grow?", "Can #1 grow on Adrenal glands?", "Does #2 cause excess adrenaline production?"], "evidence": [[[["Cancer cell-5"]], [["Adrenal tumor-9"]], [["Adrenal tumor-1"]]], [[["Cancer-1"]], [["Adrenal gland-3"]], [["Adrenal tumor-10"], "operation"]], [[["Causes of cancer-1"]], [["Adrenal tumor-5"]], [["Adrenal tumor-8"]]]], "golden_sentence": [[""], [""], [""]]}, {"qid": "5062309b1de734c026f0", "term": "Frigatebird", "description": "A family of seabirds found across tropical and subtropical oceans", "question": "Would a Frigatebird in Ontario be a strange sight?", "answer": true, "facts": ["Ontario is a province of Canada.", "Canada is surrounded by temperate oceans."], "decomposition": ["Where are Frigatebirds usually found?", "Which oceans can be found around Ontario?", "Do all of #2 fail to fit the description of #1?"], "evidence": [[[["Frigatebird-1"]], [["Hudson Bay-1", "James Bay-4"]], [["Frigatebird-1", "Hudson Bay-1", "James Bay-4"]]], [[["Frigatebird-1"]], [["Hudson Bay-12", "Ontario-2"]], ["operation"]], [[["Frigatebird-17"]], [["Geography of Ontario-8"]], ["operation"]]], "golden_sentence": [["Frigatebirds (also listed as \"frigate bird\", \"frigate-bird\", \"frigate\", \"frigate-petrel\") are a family of seabirds called Fregatidae which are found across all tropical and subtropical oceans."], ["", ""], ["", "", ""]]}, {"qid": "da4577e8d695c5de7f15", "term": "Intel", "description": "American semiconductor chip manufacturer", "question": "Would a silicon shortage be bad for Intel's sales?", "answer": true, "facts": ["Silicon is a key material for the production of semiconductor chips.", "A silicon shortage would mean fewer semiconductor chips could be produced.", "A business that produces fewer products than normal will receive lower than normal revenue."], "decomposition": ["What kind of products does Intel make?", "What are the key materials used in the production of #1?", "Is silicon in #2?"], "evidence": [[[["Intel-1"]], [["Integrated circuit-29"]], ["operation"]], [[["Intel-1"]], [["Integrated circuit-1"]], ["operation"]], [[["Intel-1"]], [["Integrated circuit-1"]], ["operation"]]], "golden_sentence": [[""], ["Starting with copper oxide, proceeding to germanium, then silicon, the materials were systematically studied in the 1940s and 1950s."]]}, {"qid": "3f9feb0ea6276527d3d0", "term": "Ivan the Terrible", "description": "Grand Prince of Moscow and 1st Tsar of Russia", "question": "Has Ivan the Terrible flown to Europe?", "answer": false, "facts": ["Ivan the Terrible was the 1st Tsar of Russia.", "Ivan the Terrible died in 1584.", "The first confirmed person to fly was Jean Francois Pilatre de Rozier in 1783."], "decomposition": ["When did Ivan the Terrible die?", "When was the airplane invented?", "Is #2 before #1?"], "evidence": [[[["Ivan the Terrible-1"]], [["Airplane-2"]], ["operation"]], [[["Ivan the Terrible-1"]], [["Airplane-2"]], ["operation"]], [[["Ivan the Terrible-1"]], [["Airplane-14"]], ["operation"]]], "golden_sentence": [["18 March]\u00a01584), commonly known as Ivan the Terrible (Russian: \u0418\u0432\u0430\u0301\u043d \u0413\u0440\u043e\u0301\u0437\u043d\u044b\u0439\u200b\u00a0(help\u00b7info) Ivan Grozny; \"Ivan the Formidable\" or \"Ivan the Fearsome\", Latin: Ioannes Severus ), was the Grand Prince of Moscow from 1533 to 1547 and the first Tsar of Russia from 1547 to 1584."], ["The Wright brothers invented and flew the first airplane in 1903, recognized as \"the first sustained and controlled heavier-than-air powered flight\"."]]}, {"qid": "d7df608a14917819dee2", "term": "Oprah Winfrey", "description": "American businesswoman, talk show host, actress, producer, and philanthropist", "question": "Could Oprah Winfrey buy dozens of her staff Bugatti luxury cars?", "answer": true, "facts": ["Oprah Winfrey is a billionaire", "A new Bugatti costs a few million dollars"], "decomposition": ["How much is Oprah Winfrey worth?", "How much does a  Bugatti cost?", "Is #2 times 2 dozen less than #1?"], "evidence": [[[["Oprah Winfrey-54"]], [["Bugatti Chiron-8"]], ["operation"]], [[["Oprah Winfrey-54"]], [["Bugatti Chiron-8"]], ["operation"]], [[["Oprah Winfrey-54"]], [["Bugatti Chiron-8"]], ["operation"]]], "golden_sentence": [["As of 2014, Winfrey had a net worth in excess of 2.9 billion dollars and had overtaken former eBay CEO Meg Whitman as the richest self-made woman in America."], ["The base price is \u20ac2,400,000 and buyers were required to place a \u20ac200,000 deposit before their purchase."]]}, {"qid": "94232ca251fc8d8bab78", "term": "Wehrmacht", "description": "unified armed forces of Germany from 1935 to 1945", "question": "Did the Wehrmacht affect the outcome of the War to End All Wars?", "answer": false, "facts": ["The Wehrmacht was the unified military of Germany from 1935 to 1945", "The War to End All Wars is a nickname for World War I", "World War I ended in 1918"], "decomposition": ["What war was the War to End All Wars?", "When did #1 end?", "When was the Wehrmacht formed?", "Is #3 before #2?"], "evidence": [[[["The war to end war-1"]], [["World War I-1"]], [["Wehrmacht-1"]], ["operation"]], [[["The war to end war-1"]], [["The war to end war-1"]], [["Wehrmacht-1"]], ["operation"]], [[["World War I-1"]], [["Armistice of 11 November 1918-1"]], [["Wehrmacht-1"]], ["operation"]]], "golden_sentence": [["\"The war to end war\" (also called \"The war to end all wars\") was a term for the First World War of 1914\u20131918."], [""], [""]]}, {"qid": "d0014c7d8b27b8a13402", "term": "Leopard cat", "description": "Small wild cat", "question": "Are Leopard cats in less dire straits than Bornean Orangutan?", "answer": true, "facts": ["Leopard cats are classified as Least Concern on IUCN endangered list.", "Bornean Orangutan's are classified as Endangered on IUCN endangered list."], "decomposition": ["What are the recognized threats to the Bornean orangutan?", "What are the recognized threats to the leopard cat?", "Is #1 worse than #2?"], "evidence": [[[["Bornean orangutan-25"]], [["Leopard cat-28"]], [["Bornean orangutan-26", "Leopard cat-30"]]], [[["Bornean orangutan-26"]], [["Leopard-3"]], [["Bornean orangutan-25"], "no_evidence", "operation"]], [[["Bornean orangutan-2"]], [["Leopard cat-1"]], ["operation"]]], "golden_sentence": [[""], [""], ["", ""]]}, {"qid": "fd3286892e0ee533838a", "term": "Snoop Dogg", "description": "American rapper", "question": "Did Snoop Dogg refuse to make music with rival gang members?", "answer": false, "facts": ["American rapper Snoop Dogg is a member of the Crips gang.", "The Crips are enemies of their rival gang, The Bloods.", "Rapper, The Game is a member of The Bloods gang.", "Tha Blue Carpet Treatment was a Snoop Dogg mixtape featuring the song California Vacation.", "Snoop Dogg collaborates with Xzibit and The Game on the song California Vacation."], "decomposition": ["What is the name of the gang that Snoop Dogg is part of?", "Which gang is the rival of #1?", "In Snoop Dogg's song California Vacation, which rapper did he collaborate with?", "Is #3 not associated with #2?"], "evidence": [[[["Snoop Dogg-7"]], [["Crips-14"]], [["Doctor's Advocate-4"], "no_evidence"], [["The Game (rapper)-5"], "operation"]], [[["Snoop Dogg-7"]], [["Crips-3"]], [["Doctor's Advocate-10"], "no_evidence"], [["The Game (rapper)-5"], "operation"]], [[["Snoop Dogg-7"]], [["Crips-3"]], [["Doctor's Advocate-4"]], [["The Game (rapper)-5"]]]], "golden_sentence": [["He was a member of the Rollin' 20s Crips gang in the Eastside area of Long Beach; although in 1993 he denied reports then in rampant circulation among police and media sources by stating that he never joined a gang."], ["The Bloods are the main rival of the Crips."], ["Guests featured on Doctor's Advocate include Busta Rhymes, Kanye West, Nas, Nate Dogg, Snoop Dogg, Tha Dogg Pound, Jamie Foxx and Xzibit."], [""]]}, {"qid": "b1a228b74d9edea4d44e", "term": "Helium", "description": "Chemical element with atomic number 2", "question": "Does the density of helium cause voices to sound deeper?", "answer": false, "facts": ["Helium is less dense than air.", "Sound travels more quickly through helium than it does through air. ", "When sound travels more quickly, the tone of it raises and sounds higher."], "decomposition": ["What is the density of helium compared to air?", "As a result of #1, what is the speed in which air travel throughs helium compared to air", "When #2 happens, does the tone go deeper?"], "evidence": [[[["Lifting gas-1"]], [["Lifting gas-6"], "no_evidence"], [["Helium-4"], "no_evidence"]], [[["Helium-1"], "no_evidence"], [["Helium-77"]], ["operation"]], [[["Helium-64"]], [["Helium-27"]], [["Helium-77"], "operation"]]], "golden_sentence": [[""], [""], [""]]}, {"qid": "db3fb90a7ffcf91686aa", "term": "Romeo and Juliet", "description": "tragedy by William Shakespeare", "question": "Is Romeo and Juliet an unusual title to teach high schoolers?", "answer": false, "facts": ["Romeo and Juliet has topped multiple 'Top Read Books In High School' lists.", "Romeo and Juliet is available in multiple editions targeted at school age children."], "decomposition": ["What academic sources teach Romeo and Juliet?", "Are high schools included in #1?"], "evidence": [[["no_evidence"], ["no_evidence"]], [[["Romeo and Juliet-73"]], ["no_evidence", "operation"]], [[["Romeo and Juliet-79"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": []}, {"qid": "e4b84837ede6597d5d88", "term": "Winter", "description": "one of the Earth's four temperate seasons, occurring between autumn and spring", "question": "Are there multiple American government holidays during winter?", "answer": true, "facts": ["Winter runs from about December 20 to about March 20.", "Government holidays include Christmas, New Year, King Day, and President's Day.", "Christmas is always December 25, New Year is always January 1, King Day is a Monday in the middle of January, and President's Day is a Monday in late February."], "decomposition": ["Through which period of the year does winter usually last in the US?", "How many government holidays fall within the span of #1?", "Is #2 considerably greater than one?"], "evidence": [[[["Winter-1"], "no_evidence"], [["Federal holidays in the United States-14", "Thanksgiving (United States)-1", "Veterans Day-10", "Washington's Birthday-1"], "no_evidence"], ["operation"]], [[["Northern Hemisphere-2"]], [["Christmas-28", "Federal holidays in the United States-6", "New Year's Day-12"]], ["operation"]], [[["Winter-9"]], [["Holiday-6"]], ["operation"]]], "golden_sentence": [["When it is winter in the Northern Hemisphere, it is summer in the Southern Hemisphere, and vice versa."], ["", "", "", "The Uniform Monday Holiday Act of 1971 moved this holiday to the third Monday, which can fall from February 15 to 21, inclusive."]]}, {"qid": "111ac881a57c0900e7f5", "term": "Islamophobia", "description": "Fear, hatred of, or prejudice against the Islamic religion or Muslims generally,", "question": "Is Islamophobia against Cyprus majority religion misdirected?", "answer": true, "facts": ["Islamophobia is prejudice and fear against Muslims.", "Cyprus is a country in the Middle East, which is a predominantly Muslim region.", "Cyprus is the only Christian majority country in the Middle East, with Christians forming between 76% and 78% of the country's total population, and most of them adhere to Eastern Orthodox Christianity."], "decomposition": ["What religion is targeted by Islamophobia?", "What is the most common religion in Cyprus?", "Is #1 different than #2?"], "evidence": [[[["Islamophobia-1"]], [["Cyprus-100"]], ["operation"]], [[["Islamophobia-54"], "no_evidence"], [["Religion in Cyprus-1"], "operation"], ["no_evidence"]], [[["Islamophobia-1"]], [["Religion in Cyprus-1"]], ["operation"]]], "golden_sentence": [["Islamophobia is the fear, hatred of, or prejudice against the Islamic religion or Muslims generally, especially when seen as a geopolitical force or the source of terrorism."], ["The majority of Greek Cypriots identify as Greek Orthodox, whereas most Turkish Cypriots are adherents of Sunni Islam."]]}, {"qid": "2e5e136aa949f28a4255", "term": "Joke", "description": "something spoken, written, or done with humorous intention", "question": "Have jokes killed more people than rats in history?", "answer": false, "facts": ["Greek philosopher Chrysippus was said to have died from laughter after seeing a donkey eating figs.", "There are only a handful of deaths attributed to jokes throughout history including King Martin of Aragon.", "There are an average of 30 deaths by rat attacks every century.", "The plague which is sometimes associated with diseased rats killed millions of people."], "decomposition": ["How many people have been killed by laughing to jokes?", "Which diseases are spread by rats?", "How many people have been killed by #2 over time", "Is #1 greater than #3?"], "evidence": [[[["Death from laughter-1"]], [["Rat-30"]], [["Bubonic plague-21"]], [["Bubonic plague-21", "Death from laughter-1"]]], [[["Death from laughter-1"], "no_evidence"], [["Rat-28"]], [["Diseases and epidemics of the 19th century-25"], "no_evidence"], ["operation"]], [[["Death from laughter-1"], "no_evidence"], [["Black Death-23"]], [["Black Death-4"]], ["operation"]]], "golden_sentence": [[""], [""], ["The pandemic resulted in the deaths of an estimated 25 million (6th century outbreak) to 50\u00a0million people (two centuries of recurrence)."], ["", ""]]}, {"qid": "a86b3bdb57c5233747df", "term": "Ham", "description": "Pork from a leg cut that has been preserved by wet or dry curing, with or without smoking", "question": "Will parma ham be ready for New Year's if the pig is slaughtered in December?", "answer": false, "facts": ["Parma ham requires two months to cure", "New Year's is at most one month away from December"], "decomposition": ["What is the minimum period of time required for parma ham to cure?", "How long is New Year's Day from December?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Prosciutto-5"]], [["New Year's Day-1"]], ["operation"]], [[["Prosciutto-5"]], [["New Year's Day-1"]], ["operation"]], [[["Ham-10"]], [["New Year's Day-8"], "operation"], ["operation"]]], "golden_sentence": [["The process of making prosciutto can take from nine months to two years, depending on the size of the ham."], [""]]}, {"qid": "3a7bd7d09030271474e5", "term": "Amtrak", "description": "Intercity rail operator in the United States", "question": "Does Amtrak operate four wheel vehicles?", "answer": true, "facts": ["Amtrak is a transportation service.", "Amtrak transports people with trains and buses.", "A bus is a four wheel vehicle. "], "decomposition": ["What kinds of vehicles does Amtrak use?", "Do any of #1 have four wheels?"], "evidence": [[[["Amtrak-1"]], [["Wheelset (rail transport)-1"], "operation"]], [[["International (Amtrak train)-14"]], [["Wheelset (rail transport)-1"]]], [[["Amtrak-3"]], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["Most North American freight cars have two bogies with two or three wheelsets, depending on the type of car; short freight cars generally have no bogies but instead have two wheelsets."]]}, {"qid": "c9bc66d4d32b11678478", "term": "Chlorine", "description": "Chemical element with atomic number 17", "question": "Does chlorine inhibit photosynthesis?", "answer": true, "facts": ["Chlorine prevents algae from growing in pools", "Algae photosynthesize "], "decomposition": ["What does Chlorine prevent from growing in a pool?", "Does #1 do photosynthesis?"], "evidence": [[[["Swimming pool-67"]], [["Algae-1"], "operation"]], [[["Chlorine-66"]], [["Photosynthesis-6"]]], [[["Chlorine dioxide-25"]], [["Bacteria-3"], "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "bb1f21d6f67c42a43190", "term": "Clove", "description": "species of plant", "question": "Do people who smoke Djarum's like cloves?", "answer": true, "facts": ["Djarum is a brand of cigarette popular around the world.", "Djarum cigarettes are made with a blend of cloves and tobacco."], "decomposition": ["What are Djarum cigarettes made of?", "Does #1 include cloves?"], "evidence": [[[["Djarum-1"]], ["operation"]], [[["Djarum-1"]], ["operation"]], [[["Djarum-1", "Kretek-1"]], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "d95ee62515fda51eb930", "term": "Astronomer", "description": "Scientist who studies celestial bodies", "question": "Is an astronomer interested in drosophila?", "answer": false, "facts": ["Astronomers study celestial bodies like planets and stars", "Drosophila are a type of fly commonly studied by scientists in fields related to biology"], "decomposition": ["What do astronomers study?", "What kind of entity is Drosophila?", "Is #2 included in #1?"], "evidence": [[[["Astronomer-1"]], [["Drosophila-4"]], [["Drosophila-4"], "operation"]], [[["Astronomer-1"]], [["Drosophila-1"]], ["operation"]], [[["Astronomer-1"]], [["Drosophila-1"]], ["operation"]]], "golden_sentence": [["Examples of topics or fields astronomers study include planetary science, solar astronomy, the origin or evolution of stars, or the formation of galaxies."], ["Drosophila species are small flies, typically pale yellow to reddish brown to black, with red eyes."], [""]]}, {"qid": "a1e7ff60ee3405b77500", "term": "Atlantic cod", "description": "benthopelagic fish of the family Gadidae", "question": "Is Atlantic cod found in a vegemite sandwich?", "answer": false, "facts": ["Vegemite is a spread popular in Australia.", "Vegemite is made from leftover brewers' yeast extract with various vegetable and spice additives. ", "The Atlantic cod is found mostly in North America and Greenland."], "decomposition": ["To what taxonomic kingdom does the Atlantic cod belong?", "What are the ingredients of Vegemite?", "Do any of #2 belong in #1?"], "evidence": [[[["Atlantic cod-1", "Fish-1"]], [["Yeast extract-11"]], ["operation"]], [[["Atlantic cod-7"]], [["Vegemite-18"]], ["operation"]], [[["Animal-14", "Atlantic cod-1"]], [["Yeast extract-11"], "no_evidence"], ["operation"]]], "golden_sentence": [["The Atlantic cod (Gadus morhua) is a benthopelagic fish of the family Gadidae, widely consumed by humans.", "They form a sister group to the tunicates, together forming the olfactores."], ["Vegemite (/\u02c8v\u025bd\u0292\u026ama\u026at/ VEJ-i-myte) is a thick, black Australian food spread made from leftover brewers' yeast extract with various vegetable and spice additives."]]}, {"qid": "054a2b490fef6cab5d94", "term": "WWE Raw", "description": "WWE television program", "question": "Did Bruiser Brody wrestle on WWE Raw?", "answer": false, "facts": ["WWE Raw debuted on TV January 11, 1993.", "Bruiser Brody was a wrestler that was stabbed to death in 1988."], "decomposition": ["When did WWE Raw make its debut appearance?", "When did wrestler Bruiser Brody's wrestling career come to an end?", "Is #1 before #2?"], "evidence": [[[["WWE Raw-1"]], [["Bruiser Brody-1"]], ["operation"]], [[["WWE Raw-1"]], [["Bruiser Brody-3"]], ["operation"]], [[["WWE Raw-8"]], [["Bruiser Brody-7"], "no_evidence"], ["operation"]]], "golden_sentence": [["The show debuted on January 11, 1993 and is currently considered to be one of two flagship shows, along with Friday Night SmackDown."], ["Frank Donald Goodish (June 18, 1946 \u2013 July 17, 1988) was an American professional wrestler who earned his greatest fame under the ring name Bruiser Brody."]]}, {"qid": "0f0d41a170d5c678ac42", "term": "Crustacean", "description": "subphylum of arthropods", "question": "Can the largest crustacean stretch out completely on a king-sized mattress?", "answer": false, "facts": ["The largest crustacean is the Japanese spider crab ", "The largest Japanese spider crabs have a leg span of just over 12 feet ", "The longer edge of a king-sized mattress is six feet, eight inches"], "decomposition": ["What is the largest crustacean?", "How long is the largest #1?", "How long is a king-sized matress?", "Is #2 smaller than #3?"], "evidence": [[[["Japanese spider crab-1"]], [["Japanese spider crab-2"]], [["Bed size-6"], "no_evidence"], ["operation"]], [[["Crustacean-15"]], [["Crustacean-15"]], [["Mattress-8"], "no_evidence"], ["operation"]], [[["Japanese spider crab-1"]], [["Crustacean-15"]], [["Bed size-17"]], ["operation"]]], "golden_sentence": [[""], ["The body may grow to a size of 40\u00a0cm (16\u00a0in) in carapace width and the whole crab can weigh up to 19 kilograms (42\u00a0lb)\u2014second in mass only to the American lobster among all living arthropod species."], ["193\u00a0cm \u00d7\u00a0202\u00a0cm (76\u00a0in \u00d7\u00a080\u00a0in) in the US."]]}, {"qid": "967986f3f0f0b7505a15", "term": "Linux", "description": "Family of free and open-source software operating systems based on the Linux kernel", "question": "Do Windows or Android smartphones run newer versions of Linux?", "answer": false, "facts": ["Android smartphones are based on Linux.", "Windows smartphones are based on the Windows operating system."], "decomposition": ["Which operating system do Windows smartphones run on?", "Which operating system do android smartphones run on?", "Is #1 or #2 Linux-based?"], "evidence": [[[["Windows 10 Mobile-1"]], [["Android (operating system)-1"]], ["operation"]], [[["Windows 10 Mobile-1"]], [["Android 11-1"]], [["Linux-3"]]], [[["Windows Phone-2"]], [["Linux-65"]], [["Linux-66"], "operation"]]], "golden_sentence": [["First released in 2015, it is a successor to Windows Phone 8.1, but was marketed by Microsoft as being an edition of its PC operating system Windows 10."], ["Android is a mobile operating system based on a modified version of the Linux kernel and other open source software, designed primarily for touchscreen mobile devices such as smartphones and tablets."]]}, {"qid": "118afeba694bea67e88d", "term": "Courage", "description": "quality of mind or spirit that enables a person to face difficulty, danger, or pain", "question": "Does Neville Longbottom have more courage as a child than as an adult?", "answer": false, "facts": ["Neville Longbottom is a character from the Harry Potter series.", "In the first few books of the Harry Potter series, Neville is a child.", "In the final few books of the Harry Potter series Neville is becoming an adult. ", "Neville's first appearances in the series show him to be very cowardly.", "Neville is considered a hero by the end of the series."], "decomposition": ["Did Neville Longbottom's first appearances in the series show him to be very cowardly?", "Was #1's a child in the first few books of the Harry potter series?", "Was Neville Longbottom considered a hero by the end of the series?", "Was #3's an adult in the final few books of the Harry potter series?", "Was he more courageous in #2 than #4?"], "evidence": [[[["Dumbledore's Army-17"]], [["Harry Potter and the Philosopher's Stone-8"]], [["Dumbledore's Army-19"]], [["Dumbledore's Army-19"]], ["operation"]], [[["Dumbledore's Army-17"]], [["Harry Potter (character)-1"]], [["Harry Potter and the Deathly Hallows-15"]], [["Harry Potter and the Deathly Hallows-5"]], [["Harry Potter and the Deathly Hallows \u2013 Part 2-10"], "operation"]], [[["Harry Potter and the Philosopher's Stone-6"], "no_evidence"], ["operation"], [["Dumbledore's Army-17"]], [["Harry Potter (character)-38"], "no_evidence", "operation"], ["operation"]]], "golden_sentence": [["Neville plays a minor role in the first four books, but Rowling wanted him to perform an act of bravery in Philosopher's Stone, in which Neville \"finds true moral courage in standing up to his closest friends\u00a0\u2014 the people who are on his side\" towards the climax."], [""], [""], [""]]}, {"qid": "d7d7b2d2bf82f931f6e3", "term": "Atlantic salmon", "description": "species of fish", "question": "Would Atlantic Salmon be within David Duchovny's dietary guidelines?", "answer": true, "facts": ["David Duchovny is a pescatarian. ", "Pescatarians do not eat chicken, pork, or beef, but will eat fish."], "decomposition": ["What kind of diet does David Duchovny follow?", "What type of food is Atlantic Salmon?", "Do people who follow #1 diets eat #2?"], "evidence": [[[["David Duchovny-12"]], [["Atlantic salmon-1"]], [["Pescetarianism-1"]]], [[["David Duchovny-12"]], [["Atlantic salmon-1", "Seafood-1"]], [["Pescetarianism-1"]]], [[["David Duchovny-3"], "no_evidence"], [["Atlantic salmon-1"]], ["operation"]]], "golden_sentence": [["Duchovny is a former vegetarian and a pescetarian."], ["Atlantic salmon is considered a very healthy food and one of the fish with a more refined taste in many cultures and as such it features in numerous popular traditional cuisines and can fetch a higher price than some other fish."], [""]]}, {"qid": "ef1c3016b40fd978317b", "term": "Winemaking", "description": "the production of wine, starting with the selection of the fruit, its fermentation into alcohol, and the bottling of the finished liquid", "question": "Do people remember Lucille Ball's winemaking as successful?", "answer": false, "facts": ["Lucille Ball was the star of \"I Love Lucy\".", "On \"I Love Lucy\", Lucille's character fails miserably while stomping grapes for wine."], "decomposition": ["What show was Lucille Ball a star of?", "On #1, was Lucille's character successful in making wine?"], "evidence": [[[["Lucille Ball-1"]], [["Grape treading-3"], "no_evidence", "operation"]], [[["I Love Lucy-1"]], [["Grape treading-3"], "no_evidence", "operation"]], [[["Lucille Ball-24"]], ["no_evidence"]]], "golden_sentence": [["She was the star and producer of sitcoms I Love Lucy, The Lucy Show, Here's Lucy, and Life with Lucy, as well as comedy television specials aired under the title The Lucy-Desi Comedy Hour."], [""]]}, {"qid": "3ac1a7f4aceb4292afcf", "term": "Wool", "description": "Textile fibre from the hair of sheep or other mammals", "question": "Can a Sphynx cat be used for wool?", "answer": false, "facts": ["A Sphynx cat is a breed of cats that lacks hair.", "Wool is a soft smooth fabric derived from the hair of animals.", "Sphynx cats skin are covered in an oily sticky substance."], "decomposition": ["Which animals can wool be derived from?", "Is the Sphynx cat likely to be included in #1?"], "evidence": [[[["Wool-1"]], [["Sphynx cat-5"], "operation"]], [[["Wool-1"]], ["operation"]], [[["Wool-7"]], [["Sphynx cat-10"], "operation"]]], "golden_sentence": [["Wool is the textile fiber obtained from sheep and other animals, including cashmere and mohair from goats, qiviut from muskoxen, hide and fur clothing from bison, angora from rabbits, and other types of wool from camelids."], [""]]}, {"qid": "1d38ce42eec45bccef65", "term": "Emulator", "description": "system that emulates a real system such that the behavior closely resembles the behavior of the real system", "question": "Are classic nintendo games for emulator legal?", "answer": false, "facts": ["Distribution of copyrighted games by anyone other than the owner is considered theft.", "Nintendo has not released any games for emulators."], "decomposition": ["Who owns the copyright for classic Nintendo games?", "Has #1 issued any versions of classic Nintendo games for emulators?"], "evidence": [[[["Nintendo Switch-65"], "no_evidence"], ["no_evidence"]], [[["Video game console emulator-5"], "no_evidence"], [["NES Classic Edition-1"], "no_evidence", "operation"]], [[["NES Classic Edition-17"], "no_evidence"], [["Video game-42"], "no_evidence", "operation"]]], "golden_sentence": [["Further, eShop purchases, while still tied to the Nintendo Account, are not tied to the specific Switch console, as was the case for previous Nintendo hardware."]]}, {"qid": "230e63ccfd781aaea021", "term": "T-Mobile", "description": "global telecommunication company", "question": "Can you use the T-Mobile tuesdays app if you aren't a T-Mobile customer?", "answer": false, "facts": ["T-Mobile tuesdays is a rewards app for T-Mobile subscribers.", "T-Mobile Tuesdays verifies users by making sure they have a T-Mobile phone number."], "decomposition": ["Who can use the T-Mobile tuesdays app?", "Does T-Mobile allow use of the app if you aren't #1?"], "evidence": [[[["Un-carrier-22"]], [["Un-carrier-22"]]], [[["T-Mobile-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Un-carrier-22"]], [["Un-carrier-22"]]]], "golden_sentence": [[""], [""]]}, {"qid": "8b8b2421c5ff3269182a", "term": "Aretha Franklin", "description": "American singer, songwriter, and pianist", "question": "Could Aretha Franklin vote for a president when her second child was born?", "answer": false, "facts": ["Aretha Franklin was born in 1942.", "Aretha Franklin had her second child in 1957.", "You must be at least 18 years old to vote in a presidential election in the United States."], "decomposition": ["How old was Aretha Franklin when her second child was born?", "What is the minimum age required to vote in a U.S. election?", "Is #2 lower than #1?"], "evidence": [[[["Aretha Franklin-1", "Aretha Franklin-32"]], [["Voting age-9"]], ["operation"]], [[["Aretha Franklin-1", "Aretha Franklin-32"]], [["United States presidential election-24"]], ["operation"]], [[["Aretha Franklin-32"]], [["Voting age-4"]], ["operation"]]], "golden_sentence": [["", ""], ["In Oregon, Senate Joint Resolution 22 has been introduced to reduce the voting age from 18 to 16."]]}, {"qid": "76df1d039e70fae55f0f", "term": "Eighth Amendment to the United States Constitution", "description": "prohibits cruel and unusual punishment and excessive bail", "question": "Is Eighth Amendment to the United States Constitution popular in court?", "answer": false, "facts": ["The Eighth Amendment prohibits cruel and unusual punishment.", "The Fifth Amendment prevents a person from incriminating themselves.", "The Fifth Amendment is often invoked in criminal cases.", "The Fourteenth Amendment regards equal protection under the law and has been in numerous landmark cases."], "decomposition": ["How many cases have involved the 8th amendment?", "How many cases have involved the other amendments?", "Is #1 the highest out of #2?"], "evidence": [[[["Eighth Amendment to the United States Constitution-30", "Eighth Amendment to the United States Constitution-41"], "no_evidence"], [["Supreme Court of the United States-73"], "no_evidence"], ["operation"]], [[["Eighth Amendment to the United States Constitution-2", "Eighth Amendment to the United States Constitution-3"], "no_evidence"], [["First Amendment to the United States Constitution-3", "First Amendment to the United States Constitution-4"], "no_evidence"], ["operation"]], [[["Eighth Amendment to the United States Constitution-1"], "no_evidence"], [["American Civil Liberties Union-36"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["", ""], [""]]}, {"qid": "1d0e2a852ae2d80b62bf", "term": "Social Democratic Party of Germany", "description": "Social-democratic political party in Germany", "question": "Did the Social Democratic Party of Germany help Frederick II become King of Prussia?", "answer": false, "facts": ["The Social Democratic Party of Germany was founded in 1863.", "Frederick II was King of Prussia from 1740-1786."], "decomposition": ["In what year was the Social Democratic Party of Germany founded?", "In what year did Frederick II become King of Prussia?", "Is #1 before #2?"], "evidence": [[[["Social Democratic Party of Germany-3"]], [["Frederick the Great-1"]], ["operation"]], [[["Social Democratic Party of Germany-3"]], [["Frederick the Great-1"]], ["operation"]], [[["Social Democratic Party of Germany-3"]], [["Frederick the Great-1"]], ["operation"]]], "golden_sentence": [["Established in 1863, the SPD is by far the oldest existing political party represented in the German parliament and was one of the first Marxist-influenced parties in the world."], ["Frederick was the last Hohenzollern monarch titled King in Prussia and declared himself King of Prussia after achieving sovereignty over most historically Prussian lands in 1772."]]}, {"qid": "3621798e7bcd64b558f3", "term": "Pride", "description": "inwardly directed emotion that carries two common meanings", "question": "Would a Catholic priest commend someone's pride?", "answer": false, "facts": ["Adherents to Catholicism subscribe to the notion of the '7 deadly sins'.", "Pride is one of the 7 deadly sins."], "decomposition": ["According to Catholic beliefs, what are the seven deadly sins?", "Is pride excluded from #1?"], "evidence": [[[["Seven deadly sins-1"]], ["operation"]], [[["Seven deadly sins-1"]], [["Seven deadly sins-1"], "operation"]], [[["Seven deadly sins-1"]], ["operation"]]], "golden_sentence": [["According to the standard list, they are pride, greed, wrath, envy, lust, gluttony, and sloth, which are also contrary to the seven heavenly virtues."]]}, {"qid": "0dbc0c716ce9c7b055cc", "term": "PlayStation 4", "description": "Sony's eighth-generation home video game console", "question": "Did Bill Gates help to develop the PlayStation 4?", "answer": false, "facts": ["The PlayStation 4 was developed by Sony Interactive Entertainment.", "Bill Gates works for Microsoft Corporation, which is a competitor of Sony."], "decomposition": ["Which organization does Bill Gate work for?", "Which organization developed PlayStation 4?", "Is #1 the same as #2?"], "evidence": [[[["Bill Gates-1"]], [["PlayStation 4-1"]], ["operation"]], [[["Bill Gates-1"]], [["PlayStation 4 system software-1"]], ["operation"]], [[["Bill Gates-1"]], [["PlayStation 4-1"]], ["operation"]]], "golden_sentence": [["During his career at Microsoft, Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect, while also being the largest individual shareholder until May 2014."], ["The PlayStation 4 (officially abbreviated as PS4) is an eighth-generation home video game console developed by Sony Interactive Entertainment."]]}, {"qid": "5d04249dc97682ef5b6a", "term": "Ape", "description": "superfamily of mammals", "question": "Would a teacher still have their job if they called a black student an ape?", "answer": false, "facts": ["'Ape' and 'monkey' are words that have been used in a derogatory manner against black people.", "Teachers are held to a level of professionalism and cannot act in an abusive way towards children."], "decomposition": ["What kind of term would \"Ape\" be if used to describe a black person?", "What standards are teachers held up to?", "If a teacher used #1, would they be upholding #2?"], "evidence": [[[["Race and ethnicity in the United States-8"], "no_evidence"], [["Teacher-74"], "no_evidence"], ["no_evidence"]], [[["Monkey chanting-1"]], [["Standards-based education reform in the United States-9"]], ["operation"]], [[["Racism-54"]], [["Teacher-28"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["Many charter schools do not require that their teachers be certified, provided they meet the standards to be highly qualified as set by No Child Left Behind."]]}, {"qid": "32cc4fcdbc130ecd2685", "term": "Compact disc", "description": "Optical disc for storage and playback of digital audio", "question": "Did compact discs make computer gaming more popular?", "answer": true, "facts": ["Compact discs contained significantly more storage space than the previously popular floppy disc format.", "Gaming studios were therefore able to significantly improve the graphics, sounds, and features of their games to make them more immersive.", "The better games led to a massive increase in popularity for computer gaming."], "decomposition": ["Which external storage device was previously popular before compact discs?", "What features of compact disc made them much better than #1?", "What new possibilities did #2 create for computer games?", "Did #3 lead to increased interest in computer games?"], "evidence": [[[["Floppy disk-3"]], [["CD-ROM-21"]], [["PC game-22"]], [["PC game-4"], "operation"]], [[["Floppy disk-2", "Floppy disk-22"]], [["Compact disc-3", "Video CD-2"]], [["Video CD-2"], "no_evidence"], [["Compact disc-59", "PC game-22"]]], [[["Floppy disk-1"]], [["Compact disc-2", "Compact disc-41"]], [["Fifth generation of video game consoles-4"]], ["no_evidence", "operation"]]], "golden_sentence": [["While floppy disk drives still have some limited uses, especially with legacy industrial computer equipment, they have been superseded by data storage methods with much greater capacity and transfer speeds, such as USB flash drives, memory cards, optical discs and storage available through computer networks such as cloud storage."], [""], ["While many companies used the additional storage to release poor-quality shovelware collections of older software, or \"enhanced\" versions of existing ones\u2014often with what the magazine mocked as \"amateur acting\" in the added audio and video\u2014new games such as Myst included many more assets for a richer game experience."], [""]]}, {"qid": "1e023599f15927713760", "term": "Stork", "description": "family of birds", "question": "Do storks need golden toads to survive?", "answer": false, "facts": ["Storks feed on a number of reptiles, amphibians, and ammals, and insects.", "The golden toad is an amphibian.", "The golden toad is a rare animal that has not been seen since 1989."], "decomposition": ["What is the most current population estimate of storks?", "What is the most current population estimate of golden toads?", "If storks exclusively ate golden toads, would #2 have been enough to sustain #1?"], "evidence": [[[["Stork-1"], "no_evidence"], [["Golden toad-1"]], ["operation"]], [[["Stork-10"], "no_evidence"], [["Golden toad-1"]], ["no_evidence", "operation"]], [[["Stork-1"], "no_evidence"], [["Golden toad-2"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "3c801eb06a5185fa097b", "term": "H", "description": "letter in the Latin alphabet", "question": "Are any letters directly before and after H missing from Roman numerals?", "answer": true, "facts": ["The Roman numerals are: I, V, X, L, C, D, and M.", "The letter H in the alphabet is preceded by G and followed by I."], "decomposition": ["What is the letter before \"H\"?", "What is the letter after \"H\"?", "What are the Roman numerals? ", "Is it the case that not both #1 and #2 are in #3?"], "evidence": [[["operation"], ["operation"], [["Roman numerals-43"]], ["operation"]], [[["G-1"]], [["I-1"]], [["1444-1"]], ["operation"]], [[["G-1", "H-1"]], [["I-1"]], [["Roman numerals-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["The chronogram would be a phrase containing the letters I, V, X, L, C, D, and M. By putting these letters together, the reader would obtain a number, usually indicating a particular year."]]}, {"qid": "8d4fb91b7d12b58cacdc", "term": "Hammer and sickle", "description": "Communist symbol", "question": "Did the Nazis use the Hammer and sickle flag?", "answer": false, "facts": ["Hammer and sickle is a communist symbol used on flags", "The Nazi flag had a large symbol of a swastika. ", "The hammer and sickle was used as a anti Nazi symbol during World War II."], "decomposition": ["Which symbol is featured in the Nazi flag?", "Is #1 a hammer and sickle symbol?"], "evidence": [[[["Flag of Nazi Germany-1"]], [["Hammer and sickle (disambiguation)-1"]]], [[["Flag of Nazi Germany-4"]], ["operation"]], [[["Flag of Nazi Germany-1"]], [["Swastika-1"]]]], "golden_sentence": [["The flag of Nazi Germany, officially the flag of the German Reich, featured a red flag with the swastika on a white disc."], ["The hammer and sickle (Russian: Serp i Molot: \u0421\u0435\u0440\u043f \u0438 \u043c\u043e\u043b\u043e\u0442, \"sickle and hammer\") is the international symbol of Communism."]]}, {"qid": "11157c0665d144b96dbc", "term": "Johnny Cash", "description": "American singer-songwriter and actor", "question": "Are there enough Jonny Cash records in the world to give one to each French citizen?", "answer": true, "facts": ["Johnny Cash has sold about 90 million albums", "The population of France is around 66 million "], "decomposition": ["How many Johnny Cash records have been sold?", "What is the population of France?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Johnny Cash-1"]], [["France-1"]], ["operation"]], [[["Johnny Cash-1"]], [["France-1"]], ["operation"]], [[["Johnny Cash-1"]], [["France-1"]], ["operation"]]], "golden_sentence": [["He is one of the best-selling music artists of all time, having sold more than 90 million records worldwide."], ["The country's 18 integral regions (five of which are situated overseas) span a combined area of 643,801 square kilometres (248,573\u00a0sq\u00a0mi) and a total population of 67.08\u00a0million (as of March\u00a02020[update])."]]}, {"qid": "67c935bddeecc672ec8d", "term": "QWERTY", "description": "keyboard layout where the first line is \"QWERTYUIOP\"", "question": "Can monkeys use QWERTY keyboards?", "answer": true, "facts": ["QWERTY keyboards are an alphabet key layout that were first used on typrwriters. ", "Monkeys can be trained to push buttons.", "Typewriter key's are buttons.", "Monkeys can press keys on keyboards."], "decomposition": ["What kind of keys are found on QWERTY keyboards?", "Can #1 be likened to buttons?", "Can monkeys be trained to push buttons?", "Are #2 and #3 positive?"], "evidence": [[[["QWERTY-17"]], ["operation"], ["no_evidence", "operation"], ["no_evidence", "operation"]], [[["QWERTY-12"]], [["Keyboard layout-3"]], [["Pet monkey-4"]], ["operation"]], [[["QWERTY-16", "QWERTY-17"]], [["Push-button-1"], "no_evidence"], [["Tool use by animals-21"], "no_evidence"], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "83b76b652385cbb8f8d6", "term": "Chocolate brownie", "description": "A square, baked, chocolate dessert", "question": "Should children be kept from \"special brownies\"?", "answer": true, "facts": ["\"Special brownies\" typically refer to brownies that have been laced with THC.", "THC is an active component of cannabis, a drug meant for adults only."], "decomposition": ["What are \"special brownies\" made from that makes them special?", "Who is #1 made specifically for?", "Are children allowed to have things meant for #2?"], "evidence": [[[["Cannabis edible-1"]], [["Cannabis edible-2"]], ["no_evidence"]], [[["Cannabis edible-11"]], [["Cannabis edible-2"]], [["Cannabis edible-7"], "operation"]], [[["Cannabis edible-6"]], [["Medical cannabis-30"]], [["Medical cannabis-30"]]]], "golden_sentence": [[""], [""]]}, {"qid": "9cb120f8981329f33d84", "term": "Snoopy", "description": "cartoon dog", "question": "Would Taylor Swift refer to Snoopy as oppa?", "answer": true, "facts": ["Oppa is a Korean word used by women to address a man who is 10 or more years older than her", "Snoopy is 47 years old", "Taylor Swift is 30 years old"], "decomposition": ["What is the minimum age difference that a Korean woman would use Oppa to address an older man?", "How old is Snoopy?", "How old is Taylor Swift?", "What is #2 minus #3?", "Is #4 greater than or equal to #1?"], "evidence": [[[["Korean pronouns-20"], "no_evidence"], [["Snoopy-7"]], [["Taylor Swift-1"]], ["operation"], ["operation"]], [[["Third-person pronoun-106"], "no_evidence"], [["Snoopy-1"], "no_evidence"], [["Taylor Swift-1"]], ["operation"], ["operation"]], [[["Korean honorifics-1"], "no_evidence"], [["Snoopy-1"]], [["Taylor Swift-4"]], ["operation"], ["no_evidence", "operation"]]], "golden_sentence": [["Using a kinship term: \uc5b8\ub2c8 (eonni, \"older sister\" if speaker is female), \ub204\ub098 (nuna, \"older sister\" if speaker is male), \uc624\ube60 (oppa, \"older brother\" if speaker is female), \ud615 (hyeong, \"older brother\" if speaker is male), \uc544\uc90c\ub9c8 (ajumma, \"middle-aged woman\"), \uc544\uc8fc\uba38\ub2c8 (ajumeoni, also \"middle aged woman\" but more polite), \uc544\uc800\uc528 (ajeossi, \"middle-aged man\"), \ud560\uba38\ub2c8 (halmeoni, \"grandmother\") of \ud560\uc544\ubc84\uc9c0 (harabeoji, \"grandfather\")."], [""], ["Taylor Alison Swift (born December 13, 1989) is an American singer-songwriter."]]}, {"qid": "615959aa92b6e6d5b697", "term": "Horseradish", "description": "species of plant", "question": "Can horseradish be eaten in a religious context?", "answer": true, "facts": ["A Seder is typically held during the Jewish holiday Passover.", "The Seder involves several items representing the story of the Exodus.", "Horseradish is commonly used for the traditional bitter herb item."], "decomposition": ["What are some commonly used traditional bitter herb items for the Seder held during the Passover?", "Is horseradish included in #1?"], "evidence": [[[["Maror-1"]], [["Maror-13"], "operation"]], [[["Maror-7"]], ["operation"]], [[["Passover Seder-53"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["Many Jews use horseradish condiment (a mixture of cooked horseradish, beetroot and sugar), though the Shulchan Aruch requires that maror be used as is, that is raw, and not cooked or mixed with salt, vinegar, sugar, lemon, or beets."]]}, {"qid": "35c860dde413f2fb0a8d", "term": "Hamster", "description": "subfamily of mammals", "question": "Do hamsters provide food for any animals?", "answer": true, "facts": ["Hamsters are prey animals.", "Prey animals provide food for predators. "], "decomposition": ["What types of animal are hamsters?", "Do #1 provide food for any other animals?"], "evidence": [[[["Hamster-1"]], [["Ball python-9", "Snake-85"], "no_evidence", "operation"]], [[["Hamster-1"]], [["Cat-1", "Hamster-2"], "operation"]], [[["Hamster-1"]], [["Rodent-73"]]]], "golden_sentence": [["Hamsters are rodents (order Rodentia) belonging to the subfamily Cricetinae, which contains 19 species classified in seven genera."], ["Pythons greater than 70 cm total length and females prey almost exclusively on small mammals.", "Pet snakes can be fed relatively infrequently, usually once every 5 to 14 days."]]}, {"qid": "e5f5ae2d9f82a807b568", "term": "Chick-fil-A", "description": "American fast food chain", "question": "Will Chick-fil-A hypothetically refuse to sponsor a Pride parade?", "answer": true, "facts": ["Pride parades are annual festivals held to celebrate the gay community.", "Chick-fil-A is a fast food restaurant founded by S. Truett Cathy.", "S. Truett Cathy was a devout Southern Baptist. ", "Chick-fil-A's opposition to same-sex marriage has been the subject of public controversy."], "decomposition": ["Who founded Chick-fil-A?", "What religion was #1?", "What do pride parades typically celebrate?", "Do #2's oppose #3?"], "evidence": [[[["Chick-fil-A-3"]], [["S. Truett Cathy-3"]], [["Pride parade-1"]], [["Public image of Mike Huckabee-17"]]], [[["Chick-fil-A-3"]], [["Chick-fil-A-2"]], [["Pride parade-1"]], [["Chick-fil-A-33"], "no_evidence", "operation"]], [[["S. Truett Cathy-1"]], [["S. Truett Cathy-3"]], [["Pride parade-1"]], [["Hate group-17"], "operation"]]], "golden_sentence": [["The chain's origin can be traced to the Dwarf Grill (now the Dwarf House), a restaurant opened by S. Truett Cathy, the chain's former chairman and CEO, in 1946."], [""], ["Pride parades (also known as pride marches, pride events, and pride festivals) are outdoor events celebrating lesbian, gay, bisexual, transgender, and queer (LGBTQ) social and self acceptance, achievements, legal rights, and pride."], [""]]}, {"qid": "2603fa7e33a90e084cdb", "term": "All Nippon Airways", "description": "Japanese Airline", "question": "Are the headquarters of All Nippon Airways near a beach?", "answer": false, "facts": ["The headquarters of All Nippon Airways are located in Shiodome City Center in the Shiodome area of the Minato ward of Tokyo.", "Tokyo is a metropolitan area.", "A beach is a landform alongside a body of water.", "Metropolitan areas typically do not have bodies of water in the surrounding area."], "decomposition": ["Where city are the headquarters of All Nippon Airways?", "What kind of development area is #1?", "What is a beach characterized as?", "Do #2 areas typically have #3?"], "evidence": [[[["All Nippon Airways-1"]], [["Shiodome-2"]], [["Beach-1"]], ["operation"]], [[["All Nippon Airways-1"]], [["Shiodome-7"]], [["Beach-1"]], ["operation"]], [[["All Nippon Airways-1"]], [["Shiodome-1"]], [["Beach-1"]], ["operation"]]], "golden_sentence": [["Its headquarters are located in Shiodome City Center in the Shiodome area of Minato ward of Tokyo."], ["This area is for residential use, and there are three tall apartment buildings located there, along with a small park."], ["A beach is a landform alongside a body of water which consists of loose particles."]]}, {"qid": "cae6498b3ccbfd070a03", "term": "Evander Holyfield", "description": "American boxer", "question": "Does Evander Holyfield eat pork products?", "answer": true, "facts": ["Evander Holyfield is a born-again Christian", "Mainstream Christian denominations do not observe restrictions on the kinds of animals they can eat"], "decomposition": ["What religion was Evander Holyfield?", "Does #1 allow eating pork?"], "evidence": [[[["Evander Holyfield-62"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], [["Christian dietary laws-1"], "operation"]], [["no_evidence"], ["no_evidence"]]], "golden_sentence": [[""]]}, {"qid": "50e95ac7f557b376e0fb", "term": "John Key", "description": "38th Prime Minister of New Zealand", "question": "Could John Key issue an executive order in the USA?", "answer": false, "facts": ["An executive order is a means of issuing federal directives in the United States, used by the president of the United States.", "To serve as president of the United States, one must be a natural-born citizen of the United States.", "John Key was born in Auckland, New Zealand."], "decomposition": ["Who can issue executive orders in the USA?", "What are the requirements to become #1?", "Does John Key satisfy all of #2?"], "evidence": [[[["Executive order-1"]], [["President of the United States-37"], "no_evidence"], [["John Key-1"], "no_evidence"]], [[["Federal government of the United States-18"]], [["President of the United States-38"]], [["John Key-1"]]], [[["Executive order-1"]], [["President of the United States-38"]], [["John Key-5"]]]], "golden_sentence": [["In the United States, a federal executive order is a directive issued by the president of the United States that manages operations of the federal government."], ["To serve as president, one must:"], [""]]}, {"qid": "79e1e95b6011f4877010", "term": "Goblin shark", "description": "Deep-sea shark", "question": "Can a Goblin shark hypothetically ride a bike if it had limbs?", "answer": false, "facts": ["A Goblin shark weighs around 460 pounds.", "The weight capacity of the average bike is 300 pounds."], "decomposition": ["What is the average weight of a goblin? ", "What is the average weight a bike can hold? ", "Is #1 less than #2?"], "evidence": [[[["Goblin shark-8"], "no_evidence"], [["Birdy (bicycle)-11"], "no_evidence"], ["operation"]], [[["Goblin shark-1", "Goblin shark-8"]], [["Outline of bicycles-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Goblin shark-8"]], [["Bicycle-26"], "no_evidence"], ["operation"]]], "golden_sentence": [["The maximum weight recorded is 210\u00a0kg (460\u00a0lb) for a 3.8-m-long shark."], ["Stock bikes ranged from 10.2\u00a0kg (22.5\u00a0lb) (for the Birdy Speed with Tune and Shimano Ultegra road components) to over 12\u00a0kg (26.5\u00a0lb), depending on the setup."]]}, {"qid": "8ddaa28150eb91aeecd3", "term": "Anchor", "description": "Device used to connect a vessel to the bed of a body of water to prevent the craft from drifting", "question": "Does a Trek 9000 require an anchor in order to park?", "answer": false, "facts": ["A Trek 9000 is a mountain bike", "An anchor is used on water borne vehicles like boats"], "decomposition": ["What kind of vehicle is the Trek 9000?", "Does #1 need an anchor to park?"], "evidence": [[[["Trek Bicycle Corporation-7"]], ["operation"]], [[["International 9000-6"], "operation"], ["no_evidence"]], [[["Klein Bicycle Corporation-7"]], [["Bicycle-30"], "operation"]]], "golden_sentence": [["1992 marked another first for Trek: its first full suspension mountain bike, the 9000-series, which featured Trek's T3C (travel is three times compression) suspension system."]]}, {"qid": "bd39921a01edc4897e35", "term": "Steven Spielberg", "description": "American film director and screenwriter", "question": "Does Steven Spielberg's 1998 film take place in a period after War Horse setting?", "answer": true, "facts": ["Steven Spielberg's 1998 film, Saving Private Ryan, takes place during World War II.", "World War II took place from 1939-1945.", "The 2011 movie War Horse takes place during World War I.", "World War I took place from 1914-1918."], "decomposition": ["What time period does War Horse take place in?", "What years did #1 take place in?", "What time period does Steven Spielberg's 1998 film take place in?", "When years did #3 take place in?", "Did #4 happen after #2?"], "evidence": [[[["War Horse (film)-2"]], [["World War I-1"]], [["Saving Private Ryan-1"]], [["World War II-1"]], ["operation"]], [[["War Horse (film)-2"]], [["World War I-1"]], [["Saving Private Ryan-1"]], [["World War II-1"]], ["operation"]], [[["War Horse (film)-2"]], [["War Horse (film)-10", "War Horse (film)-5"]], [["Saving Private Ryan-1"]], [["Saving Private Ryan-6"]], ["operation"]]], "golden_sentence": [["Set before and during World War I, it tells of the journey of Joey, a bay Thoroughbred horse raised by British teenager Albert (Irvine), as he is bought by the British Army, leading him to encounter numerous individuals and owners throughout Europe, all the while experiencing the tragedies of the war happening around him."], [""], [""], [""]]}, {"qid": "2bbfb11eaac9172082eb", "term": "Plum", "description": "subgenus of plants", "question": "Is November a bad time for a photographer to take pictures of a plum tree in bloom?", "answer": true, "facts": ["A plum tree is a deciduous tree that bears fruit.", "Deciduous trees shed their leaves in the autumn.", "Autumn happens from September until the end of Deember."], "decomposition": ["What kind of tree is a plum tree?", "What season will it be in temperate regions by November?", "What do #1 do during #2", "Considering #3, will a plum tree have an unattractive appearance at that time?"], "evidence": [[[["Plum-7"]], [["Winter-10"]], [["Plum-7"]], [["Plum-7"]]], [[["Plum-5"], "no_evidence"], [["Season-3"]], [["Plum-5"], "no_evidence"], [["Plum-5", "Plum-7"]]], [[["Prunus-1"]], [["November-2"]], [["Deciduous-1"]], ["operation"]]], "golden_sentence": [["Plum cultivars include: Damson (purple or black skin, green flesh, clingstone, astringent) Greengage (firm, green flesh and skin even when ripe) Mirabelle (dark yellow, predominantly grown in northeast France) Satsuma plum (firm red flesh with a red skin) Victoria (yellow flesh with a red or mottled skin) Yellowgage or golden plum (similar to greengage, but yellow) Different plum cultivars Damsons Greengages Mirabelles Victoria plums When it flowers in the early spring, a plum tree will be covered in blossoms, and in a good year approximately 50% of the flowers will be pollinated and become plums."], ["In the United Kingdom, meteorologists consider winter to be the three coldest months of December, January and February."], [""], ["Plum cultivars include: Damson (purple or black skin, green flesh, clingstone, astringent) Greengage (firm, green flesh and skin even when ripe) Mirabelle (dark yellow, predominantly grown in northeast France) Satsuma plum (firm red flesh with a red skin) Victoria (yellow flesh with a red or mottled skin) Yellowgage or golden plum (similar to greengage, but yellow) Different plum cultivars Damsons Greengages Mirabelles Victoria plums When it flowers in the early spring, a plum tree will be covered in blossoms, and in a good year approximately 50% of the flowers will be pollinated and become plums."]]}, {"qid": "0a783c0b9585b8e2aed2", "term": "Cauliflower", "description": "cauliflower plants (for the vegetable see Q23900272)", "question": "Do more Cauliflower grow in Arizona than California?", "answer": false, "facts": ["Cauliflower grows best in cool temperatures with lots of sun.", "California is the largest producer of Cauliflower in the U.S.", "Arizona has a hot arid climate."], "decomposition": ["Which kind of climate favors the growth of Cauliflower?", "What kind of climate does Arizona have?", "What kind of weather does California have?", "Is #1 more similar to #2 than #3?"], "evidence": [[[["Cauliflower-6"]], [["Arizona-39", "Arizona-40", "Arizona-42"]], [["Climate of the United States-18"], "no_evidence"], ["operation"]], [[["Cauliflower-6"]], [["Arizona-39"]], [["California-52"]], ["operation"]], [[["Cauliflower-6"]], [["Arizona-39"]], [["Climate of the United States-18"]], ["no_evidence", "operation"]]], "golden_sentence": [["Because weather is a limiting factor for producing cauliflower, the plant grows best in cool daytime temperatures 21\u201329\u00a0\u00b0C (70\u201385\u00a0\u00b0F), with plentiful sun, and moist soil conditions high in organic matter and sandy soils."], ["In the lower elevations, the climate is primarily desert, with mild winters and extremely hot summers.", "", ""], ["[citation needed] Like most Mediterranean climates, much of coastal California has a wet winter and dry summer."]]}, {"qid": "448a552076159daa0bb0", "term": "Martyr", "description": "person who suffers persecution and death for advocating, refusing to renounce, and/or refusing to advocate a belief or cause, usually a religious one", "question": "Would Jason Voorhees hypothetically fail at being a martyr?", "answer": true, "facts": ["A martyr is someone that is killed for their beliefs.", "Jason Voorhees is the horror maniac from the Friday the 13th movies.", "Jason Voorhees is immortal and cannot be killed.", "Characters in Friday the 13th thought that dismembering Jason Voorhees would kill him but Jason even survived dismemberment."], "decomposition": ["What experience must one pass through in order to be considered a matyr?", "Can Jason Voorhes be killed?", "Would #2 being negative make Jason Voorhees unable to undergo #1?"], "evidence": [[[["Martyr-1"]], [["Jason Voorhees-40"]], ["operation"]], [[["Martyr-1"]], [["Jason Voorhees-40"]], ["operation"]], [[["Martyr-1"]], [["Jason Voorhees-40"]], ["operation"]]], "golden_sentence": [["Insofar, the martyr is a relational figure of a society's boundary work that is produced by collective memory."], ["Further characteristics that appealed to the participants included Jason's \"immortality, his apparent enjoyment of killing [and] his superhuman strength.\""]]}, {"qid": "7ed84d1fba887c22da24", "term": "Palm Beach, Florida", "description": "Town in Florida, United States", "question": "Could Palm Beach be held in the palm of your hand?", "answer": false, "facts": ["Palm Beach has a total area of 8.12 square miles.", "The average palm is around 3 inches in length.", "There are 63360 inches in a mile."], "decomposition": ["What is the total area of Palm Beach?", "What is the maximum area that can be held on the palm of a human hand?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Palm Beach, Florida-17"]], ["no_evidence"], ["operation"]], [[["Palm Beach, Florida-53"]], [["Human body-6"]], ["operation"]], [[["Palm Beach, Florida-17"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["According to the U.S. Census Bureau, the town has a total area of approximately 8.12\u00a0sq\u00a0mi (21.0\u00a0km2), with land accounting for approximately 4.20\u00a0sq\u00a0mi (10.9\u00a0km2) and water covering the remaining 3.92\u00a0sq\u00a0mi (10.2\u00a0km2)."]]}, {"qid": "e169fbab7971d6250f87", "term": "Alfred Nobel", "description": "Swedish chemist, innovator, and armaments manufacturer (1833\u20131896)", "question": "Did Alfred Nobel write a banned book?", "answer": true, "facts": ["Banned books are works which are not available for free access due to government restrictions.", "Alfred Nobel is the author of Nemesis, a prose tragedy in four acts about Beatrice Cenci", "The entire stock of Nemesis was destroyed immediately after Nobel's death except for three copies, being regarded as scandalous and blasphemous. "], "decomposition": ["What literary works did Alfred Nobel write?", "Have any of the works listed in #1 been banned?"], "evidence": [[[["Alfred Nobel-21"]], ["operation"]], [[["Alfred Nobel-21"]], [["Alfred Nobel-21"]]], [[["Alfred Nobel-21"]], ["operation"]]], "golden_sentence": [["Nobel gained proficiency in Swedish, French, Russian, English, German, and Italian."]]}, {"qid": "77e2fb8c5674a5c2a83b", "term": "Kidney", "description": "internal organ in most animals, including vertebrates and some invertebrates", "question": "Can a quarter fit inside of a human kidney?", "answer": true, "facts": ["Kidney stones are hard mineral deposits that can form in the kidneys.", "The largest kidney stone ever recorded was 13 cm wide.", "The diameter of a quarter is 2.4 cm."], "decomposition": ["How big is the largest kidney stone ever recorded?", "How wide is a quarter?", "Is #1 larger than #2?"], "evidence": [[[["Kidney stone disease-46"], "no_evidence"], [["Quarter (United States coin)-1"]], ["operation"]], [["no_evidence"], [["Quarter (United States coin)-3"]], ["operation"]], [[["Kidney-1"]], [["Quarter (United States coin)-1"]], ["operation"]]], "golden_sentence": [[""], ["It has a diameter of .955 inch (24.26\u00a0mm) and a thickness of .069 inch (1.75\u00a0mm)."]]}, {"qid": "7bcd89e38c63b0357b75", "term": "2008 Summer Olympics", "description": "Games of the XXIX Olympiad, held in Beijing in 2008", "question": "Did Boris Yeltsin watch the 2008 Summer Olympics?", "answer": false, "facts": ["The 2008 Summer Olympics were held Aug 08 - 24, 2008", "Boris Yeltsin died on Apr 23, 2007"], "decomposition": ["What were the date of the 2008 Summer Olympics?", "When did Boris Yeltsin die?", "is #2 before #1?"], "evidence": [[[["2008 Summer Olympics-1"]], [["Boris Yeltsin-77"]], ["operation"]], [[["2008 Summer Olympics-1"]], [["Boris Yeltsin-77"]], ["operation"]], [[["2008 Summer Olympics-1"]], [["Boris Yeltsin-1"]], ["operation"]]], "golden_sentence": [["The 2008 Summer Olympics (Chinese: \u4e8c\u96f6\u96f6\u516b\u5e74\u590f\u5b63\u5965\u8fd0\u4f1a; pinyin: \u00c8r l\u00edng l\u00edng b\u0101 Ni\u00e1n Xi\u00e0j\u00ec \u00c0oy\u00f9nhu\u00ec), officially known as the Games of the XXIX Olympiad (Chinese: \u7b2c\u4e8c\u5341\u4e5d\u5c4a\u590f\u5b63\u5965\u6797\u5339\u514b\u8fd0\u52a8\u4f1a; pinyin: D\u00ec \u00c8rsh\u00edji\u01d4 Ji\u00e8 Xi\u00e0j\u00ec \u00c0ol\u00ednp\u01d0k\u00e8 Y\u00f9nd\u00f2nghu\u00ec), and commonly known as Beijing 2008 (Chinese: \u5317\u4eac\u4e8c\u96f6\u96f6\u516b; pinyin: B\u011bij\u012bng \u00c8r l\u00edng l\u00edng b\u0101), was an international multi-sport event that was held from 8 to 24 August 2008 in Beijing, China."], ["Boris Yeltsin died of congestive heart failure on 23 April 2007, aged 76."]]}, {"qid": "0d7954e1b6fbf893400b", "term": "Frost", "description": "coating or deposit of ice that may form in humid air in cold conditions, usually overnight", "question": "Does frost mean that it will be a snowy day?", "answer": false, "facts": ["Frost forms regularly in areas that experience freezing temperatures and morning dew.", "Frost isn't deposited from the sky like snow, it forms on the ground."], "decomposition": ["How is frost formed?", "Does #1 usually involve the falling of snow?"], "evidence": [[[["Frost-5"]], [["Frost-5"]]], [[["Frost-1"]], ["no_evidence", "operation"]], [[["Frost-5"]], [["Frost-5"]]]], "golden_sentence": [["The ice it produces differs in some ways from crystalline frost, which consists of spicules of ice that typically project from the solid surface on which they grow."], [""]]}, {"qid": "2793c9ed1ec365b40c39", "term": "Zorro", "description": "Fictional character", "question": "Are there multiple Disney Zorro?", "answer": true, "facts": ["Zorro is a man who is a famous fictional Spanish hero and crime fighter featured in a novel.", "Disney produced a 1957 TV show featuring Zorro the spanish crime fighter. ", "Zorro is spanish for \"fox\".", "\"El Zorro y El Sabueso\" is the spanish title for Disney's animated movie called \"The Fox and The Hound\"."], "decomposition": ["Which famous hero was featured in Disney's 1957 TV show?", "What does 'The Fox' in Disney's 'The Fox and the Hound' translate to in Spanish?", "Do #1 and #2 refer to Zorro?"], "evidence": [[[["Zorro-4"]], [["The Fox and the Hound-4", "Zorro-1"]], ["operation"]], [[["Zorro-4"]], [["Zorro (disambiguation)-1"]], [["Zorro-1"], "operation"]], [[["Zorro-4"]], [["The Fox and the Hound-1", "Zorro-1"]], ["operation"]]], "golden_sentence": [["While the rest of McCulley's Zorro stories did not enjoy the same popularity, as most of them were never reprinted until the 21st century, the character also appears in over 40 films and in ten TV series, the most famous being the Disney-produced Zorro series of 1957\u201359, starring Guy Williams."], ["", "Zorro (Spanish for 'Fox') is a fictional character created in 1919 by American pulp writer Johnston McCulley, and appearing in works set in the Pueblo of Los Angeles during the era of Spanish California (1769\u20131821)."]]}, {"qid": "4eaad1583adfd0ed6df3", "term": "Chevrolet Corvette", "description": "Sports car by the Chevrolet division of General Motors (GM)", "question": "Does selling a 2020 Chevrolet Corvette almost pay for a year at Columbia University?", "answer": true, "facts": ["The price of a 2020 Chevrolet Corvette is $58,900.", "Columbia University cost $59,430 during the 2018-2019 school year."], "decomposition": ["How much does a 2020 Chevrolet Corvette cost?", "How much does a year at Columbia University cost?", "Is #1 almost as much as #2?"], "evidence": [[[["Chevrolet Corvette-1"], "no_evidence"], [["Columbia University-28"]], ["no_evidence", "operation"]], [["no_evidence"], [["Columbia University-28"]], ["no_evidence", "operation"]], [["no_evidence"], [["Columbia University-28"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], ["In 2015\u20132016, annual undergraduate tuition at Columbia was $50,526 with a total cost of attendance of $65,860 (including room and board)."]]}, {"qid": "e317496fd0ccbf2660e6", "term": "Suburb", "description": "Human settlement that is part of or near to a larger city", "question": "Does the book Revolutionary Road give a glimpse at life in a suburb?", "answer": true, "facts": ["The setting of the book is in Connecticut.", "The area of Connecticut where the book takes place is not in a major city.", "The book takes place in a suburb called Revolutionary Hill Estates."], "decomposition": ["What location is the setting of the book Revolutionary Road?", "Is #1 a suburb?"], "evidence": [[[["Revolutionary Road-1"]], [["Revolutionary Road-5"]]], [[["Revolutionary Road-4"]], ["operation"]], [[["Revolutionary Road-4"]], [["Connecticut-61"], "operation"]]], "golden_sentence": [["Revolutionary Road is American author Richard Yates's debut novel about 1950s suburban life in the East Coast."], [""]]}, {"qid": "e11885881bbc5e7e1ffa", "term": "Liberty Bell", "description": "bell that serves as a symbol of American independence and liberty", "question": "Is the Liberty Bell still in its original location?", "answer": false, "facts": ["The Liberty Bell originally was located in Independence Hall in Philadelphia.", "It was moved to a nearby pavilion to accommodate viewers in 1976."], "decomposition": ["What was the original location of the Liberty Bell?", "What is the current location of the Liberty Bell?", "Is #2 the same as #1?"], "evidence": [[[["The Liberty Bell (band)-2"], "operation"], [["Location, Location, Location-3"], "no_evidence"], ["no_evidence"]], [[["Liberty Bell-1"]], [["Liberty Bell-1"]], [["Liberty Bell-1"], "operation"]], [[["Liberty Bell-1"]], [["Liberty Bell-1"]], ["operation"]]], "golden_sentence": [["The Liberty Bell was founded in Corpus Christi, Texas in 1966, and their original name was the Zulus."], [""]]}, {"qid": "5e31b32db2d6e4fd3e66", "term": "Publishing", "description": "Process of production and dissemination of literature, music, or information", "question": "Does Buddy The Elf know anyone who works in publishing?", "answer": true, "facts": ["Buddy The Elf is a character from the movie Elf.", "Buddy The Elf's father works in a Manhattan publishing firm."], "decomposition": ["Which people are known to the movie character Buddy The Elf?", "Does any of #1 work in publishing?"], "evidence": [[[["Elf (film)-5", "Elf (film)-9"]], [["Elf (film)-6"], "no_evidence"]], [[["Elf (film)-10"]], ["operation"]], [[["Elf (film)-3"]], ["operation"]]], "golden_sentence": [["", ""], [""]]}, {"qid": "2652b7652c84a06693fc", "term": "Mail carrier", "description": "employee of the post office or postal service, who delivers mail to residences and businesses", "question": "Is unanimously elected president's birthday a break for mail carriers?", "answer": true, "facts": ["The post office has several holidays including: New Year's Day, Washington's Birthday (President's Day), and Veterans Day.", "George Washington was the only US president elected unanimously."], "decomposition": ["Which US president was elected unanimously?", "When is #1's birthday?", "Is #2 a break or holiday for the post office?"], "evidence": [[[["1788\u201389 United States presidential election-6"]], [["George Washington-1"]], [["Washington's Birthday-1"]]], [[["George Washington-107"]], [["Washington's Birthday-1"]], [["Postal holiday-3"], "operation"]], [[["Living presidents of the United States-3"], "no_evidence"], [["Jimmy Carter-5"]], [["Public holidays in the United States-16"]]]], "golden_sentence": [[""], ["George Washington (February 22, 1732 \u2013 December 14, 1799) was an American political leader, military general, statesman, and founding father who served as the first president of the United States from 1789 to 1797."], [""]]}, {"qid": "93194dc4e0cdd14788ce", "term": "Sea otter", "description": "A species of marine mammal from the northern and eastern coasts of the North Pacific Ocean", "question": "Does a sea otter eat spiders?", "answer": false, "facts": ["Sea otters prey mostly on marine invertebrates and other aquatic creatures.", "Spiders are not aquatic creatures and they reside on land."], "decomposition": ["What are sea otters known to feed on?", "Are spiders included in #1?"], "evidence": [[[["Sea otter-49"]], ["operation"]], [[["Sea otter-2"]], ["operation"]], [[["Sea otter-2"]], ["operation"]]], "golden_sentence": [["Sea otters consume over 100 prey species."]]}, {"qid": "a8a36bdc792fb4d75658", "term": "Bengal cat", "description": "Breed of cat", "question": "Could a Bengal cat hypothetically best Javier Sotomayor's record?", "answer": true, "facts": ["Javier Sotomayor is an athlete that holds the men's high jump world record of slightly over 8 feet.", "The average cat can jump from 7.5 to 9 feet.", "Bengal cats have powerful hind legs which make them jump higher than other breeds."], "decomposition": ["How high is Javier Sotomayor's world record high jump?", "Which breed of cat can jump the highest?", "If the average cat can jump up to 9 feet, then #2 can jump higher than what number?", "Is #3 greater than #1?"], "evidence": [[[["Javier Sotomayor-1"]], [["Bengal cat-21"], "no_evidence"], ["operation"], ["operation"]], [[["High jump-3"]], [["Savannah cat-1", "Savannah cat-21"]], ["operation"], ["operation"]], [[["Javier Sotomayor-1"]], [["Caracal-2"], "no_evidence"], [["Bengal cat-21"], "no_evidence", "operation"], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "05f53f3375ec01af553f", "term": "Cinnamon", "description": "spice obtained from the inner bark of several trees from the genus Cinnamomum", "question": "Are Chipotle Cinnamon Pork Chops appropriate for a Seder?", "answer": false, "facts": ["Chipotle Cinnamon Pork Chops are a popular recipe made by applying a cinnamon rub to pork chops.", "The Seder is a Jewish feast day that begins the season of Passover.", "Pork is forbidden by Jewish kosher laws."], "decomposition": ["What kind of event is a Seder?", "Which religious group observes #1?", "Which foods are considered appropriate by the dietary restrictions imposed on #2?", "What are the main ingredients of Chipotle Cinnamon Pork Chops?", "Are #4 included in #3?"], "evidence": [[[["Passover Seder-1"]], [["Passover Seder-1"]], [["American Jewish cuisine-4"]], [["Pork chops and applesauce-1"]], ["operation"]], [[["Passover Seder-1"]], [["Passover Seder-2"]], [["Kashrut-3"]], [["Pork chop-1"]], ["operation"]], [[["Passover Seder-1"]], [["Passover Seder-1"]], [["American Jewish cuisine-4"]], [["Pork chop-1"]], ["operation"]]], "golden_sentence": [["The Passover Seder /\u02c8se\u026ad\u0259r/ (Hebrew: \u05e1\u05b5\u05d3\u05b6\u05e8 [\u02c8sede\u0281] 'order, arrangement'; Yiddish: \u05e1\u05d3\u05e8\u200e seyder) is a ritual feast that marks the beginning of the Jewish holiday of Passover."], [""], ["Under these rules, some foods \u2013 for example, pork and shellfish \u2013 are forbidden."], [""]]}, {"qid": "4e657362c3768f785177", "term": "Basil", "description": "species of plant", "question": "Would the chef at Carmine's restaurant panic if there was no basil?", "answer": true, "facts": ["Carmines is an Italian family-style restaurant.", "Basil is an essential in Italian cooking."], "decomposition": ["What kind of cuisine does Carmine's serve?", "Is basil an essential ingredient in #1?"], "evidence": [[[["Carmine Romano-2"], "no_evidence"], [["Italian cuisine-27"], "no_evidence", "operation"]], [["no_evidence"], ["no_evidence"]], [["no_evidence"], ["no_evidence"]]], "golden_sentence": [[""], [""]]}, {"qid": "d9e48fa58aa149a660d9", "term": "Rice pudding", "description": "Dish made from rice mixed with water or milk", "question": "Is most store bought rice pudding made with brown rice?", "answer": false, "facts": ["Brown rice is more expensive than white rice. ", "Most store bought rice pudding is white in color.", "Brown rice, when cooked, is light brown in color."], "decomposition": ["Which rice pudding is most commonly purchased in stores?", "What color is #1", "Which types of rice are commonly used to make rice pudding?", "Is the one among #3 having color of #2 brown rice?"], "evidence": [[[["Rice pudding-10"]], ["no_evidence"], [["Rice pudding-10"]], ["operation"]], [[["Rice pudding-17"], "no_evidence"], ["no_evidence", "operation"], [["Rice pudding-4"]], ["operation"]], [[["Rice pudding-10", "Rice pudding-17"]], [["Rice pudding-4"]], [["Rice pudding-4"]], ["operation"]]], "golden_sentence": [["In the United Kingdom and Ireland, rice pudding is a traditional dessert typically made with high-starch short-grained rice sold as \"pudding rice\"."], ["In the United Kingdom and Ireland, rice pudding is a traditional dessert typically made with high-starch short-grained rice sold as \"pudding rice\"."]]}, {"qid": "74f07cbff34182e88f85", "term": "Easy Rider", "description": "1969 film by Dennis Hopper", "question": "Did Easy Rider make a profit at the theater when it was released?", "answer": true, "facts": ["Easy Rider had a filming budget of about half a million dollars.", "Upon release in 1969, it earned about 60 million dollars."], "decomposition": ["What was the budget of Easy Rider?", "How much did Easy Rider earn upon its release?", "Is #2 greater than #1?"], "evidence": [[[["Easy Rider-13"]], [["Easy Rider-3"]], [["Easy Rider-3"], "operation"]], [[["Easy Rider-13"]], [["Easy Rider-32"]], ["operation"]], [[["Easy Rider-3"]], [["Easy Rider-3"]], ["operation"]]], "golden_sentence": [["The filming budget of Easy Rider was $360,000 to $400,000."], ["Easy Rider was released by Columbia Pictures on July 14, 1969, grossing $60 million worldwide from a filming budget of no more than $400,000."], [""]]}, {"qid": "d4f6785a109354643203", "term": "Mail carrier", "description": "employee of the post office or postal service, who delivers mail to residences and businesses", "question": "Do mail carriers need multiple uniforms?", "answer": true, "facts": ["Mail carriers work throughout the year independent of the weather.", "Mail carriers must often leave their vehicle in various weather conditions."], "decomposition": ["What seasons do mail carriers work through?", "In order to make it through all of #1, does one need different clothing pieces?"], "evidence": [[[["United States Postal Service-145"], "no_evidence"], [["Clothing-2"]]], [[["United States Postal Service creed-1"], "no_evidence"], [["Winter clothing-2"], "operation"]], [[["Season-1"], "no_evidence"], [["Mail carrier-8"], "no_evidence", "operation"]]], "golden_sentence": [["City Carriers are required to work in any kind of weather, daylight or dark and carry three bundles of mail (letters in one hand, magazines in the other and advertisements in a mailbag) in addition to parcels up to a total of 70\u00a0lbs."], [""]]}, {"qid": "f80aaf02d6f7c4c58b5a", "term": "Shrimp", "description": "Decapod crustaceans", "question": "Is Steve Martin someone who would refuse a dish of shrimp pasta?", "answer": true, "facts": ["Steve Martin is allergic to shellfish.", "Shrimp are one of the many types of shellfish."], "decomposition": ["What types of food is Steve Martin allergic to?", "What type of food is shrimp?", "Is #2 included in #1?"], "evidence": [[["no_evidence"], [["Shrimp-36"]], ["no_evidence", "operation"]], [[["Steve Martin-1"], "no_evidence"], [["Shrimp-1", "Shrimp-34"]], ["no_evidence", "operation"]], [["no_evidence"], [["Shrimp-16"]], ["no_evidence", "operation"]]], "golden_sentence": [[""]]}, {"qid": "7746380005f8fb28dead", "term": "John Key", "description": "38th Prime Minister of New Zealand", "question": "As of 2020 have more women succeeded John Key than preceded him?", "answer": false, "facts": ["John Key, the 38th Prime Minister of New Zealand, has had one woman succeed him as Prime Minister.", "John key was preceded by two women as Prime Minister of New Zealand."], "decomposition": ["Which notable position did John Key occupy?", "How many women served as #1 before him?", "How many women have served as #1 after him?", "Is #3 greater than #2?"], "evidence": [[[["John Key-1"], "no_evidence"], [["Helen Clark-1"]], [["Jacinda Ardern-1"]], ["operation"]], [[["John Key-1"]], [["Prime Minister of New Zealand-23"]], [["Prime Minister of New Zealand-23"]], ["operation"]], [[["John Key-1"]], [["Helen Clark-1", "Jenny Shipley-1"]], [["Jacinda Ardern-1"]], ["operation"]]], "golden_sentence": [["Sir John Phillip Key GNZM AC (born 9 August 1961) is a New Zealand former politician who served as the 38th Prime Minister of New Zealand from 2008 to 2016 and as Leader of the New Zealand National Party from 2006 to 2016."], ["She was New Zealand's fifth-longest-serving prime minister, and the second woman to hold that office."], [""]]}, {"qid": "484026d84494680d44ca", "term": "Common Era", "description": "alternative (and religiously neutral) naming of the traditional calendar era, Anno Domini", "question": "Would Methuselah hypothetically hold a record in the Common Era?", "answer": true, "facts": ["Methuselah was a biblical figure said to have lived until 969.", "The Common Era is the years after the BC era and is alternatively referred to as A.D.", "Jeanne Louise Calment 1875\u20131997 was the oldest human whose age was well-documented, with a lifespan of 122 years and 164 days."], "decomposition": ["Which period is referred to as the Common Era?", "Who is the oldest human whose age was well documented during #1?", "How old was #2 when she died?", "How old was Methuselah when he died?", "Is #4 greater than #3?"], "evidence": [[[["Common Era-1"]], ["no_evidence"], ["no_evidence"], [["Methuselah-1"]], ["operation"]], [[["Common Era-1"]], [["Oldest people-2"]], [["Oldest people-2"]], [["Methuselah-1"]], ["operation"]], [[["Common Era-1"]], [["Oldest people-2"]], [["Oldest people-2"]], [["Methuselah-1"]], ["operation"]]], "golden_sentence": [["BCE (Before the Common Era or Before the Current Era) is the era before CE."], ["Said to have died at the age of 969, he lived the longest of all figures mentioned in the Bible."]]}, {"qid": "523dcc94f5f0108978fd", "term": "Stork", "description": "family of birds", "question": "Would you be likely to see storks at a baby shower?", "answer": true, "facts": ["Storks are a symbol of the arrival of a new baby.", "Baby showers are parties held to celebrate a woman who will soon give birth."], "decomposition": ["What do storks represent?", "Do baby showers celebrate #1?"], "evidence": [[[["White stork-46"]], [["Baby shower-1"]]], [[["White stork-46"]], [["Baby shower-1"]]], [[["White stork-46"]], ["operation"]]], "golden_sentence": [["According to European folklore, the stork is responsible for bringing babies to new parents."], ["It celebrates the delivery or expected birth of a child or the transformation of a woman into a mother."]]}, {"qid": "692d730092271062db6f", "term": "National Diet", "description": "legislature of Japan", "question": "Can Viper Room concert hypothetically be held at National Diet building?", "answer": true, "facts": ["The Viper Room has a capacity of 250 people.", "The National Diet building has two wings with over 700 seats."], "decomposition": ["What is the capacity of the The Viper Room?", "What is the capacity of the National Diet Building?", "Is #2 greater than or equal to #1?"], "evidence": [[[["The Viper Room-1"], "no_evidence"], [["National Diet Building-28"]], ["operation"]], [[["The Viper Room-1"], "no_evidence"], [["National Diet Building-11"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], [["National Diet Building-28"]], [["National Diet Building-28"], "operation"]]], "golden_sentence": [[""], ["The number of seats, however, is still not enough to accommodate the entire 722 members of the Diet, so at the opening ceremony Diet members are sometimes seen standing in the aisles or space in the back of the floor."]]}, {"qid": "1a9a6e54453e63409f0b", "term": "Fever", "description": "common medical sign characterized by elevated body temperature", "question": "Can you get a fever from consuming meat?", "answer": true, "facts": ["A fever is a medical symptom that elevates the core body temperature. ", "Eating under cooked meat can cause food poisoning.", "One of the symptoms of food poisoning is elevated core body temperature. "], "decomposition": ["What is a fever?", "What can consuming uncooked meat cause?", "Is #1 a symptom of #2?"], "evidence": [[[["Fever-1"]], [["Raw meat-4"]], [["Fever-1"]]], [[["Fever-1"]], [["Trichinosis-1", "Trichinosis-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Fever-1"]], [["Raw meat-4"]], [["Listeria-14"], "operation"]]], "golden_sentence": [["Fever, also referred to as pyrexia, is defined as having a temperature above the normal range due to an increase in the body's temperature set point."], ["These complications can include Haemolytic uraemic syndrome (HUS) and neurological problems."], [""]]}, {"qid": "d50f34e22a15b560f6e1", "term": "Byzantine calendar", "description": "The calendar used by the Eastern Orthodox Church from c. 691 to 1728", "question": "Did Ivan the Terrible use the Byzantine calendar?", "answer": true, "facts": ["Ivan the Terrible was the Tsar of Russia from 1530 to 1585. ", "The Byzantine calendar was the official calendar of the Russian government from 988 to 1700.", "The Tsar was the leader of the Russian government. "], "decomposition": ["What was Ivan the Terrible's role from 1530 to 1585?", "What country was Ivan the Terrible #1 of?", "Was the Byzantine calendar the official calendar of #2 from 1530 to 1585?"], "evidence": [[[["Ivan the Terrible-1"]], [["Ivan the Terrible-1"]], [["Byzantine calendar-1"], "operation"]], [[["Ivan the Terrible-1"]], [["Ivan the Terrible-1"]], [["Byzantine calendar-1"]]], [[["Ivan the Terrible-1"], "no_evidence"], [["Ivan the Terrible-2"]], [["Byzantine calendar-1"], "operation"]]], "golden_sentence": [["18 March]\u00a01584), commonly known as Ivan the Terrible (Russian: \u0418\u0432\u0430\u0301\u043d \u0413\u0440\u043e\u0301\u0437\u043d\u044b\u0439\u200b\u00a0(help\u00b7info) Ivan Grozny; \"Ivan the Formidable\" or \"Ivan the Fearsome\", Latin: Ioannes Severus ), was the Grand Prince of Moscow from 1533 to 1547 and the first Tsar of Russia from 1547 to 1584."], ["18 March]\u00a01584), commonly known as Ivan the Terrible (Russian: \u0418\u0432\u0430\u0301\u043d \u0413\u0440\u043e\u0301\u0437\u043d\u044b\u0439\u200b\u00a0(help\u00b7info) Ivan Grozny; \"Ivan the Formidable\" or \"Ivan the Fearsome\", Latin: Ioannes Severus ), was the Grand Prince of Moscow from 1533 to 1547 and the first Tsar of Russia from 1547 to 1584."], ["The Byzantine calendar, also called \"Creation Era of Constantinople\" or \"Era of the World\" (Ancient Greek: \u1f1c\u03c4\u03b7 \u0393\u03b5\u03bd\u03ad\u03c3\u03b5\u03c9\u03c2 \u039a\u03cc\u03c3\u03bc\u03bf\u03c5 \u03ba\u03b1\u03c4\u1f70 \u1fec\u03c9\u03bc\u03b1\u03af\u03bf\u03c5\u03c2, also \u1f1c\u03c4\u03bf\u03c2 \u039a\u03c4\u03af\u03c3\u03b5\u03c9\u03c2 \u039a\u03cc\u03c3\u03bc\u03bf\u03c5 or \u1f1c\u03c4\u03bf\u03c2 \u039a\u03cc\u03c3\u03bc\u03bf\u03c5, abbreviated as \u03b5.\u039a."]]}, {"qid": "fff02ed263d702c0c3fc", "term": "Rabbi", "description": "teacher of Torah in Judaism", "question": "Would a Rabbi celebrate Christmas?", "answer": false, "facts": ["A Rabbi is a spiritual leader or religious teacher in Judaism.", "Christmas is a holiday observed by Christians."], "decomposition": ["What religion do Rabbis belong to?", "Which religion celebrates Christmas?", "Is #1 the same as #2?"], "evidence": [[[["Rabbi-17"]], [["Christmas-7"]], ["operation"]], [[["Rabbi-1"]], [["Christmas-1"]], ["operation"]], [[["Rabbi-1"]], [["Christmas-1"]], ["operation"]]], "golden_sentence": [["Rabbis serve the Jewish community."], ["In Old English, G\u0113ola (Yule) referred to the period corresponding to December and January, which was eventually equated with Christian Christmas."]]}, {"qid": "9f90e5001ba2caf0c72b", "term": "Clementine", "description": "nothospecies of plant, Clementine", "question": "Is clementine pith highly sought after?", "answer": false, "facts": ["Pith is the white part of the clementine fruit between the orange colored peel and the edible fruit.", "Most people discard the pith after peeling."], "decomposition": ["What is a pith?", "Do people usually like to keep #1 after peeling?"], "evidence": [[[["Pith-1"]], [["Pith-1"]]], [[["Pith-1"]], [["Clementine-1"], "no_evidence", "operation"]], [[["Pith-3"]], ["no_evidence"]]], "golden_sentence": [["Pith, or medulla, is a tissue in the stems of vascular plants."], [""]]}, {"qid": "006fb764cdab12dfe8bb", "term": "Nepalese Civil War", "description": "civil war in Nepal between 1996 and 2006", "question": "Did the Nepalese Civil War take place near India?", "answer": true, "facts": ["The Nepalese Civil War happened in Nepal.", "Nepal is a country that shares a border with India."], "decomposition": ["Where did the Nepalese Civil War take place?", "Is #1 near India?"], "evidence": [[[["Nepalese Civil War-1"]], [["Nepal-1"], "operation"]], [[["Nepalese Civil War-1"]], [["Nepal-1"]]], [[["Nepalese Civil War-1"]], [["Nepal-1"], "operation"]]], "golden_sentence": [["The Nepalese Civil War (also known as the Maoist Conflict (Nepali: \u092e\u093e\u0913\u0935\u093e\u0926\u0940 \u091c\u0928\u092f\u0941\u0926\u094d\u0927; IAST: m\u0101ov\u0101d\u012b janayuddha), the Maoist Insurgency or the Maoist Revolution) was a civil war in Nepal fought between the Communist Party of Nepal (Maoist) (CPN-M) and the government of Nepal from 1996 to 2006."], ["It is landlocked, and borders China in the north and India in the south, east and west, while Bangladesh is located within only 27\u00a0km (17\u00a0mi) of its southeastern tip and Bhutan is separated from it by the Indian state of Sikkim."]]}, {"qid": "144366508a80167e0035", "term": "Paratrooper", "description": "Military parachutists functioning as part of an airborne force", "question": "Can paratroopers be used in a vacuum?", "answer": false, "facts": ["Paratroopers use parachutes to glide", "Parachutes function by creating drag in an atmosphere", "There is no atmosphere in a vacuum"], "decomposition": ["What equipment do paratroopers use?", "What does #1 need to create in order to function?", "In what does #1 create #2?", "Is #3 present in a vacuum?"], "evidence": [[[["Paratrooper-1"]], [["Parachute-1"]], [["Drag (physics)-1"]], [["Vacuum-1"], "operation"]], [[["Parachuting-1", "Paratrooper-1"]], [["Drag (physics)-1"]], [["Atmosphere of Earth-1"], "no_evidence"], [["Vacuum-1"], "operation"]], [[["Paratrooper-1"]], [["Parachute-1"]], [["Parasitic drag-3"]], [["Vacuum-16"]]]], "golden_sentence": [["Military parachutists (troops) and parachutes were first used on a large scale during World War II for troop distribution and transportation."], [""], [""], [""]]}, {"qid": "e85c6cda526ace2aeb7f", "term": "Black Sea", "description": "Marginal sea of the Atlantic Ocean between Europe and Asia", "question": "Could the moon fit inside the Black Sea?", "answer": false, "facts": ["The volume of the Black Sea is 547,000 cubic kilometers.", "The volume of the moon is 21.9 billion cubic kilometers."], "decomposition": ["What is the volume of the Black Sea?", "What is the volume of the moon?", "Is #1 higher than #2?"], "evidence": [[[["Black Sea-2"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Black Sea-2"]], [["Moon-48"], "no_evidence"], ["operation"]], [[["Black Sea-28"], "no_evidence"], [["Earth-85"], "no_evidence"], ["operation"]]], "golden_sentence": [["The Black Sea has an area of 436,400\u00a0km2 (168,500\u00a0sq\u00a0mi) (not including the Sea of Azov), a maximum depth of 2,212\u00a0m (7,257\u00a0ft), and a volume of 547,000\u00a0km3 (131,000\u00a0cu\u00a0mi)."]]}, {"qid": "6fcea95cbcf586bc8741", "term": "Thesis", "description": "document submitted in support of candidature for an academic degree", "question": "Could R. Kelly write a college thesis?", "answer": false, "facts": ["A college thesis is a long and complicated written document.", "R. Kelly claims to be illiterate, which means he cannot read and write. "], "decomposition": ["What does writing a college thesis require a person be able to do?", "What does R. Kelly claim to be?", "Can someone who is #2 do #1?"], "evidence": [[[["Reading-1"]], [["R. Kelly-9"]], [["Dyslexia-20"]]], [[["Thesis-1"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Thesis-1"], "no_evidence"], [["R. Kelly-9"]], [["Dyslexia-1"], "operation"]]], "golden_sentence": [[""], ["An undiagnosed and untreated learning disability, believed to be dyslexia, left Kelly unable to read or write."], [""]]}, {"qid": "f958ba8598e02817e290", "term": "Shogi", "description": "Game native to Japan", "question": "Can a chess board be converted to a Shogi board?", "answer": false, "facts": ["Chess is a game that is made up of 64 squares.", "The Japanese game of Shogi requires a total of 81 squares."], "decomposition": ["How many squares are on a chessboard?", "How many squares are on a Shogi board?", "Is #1 equal to #2?"], "evidence": [[[["Chess-1"]], [["Shogi-4"]], ["operation"]], [[["Chess-1"]], [["Shogi-4"]], ["operation"]], [[["Chessboard-3"]], [["Shogi-4"]], ["operation"]]], "golden_sentence": [["Chess is a two-player strategy board game played on a checkered board with 64\u00a0squares arranged in an 8\u00d78 grid."], ["Two players face each other across a board composed of rectangles in a grid of 9 ranks (rows, \u6bb5) by 9 files (columns, \u7b4b) yielding an 81 square board."]]}, {"qid": "6daa6b34eb7c90a4978c", "term": "Short-eared dog", "description": "species of canid", "question": "Has Cesar Millan ever tamed a short-eared dog?", "answer": false, "facts": ["Cesar Millan is a Mexican-American dog trainer with over 25 years of canine experience.", "The short-eared dog lives in various parts of the rainforest environment, preferring areas with little human disturbance.", "The short-eared dog is a solitary animal and prefers moving in trees away from human and other animal interactions.", "The short-eared dog is a wild animal that is not suitable as a pet."], "decomposition": ["Which kind of dogs does Cesar Millan's train?", "What are the social characteristics of the short-eared dog?", "Does #2 match the characteristics of #1?"], "evidence": [[[["Cesar Millan-11"]], [["Short-eared dog-9"]], ["operation"]], [[["Cesar Millan-2"], "no_evidence"], [["Short-eared dog-1", "Short-eared dog-9"]], ["operation"]], [[["Cesar Millan-2"]], [["Short-eared dog-9"]], ["operation"]]], "golden_sentence": [["The show consists of one-half lecture and one-half demonstration with local shelter dogs, in which he uses his pack-leader training techniques to modify negative behaviors."], ["It prefers a solitary lifestyle, in forest areas."]]}, {"qid": "cf59df3507c7313e64e1", "term": "Byzantine calendar", "description": "The calendar used by the Eastern Orthodox Church from c. 691 to 1728", "question": "Did the Eastern Orthodox Church and the Byzantine Empire ever use the same calendar?", "answer": true, "facts": ["Eastern Orthodox Church used the Byzantine calendar from c. 691 to 1728", "The Byzantine Empire used the Byzantine calendar from c. 988 to 1453"], "decomposition": ["What calendar did the Eastern Orthodox Church use from  c. 691 to 1728?", "What calendar did the Byzantine Empire use from c. 988 to 1453?", "Is #1 and #2 the same?"], "evidence": [[[["Eastern Orthodox Church-77"]], [["Byzantine calendar-8"]], ["operation"]], [[["Julian calendar-82"], "no_evidence"], [["Julian calendar-69"], "operation"], ["operation"]], [[["Eastern Orthodox Church-207"], "no_evidence"], [["Byzantine calendar-1"], "operation"], ["no_evidence"]]], "golden_sentence": [["The autonomous Church of Finland of the Ecumenical Patriarchate, as well as parts of the Church of the Czech Lands and Slovakia, use the Gregorian calendar."], ["It was only in AD 1700 that the Byzantine calendar in Russia was changed to the Julian calendar by Peter the Great."]]}, {"qid": "ee52a5232a8c3bb13cc4", "term": "Ancient Greece", "description": "Civilization belonging to an early period of Greek history", "question": "Were number of states in Ancient Greece underwhelming compared to US states in 1900?", "answer": false, "facts": ["In the year 1900 there were 42 US states.", "Ancient Greece had several hundred relatively independent city-states called poleis."], "decomposition": ["How many states were in the United States in 1900?", "How many city-states were there in Ancient Greece?", "Is #2 less than #1?"], "evidence": [[[["Oklahoma-2", "Utah-2"]], [["City-state-6"], "no_evidence"], ["operation"]], [[["Oklahoma-2", "Utah Territory-1"]], [["Ancient Greece-47"]], ["operation"]], [[["Oklahoma Territory-52", "Utah-2"]], [["Ancient Greece-22"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["", ""], [""]]}, {"qid": "5d6a95a11d37619b9d3f", "term": "Agriculture", "description": "Cultivation of plants and animals to provide useful products", "question": "Is the rise of agriculture attributed to rivers?", "answer": true, "facts": ["Some of the earliest civilizations on record are in river valleys.", "Early civilizations used water to irrigate crops, leading to the growth of agriculture."], "decomposition": ["What are the basic factors of agricultural production?", "Do rivers provide any of #1?"], "evidence": [[[["Irrigation-45"]], [["Irrigation-45"]]], [[["Agriculture-1"]], [["Agriculture-7"], "operation"]], [[["Agriculture-7"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], ["Irrigation water can come from groundwater (extracted from springs or by using wells), from surface water (withdrawn from rivers, lakes or reservoirs) or from non-conventional sources like treated wastewater, desalinated water, drainage water, or fog collection."]]}, {"qid": "c7b0c9e6f66524cf0ab8", "term": "Goldfish", "description": "freshwater fish, common in aquariums", "question": "Are goldfish more difficult to care for than isopods?", "answer": true, "facts": ["Isopod care is compared to that of a houseplant.", "Goldfish are notorious for making their tanks dirty quite often."], "decomposition": ["What is isopod care comparable to?", "What challenges do Goldfish pose to keepers?", "Is #1 easier to deal with than #2?"], "evidence": [[[["Isopoda-14", "Isopoda-15"]], [["Goldfish-28"]], ["operation"]], [[["Isopoda-1", "Isopoda-13"]], [["Goldfish-15"], "no_evidence"], ["operation"]], [[["Isopoda-1"]], [["Goldfish-17", "Goldfish-21", "Goldfish-22"], "no_evidence"], ["operation"]]], "golden_sentence": [["", "In general, isopod parasites have diverse lifestyles and include Cancricepon elegans, found in the gill chambers of crabs; Athelges tenuicaudis, attached to the abdomen of hermit crabs; Crinoniscus equitans living inside the barnacle Balanus perforatus; cyproniscids, living inside ostracods and free-living isopods; bopyrids, living in the gill chambers or on the carapace of shrimps and crabs and causing a characteristic bulge which is even recognisable in some fossil crustaceans; and entoniscidae living inside some species of crab and shrimp."], [""]]}, {"qid": "372d1c3388cdf2b22220", "term": "Mario", "description": "fictional character from Nintendo's ''Mario'' franchise and the company's mascot", "question": "Does Mario use mushrooms to run faster?", "answer": false, "facts": ["Mushrooms in the Mario universe are used to provide extra lives and to make Mario grow, providing him an extra hit point. ", "Mario is made to run faster when he wears bunny ears or uses a starman."], "decomposition": ["In the Mario Universe what abilities do mushrooms give to Mario when collected?", "Is any of #1 increased speed?"], "evidence": [[[["Super Mario-33", "Super Mario-35", "Super Mario-36"]], [["Super Mario-33"]]], [[["Goomba-1"]], ["operation"]], [[["Super Mario-33"]], ["operation"]]], "golden_sentence": [["When Mario is in his \"Super\" form, most blocks that would contain a Super Mushroom instead offer a more powerful power-up such as the Fire Flower.", "", "Super Mario 64 DS features an item simply called \"Mushroom\" that grants the same abilities as the Mega Mushroom."], [""]]}, {"qid": "8f11d46b1e260685121d", "term": "Celery", "description": "species of plant", "question": "Can eating your weight in celery prevent diabetes?", "answer": false, "facts": ["Diabetes is a disease in which the body cannot process sugar.", "Celery is known as a healthy snack and has 1 gram of sugar per serving.", "The recommended daily intake of sugar to prevent diabetes is less than 36 grams per day for an adult male.", "The average weight of an adult male is 197 pounds."], "decomposition": ["If a person has diabetes, what is there body unable to process?", "To prevent diabetes, what is the average amount of #1 an adult man should eat daily in grams?", "How much does the average male weigh in pounds?", "How many grams of sugar does a pound of celery have?", "Is #3 times #4 less than #2?"], "evidence": [[[["Diabetes-27", "Diabetes-28"]], [["Diabetic diet-4"], "no_evidence"], [["Human-46"]], ["no_evidence"], [["Celery-28"], "operation"]], [[["Diabetes-1"], "no_evidence"], [["Prevention of type 2 diabetes-2"], "no_evidence"], [["Man-6"], "no_evidence"], [["Celery-41"], "no_evidence"], ["no_evidence", "operation"]], [[["Diabetes-2", "Outline of diabetes-2"]], [["Diabetes-36"]], [["Human-46"]], [["Celery-28"]], ["operation"]]], "golden_sentence": [["", ""], [""], ["The average mass of an adult human is 54\u201364\u00a0kg (119\u2013141\u00a0lb) for females and 70\u201383\u00a0kg (154\u2013183\u00a0lb) for males."], [""]]}, {"qid": "3e6e6ea5bb51010aafe4", "term": "Meatball", "description": "dish made from ground meat rolled into a small ball-like form", "question": "Do restaurants associate meatballs with the wrong country of origin?", "answer": true, "facts": ["Spaghetti and meatballs are a staple on Italian pizzeria menus in the US.", "The Olive Garden, an Italian family restaurant, has several dishes with meatballs.", "Meatballs originated in the Chinese Qin dynasty (221 BC to 207 BC)."], "decomposition": ["In what country is the oldest evidence of people eating meatballs found?", "What dish involving meatballs became popular in the United States after being invented in New York City in the 20th century?", "With which national cuisine do Americans typically associate #2?", "Are #3 and #1 different?"], "evidence": [[[["Meatball-2"]], [["Spaghetti and meatballs-2"]], [["Spaghetti and meatballs-2"]], [["Meatball-2", "Spaghetti and meatballs-2"], "operation"]], [[["Meatball-2"]], [["Meatball-8"]], [["Spaghetti and meatballs-3"]], ["operation"]], [[["Meatball-2"]], [["Spaghetti and meatballs-2"]], [["Spaghetti and meatballs-2"]], ["operation"]]], "golden_sentence": [["The Chinese recipe \"Four Joy Meatballs\" (\u56db\u559c\u4e38\u5b50\u2014S\u00ec x\u01d0 w\u00e1nzi) is derived from Shandong cuisine, which originated in the native cooking styles of Shandong."], ["It is widely believed that spaghetti and meatballs was an innovation of early 20th-century Italian immigrants in New York City, who had access to a more plentiful meat supply than in Italy."], ["It is widely believed that spaghetti and meatballs was an innovation of early 20th-century Italian immigrants in New York City, who had access to a more plentiful meat supply than in Italy."], ["", ""]]}, {"qid": "ac9d2f1784956463de4f", "term": "Central Park Zoo", "description": "Zoo in Central Park, Manhattan, New York City", "question": "Would it be wise to bring a robusto into Central Park Zoo?", "answer": false, "facts": ["A robusto is a short, fat cigar that is very popular in America.", "The Central Park Zoo has several rules including: no feeding the animals and no smoking.", "NYPD's 19th precinct is only an 11 minute walk away from the Central Park Zoo."], "decomposition": ["What is a robusto?", "According to the rules, what can you not bring into The Central Park Zoo?", "Is #1 not in #2?"], "evidence": [[[["Cigar-1"]], [["Central Park Zoo-1", "Smoking ban-10"], "no_evidence"], ["operation"]], [[["Cigar-1"], "no_evidence"], ["no_evidence"], ["operation"]], [[["Cigar-1"], "no_evidence"], [["Passive smoking-61"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], ["", ""]]}, {"qid": "84f613ba0ef0649121bb", "term": "Theocracy", "description": "Form of government with religious leaders", "question": "Were the Great Pyramids built by a theocratic government?", "answer": true, "facts": ["The Great Pyramids were built by the Ancient Egyptians.", "A theocracy is a government that is led by religious leaders or who worships their leader as a god.", "The Pharaohs of Ancient Egypt were worshipped as children of the Sun god Ra."], "decomposition": ["Who were the builders of the Great Pyramids?", "How did #1 serve their leaders?", "Could #2 be described as a theocracy?"], "evidence": [[[["Great Pyramid of Giza-8"]], ["no_evidence"], [["Theocracy-1"]]], [[["Giza pyramid complex-17"]], [["Ancient Egypt-73"], "no_evidence"], [["Theocracy-1"], "operation"]], [[["Giza pyramid complex-1"]], [["Giza pyramid complex-17"]], [["Ancient Egypt-3"], "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "19ccb88d92c668ff7576", "term": "Sesame", "description": "species of plant", "question": "Are sesame seeds glued onto hamburger buns?", "answer": false, "facts": ["Glue is toxic and not used in food production.", "Sesame seeds add texture and visual appeal to hamburger buns.", "Beaten eggwhites are often used to adhere foods to other foods. "], "decomposition": ["What do people usually do with hamburger buns?", "Can you #1 sesame seeds?"], "evidence": [[[["Hamburger-1"]], [["Sesame-1"]]], [[["Bread-1", "Bun-1"]], [["Sesame-2"], "operation"]], [[["Hamburger-1"]], ["no_evidence", "operation"]]], "golden_sentence": [["Hamburgers are often served with cheese, lettuce, tomato, onion, pickles, bacon, or chiles; condiments such as ketchup, mustard, mayonnaise, relish, or \"special sauce\"; and are frequently placed on sesame seed buns."], [""]]}, {"qid": "f9bfc4f4881846737dc4", "term": "Snoop Dogg", "description": "American rapper", "question": "Was Snoop Dogg an adult when Tickle Me Elmo was popular?", "answer": true, "facts": ["Tickle Me Elmo was first popular in 1996.", "Snoop Dogg would have been 25 when the Tickle Me Elmo craze took off."], "decomposition": ["In what year did Tickle Me Elmo become popular?", "In what year was Snoop Dogg born?", "What is the difference between #1 and #2?", "Is #3 greater than or equal to 18?"], "evidence": [[[["Tickle Me Elmo-2"]], [["Snoop Dogg-1"]], ["operation"], ["operation"]], [[["Tickle Me Elmo-2"]], [["Snoop Dogg-1"]], ["operation"], ["operation"]], [[["Tickle Me Elmo-2"]], [["Snoop Dogg-1"]], ["operation"], ["operation"]]], "golden_sentence": [["The toy was first produced in the United States in 1996 and slowly became a fad."], ["Calvin Cordozar Broadus Jr. (born October 20, 1971), better known by his stage name Snoop Dogg, is an American rapper, singer, songwriter, producer, media personality, entrepreneur, and actor."]]}, {"qid": "1ba67bb2fcba4a15b5a2", "term": "James Watson", "description": "American molecular biologist, geneticist, and zoologist", "question": "Did James Watson's partner in studying the double helix outlive him? ", "answer": false, "facts": ["James Watson studied the double helix with Francis Crick.", "Francis Crick passed away in 2004 at 88 years of age.", "James Watson is alive and is 92 years old."], "decomposition": ["Who did James Watson study the double helix with?", "How old was #1 at their death?", "How old is James Watson currently?", "Is #2 greater than #3?"], "evidence": [[[["Francis Crick-1"]], [["Francis Crick-1"]], [["James Watson-1"]], ["operation"]], [[["James Watson-1"]], [["Francis Crick-1"], "no_evidence"], ["no_evidence"], ["operation"]], [[["James Watson-1"]], [["Francis Crick-1"]], [["James Watson-1"]], ["operation"]]], "golden_sentence": [[""], [""], [""]]}, {"qid": "28595a667f1c97853fb4", "term": "Atheism", "description": "Absence of belief in the existence of deities", "question": "Can atheism surpass Christianity in American black communities by 2021?", "answer": false, "facts": ["83% of blacks in the US identify as Christians.", " In the United States, blacks are less likely than other ethnic groups to be religiously unaffiliated, let alone identifying as atheist.", "African American history, slavery and the civil rights movement are all closely tied to Christianity in America."], "decomposition": ["What is the population of black Americans?", "How many out of #1 follow a religion?", "Is #2 close to or less than 50%?"], "evidence": [[[["African Americans-38"]], [["African Americans-104"]], ["operation"]], [["no_evidence"], [["African Americans-103", "African Americans-110"], "no_evidence"], ["operation"]], [[["Americans-19"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["By 1990, the African-American population reached about 30 million and represented 12% of the U.S. population, roughly the same proportion as in 1900."], [""]]}, {"qid": "334a82787461ad4fb509", "term": "Led Zeppelin", "description": "English rock band", "question": "Did the band Led Zeppelin own a prime number of gilded gramophones?", "answer": true, "facts": ["5 is a prime number", "A Grammy Award trophy is a gilded gramophone", "Led Zeppelin won 5 Grammy Awards"], "decomposition": ["What award has a trophy which consists of a gilded gramophone?", "How many #1 have Led Zeppelin won?", "Is #2 a prime number?"], "evidence": [[[["Grammy Award-1"]], [["Led Zeppelin-57"], "no_evidence"], [["Prime number-14"]]], [[["Grammy Award-1"]], [["Led Zeppelin-57"]], ["operation"]], [[["Grammy Award-1"]], [["Led Zeppelin-57"]], [["Prime number-1"]]]], "golden_sentence": [["A Grammy Award (stylized as GRAMMY, originally called Gramophone Award), or Grammy, is an award presented by The Recording Academy to recognize achievements in the music industry."], ["Led Zeppelin were the recipient of a Grammy Lifetime Achievement Award in 2005, and four of their recordings have been inducted into the Grammy Hall of Fame."], [""]]}, {"qid": "38394c4214a4d9f85107", "term": "Quartz", "description": "mineral composed of silicon and oxygen atoms in a continuous framework of SiO\u2084 silicon\u2013oxygen tetrahedra, with each oxygen being shared between two tetrahedra, giving an overall chemical formula of SiO\u2082", "question": "Are Doctors of Homeopathy more likely than Doctors of Internal Medicine to recommend Quartz as a treatment?", "answer": true, "facts": ["Doctors of Homeopathy are practitioners of \"alternative medicine\" ", "In alternative medicine practices, Quartz is believed to have powers.", "Doctors of Internal Medicine have completed a medical residency and do not recommend alternative medicine."], "decomposition": ["What do doctors of homeopathy practice?", "What is Quartz believed to have in #1?", "What do doctors of internal medicine study?", "Are #1 or #2 not included in #3?"], "evidence": [[[["Homeopathy-1"]], [["Crystal healing-1"]], [["Internal medicine-1"]], ["operation"]], [[["Homeopathy-1"]], [["Quartz-1"]], [["Internal medicine-1"]], ["no_evidence", "operation"]], [[["Homeopathy-1"]], [["Crystal healing-1", "Quartz-1"]], [["Internal medicine-1"]], ["operation"]]], "golden_sentence": [["Between the dilution iterations homeopaths practice hitting and/or violently shaking the product, and claim that it makes the diluent remember the original substance after its removal."], ["Adherents of the technique claim that these have healing powers, although there is no scientific basis for this claim."], ["Physicians specializing in internal medicine are called internists, or physicians (without a modifier) in Commonwealth nations."]]}, {"qid": "9cc3a09318aed9fbc2a1", "term": "Rock in Rio", "description": "Brazilian music festival", "question": "Would it be difficult to host Stanley Cup Finals at Rock in Rio?", "answer": true, "facts": ["The Stanley Cup Finals is the last series in hockey each year.", "Hockey rinks are indoors and require ice for players to skate on.", "Rock in Rio is a massive outdoor music festival in Brazil.", "Rock in Rio takes place in June each year.", "The temperature in Brazil during June averages around 80F."], "decomposition": ["Which sport has the Stanley Cup Finals?", "Which kind of surface is required to play #1?", "What time of the year does Rock in Rio take place, and where?", "Do weather conditions in #3 at that time not favor the formation of #2?"], "evidence": [[[["Stanley Cup-1"]], [["Ice hockey-1"]], [["Rock in Rio-1", "Rock in Rio-21"]], ["operation"]], [[["Stanley Cup-1"]], [["Ice hockey-1"]], [["Rock in Rio-1", "Rock in Rio-31"]], [["Brazil-47"], "operation"]], [[["Stanley Cup-1"]], [["Hockey-16"]], [["Rock in Rio-16"]], [["Rio de Janeiro-31"]]]], "golden_sentence": [["It is the oldest existing trophy to be awarded to a professional sports franchise in North America, and the International Ice Hockey Federation (IIHF) considers it to be one of the \"most important championships available to the sport\"."], ["Ice hockey is a contact team sport played on ice, usually in a rink, in which two teams of skaters use their sticks to shoot a vulcanized rubber puck into their opponent's net to score goals."], ["", "Friday 18 September Palco Mundo: Queen + Adam Lambert, OneRepublic, The Script, Rock in Rio 30 Anos Palco Sunset: Tribute to C\u00e1ssia Eller, Lenine + Na\u00e7\u00e3o Zumbi + Martin Fondse, Ira!"]]}, {"qid": "9fd6e38891dccf8ab5d3", "term": "Auburn, New York", "description": "City in New York, United States", "question": "Would the trees in Auburn, New York be changing colors in September?", "answer": true, "facts": ["In New York, fall begins during the month of September. ", "When the weather gets cooler in the fall, leaves begin to change colors. "], "decomposition": ["In the month of September, what season would it be in New York?", "Do trees change color during #1?"], "evidence": [[[["Climate of New York-2"]], [["Autumn leaf color-1"]]], [[["Autumn-1"]], [["Autumn-3"], "operation"]], [[["Autumn-3"]], [["Autumn-3"]]]], "golden_sentence": [["Seasonally, summer-like conditions prevail from June to early September statewide, while areas in far southern New York and New York City have summer conditions from late May through late September."], ["Autumn leaf color is a phenomenon that affects the normal green leaves of many deciduous trees and shrubs by which they take on, during a few weeks in the autumn season, various shades of yellow, orange, red, purple, and brown."]]}, {"qid": "f2e2eae11545956425cd", "term": "Macaque", "description": "genus of Old World monkeys", "question": "Could an elephant easily defeat a male macaque?", "answer": true, "facts": ["Male macaques range from 16 to 28 inches tall with a weight between 12.13 to 39.7 pounds.", "Elephants are between 7 to 11 feet tall and weigh several thousand pounds.", "Elephants contain large, sharp tusks that can injure or kill other animals."], "decomposition": ["How much does a male macaques weigh?", "How much can an elephant weigh?", "How tall is a male macaque?", "How tall is an elephant?", "Is #2 more than #1 and is #4 more than #3?"], "evidence": [[[["Macaque-4"]], [["Elephant-14"]], [["Macaque-4"]], [["Elephant-12"]], ["operation"]], [[["Macaque-4"]], [["Elephant-15"]], [["Macaque-4"]], [["Elephant-15"]], ["operation"]], [[["Macaque-4"]], [["Elephant-15", "Elephantidae-1"], "no_evidence"], [["Macaque-4"]], [["Elephant-15"]], ["operation"]]], "golden_sentence": [["Females can range from a weight of 2.4 to 13\u00a0kg (5.3 to 28.7\u00a0lb)."], ["This species reached a height of 120\u2013180\u00a0cm (3\u00a0ft 11\u00a0in\u20135\u00a0ft 11\u00a0in) and weighed 200\u20132,000\u00a0kg (400\u20134,400\u00a0lb)."], ["Males from all species can range from 41 to 70\u00a0cm (16 to 28 inches) in head and body length, and in weight from 5.5 to 18\u00a0kg (12.13 to 39.7\u00a0lb)."], ["Proboscideans experienced several evolutionary trends, such as an increase in size, which led to many giant species that stood up to 500\u00a0cm (16\u00a0ft 5\u00a0in) tall."]]}, {"qid": "88ead5a1507f1517e47b", "term": "Japanese people", "description": "Ethnic group native to Japan", "question": "Did Pedubastis I know Japanese people?", "answer": false, "facts": ["Pedubastis I was a pharaoh that died in 800 BC", "Japan's origins are in 600 BC according to a letter of the Sui dynasty.", "Pedubastis I ruled over the country of Egypt."], "decomposition": ["When did Pedubastis I die?", "When did the nation of Japan form?", "Is #2 before #1?"], "evidence": [[[["Pedubastis-2"]], [["Graphic pejoratives in written Chinese-14"]], [["Graphic pejoratives in written Chinese-14", "Pedubastis-2"], "operation"]], [[["Pedubastis-2"]], [["Graphic pejoratives in written Chinese-14"]], ["operation"]], [[["Pedubast I-1"], "no_evidence"], [["Japan-9"]], ["operation"]]], "golden_sentence": [["Pedubast I (c. 829 \u2013 804 BCE), a pharaoh of the 23rd Dynasty Pedubast II (8th or 7th century BCE), a ruler of Tanis Pedubast III (c. 522 \u2013 520 BCE), a rebel pharaoh during the 27th Dynasty Pedubast (high steward), official during the 26th Dynasty In addition, various kinglets of the Third Intermediate Period of Egypt bore the name Pedubast."], [""], ["", ""]]}, {"qid": "c52e3734812f8deff0b4", "term": "Drum", "description": "type of musical instrument of the percussion family", "question": "Would a cattle farmer be useful to a drum maker?", "answer": true, "facts": ["Cattle are often slaughtered for meat and other products, like leather.", "Drums are often made with leather."], "decomposition": ["Which animal products would a drum maker need?", "Are #1 commonly obtained from cattle?"], "evidence": [[[["Drumhead-3"], "no_evidence"], [["Leather-1"]]], [[["Drumhead-5"]], ["operation"]], [[["Drum-7"]], [["Drum-7", "Leather-5"]]]], "golden_sentence": [[""], ["The most common raw material is cattle hide."]]}, {"qid": "ed16e2e91adc97b7ffd3", "term": "Jalape\u00f1o", "description": "Hot pepper", "question": "Can children be hurt by jalapeno peppers?", "answer": true, "facts": ["Jalapeno peppers contain capsaicin. ", "Capsaicin creates a burning sensation in the eyes and can lead to surface injuries. ", "Small children do not understand how to protect themselves from peppers or how to wash their hands properly."], "decomposition": ["What do Jalapeno peppers contain?", "Can #1 cause injuries to a child if they are not careful?"], "evidence": [[[["Jalape\u00f1o-15"]], ["no_evidence"]], [[["Capsaicin-1"]], [["Capsaicin-17"], "operation"]], [[["Capsaicin-1"]], [["Capsaicin-17"], "operation"]]], "golden_sentence": [["A raw jalape\u00f1o is 92% water, 6% carbohydrates, 1% protein, and contains negligible fat (table)."]]}, {"qid": "ad0d1f840d6c4a47be62", "term": "Vice President of the United States", "description": "Second highest executive office in United States", "question": "Can Vice President of the United States kill with impunity?", "answer": true, "facts": [" Vice President Aaron Burr fatally wounded Alexander Hamilton in a duel on July 11, 1804.", "Aaron Burr continued his term as Vice President of the United States after killing Alexander Hamilton.", "US stand-your-ground laws allow a person to defend themselves even to the point of applying lethal force."], "decomposition": ["What was the outcome of Vice President Aaron Burr's duel in July, 1804?", "Did#1 lead to loss of his opponent's life and did he continue his term afterwards?"], "evidence": [[[["Aaron Burr-1"]], [["Aaron Burr-34", "Aaron Burr-35"], "operation"]], [[["Burr\u2013Hamilton duel-35"]], [["Aaron Burr-35", "Burr\u2013Hamilton duel-35"], "no_evidence"]], [[["Aaron Burr-4"]], [["Aaron Burr-35", "Aaron Burr-4"]]]], "golden_sentence": [[""], ["", ""]]}, {"qid": "29134df6473e8ca15de5", "term": "Amtrak", "description": "Intercity rail operator in the United States", "question": "Can Amtrak's Acela Express break the sound barrier?", "answer": false, "facts": ["Amtrak's Acela Express is the fastest train in the Western Hemisphere and can reach 150 mph.", "A US Navy plane would need to travel 770 mph to break the sound barrier."], "decomposition": ["What is the maximum speed of Amtrak's Acela Express?", "What is the minimum speed needed to break the sound barrier?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Acela Express-1"]], [["Sound barrier-2"]], ["operation"]], [[["Acela Express-1"]], [["Sound barrier-2"]], ["operation"]], [[["Acela Express-20"]], [["Sonic boom-5"]], [["Counting-14"]]]], "golden_sentence": [["The route contains segments of high-speed rail, and Acela Express trains are the fastest trainsets in the Americas; they attain 150\u00a0mph (240\u00a0km/h) on 33.9\u00a0mi (54.6\u00a0km) of the route."], ["In dry air at 20\u00a0\u00b0C (68\u00a0\u00b0F), the speed of sound is 343 metres per second (about 767\u00a0mph, 1234\u00a0km/h or 1,125\u00a0ft/s)."]]}, {"qid": "99187d8cdc27e48b2d10", "term": "Legend", "description": "Traditional story of heroic humans.", "question": "Are all characters in Legend of Robin Hood fictional?", "answer": false, "facts": ["The Legend of Robin Hood tells of an archer that stole from the rich and gave to the poor.", "Robin Hood's main antagonist is the Sheriff of Nottingham.", "The Sheriff of Nottingham is an agent for Prince John who has usurped the throne from his brother Richard.", "Richard I was King of England from 1189-1199.", "Prince John became John, King of England and reigned from 1199-1216."], "decomposition": ["Who is Robin Hood's main antagonist in the Legend of Robin Hood?", "Who is #1's employer?", "Who is #2's brother who was usurped from the throne by him?", "Are #2 and #3 completely fictional characters?"], "evidence": [[[["Robin Hood-2"]], [["Sheriff of Nottingham-3"]], [["The Legend of Robin Hood-2"]], [["John, King of England-1"], "operation"]], [[["Sheriff of Nottingham-1"]], [["Sheriff of Nottingham-5"]], [["John, King of England-67"]], [["John, King of England-1", "Sheriff of Nottingham-2"]]], [[["Sheriff of Nottingham-1"]], [["Sheriff of Nottingham-5"]], [["The Legend of Robin Hood-2"]], [["Richard I of England-1"], "operation"]]], "golden_sentence": [["These include his lover, Maid Marian, his band of outlaws, the Merry Men, and his chief opponent, the Sheriff of Nottingham."], [""], ["He comes into conflict with a plot to replace King Richard I by his brother Prince John involving the Sheriff of Nottingham and Sir Guy of Gisbourne."], [""]]}, {"qid": "d9b02f2e842ada0dc8de", "term": "Carrot", "description": "Root vegetable, usually orange in color", "question": "Are raw carrots better for maximizing vitamin A intake?", "answer": false, "facts": [" 3% of the \u03b2-carotene in raw carrots is released during digestion, which can be improved to 39% by pulping, cooking and adding cooking oil", "Retinal is a form of Vitamin A", "Human bodies break down \u03b2-carotene into retinal"], "decomposition": ["What is the source of Vitamin A in carrots?", "Is absorption of #1 reduced by cooking?"], "evidence": [[[["Vitamin A-13"]], [["Carrot-35"], "operation"]], [[["Carrot-42"]], ["no_evidence", "operation"]], [[["Carrot-42"]], [["Carotene-9"]]]], "golden_sentence": [["The carotenoid forms (for example, beta-carotene as found in carrots) give no such symptoms, but excessive dietary intake of beta-carotene can lead to carotenodermia, a harmless but cosmetically displeasing orange-yellow discoloration of the skin."], [""]]}, {"qid": "64f9d928e135606c1a0d", "term": "Radioactive waste", "description": "wastes that contain nuclear material", "question": "Is radioactive waste a plot device for many shows?", "answer": true, "facts": ["Radioactive isotopes in an ooze-like waste cause turtles to become the Teenage Mutant Ninja Turtles.", "In the Fox animated hit, Family Guy, radioactive waste is used to turn give the main characters superpowers. ", "The superhero 'Daredevil' encounters radioactive waste that blinds him as a child and gives him super powers."], "decomposition": ["What turned turtles into mutant turtles in Teenage Mutant Ninja Turtles?", "Which substance gives the main characters of Family Guy superpowers?", "Which substance gave Daredevil his super powers?", "Are #1. #2 and #3 radioactive waste?"], "evidence": [[[["Teenage Mutant Ninja Turtles (1990 film)-4"]], [["Griffin family-1"], "no_evidence"], [["Daredevil (Marvel Comics character)-2"]], ["no_evidence"]], [[["Teenage Mutant Ninja Turtles II: The Secret of the Ooze-6"]], ["no_evidence"], [["Alternative versions of Daredevil-33"], "no_evidence"], ["operation"]], [[["Teenage Mutant Ninja Turtles (Mirage Studios)-9"], "no_evidence"], ["no_evidence"], [["Daredevil (Marvel Comics character)-2"]], ["no_evidence", "operation"]]], "golden_sentence": [["Splinter, their rat master, explains to April that he and the turtles were once ordinary animals, but were mutated into intelligent creatures by toxic waste, and trained by Splinter in the art of ninjitsu."], [""], ["While he can no longer see, his exposure to the radioactive material heightens his remaining senses beyond normal human ability, and gives him a \"radar sense.\""]]}, {"qid": "4713a6b4d0d5db3c4495", "term": "Firewall (computing)", "description": "Software or hardware-based network security system", "question": "Could a firewall be destroyed by a hammer?", "answer": false, "facts": ["A firewall is not a physical entity and only exists on a network.", "Hammers cannot be used to destroy non-physical entities."], "decomposition": ["What enables firewall software to work on a computer?", "Can #1 be physically removed from a computer?", "Can a hammer do #2?"], "evidence": [[[["Firewall (computing)-1"]], ["no_evidence"], [["Hammer-1"], "no_evidence"]], [[["Firewall (computing)-1"]], ["no_evidence"], [["Hammer-1"], "no_evidence"]], [[["NPF (firewall)-1"]], ["operation"], [["Hammer-3"]]], [[["Firewall (computing)-1"]], [["Firewall (computing)-13"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "05688d8e9e0fae63bb28", "term": "President of Mexico", "description": "Head of state of the country of Mexico", "question": "Can the President of Mexico vote in New Mexico primaries?", "answer": false, "facts": ["Mexico is an independent country located in  North America.", "New Mexico is a state located in the United States.", "US laws require a voter to be a citizen of the United States.", "The President of Mexico in 2020, Andr\u00e9s Manuel L\u00f3pez Obrador, is a Mexican citizen."], "decomposition": ["What is the citizenship requirement for voting in US states such as New Mexico?", "What is the citizenship requirement of any President of Mexico?", "Is #2 the same as #1?"], "evidence": [[[["Article Two of the United States Constitution-22"]], [["President of Mexico-5"]], ["operation"]], [[["Elections in the United States-7"]], [["President of Mexico-4"], "no_evidence"], ["no_evidence", "operation"]], [[["Voting rights in the United States-101"]], [["President of Mexico-5"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "9afe4d60721b4fead410", "term": "Samsung Galaxy", "description": "series of Android mobile computing devices", "question": "Would the operating system of a Samsung Galaxy 1 sound edible?", "answer": true, "facts": ["The first Samsung Galaxy device ran a version of Android from 2009.", "In 2009, the Android edition was called \"cupcake.\" "], "decomposition": ["What are the operating systems of a Samsung Galaxy 1?", "Does #1 sound like something that is edible?"], "evidence": [[[["Samsung Galaxy S-22"]], ["operation"]], [[["Android Cupcake-1", "Samsung Galaxy (original)-1"]], [["Cupcake-1"], "operation"]], [[["Samsung Galaxy S-26"], "no_evidence"], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "7c7fc86eedb6d024b639", "term": "Metallica", "description": "American heavy metal band", "question": "Does Metallica use Soulseek?", "answer": false, "facts": ["Soulseek is a computer program used to illegally share music files", "Metallica has been outspoken in their opposition of digital piracy"], "decomposition": ["Does Soulseek enable users to illegally share music files?", "Does Metallica encourage illegal sharing of digital files?", "Are the answers to #1 and #2 the same?"], "evidence": [[[["Soulseek-20"]], [["Metallica-29"]], ["operation"]], [[["Soulseek-22"]], [["Metallica v. Napster, Inc.-1"]], ["operation"]], [[["Soulseek-1"]], [["Metallica-3"]], ["operation"]]], "golden_sentence": [["Soulseek claims to be against copyright violation and that the purpose of their service is to promote unsigned artists."], ["Legal action was initiated against Napster; Metallica filed a lawsuit at the U.S. District Court, Central District of California, alleging that Napster violated three areas of the law: copyright infringement, unlawful use of digital audio interface device, and the Racketeer Influenced and Corrupt Organizations Act (RICO)."]]}, {"qid": "577eaa0c83251e646569", "term": "Monk", "description": "member of a monastic religious order", "question": "Are monks forbidden from engaging in warfare?", "answer": false, "facts": ["Monks are members of religious orders that usually take vows of poverty, chastity, and obedience.", "The Knights Templar were a religious order that fought during the Crusades and captured Jerusalem in 1099.", "Buddhist Shaolin monks developed very powerful martial arts skills, have defended temples during conquests."], "decomposition": ["What role did the Knights Templar play during the Crusades?", "What role have Shaolin monks played at temples during conquests?", "Did #1 or #2 not involve warfare?"], "evidence": [[[["Knights Templar-2"]], [["Shaolin Kung Fu-9"], "no_evidence"], ["operation"]], [[["Knights Templar-10"]], [["Shaolin Kung Fu-9"]], ["operation"]], [[["Knights Templar-2"]], [["Shaolin Monastery-11"]], ["no_evidence"]]], "golden_sentence": [["Templar knights, in their distinctive white mantles with a red cross, were among the most skilled fighting units of the Crusades."], [""]]}, {"qid": "2bb313cf26007cbe9a7f", "term": "Tibia", "description": "larger of the two bones of the leg below the knee for vertebrates", "question": "Can a minotaur hypothetically injure a tibia playing football?", "answer": true, "facts": ["A minotaur is a mythological creature with the head of a bull and the body of a human.", "The tibia is the bone between the knee and the ankle in humans.", "Tibia injuries are common injuries in contact sports such as football."], "decomposition": ["What is the body structure of a Minotaur?", "Where in the human body is the tibia located?", "Does #1 account for the presence of #2 in a Minotaur?", "Are injuries to #2 common during football?", "Are #3 and #4 positive"], "evidence": [[[["Minotaur-1"]], [["Tibia-1"]], ["operation"], [["Running injuries-1"], "no_evidence"], ["operation"]], [[["Minotaur-1"]], [["Tibia-1"]], ["operation"], [["Anterior cruciate ligament injury-2", "Intercondylar area-6"]], ["operation"]], [[["Minotaur-1"]], [["Tibia-1"]], ["operation"], [["Shin splints-1", "Shin splints-2"], "no_evidence"], ["operation"]]], "golden_sentence": [["In Greek mythology, the Minotaur (Ancient Greek: \u039c\u03b9\u03bd\u03ce\u03c4\u03b1\u03c5\u03c1\u03bf\u03c2 [mi\u02d0n\u0254\u030c\u02d0tau\u032fros]; in Latin as Minotaurus [mi\u02d0no\u02d0\u02c8tau\u032fr\u028as]) is a mythical creature portrayed in Classical times with the head and tail of a bull and the body of a man or, as described by Roman poet Ovid, a being \"part man and part bull\"."], ["The tibia is found on the medial side of the leg next to the fibula and closer to the median plane or centre-line."], ["These are often the result of overuse."]]}, {"qid": "b30b1694e7fd91322b6a", "term": "Portuguese Empire", "description": "Global empire centered in Portugal", "question": "Did Columbus obtain his funding from the rulers of the Portugese Empire?", "answer": false, "facts": [" King Ferdinand and Queen Isabella funded Columbus' voyage to the New World.", "King Ferdinand of Argon and Queen Isabella of Castille were the joint rulers of kingdoms of the Iberian Peninsula, which included modern-day Spain but excludes Portugal. ", "King John II of Portugal rejected Columbus' request for funding. "], "decomposition": ["Which major voyage did Columbus require funding to embark upon?", "Who funded #1?", "Which kingdoms did #2 rule over?", "Is the Portuguese Empire included in #3?"], "evidence": [[[["Voyages of Christopher Columbus-62"]], [["Voyages of Christopher Columbus-6"]], [["The empire on which the sun never sets-12"]], ["operation"]], [[["Christopher Columbus-1"]], [["Christopher Columbus-2"]], [["Isabella I of Castile-1"]], ["operation"]], [[["Voyages of Christopher Columbus-7"]], [["Voyages of Christopher Columbus-12"]], [["Voyages of Christopher Columbus-9"]], [["Portuguese Empire-4"], "operation"]]], "golden_sentence": [[""], ["Columbus had previously failed to convince King John II of Portugal to fund his exploration of a western route, but the new king and queen of the re-conquered Spain decided to fund Columbus's expedition in hopes of bypassing Portugal's lock on Africa and the Indian Ocean, reaching Asia by traveling west."], [""]]}, {"qid": "1a883f5a0bd80ea6cb38", "term": "Shrimp", "description": "Decapod crustaceans", "question": "Do shrimp taste best when cooked for a long time?", "answer": false, "facts": ["Shrimp becomes tough and rubbery if cooked for a long time.", "The ideal texture for shrimp is soft and easily chewed."], "decomposition": ["What happens when shrimp is cooked for a long time?", "What is the ideal texture for shrimp?", "Are #1 and #2 the same?"], "evidence": [[[["Shrimp and prawn as food-11"], "no_evidence"], [["Shrimp and prawn as food-8"], "no_evidence"], ["operation"]], [[["Shrimp and prawn as food-10"]], [["Longjing prawns-1"]], ["operation"]], [[["Shrimp and prawn as food-9"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["There's pineapple shrimp, lemon shrimp, coconut shrimp, pepper shrimp, shrimp soup, shrimp stew, shrimp salad, shrimp and potatoes, shrimp burger, shrimp sandwich... American soldier \"Bubba\" in Vietnam, in the 1994 romantic-comedy film Forrest Gump Removing the \"sand vein\" (a euphemism for the digestive tract) is referred to as \"deveining\"."]]}, {"qid": "d935cafaad6ed07b5c66", "term": "Birdwatching", "description": "hobby", "question": "Would a birdwatcher pursue their hobby at a Philadelphia Eagles game?", "answer": false, "facts": ["Birdwatching is a recreational activity in which people observe and/or listen to the sounds of birds.", "Despite their name, the Philadelphia Eagles are a professional American Football team comprised of humans, not birds."], "decomposition": ["What is a birdwwatcher interested in watching?", "What kind of sport does the Philadelphia eagles play?", "Can #1 be found at #2?"], "evidence": [[[["Birdwatching-8"], "no_evidence"], [["Philadelphia Eagles-1"], "no_evidence"], ["operation"]], [[["Birdwatching-1"]], [["Philadelphia Eagles-1"]], [["American football-1"]]], [[["Birdwatching-1"]], [["Philadelphia Eagles-1"]], ["operation"]]], "golden_sentence": [["The early interest in observing birds for their aesthetic rather than utilitarian (mainly food) value is traced to the late 18th century in the works of Gilbert White, Thomas Bewick, George Montagu and John Clare."], ["The Philadelphia Eagles are a professional American football team based in Philadelphia."]]}, {"qid": "345421e0e2964f5adc1a", "term": "Cantonese", "description": "Standard dialect of Yue language that originated in the vicinity of Guangzhou (Canton) in southern China", "question": "Is Cantonese spoken in Japan?", "answer": false, "facts": ["Cantonese is a dialect of Chinese language used in southern China.", "There is no relation to the Japanese language."], "decomposition": ["Where is Cantonese widely spoken?", "Is Japan included in #1?"], "evidence": [[[["Cantonese-1"]], [["Japan-1"], "operation"]], [[["Cantonese-2"]], ["operation"]], [[["Cantonese-11", "Cantonese-16"]], ["operation"]]], "golden_sentence": [["Cantonese is a variety of Chinese originating from the city of Guangzhou (also known as Canton) and its surrounding area in Southeastern China."], ["Part of the Pacific Ring of Fire, Japan encompasses an archipelago of 6,852 islands; the five main islands, from north to south, are Hokkaido, Honshu, Shikoku, Kyushu and Okinawa."]]}, {"qid": "30a43b4248268a599a8d", "term": "Ape", "description": "superfamily of mammals", "question": "Do ants outperform apes on language ability?", "answer": false, "facts": ["Language involves grammar and vocabulary", "Ants have not shown any understanding of grammar", "Apes include humans who use language to communicate"], "decomposition": ["What faculties are required for language?", "Which of #1 do ants possess?", "Which of #1 do apes possess?", "Does #2 have more overlap with #1 than #3 does?"], "evidence": [[[["Larynx-18"]], ["no_evidence", "operation"], ["no_evidence"], ["operation"]], [[["Language-1"]], [["Ant-28"]], [["Gorilla-28"]], [["Ant-28", "Gorilla-28"]]], [[["Language-10"]], ["no_evidence"], [["Ape-26"], "no_evidence"], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "a3eb02fb2ee0cc4ca932", "term": "Tonsure", "description": "hairstyle related to religious devotion", "question": "Would Christopher Hitchens be very unlikely to engage in tonsure?", "answer": true, "facts": ["Tonsure is the practice of cutting or shaving some or all of the hair on the scalp as a sign of religious devotion or humility.", "Christopher Hitchens was an anti-theist, and he regarded all religions as false, harmful, and authoritarian."], "decomposition": ["What were Christopher Hitchens' views on religion?", "What is the purpose of tonsure?", "Would a proponent of #1 have a negative opinion of #2?"], "evidence": [[[["Christopher Hitchens-2"]], [["Tonsure-1"]], ["operation"]], [[["Christopher Hitchens-2"]], [["Tonsure-1"]], ["operation"]], [[["Christopher Hitchens-32"]], [["Tonsure-5"]], [["Christopher Hitchens-33"]]]], "golden_sentence": [["He argued in favour of free expression and scientific discovery, and that it was superior to religion as an ethical code of conduct for human civilization."], ["Tonsure (/\u02c8t\u0252n\u0283\u0259r/) is the practice of cutting or shaving some or all of the hair on the scalp as a sign of religious devotion or humility."]]}, {"qid": "eb09b7d29e48319b2f89", "term": "Menthol", "description": "chemical compound", "question": "Is menthol associated with Christmas?", "answer": true, "facts": ["Menthol is the chemical in mint products that give mint its characteristic cool and tangy taste.", "Peppermint is a popular candy flavor during Christmas season."], "decomposition": ["What is a popular candy flavor during Christmas?", "Is menthol an ingredient in #1?"], "evidence": [[[["Candy cane-1"]], [["Peppermint-2"]]], [[["Candy cane-1"]], [["Menthol-1"]]], [[["Candy cane-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["It is traditionally white with red stripes and flavored with peppermint, but they also come in a variety of other flavors and colors."], [""]]}, {"qid": "6ab5b383cbc62810820f", "term": "Snoop Dogg", "description": "American rapper", "question": "Does Snoop Dogg advocate a straight edge lifestyle?", "answer": false, "facts": ["A straight edge lifestyle requires abstaining from the usage of recreational drugs or alcohol.", "Snoop Dogg is famous for his chronic usage of marijuana."], "decomposition": ["What is the position of the straight edge advocates on recreational drugs?", "What is Snoop Dogg's position on recreational drugs?", "Is #1 the same as #2?"], "evidence": [[[["Straight edge-1"]], [["Snoop Dogg-89"]], ["operation"]], [[["Straight edge-1"]], [["Snoop Dogg-68"]], ["operation"]], [[["Straight edge-1"]], [["Snoop Dogg-87", "Snoop Dogg-88", "Snoop Dogg-89"]], ["operation"]]], "golden_sentence": [["Straight edge (sometimes abbreviated sXe or signified by XXX or X) is a subculture of hardcore punk whose adherents refrain from using alcohol, tobacco and other recreational drugs, in reaction to the excesses of punk subculture."], [""]]}, {"qid": "44152ecbd86680edc147", "term": "Gorilla", "description": "Genus of mammals", "question": "Is it expected that Charla Nash would be anxious near a gorilla?", "answer": true, "facts": ["In 2009, Charla Nash was attacked and nearly killed by a chimpanzee. ", "While a different species, Gorillas and Chimpanzees have similar physical appearances and are both primates."], "decomposition": ["Which animal attacked Charla Nash in 2009?", "Does #1 bear significant similarity to a gorilla?"], "evidence": [[[["Travis (chimpanzee)-8"]], ["operation"]], [[["Travis (chimpanzee)-8"]], [["Hominidae-1"]]], [[["Travis (chimpanzee)-8"]], [["Gorilla-1"], "operation"]]], "golden_sentence": [["On February 16, 2009, at around 3:40 p.m., Travis attacked Sandra Herold's 55-year-old friend Charla Nash, inflicting devastating injuries to her face and limbs."]]}, {"qid": "14b9759de43dd1877093", "term": "Oyster", "description": "salt-water bivalve mollusc", "question": "Should oysters be avoided by people with ADHD?", "answer": false, "facts": ["Oysters are an excellent source of zinc.", "ADHD is  a mental disorder of the neurodevelopmental type characterized by difficulty paying attention.", "Zinc supplementation has been reported to improve symptoms of ADHD and depression."], "decomposition": ["Which metal are oysters are known to be an excellent source of?", "Is the consumption of #1 known to worsen symptoms of ADHD?"], "evidence": [[[["Oyster-50"]], [["Attention deficit hyperactivity disorder-21", "Attention deficit hyperactivity disorder-59"], "no_evidence"]], [[["Oyster-50"]], [["Attention deficit hyperactivity disorder-59"], "operation"]], [[["Oyster-50"]], [["Attention deficit hyperactivity disorder-59"]]]], "golden_sentence": [["Oysters are an excellent source of zinc, iron, calcium, and selenium, as well as vitamin A and vitamin B12."], ["", ""]]}, {"qid": "140cd895b5d9b97c741d", "term": "KFC", "description": "American fast food restaurant chain", "question": "Does Magnus Carlsen enjoy KFC?", "answer": false, "facts": ["Magnus Carlsen is a chess grandmaster from Norway", "There are no KFC locations in Norway"], "decomposition": ["What country is Magnus Carlsen from?", "In what countries does KFC have a location?", "Is #1 included in #2?"], "evidence": [[[["Magnus Carlsen-1"]], [["KFC-3"]], ["operation"]], [[["Magnus Carlsen-1"]], [["KFC-1"], "no_evidence"], ["no_evidence"]], [[["Magnus Carlsen-1"]], [["KFC-51"], "no_evidence"], ["operation"]]], "golden_sentence": [["Sven Magnus \u00d8en Carlsen (Norwegian:\u00a0[sv\u025bn \u02c8m\u0251\u0300\u014bn\u0289s \u00f8\u02d0n \u02c8k\u0251\u02d0\u026dsn\u0329]; born 30 November 1990) is a Norwegian chess grandmaster who is the current World Chess Champion, World Rapid Chess Champion, and World Blitz Chess Champion."], ["KFC was one of the first American fast-food chains to expand internationally, opening outlets in Canada, the United Kingdom, Mexico, and Jamaica by the mid-1960s."]]}, {"qid": "d9368bce30a4d822934e", "term": "Mongols", "description": "ethnic group of central Asia", "question": "Would a packed Wembley stadium be likely to have a descendant of the Mongols inside?", "answer": true, "facts": ["Wembley stadium has a capacity of 90,000 people.", "The Mongols were an ethnic group that dominated the 13th and 14th centuries.", "Genghis Khan was the founder of the Mongol Empire.", "Geneticists have determined that 1 in every 200 men are descended from Genghis Khan."], "decomposition": ["What is the capacity of the Wembley stadium?", "Who is the founder of the Mongol empire?", "What is the minimum number of men within which at least one descendant of #2 is found?", "Is #1 divided by #3 greater than or equal to one?"], "evidence": [[[["Wembley Stadium-2"]], [["Mongol Empire-2"]], ["no_evidence"], ["operation"]], [[["Wembley Stadium-2"]], [["Mongol Empire-2"]], [["Descent from Genghis Khan-22"], "no_evidence"], ["no_evidence", "operation"]], [[["Wembley Stadium-2"]], [["Mongol Empire-2"]], [["Descent from Genghis Khan-22"]], ["operation"]]], "golden_sentence": [["With 90,000 seats, it is the largest football stadium in England, the largest stadium in the UK and the second-largest stadium in Europe."], ["The Mongol Empire emerged from the unification of several nomadic tribes in the Mongol homeland under the leadership of Genghis Khan (c.\u2009 1162\u20131227), whom a council proclaimed as the ruler of all Mongols in 1206."]]}, {"qid": "aa40ae6a77c173b007d5", "term": "Allosaurus", "description": "Genus of large theropod dinosaur", "question": "Is Oculudentavis more dangerous than Allosaurus?", "answer": false, "facts": ["Oculudentavis was a dinosaur that resembled a tiny bird with a half an inch skull.", "The Allosaurus was a carnivorous dinosaur with teeth described as saws."], "decomposition": ["What were the characteristics of the Oculudentavis?", "What were the characteristics of the Allosaurus?", "Are #1 more likely to cause harm than #2?"], "evidence": [[[["Oculudentavis-3"]], [["Allosaurus-2"]], ["operation"]], [[["Oculudentavis-1"]], [["Allosaurus-2"]], ["operation"]], [[["Oculudentavis-4"]], [["Allosaurus-3"]], ["operation"]]], "golden_sentence": [["The skull of Oculudentavis was 1.4 centimetres (0.55\u00a0in) in length, indicating that Oculudentavis would be the smallest known Mesozoic dinosaur, if such an identification is correct."], ["Allosaurus was a large bipedal predator."]]}, {"qid": "c85ba37e0f04159ecd2d", "term": "Smooth jazz", "description": "category of music", "question": "Would James Cotton's instrument be too strident for a smooth jazz band?", "answer": true, "facts": ["Smooth jazz is an offshoot of jazz music that relies on a more melodic form.", "Smooth jazz employs the following instruments: saxophone. guitar. piano. trumpet. synthesizer. electric bass. and drums.", "James Cotton was a famous blues harmonica player."], "decomposition": ["What instument is James Cotton known for?", "What instuments are used to play Smooth Jazz?", "Is #1 not  one of #2?"], "evidence": [[[["James Cotton-1"]], [["Smooth jazz-2"], "no_evidence"], ["operation"]], [[["James Cotton-1"]], [["Smooth jazz-3"], "no_evidence"], ["operation"]], [[["James Cotton-1"]], [["Musical ensemble-15"], "no_evidence"], ["operation"]]], "golden_sentence": [["James Henry Cotton (July 1, 1935 \u2013 March 16, 2017) was an American blues harmonica player, singer and songwriter, who performed and recorded with many of the great blues artists of his time and with his own band."], ["Much of the music was initially \"a combination of jazz with easy-listening pop music and lightweight R&B\"."]]}, {"qid": "d92b7c809b069c6b9166", "term": "United Airlines", "description": "Airline in the United States", "question": "Are there tearjerkers about United Airlines flights?", "answer": true, "facts": ["Tearjerkers typically refer to a genre of movie. ", "United Airlines flight 93 was involved in a terrorist attack in 2001.", "Several flights memorialize the passengers of Flight 93,."], "decomposition": ["What do tearjerkers refer to?", "Which United Airlines flight was involved in a terrorist attack in 2001?", "Are there any #1 in memory of the passengers of #2?"], "evidence": [[[["Melodrama-1"], "no_evidence"], [["September 11 attacks-2"]], [["United 93 (film)-1"], "no_evidence", "operation"]], [[["Melodrama-1"]], [["American Airlines Flight 11-1", "American Airlines Flight 77-1", "United Airlines Flight 175-1", "United Airlines Flight 93-1"]], ["no_evidence", "operation"]], [[["Tearjerker-1"]], [["United Airlines Flight 811-29"]], [["United Airlines Flight 811-29"]]]], "golden_sentence": [[""], ["Two of the planes, American Airlines Flight 11 and United Airlines Flight 175, crashed into the North and South towers, respectively, of the World Trade Center complex in Lower Manhattan."], [""]]}, {"qid": "74d1211365c95cabf67c", "term": "Rabbi", "description": "teacher of Torah in Judaism", "question": "Would a rabbi worship martyrs Ranavalona I killed?", "answer": false, "facts": ["Rabbis are teachers of Judaism.", "Ranavalona I, ruler of Madagascar, killed many Christians that were later determined by the church to be martyrs.", "Judaism does not have a group of saints and martyrs that are prayed to like Christianity.."], "decomposition": ["Which religion are rabbis teachers of?", "Which religion were the matyrs killed by Ranavalona I adherents of?", "Do adherent of #1 worship matyrs like those of #2?"], "evidence": [[[["Rabbi-1"]], [["Christianity in Madagascar-13"]], ["operation"]], [[["Rabbi-1"]], [["Christianity in Madagascar-13"]], ["operation"]], [[["Rabbi-1"]], [["Christianity in Madagascar-13"]], ["no_evidence"]]], "golden_sentence": [["A rabbi is a spiritual leader or religious teacher in Judaism."], ["Persecution of Christians intensified; in 1849, 1,900 people were fined, jailed, or otherwise punished for their Christian faith, of whom 18 were executed."]]}, {"qid": "58e51066d24ed64113e8", "term": "Mercury (element)", "description": "Chemical element with atomic number 80", "question": "Does Mercury help detect coronavirus?", "answer": true, "facts": ["Mercury is used in thermometers", "Thermometers are used in taking body temperature", "High temperature or fever is one symptom of coronavirus"], "decomposition": ["What are the basic symptoms of coronavirus?", "Which instrument is used to measure a symptom among #1 that can be measured?", "Does a type of #2 use mercury?"], "evidence": [[[["Coronavirus-26"]], [["Fever-1", "Medical thermometer-1"]], [["Medical thermometer-24"]]], [[["Coronavirus-26"]], [["Thermometer-1"]], ["operation"]], [[["Human coronavirus NL63-4"]], [["Fever-7", "Medical thermometer-12"]], [["Thermometer-44"]]]], "golden_sentence": [["Coronaviruses can cause colds with major symptoms, such as fever, and a sore throat from swollen adenoids."], ["", "A medical thermometer (also called clinical thermometer) is used for measuring human or animal body temperature."], [""]]}, {"qid": "065a4e16464629469453", "term": "The Jackson 5", "description": "American pop music family group", "question": "Was The Jackson 5 bigger family band than The Isley Brothers?", "answer": true, "facts": ["The Jackson 5 has sold over 100 million albums worldwide.", "The Eisley Brothers have sold over 18 million albums.", "The Jackson 5 consisted of Jackie, Tito, Jermaine, Marlon and Michael.", "The Isley Brothers consisted of brothers O'Kelly, Rudolph. Ronald, and Vernon."], "decomposition": ["How many albums has the Jackson 5 sold?", "How many albums has the Eisley Brothers sold?", "How many people were in the Jackson 5?", "How many people made up the Eisley Brothers?", "Is #1 greater than #2 and is #3 greater than #4?"], "evidence": [[[["The Jackson 5-4"]], [["The Isley Brothers-5"]], [["Ronnie Rancifer-1"]], [["The Isley Brothers-4"]], ["operation"]], [[["The Jackson 5-4"]], [["The Isley Brothers-5"]], [["The Jackson 5-1"]], [["The Isley Brothers-1"]], ["operation"]], [[["The Jackson 5-4"]], [["The Isley Brothers-5"]], [["The Jackson 5-1"]], [["The Isley Brothers-4"]], ["operation"]]], "golden_sentence": [["The Jackson 5 have sold more than 100 million records worldwide, making them one of the best-selling bands of all time."], ["The Isley Brothers have sold over 18 million units in the United States alone."], [""], ["The six-member band splintered in 1983, with Ernie, Marvin, and Chris Jasper forming the short-lived spinoff group Isley-Jasper-Isley."]]}, {"qid": "d011ee6187b9c0b57372", "term": "Ginger", "description": "Species of plant", "question": "Does a Starbucks passion tea have ginger in it?", "answer": false, "facts": ["Starbucks Passion tea features cinnamon, apple, licorice root, and lemongrass flavors.", "Ginger is a spicy flavored plant.", "Starbucks Passion tea is a sweet drink."], "decomposition": ["What ingredients are in the Starbucks Passion Tea?", "Is ginger part of #1?"], "evidence": [[[["Starbucks-19"], "no_evidence"], ["operation"]], [["no_evidence"], ["no_evidence"]], [["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""]]}, {"qid": "f4b514b431ccef9065a1", "term": "Ammonia", "description": "Chemical compound of nitrogen and hydrogen", "question": "Do Shivambu practitioners believe ammonia is unhealthy?", "answer": false, "facts": ["Shivambu is another term for 'Urine Therapy', an alternative belief about healing with urine.", "Human urine contains ammonia.", "Shivambu practitioners believe that ingesting urine is healthy."], "decomposition": ["What is another term for Shivambu?", "What did #1 believe could have healing properties?", "Is #2 void of ammonia?"], "evidence": [[[["Urine therapy-1"]], [["Urine therapy-1"]], [["Urine therapy-5"], "operation"]], [[["Urine therapy-1"]], [["Urine therapy-2"]], [["Urine-32"], "operation"]], [[["Urine therapy-1"]], [["Urine therapy-1"]], [["Ammonia-32", "Ammonia-32", "Ammonia-90"], "no_evidence"]]], "golden_sentence": [["In alternative medicine, urine therapy or urotherapy, (also urinotherapy, Orin Therapy, Shivambu, uropathy, or auto-urine therapy) is the application of human urine for medicinal or cosmetic purposes, including drinking of one's own urine and massaging one's skin, or gums, with one's own urine."], [""], [""]]}, {"qid": "71712be9f5a6c53362aa", "term": "Grapefruit", "description": "citrus fruit", "question": "Can eating grapefruit kill besides allergies or choking?", "answer": true, "facts": ["Grapefruit is a citrus fruit consumed mostly during the summer months.", "Chemicals in grapefruit can interact with medications such as statins.", "Grapefruit can lead to too much absorption of statin medicine.", "Too much statins can cause severe muscle pain, liver damage, kidney failure and death. "], "decomposition": ["What health risks associated with eating grapefruit could lead to death?", "Is #1 more than just allergy and choking?"], "evidence": [[[["Grapefruit-16"]], [["Grapefruit-16"]]], [[["Grapefruit-18"]], [["Drug overdose-1"]]], [[["Grapefruit\u2013drug interactions-3"]], ["operation"]]], "golden_sentence": [["If the drug's breakdown for removal is lessened, then the level of the drug in the blood may become too high or stay too long, leading to adverse effects."], [""]]}, {"qid": "428a956ad447c55fd68c", "term": "Jealousy", "description": "emotion referring to the thoughts and feelings of insecurity, fear, and envy over relative lack of possessions, status or something of great personal value", "question": "Should someone prone to jealousy be in a polyamorous relationship?", "answer": false, "facts": ["Polyamorous people are those who seek to have an intimate relationship with more than one partner.", "In relationships, untreated jealousy typically leads to a breakup."], "decomposition": ["What kind of relationship would a polyamorous person engage in?", "Would a jealous person be comfortable with #1?"], "evidence": [[[["Polyamory-10"]], [["Polyamory-22"]]], [[["Polyamory-1"]], [["Jealousy-3", "Jealousy-32"], "operation"]], [[["The Industrial Christian Home for Polygamous Wives-3"], "no_evidence"], ["no_evidence"]]], "golden_sentence": [["Ideally, a partner's partners are accepted as part of that person's life rather than merely tolerated, and usually a relationship that requires deception or a \"don't ask don't tell\" policy is seen as a less than ideal model."], ["Commentary: Compersion can be thought of as the opposite of \u201cjealousy;\u201d it is a positive emotional reaction to a lover\u2019s other relationship.\""]]}, {"qid": "ea79f13a0d92f8e4f811", "term": "B", "description": "letter in the Latin alphabet", "question": "Would early Eastern Canadian Natives language have use of the letter B?", "answer": false, "facts": ["The Early Eastern Canadian Natives were a group of people that spoke the Inuktitut language.", "The Inuktitut language began as an oral language with no letters, only uvular sounds.", "The later Inuktitut language has no letters that resemble the Latin alphabet."], "decomposition": ["What language did Eastern Canadian Natives speak?", "What kind of language is #1?", "Does #2 involve the use of letters?"], "evidence": [[[["Inuktitut-1"]], [["Inuktitut-30"]], [["Syllabary-1"]]], [[["M\u00e9tis-1"], "no_evidence"], ["no_evidence"], [["Indigenous peoples in Canada-59"], "no_evidence", "operation"]], [[["Inuktitut-1"]], [["Inuktitut-1"]], [["Inuktitut-1"], "operation"]]], "golden_sentence": [["Inuktitut (/\u026a\u02c8n\u028akt\u026at\u028at/; Inuktitut:\u00a0[inukti\u02c8tut], syllabics \u1403\u14c4\u1483\u144e\u1450\u1466; from inuk, \"person\" + -titut, \"like\", \"in the manner of\"), also Eastern Canadian Inuktitut, is one of the principal Inuit languages of Canada."], [""], [""]]}, {"qid": "06d7e148f6c6849917ac", "term": "Advertising", "description": "Form of communication for marketing, typically paid for", "question": "During the pandemic, is door to door advertising considered inconsiderate?", "answer": true, "facts": ["Door to door advertising involves someone going to several homes in a residential area to make sales and leave informational packets.", "During the COVID-19 pandemic, the CDC recommends that people limit their travel to essential needs only.", "During the COVID-19 pandemic, citizens are advised to stay home and to limit their interaction with others.", "During the COVID-19 pandemic, people are encouraged to remain six feet away from each other at all times.", "The more people that someone interacts with, the higher the likelihood of them becoming a vector for the COVID-19 virus."], "decomposition": ["What does door to door advertising involve a person to do?", "During the COVID-19 pandemic, what does the CDC advise people to do in terms of traveling?", "During the COVID-19 pandemic, what does the CDC advise people to do in terms of interaction with others?", "Does doing #1 go against #2 and #3?"], "evidence": [[[["Door-to-door-1"]], [["Cloth face mask-12"]], ["no_evidence"], ["operation"]], [[["Door-to-door-1"]], [["Coronavirus recession-13"]], [["Social distancing-30"]], ["operation"]], [[["Door-to-door-1"]], [["Stay-at-home order-18"], "no_evidence"], [["Social distancing-1"]], ["operation"]]], "golden_sentence": [["Door-to-door is a canvassing technique that is generally used for sales, marketing, advertising, or campaigning, in which the person or persons walk from the door of one house to the door of another, trying to sell or advertise a product or service to the general public or gather information."], ["The U.S. Centers for Disease Control and Prevention (CDC) in March 2020 recommended that if neither respirators nor surgical masks are available, as a last resort, it may be necessary for healthcare workers to use masks that have never been evaluated or approved by NIOSH or homemade masks, though caution should be exercised when considering this option."]]}, {"qid": "64917f17428f00c7a5e8", "term": "Mickey Mouse", "description": "Disney cartoon character", "question": "Is Mickey Mouse hypothetically unlikely to make a purchase at Zazzle?", "answer": true, "facts": ["Mickey Mouse is a Disney character that has starred in numerous movies and TV specials.", "Mickey Mouse wears a pair of red pants and never wears a shirt.", "Zazzle is a website that specializes in custom T-shirts."], "decomposition": ["What clothing pieces does Micky Mouse typically wear?", "What clothing pieces does Zazzle specialize in?", "Is there no overlap between #1 and #2?"], "evidence": [[[["Mickey Mouse-49"]], [["Zazzle-1"], "no_evidence"], ["operation"]], [[["Mickey Mouse-1"]], [["Zazzle-3"]], [["Mickey Mouse-1", "Zazzle-1"]]], [[["Mickey Mouse-49"]], [["Zazzle-1"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["When Mickey is not wearing his red shorts, he is often still wearing red clothing such as a red bandmaster coat (The Band Concert, The Mickey Mouse Club), red overalls (Clock Cleaners, Boat Builders), a red cloak (Fantasia, Fun and Fancy Free), a red coat (Squatter's Rights, Mickey's Christmas Carol), or a red shirt (Mickey Down Under, The Simple Things)."], ["Zazzle is an American online marketplace that allows designers and customers to create their own products with independent manufacturers (clothing, posters, etc."]]}, {"qid": "83480898570828e5e7d7", "term": "Linus Torvalds", "description": "Creator and lead developer of Linux kernel", "question": "Is Maruti Suzuki Baleno an efficient car for Linus Torvald's family?", "answer": true, "facts": ["Linus Torvald has a family consisting of five people including his wife and children.", "The Maruti Suzuki Baleno is and Indian car that can seat five people."], "decomposition": ["How many people can sit in a  Maruti Suzuki Baleno?", "How many people are in Linus Torvald's family?", "Is #1 at least equal or greater than #2?"], "evidence": [[[["Suzuki Baleno (2015)-4"]], [["Linus Torvalds-20"]], ["operation"]], [[["Suzuki Baleno (2015)-14"], "no_evidence"], [["Linus Torvalds-20"]], ["no_evidence", "operation"]], [["no_evidence"], [["Linus Torvalds-20"]], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "6489e8ee3b0d1baeb2be", "term": "Benjamin Franklin", "description": "American polymath and a Founding Father of the United States", "question": "Is Benjamin Franklin a prime candidate to have his statues removed by Black Lives Matter movement?", "answer": true, "facts": ["The Black Lives Matter movement is a social movement advocating for racial equality.", "Benjamin Franklin, a famous founding father, has his image on many monuments and on American currency.", "Members of the Black Lives Matter movement petitioned for statues of Christopher Columbus to be removed due to his subjugation of Native Americans.", "Benjamin Franklin's 1730s newspaper, The Philadelphia Gazette, posted ads for black slaves.", "Benjamin Franklin owned two slaves, George and King, who worked as personal servants."], "decomposition": ["What social issue motivates the Black Lives Matter movement?", "Did Benjamin Franklin act against achieving #1?"], "evidence": [[[["Black Lives Matter-10"]], [["Benjamin Franklin-135", "Benjamin Franklin-136"]]], [[["Black Lives Matter-14"], "no_evidence"], [["Benjamin Franklin-5"], "operation"]], [[["Black Lives Matter-10"]], [["Benjamin Franklin-5"], "operation"]]], "golden_sentence": [["The phrase \"Black Lives Matter\" can refer to a Twitter hashtag, a slogan, a social movement, or a loose confederation of groups advocating for racial justice."], ["", ""]]}, {"qid": "b3245b4306999b19c121", "term": "Pharmacy", "description": "academic discipline studying preparation and dispensation of medicinal", "question": "Is ID required to get all medications from all pharmacies?", "answer": false, "facts": ["Controlled substance prescriptions can require an ID for pickup depending on state law.", "Non controlled substances can be picked up without ID by anybody who knows the patient information.", "State laws regarding pharmacies ID restrictions are not the same across the country."], "decomposition": ["Which category of medications usually require an ID for pickup?", "What are the regulations guiding #1 across states in the US?", "Are #2 the same across all states?"], "evidence": [[[["Medication-37"]], [["Medication-37", "Over-the-counter drug-5"]], [["Over-the-counter drug-5"], "no_evidence"]], [[["Controlled Substances Act-45"]], [["Combat Methamphetamine Epidemic Act of 2005-6"]], [["Controlled Substances Act-14"], "operation"]], [[["Opiate-1"], "no_evidence"], [["Uniform Controlled Substances Act-1"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Depending upon the jurisdiction, drugs may be divided into over-the-counter drugs (OTC) which may be available without special restrictions, and prescription drugs, which must be prescribed by a licensed medical practitioner in accordance with medical guidelines due to the risk of adverse effects and contraindications."], ["", ""], [""]]}, {"qid": "81bc2060211d86ee5354", "term": "Fiat Chrysler Automobiles", "description": "Multinational automotive manufacturing conglomerate", "question": "Is Fiat Chrysler gaining a new overall corporate identity?", "answer": true, "facts": ["The company is renaming itself Stellantis following the completion of its merger.", "There are 14 automobile brands owned by the company, which will be keeping their names and logos."], "decomposition": ["What plans are underway as regards naming after the completion of the Fiat Chrysler merger?", "Does #1 involve a change of the collective corporate identity?"], "evidence": [[[["Fiat Chrysler Automobiles-37"], "no_evidence"], [["Corporate identity-2"], "operation"]], [[["Fiat Chrysler Automobiles-1"]], [["Fiat Chrysler Automobiles-1"], "no_evidence"]], [[["Groupe PSA-23"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "606718d82a6cf5a29ca4", "term": "The Jungle Book", "description": "1894 children's book by Rudyard Kipling", "question": "Did Disney get most of Rudyard Kipling's The Jungle Book profits?", "answer": true, "facts": ["Rudyard Kipling's 1894 book The Jungle Book has was adapted into several Disney films.", "The 2016 film The Jungle Book grossed over 966 million dollars.", "Disney's 1967 film version of The Jungle Book grossed over 378 million dollars.", "The Jungle Book was not a worldwide phenomenon in Kipling's time."], "decomposition": ["When did Rudyard Kipling write \"The Jungle Book\"?", "The 1967 and 2016 adaptations of the book was produced by which media company?", "When did #2 produce these movies?", "Is #3 several decades after #1 and #2 the same as Disney?"], "evidence": [[[["Rudyard Kipling-2"]], [["The Jungle Book (2016 film)-1"]], [["The Jungle Book (2016 film)-1"]], ["operation"]], [[["The Jungle Book-1"]], [["The Jungle Book (franchise)-1"], "no_evidence"], [["The Jungle Book (franchise)-1"]], ["no_evidence", "operation"]], [[["The Jungle Book-1"]], [["The Jungle Book-26"]], [["Adventures of Mowgli-1"]], ["operation"]]], "golden_sentence": [["Kipling's works of fiction include The Jungle Book (1894), Kim (1901), and many short stories, including \"The Man Who Would Be King\" (1888)."], ["The Jungle Book is a 2016 American fantasy adventure film directed and produced by Jon Favreau, from a screenplay written by Justin Marks and produced by Walt Disney Pictures."], [""]]}, {"qid": "3c8b64409e8a8447fcf6", "term": "Kidney", "description": "internal organ in most animals, including vertebrates and some invertebrates", "question": "Does an organ donor need to be dead to donate a kidney?", "answer": false, "facts": ["The average human has two kidneys.", "Only one kidney is required to function as a healthy person.", "Living organ donors will sometimes donate their spare kidney to someone experiencing failure of both their kidneys."], "decomposition": ["How many kidneys does the average person have?", "How many kidneys does a person require to function?", "Is #1 the same as #2?"], "evidence": [[[["Kidney-1"]], [["Kidney-33"]], ["operation"]], [[["Kidney-1"]], [["Organ donation-3"], "no_evidence"], ["no_evidence", "operation"]], [[["Kidney-1"]], [["Kidney-33"]], ["operation"]]], "golden_sentence": [["The kidneys are two bean-shaped organs found in vertebrates."], ["Kidney failure Acute kidney failure Stage 5 Chronic Kidney Disease Renal artery stenosis Renovascular hypertension Generally, humans can live normally with just one kidney, as one has more functioning renal tissue than is needed to survive."]]}, {"qid": "71a73e51beec977d2985", "term": "Kelly Clarkson", "description": "American singer-songwriter, actress, and television personality", "question": "Would Kelly Clarkson's voice shake glass?", "answer": true, "facts": ["Glass vibrates at its resonant frequency which is around a middle C note.", "Kelly Clarkson has an impressive three octave vocal range.", "Kelly Clarkson's Never Enough is in the key of A-flat.", "A-flat is above middle C in terms of notes."], "decomposition": ["At what note would glass start to vibrate?", "In Kelly Clarkson's song Never Enough, what key is the song sung in?", "Is #2 above #1?"], "evidence": [[[["Acoustic resonance-51"], "no_evidence"], [["Never Again (Kelly Clarkson song)-5"]], ["operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Crystallophone-2", "Resonance-1", "Resonance-8"], "no_evidence"], [["Kelly Clarkson-9"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["A glass has a natural resonance, a frequency at which the glass will vibrate easily."], ["According to the digital music sheet published at Musicnotes.com by Alfred Publishing Company, Inc, it is written in the key of G minor."]]}, {"qid": "bf05fdbb4de5ae3e6f2a", "term": "Railroad engineer", "description": "person who operates a train on a railroad or railway", "question": "Is a railroad engineer needed during NASCAR events?", "answer": false, "facts": ["Railroad engineers work on trains and railway systems", "NASCAR events feature automobile races"], "decomposition": ["On what kind of transportation do railroad engineers work?", "NASCAR involves what kind of transportation?", "Is #1 and #2 the same?"], "evidence": [[[["Edward Banfield (railroad engineer)-1"]], [["Safety car-34"]], ["operation"]], [[["Train driver-1"]], [["NASCAR-1"]], ["operation"]], [[["Train driver-1"]], [["NASCAR-1"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "35b674ee9ae29a5760a2", "term": "Crane (bird)", "description": "family of birds", "question": "Can crane slamdunk?", "answer": false, "facts": ["Crane are a type of bird. ", "Slamdunking is a basketball maneuver in which the player puts the basketball in the basket with one or two hands above the rim.", "Birds don't have hands."], "decomposition": ["What is a slamdunk?", "What body parts are needed to perform #1?", "Do cranes have #2?"], "evidence": [[[["Slam dunk-1"]], [["Slam dunk-6"]], [["Crane (bird)-1"], "operation"]], [[["Slam dunk-1"]], [["Hand-1"]], [["Crane (bird)-1"], "operation"]], [[["Slam dunk-1"]], [["Slam dunk-1"]], [["Crane (bird)-1"], "no_evidence"]]], "golden_sentence": [["A slam dunk, also simply dunk, is a type of basketball shot that is performed when a player jumps in the air, controls the ball above the horizontal plane of the rim, and scores by putting the ball directly through the basket with one or both hands above the rim."], [""], ["Unlike the similar-looking but unrelated herons, cranes fly with necks outstretched, not pulled back."]]}, {"qid": "0773b2ab4e3776a4f1b4", "term": "QWERTY", "description": "keyboard layout where the first line is \"QWERTYUIOP\"", "question": "Did the Qwerty keyboard layout predate computers?", "answer": true, "facts": ["The Qwerty layout was originally developed for mechanical typewriters in the 1870s.", "ENIAC was considered to be the first computer, built in the late 1940s."], "decomposition": ["When was the QWERTY keyboard layout developed?", "When was the modern computer invented?", "Is #1 before #2?"], "evidence": [[[["QWERTY-1"]], [["Computer-3", "QWERTY-20"]], ["operation"]], [[["QWERTY-1"]], [["Computer-25", "Computer-26"]], ["operation"]], [[["QWERTY-8"]], [["Computer-23"]], ["operation"]]], "golden_sentence": [["The QWERTY design is based on a layout created for the Sholes and Glidden typewriter and sold to E. Remington and Sons in 1873."], ["", ""]]}, {"qid": "43d38a1113752361c696", "term": "Koala", "description": "An arboreal herbivorous marsupial native to Australia.", "question": "Would Alexander Hamilton have known about koalas?", "answer": false, "facts": ["Alexander Hamilton died in 1804.", "The first published depiction of a koala was in 1810."], "decomposition": ["When were Koalas first sighted?", "When did Alexander Hamilton die?", "Is #1 before #2?"], "evidence": [[[["Koala-33", "Koala-35"]], [["Alexander Hamilton-1"]], ["operation"]], [[["Koala-3"], "no_evidence"], [["Alexander Hamilton-1"]], ["operation"]], [[["Koala-3"]], [["Alexander Hamilton-109"]], ["operation"]]], "golden_sentence": [["In 1802, French-born explorer Francis Louis Barrallier encountered the animal when his two Aboriginal guides, returning from a hunt, brought back two koala feet they were intending to eat.", "The first published image of the koala appeared in George Perry's (1810) natural history work Arcana."], ["Alexander Hamilton (January 11, 1755 or 1757\u00a0\u2013 July 12, 1804) was an American statesman, politician, legal scholar, military commander, lawyer, banker, and economist."]]}, {"qid": "c3c61a534ba4eefa7f93", "term": "Torso", "description": "the central part of the living body", "question": "Will the torso be safe from blows to the largest and smallest bones in body?", "answer": true, "facts": ["The three smallest bone in the body are malleus, incus, and stapes.", "Malleus, incus, and stapes are located in the ear.", "The femur is the largest bone in the body.", "The femur is located in the leg.", "The torso is located in the center of the body."], "decomposition": ["Which part of the human body is the torso?", "Which is the largest and smallest bone in the human body?", "Where are #2 located?", "Is any of #3 part of #1?"], "evidence": [[[["Torso-1"]], [["Femur-1", "Stapes-1"]], [["Human leg-1", "Stapes-1"]], [["Abdomen-1", "Perineum-1", "Thorax-1"], "operation"]], [[["Torso-1"]], [["Bone-3"]], [["Femur-7"]], [["Femur-7"], "operation"]], [[["Torso-1"]], [["Bone-3"]], [["Thigh-1"]], ["operation"]]], "golden_sentence": [["The torso or trunk is an anatomical term for the central part or core of many animal bodies (including humans) from which extend the neck and limbs."], ["femurs or femora /\u02c8f\u025bm\u0259r\u0259/), or thigh bone, is the proximal bone of the hindlimb in tetrapod vertebrates (for example, the largest bone of the human thigh).", "The stapes is the smallest and lightest bone in the human body, and is so-called because of its resemblance to a"], ["", "This stirrup-shaped bone is connected to the oval window by its annular ligament, which allows the footplate to transmit sound energy through the oval window into the inner ear."], ["", "", ""]]}, {"qid": "356e0934151bfc25dff9", "term": "Dalai Lama", "description": "Tibetan Buddhist spiritual teacher", "question": "Does the Dalai Lama believe in the divine barzakh?", "answer": false, "facts": ["The Dalai Lama is the spiritual leader of Tibetan Buddhism", "The divine barzakh is a concept from Islam"], "decomposition": ["What religion contains the concept of the divine barzakh?", "Is the Dalai Lama a member of #1?"], "evidence": [[[["Barzakh-9"]], [["Dalai Lama-1"]]], [[["Barzakh-9"]], [["Dalai Lama-1"], "operation"]], [[["Astral plane-3"]], [["Dalai Lama-1"], "operation"]]], "golden_sentence": [["In Sufism the Barzakh or Alam-e-Araf is not only where the human soul resides after death but it is also a place that the soul can visit during sleep and meditation."], [""]]}, {"qid": "d18a7c2fc15dedcddd71", "term": "Pig Latin", "description": "secret language game", "question": "Is Pig Latin related to real Latin?", "answer": false, "facts": ["Pig Latin is based on English.", "It is formed by moving consonants and syllables.", "Real Latin is a separate language distinct from English."], "decomposition": ["Which language is Pig Latin based on?", "Is #1 Latin?"], "evidence": [[[["Pig Latin-1"]], ["operation"]], [[["Pig Latin-6"]], [["Pig Latin-6"], "operation"]], [[["Pig Latin-1"]], [["English language-1"], "operation"]]], "golden_sentence": [["The reference to Latin is a deliberate misnomer; Pig Latin is simply a form of argot or jargon unrelated to Latin, and the name is used for its English connotations as a strange and foreign-sounding language."]]}, {"qid": "7aae94534417ec6787ef", "term": "Woodrow Wilson", "description": "28th president of the United States", "question": "Would Woodrow Wilson support Plessy v. Ferguson decision?", "answer": true, "facts": ["Plessy v Ferguson was a landmark case that stated segregation did not violate the constitution.", "President Woodrow Wilson escalated the discriminatory hiring policies and segregation of government offices.", "By the end of 1913, under President Wilson, many departments, including the navy, had segregated work spaces, restrooms, and cafeterias."], "decomposition": ["What was the topic of Plessy v. Ferguson?", "Does Woodrow Wilson's veiws on #1 agree more with Plessy or Ferguson?", "Who did the court rule in favor of in Plessy v. Ferguson?", "Are #2 and #3 the same?"], "evidence": [[[["Plessy v. Ferguson-1"]], [["Woodrow Wilson-77"]], [["Plessy v. Ferguson-3"]], ["operation"]], [[["Plessy v. Ferguson-2"]], [["Woodrow Wilson-77", "Woodrow Wilson-78"], "no_evidence"], [["Plessy v. Ferguson-1"]], ["no_evidence", "operation"]], [[["Plessy v. Ferguson-9"]], [["Woodrow Wilson-78"]], [["Plessy v. Ferguson-7"]], ["operation"]]], "golden_sentence": [["Plessy v. Ferguson, 163 U.S. 537 (1896), was a landmark decision of the U.S. Supreme Court that upheld the constitutionality of racial segregation laws for public facilities as long as the segregated facilities were equal in quality\u00a0\u2013 a doctrine that came to be known as \"separate but equal\"."], [""], ["In May 1896, the Supreme Court issued a 7\u20131 decision against Plessy ruling that the Louisiana law did not violate the Fourteenth Amendment to the U.S. Constitution, stating that although the Fourteenth Amendment established the legal equality of white and black Americans, it did not and could not require the elimination of all social or other \"distinctions based upon color\"."]]}, {"qid": "30fa53d7e523326fc510", "term": "Christmas Eve", "description": "Evening or entire day before Christmas Day", "question": "Would a Bulgarian priest eat a four-course meal on Christmas Eve?", "answer": false, "facts": ["A four-course meal consists of a soup, an appetizer, an entr\u00e9e, and dessert.", "The Bulgarian Christmas Eve meal has an odd number of dishes and an odd number of people sitting around the table."], "decomposition": ["Is the number of dishes served at a Bulgarian Christmas Eve meal odd or even?", "Is the number \"four\" odd or even?", "Is #1 the same as #2?"], "evidence": [[[["Christmas Eve-20"]], [["4-3"]], ["operation"]], [[["Christmas Eve-20"]], [["4-3"]], ["operation"]], [[["Christmas Eve-20"]], [["Parity (mathematics)-1"]], ["operation"]]], "golden_sentence": [["In Bulgaria, the meal consists of an odd number of lenten dishes in compliance with the rules of fasting."], ["4 is the smallest squared prime (p2) and the only even number in this form."]]}, {"qid": "d495ad1e9d7a5a510010", "term": "U.S. Route 66", "description": "Former US Highway between Chicago and Los Angeles", "question": "Is Route 66 generally unknown to Americans?", "answer": false, "facts": ["Route 66 was immortalized in the hit \"Route 66\" by Bobby Troupe.", "\"Route 66\" as a song has reached the Billboard Top Charts multiple times and is still played often."], "decomposition": ["In what hit song was Route 66 mentioned?", "Is #1 a little-known song in America?"], "evidence": [[[["(Get Your Kicks on) Route 66-1"]], [["(Get Your Kicks on) Route 66-3"]]], [[["(Get Your Kicks on) Route 66-1"]], [["(Get Your Kicks on) Route 66-2"]]], [[["U.S. Route 66-1"]], [["(Get Your Kicks on) Route 66-1", "(Get Your Kicks on) Route 66-3"], "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "3d20c8efd96389ed5e05", "term": "1936 Summer Olympics", "description": "games of the XI Olympiad, celebrated in Berlin in 1936", "question": "Did the Berlin Wall prevent any athletes from competing in the 1936 Summer Olympics?", "answer": false, "facts": ["The 1936 Olympics were held in 1936.", "The Berlin Wall was not constructed until 1961."], "decomposition": ["When were the 1936 Olympics?", "When was the Berlin Wall built?", "Did #2 occur before #1?"], "evidence": [[[["1936 Summer Olympics-1"]], [["Berlin Wall-1"]], ["operation"]], [[["1936 Summer Olympics-1"]], [["Berlin Wall-1"]], ["operation"]], [[["1936 Summer Olympics-1"]], [["Berlin Wall-1"]], ["operation"]]], "golden_sentence": [["The 1936 Summer Olympics (German: Olympische Sommerspiele 1936), officially known as the Games of the XI Olympiad (German: Spiele der XI."], ["Construction of the Wall was commenced by the German Democratic Republic (GDR, East Germany) on 13 August 1961."]]}, {"qid": "42b4a8f9033fc8a0525d", "term": "Vietnamese people", "description": "ethnic group originally from northern Vietnam", "question": "Are the Vietnamese people a great untapped resource for NBA players?", "answer": false, "facts": ["Vietnam was ranked as one of the countries with the shortest people on average, in 2019.", "The average height of a Vietnamese man is 5 feet 4.74 inches.", "The average height of an NBA player in 2018 was 6 foot 7 inches tall."], "decomposition": ["What is the average height of NBA players?", "What is the average height of Vietnamese males?", "Is #2 close to being the same as #1?"], "evidence": [[[["Basketball-85"]], ["no_evidence"], ["operation"]], [[["Wilt Chamberlain-1"], "no_evidence"], [["Vietnamese people-7"], "no_evidence"], ["operation"]], [[["Basketball-85"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["the average height of all NBA players is just under 6\u00a0feet 7\u00a0inches (2.01\u00a0m), with the average weight being close to 222 pounds (101\u00a0kg)."]]}, {"qid": "efcffecda33202d2621f", "term": "Los Angeles County, California", "description": "County in California, United States", "question": "Will every resident of Los Angeles County, California go to Jehovah Witnesses's heaven?", "answer": false, "facts": ["Jehovah Witnesses believe that exactly 144,000 people will be saved and go to heaven.", "There are over 10 million residents of Los Angeles County, California as of 2019."], "decomposition": ["What is the estimated population of Los Angeles County, California?", "According to the Jehovah's Witnesses, how many people will go to heaven?", "Is #1 less than or equal to #2?"], "evidence": [[[["Westside (Los Angeles County)-6"]], [["Jehovah's Witnesses-31"]], ["operation"]], [[["Los Angeles County, California-8"]], [["Jehovah's Witnesses-30"]], ["operation"]], [[["Los Angeles County, California-1"]], [["Jehovah's Witnesses and salvation-5"]], ["operation"]]], "golden_sentence": [[""], ["Jehovah's Witnesses believe that God's Kingdom is a literal government in heaven, ruled by Jesus Christ and 144,000 \"spirit-anointed\" Christians drawn from the earth, which they associate with Jesus' reference to a \"new covenant\"."]]}, {"qid": "b09d0307a45c4cd97bf3", "term": "Kangaroo", "description": "\u0441ommon name of family of marsupials", "question": "Do Australians ride Kangaroos to work?", "answer": false, "facts": ["Kangaroos can become aggressive if they feel a human is too close or is threatening them.", "There are no parking areas or stalls for Kangaroos in Australia. ", "It would be considered animal abuse to ride on a kangaroo and leave it at one's job."], "decomposition": ["Do kangaroos live freely with people?", "Are there any kangaroo parking lots in Australia?", "Is #1 or #2 positive?"], "evidence": [[[["Kangaroo-38"]], ["no_evidence"], [["Kangaroo-38"]]], [[["Kangaroo-35"], "no_evidence"], [["Parking lot-1"], "no_evidence"], ["operation"]], [[["Red kangaroo-10"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "65f899de64c4d5af256b", "term": "Go (game)", "description": "Abstract strategy board game for two players", "question": "Did origin dynasty of Go precede Jia Sidao?", "answer": true, "facts": ["Go is a strategy game that originated in the Zhou dynasty.", "The Zhou dynasty lasted from 1046 BC \u2013 256 BC.", "Jia Sidao was a chancellor during the late Song dynasty.", "The Song dynasty started in 960 AD and lasted until 1279 AD."], "decomposition": ["During which Chinese dynasty did the game Go originate?", "Which Chinese dynasty was Jia Sidao a part of during his lifetime?", "Did #1 precede #2?"], "evidence": [[[["Go (game)-1"]], [["Jia Sidao-1"]], ["operation"]], [[["History of Go-9"]], [["Jia Sidao-1"]], [["Government of the Han dynasty-1", "Song dynasty-1"]]], [[["History of Go-4"]], [["Jia Sidao-1"]], ["operation"]]], "golden_sentence": [[""], ["Jia Sidao (1213-1275), courtesy name Shixian, was a chancellor of the late Song dynasty of China, the younger brother of a concubine of Emperor Lizong, who subsequently had a relationship of special favor with Emperor Duzong, and had roles in the Mongol-Song Battle of Xiangyang and an unpopular land nationalization program in the 1260s."]]}, {"qid": "dbd8881ff38ad80c2688", "term": "Star Wars", "description": "Epic science fantasy space opera franchise", "question": "Are there multiple Star Wars TV shows produced by Disney?", "answer": true, "facts": ["Star Wars Rebels and Star Wars Resistance were released after Disney bought ownership of Star Wars.", "Disney also produced the first live-action TV show set in the Star Wars galaxy, The Mandalorian.", "Disney produced one additional revival season of Star Wars Clone Wars which was originally produced before Disney owned Star Wars."], "decomposition": ["When did Disney acquire Star Wars?", "How many Star Wars TV shows have been produced since #1?", "Is #2 greater than one?"], "evidence": [[[["The Walt Disney Company-38"]], [["Star Wars Resistance-2", "The Mandalorian-2"]], ["operation"]], [[["The Walt Disney Company-38"]], [["Star Wars Rebels-2", "Star Wars Resistance-2"]], ["operation"]], [[["Star Wars-2"]], [["Star Wars-23"], "no_evidence"], ["operation"]]], "golden_sentence": [["The sale was completed on December 21, 2012."], ["", ""]]}, {"qid": "495cb36445e0358e960b", "term": "Private investigator", "description": "person hired to undertake investigatory law services", "question": "Would Emma Roberts's Nancy Drew be considered a private investigator?", "answer": false, "facts": ["Emma Roberts starred as Nancy Drew in the 2007 film titled Nancy Drew.", "A private investigator is hired by an individual to solve a crime.", "Nancy Drew from the 2007 film was described as an amateur sleuth.", "Nancy Drew in the 2007 film was interested in a case and decided to pursue it on her own, without being paid for it."], "decomposition": ["Who did Emma Roberts play in the 2007 film titled Nancy Drew?", "Was #1 considered a private investigator?"], "evidence": [[[["Nancy Drew (2007 film)-1"]], [["Nancy Drew (2007 film)-2"], "operation"]], [[["Nancy Drew (2007 film)-1"]], [["Nancy Drew (2007 film)-2"]]], [[["Nancy Drew (2007 film)-1"]], [["Nancy Drew (2007 film)-2"]]]], "golden_sentence": [["It stars Emma Roberts as Nancy Drew, with Josh Flitter and Max Thieriot."], [""]]}, {"qid": "49ac663ec445fb757697", "term": "Seven Years' War", "description": "Global conflict between 1756 and 1763", "question": "Was the AK-47 used in the Seven Years' War?", "answer": false, "facts": ["The Seven Years' War took place between 1756 and 1763.", "The AK-47 was developed in the 1940s."], "decomposition": ["Between what years did the Seven Years' War take place?", "When was the AK-47 developed?", "Is #2 before #1?"], "evidence": [[[["Seven Years' War-1"]], [["AK-47-2"]], ["operation"]], [[["France in the Seven Years' War-1"]], [["AK-47-2"]], ["operation"]], [[["Seven Years' War-9"]], [["AK-47-2"]], ["operation"]]], "golden_sentence": [["The Seven Years' War was a global war fought between 1756 and 1763."], ["Design work on the AK-47 began in 1945."]]}, {"qid": "923974811adaa99aed18", "term": "J. K. Rowling", "description": "English novelist", "question": "Did Helen Keller ever read a novel by J. K. Rowling?", "answer": false, "facts": ["Helen Keller died in 1968.", "J. K. Rowling's first novel was published in 1997."], "decomposition": ["When was J. K. Rowling's first novel published?", "When did Helen Keller die?", "Is #1 before #2?"], "evidence": [[[["J. K. Rowling-15"]], [["Helen Keller-1"]], ["operation"]], [[["Harry Potter and the Philosopher's Stone-2"]], [["Helen Keller-1"]], ["operation"]], [[["J. K. Rowling-2"], "no_evidence"], [["Helen Keller-45"]], ["operation"]]], "golden_sentence": [[""], ["Helen Adams Keller (June 27, 1880 \u2013 June 1, 1968) was an American author, political activist, and lecturer."]]}, {"qid": "bf86d02dee2fddaa7f75", "term": "Garfield", "description": "Comic strip created by Jim Davis", "question": "Would Garfield like canid food?", "answer": false, "facts": ["Garfield is a fictional comic strip character that is a cat. ", "Garfield loves to eat spaghetti.", "Canid refers to the species that dogs belong to.", "Dogs like to eat meat and dog food."], "decomposition": ["What is Garfield's favorite food?", "Is the answer to #1 a type of canned food?"], "evidence": [[[["Garfield (character)-1"]], [["Lasagne-2"]]], [[["Garfield-29"]], [["Lasagne-1"]]], [[["Garfield-43"]], ["operation"]]], "golden_sentence": [["He is noted for his love of lasagna and sleeping, and his hatred of Mondays, fellow cat Nermal and exercise."], [""]]}, {"qid": "397522bfdec377fb0a9e", "term": "Hamlet", "description": "tragedy by William Shakespeare", "question": "Did Hamlet's author use email?", "answer": false, "facts": ["Hamlet was written by William Shakespeare.", "William Shakespeare was born in 1564.", "Email was not widely used until the 1970s."], "decomposition": ["Who was the author of Hamlet?", "When did #1 pass away?", "When did email become commonly used?", "Did #3 occur before #2?"], "evidence": [[[["Hamlet-1"]], [["William Shakespeare-17"]], [["Email-1"]], ["operation"]], [[["Hamlet-4"]], [["William Shakespeare-5"]], [["History of email-12"]], ["operation"]], [[["Hamlet-2"]], [["William Shakespeare-17"]], [["Email-1"]], ["operation"]]], "golden_sentence": [["The Tragedy of Hamlet, Prince of Denmark, often shortened to Hamlet (/\u02c8h\u00e6ml\u026at/), is a tragedy written by William Shakespeare sometime between 1599 and 1601."], ["Shakespeare died on 23 April 1616, at the age of 52."], ["Email entered limited use in the 1960s, but users could only send to users of the same computer, and some early email systems required the author and the recipient to both be online simultaneously, similar to instant messaging."]]}, {"qid": "510edcf28368bca73223", "term": "Larry King", "description": "American television and radio host", "question": "Can Larry King's ex-wives form a water polo team?", "answer": true, "facts": ["Water polo is a sport played by teams of seven competitors", "Larry King has seven ex-wives"], "decomposition": ["How many ex wives does Larry King have?", "How many players  are on a water polo team?", "Is #1 equal to #2?"], "evidence": [[[["Larry King-43"]], [["Water polo-1"]], ["operation"]], [[["Larry King-37"]], [["Water polo-1"]], ["operation"]], [[["Larry King-37"]], [["Water polo-1"]], ["operation"]]], "golden_sentence": [["From his eight marriages, King has five children and nine grandchildren, as well as four great-grandchildren."], ["Each team is made up of six field players and one goalkeeper."]]}, {"qid": "34a656097b1b8bf1e163", "term": "Hypertension", "description": "Long term medical condition", "question": "Are a dozen pickles good for easing hypertension?", "answer": false, "facts": ["Hypertension is high blood pressure that can come from a number of factors including excess salt.", "Pickles are food that are high in sodium, or salt."], "decomposition": ["What nutrients cause hypertension?", "Are pickles low in #1?"], "evidence": [[[["Hypertension-2"]], [["Pickled cucumber-3"], "operation"]], [[["Hypertension-24"]], [["Pickled cucumber-20"], "operation"]], [[["Hypertension-2"]], [["Pickled cucumber-3"]]]], "golden_sentence": [[""], [""]]}, {"qid": "1a7a09deb8572d94cd59", "term": "Saint Peter", "description": "apostle and first pope", "question": "Was Florence Nightingale's death more painful than Saint Peter's?", "answer": false, "facts": ["Florence Nightingale was a social reformer that is the founder of modern medicine.", "Florence Nightingale died in her sleep.", "Saint Peter was a Christian preacher.", "Saint Peter was crucified by the Romans."], "decomposition": ["How did Saint Peter die?", "How did Florence Nightingale die?", "Can #2 be considered more painful than #1?"], "evidence": [[[["Saint Peter-64"]], [["Florence Nightingale-42"]], [["Florence Nightingale-42", "Saint Peter-64"], "operation"]], [[["Saint Peter-57"]], [["Florence Nightingale-42"]], ["operation"]], [[["Saint Peter-81"]], [["Florence Nightingale-42"]], ["operation"]]], "golden_sentence": [[""], ["Florence Nightingale died peacefully in her sleep in her room at 10\u00a0South Street, Mayfair, London, on 13 August 1910, at the age of 90."], ["", ""]]}, {"qid": "c72be630d6bc6399c80a", "term": "Amazon (company)", "description": "American electronic commerce and cloud computing company", "question": "Would Iceland lose to Amazon in a bidding war?", "answer": true, "facts": ["Iceland had a nominal GDP of $27 billion as of a 2018 estimate.", "Amazon recorded revenues of $232.887 billion in the 2018 fiscal year."], "decomposition": ["What was the nominal GDP of Iceland in 2018?", "What was Amazon's recorded revenues in 2018?", "Is #2 greater than #1?"], "evidence": [[[["Economy of Iceland-1"]], [["Amazon (company)-64"]], ["operation"]], [[["Economy of Iceland-1"]], [["Amazon (company)-64"]], ["operation"]], [[["Economy of Iceland-1"], "operation"], [["Advertising revenue-10"], "operation"], ["no_evidence"]]], "golden_sentence": [["In 2011, gross domestic product was US$12bn, but by 2018 it had increased to a nominal GDP of US$27bn."], ["For the fiscal year 2018, Amazon reported earnings of US$10.07 billion, with an annual revenue of US$232.887 billion, an increase of 30.9% over the previous fiscal cycle."]]}, {"qid": "06f1e6f8757070bb95df", "term": "Boolean algebra", "description": "Algebra involving variables containing only \"true\" and \"false\" (or 1 and 0) as values", "question": "Could boolean algebra be described as binary?", "answer": true, "facts": ["Binary options tend to have 2 instead of 10 as a base. ", "Binary directly describes something composed of 2 things. "], "decomposition": ["How many digits are used in boolean algebra?", "How many does 'binary' denote?", "Is #1 the same as #2?"], "evidence": [[[["Boolean algebra-1"]], [["Binary number-1"]], ["operation"]], [[["Boolean algebra-1"]], [["Binary number-1"]], ["operation"]], [[["Boolean algebra-1"]], [["Binary number-1"]], ["operation"]]], "golden_sentence": [["In mathematics and mathematical logic, Boolean algebra is the branch of algebra in which the values of the variables are the truth values true and false, usually denoted 1 and 0 respectively."], ["In mathematics and digital electronics, a binary number is a number expressed in the base-2 numeral system or binary numeral system, which uses only two symbols: typically \"0\" (zero) and \"1\" (one)."]]}, {"qid": "7324caa8b4b90bac869e", "term": "Jujutsu", "description": "Japanese martial art", "question": "Could a Jujutsu expert hypothetically defeat a Janissary?", "answer": false, "facts": ["Jujutsu is a form of unarmed combat.", "Janissaries were the elite infantry of the Ottoman Empire.", "Janissaries wore chain mail and armor and wielded sharp swords."], "decomposition": ["What equipment does Jujutsu use?", "What equipment does Janissary use?", "Would someone with #1 likely defeat someone with #2?"], "evidence": [[[["Jujutsu-1"]], [["Janissaries-25"]], ["no_evidence"]], [[["Jujutsu-1"]], [["Janissaries-1", "Janissaries-12"]], ["operation"]], [[["Jujutsu-1"]], [["Janissaries-25"]], ["operation"]]], "golden_sentence": [["Jujutsu (English: /d\u0292u\u02d0\u02c8d\u0292\u028atsu\u02d0/ joo-JOOT-soo; Japanese: \u67d4\u8853 j\u016bjutsu listen\u00a0(help\u00b7info)), also known as Japanese Jujutsu, JJJ, Jujitsu or Japanese Ju-Jitsu, is a family of Japanese martial arts and a method of close combat for defeating an opponent in which one uses either a short weapon or bare hands, and selected subset of techniques from certain styles of Japanese Jujutsu were used to develop modern martial arts and combat sports, such as Judo, Sambo, ARB, Brazilian jiu-jitsu and mixed martial arts."], ["Janissaries who guarded the palace (Z\u00fcl\u00fcfl\u00fc Baltac\u0131lar) carried long-shafted axes and halberds."]]}, {"qid": "1e96f55f483e1e040ebb", "term": "JPEG", "description": "Lossy compression method for reducing the size of digital images", "question": "Does the JPEG acronym stand for a joint committee?", "answer": true, "facts": ["The term \"JPEG\" is an initialism/acronym for the Joint Photographic Experts Group.", "They created the standard in 1992.", "The Joint Photographic Experts Group (JPEG) is the joint committee between ISO/IEC JTC 1 and ITU-T Study Group 16 (formerly CCITT) . ", "The Joint Photographic Experts Group created and maintains the JPEG, JPEG 2000, and JPEG XR standards. "], "decomposition": ["What does the acronym JPEG represent?", "Is #1 a coalition of different groups?"], "evidence": [[[["JPEG-1"]], [["Coalition-1"]]], [[["JPEG-2"]], [["Joint Photographic Experts Group-1"], "operation"]], [[["JPEG-2"]], [["Joint Photographic Experts Group-1"]]]], "golden_sentence": [["Play media JPEG (/\u02c8d\u0292e\u026ap\u025b\u0261/ JAY-peg) is a commonly used method of lossy compression for digital images, particularly for those images produced by digital photography."], ["The term \"coalition\" is the denotation for a group formed when two or more people, factions, states, political parties, militaries etc."]]}, {"qid": "8fb8ee5c759f2b75bad8", "term": "Mike Tyson", "description": "American boxer", "question": "Did Mike Tyson train to use the gogoplata?", "answer": false, "facts": ["Mike Tyson is a boxer", "The gogoplata is a chokehold used in mixed martial arts and various submission grappling disciplines"], "decomposition": ["In what sports is a gogoplata used?", "Did Mike Tyson participate in #1?"], "evidence": [[[["Gogoplata-4"]], [["Mike Tyson-1"], "operation"]], [[["Gogoplata-1"]], [["Mike Tyson-1"]]], [[["Gogoplata-1"]], ["no_evidence"]]], "golden_sentence": [[""], [""]]}, {"qid": "223dbc63776281eb679b", "term": "The Mentalist", "description": "American police procedural drama television series (2008-2015)", "question": "Was the Mentalist filmed in black and white?", "answer": false, "facts": ["The Mentalist first aired in 2008.", "Black and white television shows were no longer being made in 2008."], "decomposition": ["When did The Mentalist first air?", "When did they stop filming black and white television?", "Was #1 before #2?"], "evidence": [[[["The Mentalist-1"]], [["Black and white-5"]], ["operation"]], [[["The Mentalist-1"]], [["Black and white-5"]], ["operation"]], [[["The Mentalist-1"]], [["Black and white-5"]], ["operation"]]], "golden_sentence": [["The Mentalist is an American drama television series that ran from September 23, 2008, until February 18, 2015, broadcasting 151 episodes over seven seasons, on CBS."], ["Australia experimented with color television in 1967 but continued to broadcast in black-and-white until 1975, and New Zealand experimented with color broadcasting in 1973 but did not convert until 1975."]]}, {"qid": "4fa8233b3d2f870dfad6", "term": "Ku Klux Klan", "description": "American white supremacy group", "question": "Would the Ku Klux Klan welcome Opal Tometi into their group?", "answer": false, "facts": ["The Ku Klux Klan is an American white supremacist hate group whose primary targets are African Americans, as well as Jews, immigrants, leftists, and homosexuals.", "Opal Tometi is an African American woman.", "Opal Tometi is a co-founder of Black Lives Matter.", "Black Lives Matter (BLM) is a decentralized movement advocating for non-violent civil disobedience in protest against incidents of police brutality and all racially motivated violence against African-American people."], "decomposition": ["Which groups of people are enemies of the Ku Klux Klan?", "What is Opal Tometi's ethnicity?", "Is #2 absent from #1?"], "evidence": [[[["Ku Klux Klan-1"]], [["Opal Tometi-1", "Opal Tometi-4"]], ["operation"]], [[["Ku Klux Klan-1"]], [["Opal Tometi-1"]], ["operation"]], [[["Ku Klux Klan-1"]], [["Opal Tometi-8"]], ["operation"]]], "golden_sentence": [["In each era, membership was secret and estimates of the total were highly exaggerated by both friends and enemies."], ["Opal Tometi is a Nigerian-American human rights activist, writer, strategist, and community organizer.", "Opal Tometi is the daughter of Nigerian immigrants."]]}, {"qid": "55c8dbf081d444a2c058", "term": "Emu", "description": "Large flightless bird endemic to Australia", "question": "Can an emu chase a bogan?", "answer": true, "facts": ["Emus are endemic to the continent of Australia", "Bogan is a pejorative term for certain citizens of Australia"], "decomposition": ["Where are emus endemic to?", "Where is a \"bogan\" found?", "Do areas #1 and #2 overlap?"], "evidence": [[[["Emu-1"]], [["Bogan-2"]], ["operation"]], [[["Emu-1"]], [["Bogan-25"]], ["operation"]], [[["Emu-1"]], [["Bogan-1"]], ["operation"]]], "golden_sentence": [["It is endemic to Australia where it is the largest native bird and the only extant member of the genus Dromaius."], ["It has antecedents in the Australian larrikin and ocker, and various localised names exist that describe the same or very similar people to the bogan."]]}, {"qid": "192e8d120227e3adc29d", "term": "ABBA", "description": "Swedish pop group", "question": "Is calling ABBA the Swedish Beatles a preposterous claim?", "answer": true, "facts": ["ABBA was a Swedish band that had 1 Billboard number 1 hit and 4 top 10 hits.", "The Beatles had 20 Billboard number 1 hits and 34 top 10 hits."], "decomposition": ["How many Billboard number ones did ABBA have?", "How many Billboard number ones did the Beatles have?", "Is #1 lower than #2?"], "evidence": [[[["ABBA-38"]], [["Billboard 200-25"]], ["operation"]], [[["ABBA-120"]], [["The Beatles-111", "The Beatles-4"], "no_evidence"], ["operation"]], [[["ABBA-121"]], [["Billboard 200-26"]], [["Billboard 200-26"], "operation"]]], "golden_sentence": [["The band's popularity in the United States would remain on a comparatively smaller scale, and \"Dancing Queen\" became the only Billboard Hot 100 number-one single ABBA had there (they did, however, get three more singles to the number-one position on other Billboard charts, including Billboard Adult Contemporary and Hot Dance Club Play)."], ["Sources: The Beatles (132) Garth Brooks (52) Michael Jackson (51) Whitney Houston (46) Taylor Swift (40) Elton John (39) Fleetwood Mac (38) (tie) The Rolling Stones (38) (tie) The Monkees (37) Prince (34) (tie) Adele (34) (tie) The following artists are the only ones with 30 or more top-10 albums:"]]}, {"qid": "e6e7e91bd8381ab395b8", "term": "Jane Austen", "description": "English novelist", "question": "Did Jane Austen suffer from middle child syndrome?", "answer": false, "facts": ["Jane Austen was the second youngest of 8 children.", "Middle child syndrome is the feeling of exclusion by middle children, due directly to their placement in their family's birth order."], "decomposition": ["What would a child have to be to suffer from middle child syndrome?", "What is Jane Austen's position among her siblings?", "Does being #2 make her #1?"], "evidence": [[[["Middle child syndrome-1"]], [["Jane Austen-7", "Timeline of Jane Austen-4"]], ["operation"]], [[["Middle child syndrome-1"]], [["Jane Austen-11"]], ["operation"]], [[["Middle child syndrome-1"]], [["Jane Austen-5", "Jane Austen-7"]], ["operation"]]], "golden_sentence": [["Middle child syndrome is the feeling of exclusion by middle children, due directly to their placement in their family's birth order."], ["He added that her arrival was particularly welcome as \"a future companion to her sister\".", ""]]}, {"qid": "f5c3a207edc46f5054b5", "term": "Hunting", "description": "Searching, pursuing, catching and killing wild animals", "question": "Would a pacifist be opposed to hunting?", "answer": false, "facts": ["Pacifists are a group opposed to violence and war.", "Amish people are well known for their pacifism.", "Amish people hunt for meat and sport."], "decomposition": ["What is the purpose of hunting?", "What are Pacifists opposed to?", "Is #2 listed in #1?"], "evidence": [[[["Hunting-1"]], [["Pacifism-1"]], [["Violence-1"], "operation"]], [[["Hunting-16"], "no_evidence"], [["Opposition to World War I-9"], "operation"], ["no_evidence"]], [[["Hunting-1"]], [["Pacifism-1"]], ["operation"]]], "golden_sentence": [["Hunting wildlife or feral animals is most commonly done by humans for meat, recreation, to remove predators that can be dangerous to humans or domestic animals, to remove pests that destroy crops or kill livestock, or for trade."], ["Pacifism is opposition to war, militarism or violence."], [""]]}, {"qid": "e8d659920a389d8f35bb", "term": "Dance", "description": "A performing art consisting of movement of the body", "question": "Is a person with St. Vitus's Dance likely to win a ballet competition?", "answer": false, "facts": ["St. Vitus's Dance, also called Sydenham's chorea, is a disease characterized by sudden jerking movements of the body.", "Ballet is a performance dance known for elegant and fluid motions."], "decomposition": ["What are the characteristic movements of St. Vitus' Dance?", "What are the characteristic movements of well trained ballet dancers?", "Is #1 the same as #2?"], "evidence": [[[["Sydenham's chorea-1"]], [["Ballet-2"]], ["operation"]], [[["Sydenham's chorea-1"]], [["Ballet-22"]], ["operation"]], [[["Vitus-15"], "operation"], ["no_evidence"], ["no_evidence"]]], "golden_sentence": [["Sydenham's chorea, also known as chorea minor and historically and occasionally referred to as St Vitus' dance, is a disorder characterized by rapid, uncoordinated jerking movements primarily affecting the face, hands and feet."], [""]]}, {"qid": "63471dc150e5dd73f28a", "term": "Retail", "description": "Sale of goods and services from individuals or businesses to the end-user", "question": "Would a retail associate envy the retailer's CEO's pay?", "answer": true, "facts": ["The average retail CEO makes 14 million yearly.", "Retail associates typically make between $8 and $13 hourly."], "decomposition": ["How much does a retail CEO make yearly?", "How much does a retail associate make yearly?", "Is #1 greater than #2?"], "evidence": [[["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Executive compensation in the United States-5"], "no_evidence"], [["Executive compensation in the United States-5"], "no_evidence"], [["Executive compensation in the United States-5"], "operation"]], [[["Chief risk officer-15"], "no_evidence"], [["Minimum wage-87"]], ["operation"]]], "golden_sentence": []}, {"qid": "19dcd70c4a5675758bcc", "term": "Menstruation", "description": "Regular discharge of blood and tissue from the inner lining of the uterus through the vagina", "question": "Are there people who are men who experience menstruation?", "answer": true, "facts": ["Menstruation can occur in any human being who has a uterus and vagina. ", "People who are born with a vagina may transition socially and/or medically to being male. ", "Someone with a vagina who has transitioned to being male is a man. "], "decomposition": ["What body organs are involved in menstruation?", "Do some men possess #1? "], "evidence": [[[["Menstruation-9"]], [["Transgender pregnancy-2"], "operation"]], [[["Menstruation-1"]], [["Male menstruation-1"], "no_evidence", "operation"]], [[["Menstruation-1"]], [["Sex and gender distinction-1"], "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "936ffc7601dc60d71099", "term": "Disc jockey", "description": "Person who plays recorded music for an audience", "question": "Was disc jockey Jay Thomas enemies with Clayton Moore?", "answer": false, "facts": ["Jay Thomas was a radio disc jockey and Emmy winning actor. ", "Clayton Moore played the Lone Ranger.", "Jay Thomas was a yearly recurring guest on the Dave Letterman Show every Christmas.", "Jay Thomas told his Lone Ranger Story every year about a funny encounter he had with Clayton Moore."], "decomposition": ["Did Jay Thomas appear on the David Letterman Show?", "Did Jay Thomas tell a story about Clayton Moore who is the Lone Ranger?", "Is the story a humorous story?", "Is #1, #2 or #3 a no answer?"], "evidence": [[[["Jay Thomas-3"]], [["Clayton Moore-9"]], ["no_evidence"], [["Jay Thomas-3"]]], [[["Jay Thomas-5"]], [["Jay Thomas-6"]], [["Jay Thomas-8"], "operation"], ["operation"]], [[["Jay Thomas-3"]], [["Clayton Moore-9"]], [["Jay Thomas-8"]], ["operation"]]], "golden_sentence": [["He was also an annual guest on The Late Show with David Letterman during the Christmas season, where he told a story about how he met Clayton Moore, who portrayed the title character on The Lone Ranger."], ["citation needed] Jay Thomas was an annual guest on The Late Show with David Letterman during the Christmas season, where he told a true story about how he met Moore which he called his \"Lone Ranger story\"."], [""]]}, {"qid": "a35d9fcc64cbd5914d7e", "term": "Cactus", "description": "Family of mostly succulent plants, adapted to dry environments", "question": "Should cactus soil always be damp?", "answer": false, "facts": ["The amount of water a cactus needs depends on the weather and season.", "When the weather isn't hot, a cactus needs very little water."], "decomposition": ["What are the factors that determine the amount of water a cactus needs?", "Are #1 always constant?"], "evidence": [[[["Cactus-2"]], [["Cactus-2"]]], [[["Cactus-77"]], ["operation"]], [[["Cactus-77"]], [["Cactus-77"]]]], "golden_sentence": [[""], [""]]}, {"qid": "2128dac5395d3b16f3bb", "term": "United States Department of Education", "description": "United States government department", "question": "Does the United States Department of Education oversee services benefiting undocumented migrants? ", "answer": true, "facts": ["The United States Department of Education oversees public education across the United States.", "Public education is a service.", "Public education services are given to students of migrant families that may be undocumented."], "decomposition": ["Which service does the United States Department of Education oversee?", "Which services could children from undocumented migrant families benefit from?", "Is #1 included in #2?"], "evidence": [[[["United States Department of Education-3", "United States Department of Education-4"]], [["Office of Migrant Education-1"]], ["operation"]], [[["United States Department of Education-3"]], ["no_evidence"], ["no_evidence", "operation"]], [[["United States Department of Education-3"]], [["Office of Migrant Education-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["The primary functions of the Department of Education are to \"establish policy for, administer and coordinate most federal assistance to education, collect data on US schools, and to enforce federal educational laws regarding privacy and civil rights.\"", ""], ["The Office of Migrant Education (OME) is a program within the U.S. Department of Education's Office of Elementary and Secondary Education (OESE) that administers grant programs that provide academic and supportive services to the children of families who migrate to find work in the agricultural and fishing industries."]]}, {"qid": "7ae81ce58f63f4632794", "term": "Pea", "description": "species of plant", "question": "Does Soylent use Pea for their source of protein? ", "answer": false, "facts": ["Soylent is a meal replacement drink that offers 20mg protein.", "The protein in Soylent is derived from Soy."], "decomposition": ["What type of protein does Soylent use?", "Is #1 the same as pea protein?"], "evidence": [[[["Soylent (meal replacement)-1", "Soylent (meal replacement)-16"], "no_evidence"], ["no_evidence", "operation"]], [[["Soylent (meal replacement)-3"]], [["Pea-10"], "operation"]], [[["Soylent (meal replacement)-3"]], ["operation"]]], "golden_sentence": [["", "The company initially suspected soy or sucralose intolerance."]]}, {"qid": "f6a5e69a02ba6e52ba4c", "term": "Prime number", "description": "Integer greater than 1 that has no positive integer divisors other than itself and 1", "question": "Are Brian Cranston and Saoirse Ronan's combined Emmy Awards a prime number?", "answer": false, "facts": ["Brian Cranston has won 6 Emmy Awards.", "Saoirse Ronan has won 0 Emmy awards.", "6 is divisible by the following numbers: 1,2,3, and 6."], "decomposition": ["How many Emmy Awards has Brian Cranston won?", "How many Emmy Awards has Saoirse Ronan won?", "What is #1 plus #2?", "Is #3 not evenly divisible by any other number than 1 and #3?"], "evidence": [[[["Bryan Cranston-2"]], [["Saoirse Ronan-1"]], ["operation"], [["Composite number-4"], "operation"]], [[["Bryan Cranston-27"]], [["Saoirse Ronan-1"]], ["operation"], ["operation"]], [[["Bryan Cranston-12"]], [["Saoirse Ronan-1"], "no_evidence"], ["operation"], ["operation"]]], "golden_sentence": [["Cranston's performance on Breaking Bad is widely regarded as one of the best in television history, earning him the Primetime Emmy Award for Outstanding Lead Actor in a Drama Series four times (2008, 2009, 2010, and 2014)."], ["Primarily noted for her roles in period dramas since adolescence, Ronan has received several awards including a Golden Globe Award and nominations for four Academy Awards and five British Academy Film Awards."], ["4, 6, 8, 9, 10, 12, 14, 15, 16, 18, 20, 21, 22, 24, 25, 26, 27, 28, 30, 32, 33, 34, 35, 36, 38, 39, 40, 42, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 60, 62, 63, 64, 65, 66, 68, 69, 70, 72, 74, 75, 76, 77, 78, 80, 81, 82, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 102, 104, 105, 106, 108, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 128, 129, 130, 132, 133, 134, 135, 136, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150."]]}, {"qid": "cee5e957a5c04b1f389c", "term": "Plastic", "description": "material of a wide range of synthetic or semi-synthetic organic solids", "question": "Do beeswax alternatives to cling wrap use plsatic?", "answer": false, "facts": ["Beeswax food wrapping is typically made of two ingredients.", "Beeswax food wrap is composed of cotton fabric and beeswax.", "Neither cotton nor beeswax contains plastic."], "decomposition": ["What are the components of Beeswax food wrap?", "Do any among #1 contain plastic?"], "evidence": [[[["Beeswax wrap-1"]], ["operation"]], [[["Beeswax wrap-1"]], ["operation"]], [[["Beeswax wrap-8"]], ["operation"]]], "golden_sentence": [["The wrap is mouldable, grippable and tacky."]]}, {"qid": "ad9f520df54670bdb9b3", "term": "Doctorate", "description": "academic or professional degree", "question": "Should you be skeptical of a 21 year old claiming to have a doctorate?", "answer": true, "facts": ["The average age that someone gets their doctorate at is 33. ", "A doctorate takes an average of 8.5 years."], "decomposition": ["What is the average age at which people get their doctorate?", "Is 21 very much less than #1 ?"], "evidence": [[[["Graduate science education in the United States-6"]], ["operation"]], [[["Doctorate-1", "Graduate science education in the United States-6"], "no_evidence"], ["operation"]], [[["Doctorate-1", "Doctorate-18"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""]]}, {"qid": "7646d38e9a2966b4281e", "term": "Richard Dawkins", "description": "English ethologist, evolutionary biologist and author", "question": "Would Richard Dawkins hypothetically refuse an offering of the Last rites?", "answer": true, "facts": ["Richard Dawkins is known as an outspoken atheist, well known for his criticism of creationism and intelligent design.", "The Last rites, in Catholicism, are the last prayers and ministrations given to an individual of the faith, when possible, shortly before death."], "decomposition": ["Which religious beliefs are the Last rites affiliated with?", "What was Richard Dawkins' belief regarding religion?", "Would a #2 refuse to practice #1?"], "evidence": [[[["Last rites-1"]], [["Richard Dawkins-31", "Richard Dawkins-32"]], ["operation"]], [[["Last rites-1"]], [["Richard Dawkins-3"]], [["Atheism-1", "Richard Dawkins-24"]]], [[["Last rites-1"]], [["Richard Dawkins-3"]], ["operation"]]], "golden_sentence": [["The last rites, in Catholicism, are the last prayers and ministrations given to an individual of the faith, when possible, shortly before death."], ["Dawkins suggests that atheists should be proud, not apologetic, stressing that atheism is evidence of a healthy, independent mind.", ""]]}, {"qid": "b6ebc7f4d12f4c01de1c", "term": "Christopher Columbus", "description": "Italian explorer, navigator, and colonizer", "question": "Did Christopher Columbus condone multiple deadly sins?", "answer": true, "facts": ["The seven deadly sins are:  pride, greed, wrath, envy, lust, gluttony, and sloth.", "Under Columbus, every native of fourteen years of age or upward was to pay a large hawk's bell of gold dust or cotton and those who could not pay were punished.", " in just two years under Columbus's governorship, over 125,000 of the 250,000\u2013300,000 natives in Haiti were dead."], "decomposition": ["What are the deadly sins?", "What were Christopher Columbus's actions in the New World?", "Did #2 include more than one of #1?"], "evidence": [[[["Seven deadly sins-1"]], [["Christopher Columbus-56", "Christopher Columbus-68"]], ["operation"]], [[["Seven deadly sins-1"]], [["Christopher Columbus-93"], "no_evidence"], ["no_evidence", "operation"]], [[["Seven deadly sins-1"]], [["Christopher Columbus-43", "Christopher Columbus-98"]], ["operation"]]], "golden_sentence": [["The seven deadly sins, also known as the capital vices, or cardinal sins, is a grouping and classification of vices within Christian teachings, although it does not appear explicitly in the Bible."], ["In poor health, Columbus returned to Hispaniola on 19 August, only to find that many of the Spanish settlers of the new colony were in rebellion against his rule, claiming that Columbus had misled them about the supposedly bountiful riches of the New World.", "The 48-page report, found in 2006 in the national archive in the Spanish city of Simancas, contains testimonies from 23 people, including both enemies and supporters of Columbus, about the treatment of colonial subjects by Columbus and his brothers during his seven-year rule."]]}, {"qid": "f7c068a07b6ee13e8e94", "term": "Disneyland Paris", "description": "Theme park resort in France owned by The Walt Disney Company", "question": "Would an American feel lost due to language barriers at Disneyland Paris?", "answer": false, "facts": ["All Disneyland Paris cast members are required to know and speak English.", "Travelers from England go to Disneyland Paris often without issue."], "decomposition": ["What language do Americans mainly speak?", "At Disneyland Paris, what languages are workers required to know?", "Is #1 the same as #2?"], "evidence": [[[["Americans-34"]], [["Disneyland Paris-15"]], ["operation"]], [[["United States-80"]], [["Disneyland Paris-11"]], ["operation"]], [[["American English-2"]], [["Disneyland Paris-11", "Disneyland Paris-15"]], ["operation"]]], "golden_sentence": [["In 2007, about 226 million, or 80% of the population aged five years and older, spoke only English at home."], ["Topics of controversy also included Disney's American managers requiring English to be spoken at all meetings and Disney's appearance code for members of staff, which listed regulations and limitations for the use of makeup, facial hair, tattoos, jewellery, and more."]]}, {"qid": "729c6d2b8eba3e811866", "term": "Celery", "description": "species of plant", "question": "Could you make the kitchen 'holy trinity' without celery?", "answer": false, "facts": ["The 'Holy Trinity' in cooking is a base used for soups, stews, and more.", "The ingredients of the Holy Trinity base are onions, bell peppers, and celery."], "decomposition": ["What ingredients are part of the culinary holy trinity?", "Is celery absent from #1?"], "evidence": [[[["Holy trinity (cuisine)-3"]], [["Holy trinity (cuisine)-3"], "operation"]], [[["Holy trinity (cuisine)-1"]], ["operation"]], [[["Holy trinity (cuisine)-1"]], ["operation"]]], "golden_sentence": [["The holy trinity is the Cajun and Louisiana Creole variant of mirepoix; traditional mirepoix is two parts onions, one part carrots, and one part celery, whereas the holy trinity is typically equal measures of the three ingredients or one parts onions, one part celery, and one part green bell pepper."], [""]]}, {"qid": "54e9f8ea7b058777fd2a", "term": "Rurouni Kenshin", "description": "1994 Japanese manga series written and illustrated by Nobuhiro Watsuki", "question": "Is Rurouni Kenshin from same country as lead character in Nobunaga's Ambition?", "answer": true, "facts": ["Rurouni Kenshin is a manga series that comes from Japan.", "Nobunaga's Ambition is a video game series based on the experiences of Oda Nobunaga.", "Oda Nobunaga was a Japanese feudal lord."], "decomposition": ["Where is Rurouni Kenshin from?", "Where was Oda Nobunaga from?", "Is #1 the same as #2?"], "evidence": [[[["Rurouni Kenshin-1"]], [["Oda Nobunaga-1"]], ["operation"]], [[["Rurouni Kenshin-1"]], [["Nobunaga's Ambition-1", "Oda Nobunaga-4"]], ["operation"]], [[["Rurouni Kenshin-1"]], [["Oda Nobunaga-1"]], ["operation"]]], "golden_sentence": [["Rurouni Kenshin: Meiji Swordsman Romantic Story (Japanese: \u308b\u308d\u3046\u306b\u5263\u5fc3 -\u660e\u6cbb\u5263\u5ba2\u6d6a\u6f2b\u8b5a-, Hepburn: Rur\u014dni Kenshin -Meiji Kenkaku Romantan-), also known sometimes as Samurai X in the TV show, is a Japanese manga series written and illustrated by Nobuhiro Watsuki."], ["Oda Nobunaga (\u7e54\u7530 \u4fe1\u9577, Oda Nobunaga, listen; June 23, 1534 \u2013 June 21, 1582) was a Japanese daimy\u014d in the late 16th century who attempted to unify Japan during the late Sengoku period, and successfully gained control over most of Honshu through conquest."]]}, {"qid": "fb658aee4426cd7e9838", "term": "Aldi", "description": "Germany-based supermarket chain", "question": "Should you bring your own bags to Aldi?", "answer": true, "facts": ["Unlike most grocery stores, Aldi charges customers for use of paper bags.", "Aldi does not supply shopping carts without a deposit, so shopping bags are a good alternative."], "decomposition": ["In US Aldi stores, how do customers get shopping bags?", "How do customers get shopping carts?", "Do #1 and #2 cost money or value?"], "evidence": [[["no_evidence"], ["no_evidence"], ["no_evidence"]], [[["Aldi-23"], "no_evidence"], [["Aldi-23"], "no_evidence"], ["operation"]], [[["Aldi-1"], "no_evidence"], [["Aldi-32"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": []}, {"qid": "25161b64179ef6642fe8", "term": "Conan O'Brien", "description": "American television show host and comedian", "question": "Would most children be up past their bedtime if they were watching Conan O'Brien?", "answer": true, "facts": ["Conan O'Brien airs at 11 PM. ", "It is recommended that children are in bed before 10PM."], "decomposition": ["When does Conan O' Brian air?", "What is the recommended bedtime for children?", "Does #1 occur after #2?"], "evidence": [[[["Conan (talk show)-1"]], ["no_evidence"], [["Conan (talk show)-1"]]], [[["Conan O'Brien-34"]], [["Bedtime-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Conan (talk show)-1"]], ["no_evidence"], ["operation"]]], "golden_sentence": [["The show premiered on November 8, 2010, and is hosted by writer, comedian and performer Conan O'Brien, accompanied by his long-time \"sidekick\" Andy Richter."], [""]]}, {"qid": "f11cda26e9960db8291a", "term": "Stroke", "description": "Medical condition where poor blood flow to the brain causes cell death", "question": "Did Dale Jr.'s father crash his car due to a stroke?", "answer": false, "facts": ["Dale Earnhardt Jr. is his late father's namesake.", "Dale Earnhardt died in a crash during a NASCAR race. ", "Dale Earnhardt's car spun out of control after it tapped the car of another driver.", "Dale Earnhardt's death was a Basilar skull fracture."], "decomposition": ["Who was Dale Jr's father?", "What was the cause of the car crash that killed #1?", "Is #2 a stroke?"], "evidence": [[[["Dale Earnhardt Jr.-4"]], [["Dale Earnhardt-23"]], [["Dale Earnhardt-23"], "operation"]], [[["Dale Earnhardt Jr.-1"]], [["Dale Earnhardt-23"]], ["operation"]], [[["Dale Earnhardt Jr.-4"]], [["Dale Earnhardt-23"]], ["operation"]]], "golden_sentence": [["Dale Earnhardt Jr. was born and raised in Kannapolis, North Carolina, the son of Brenda Lorraine Jackson and Dale Earnhardt Sr. His maternal grandfather, Robert Gee Sr., was a NASCAR car builder."], [""], [""]]}, {"qid": "f408fdd7dfaed684e2f1", "term": "Governor of New Jersey", "description": "head of state and of government of the U.S. state of New Jersey", "question": "Was latest Republican governor of New Jersey as of 2020 heftiest politician ever?", "answer": false, "facts": ["Chris Christie was the latest Republican governor of New Jersey as of 2020.", "Chris Christie weighed around 322 pounds.", "President William Howard Taft weighed between 335 and 350 pounds."], "decomposition": ["Who was the latest Republican governor of New Jersey as of 2020?", "How much does #1 weigh?", "How much did President William Howard Taft weigh?", "Is #2 greater than #3?"], "evidence": [[[["Chris Christie-1", "Chris Christie-4"]], ["no_evidence"], [["William Howard Taft-107"]], ["no_evidence", "operation"]], [[["Chris Christie-1"]], [["Chris Christie-101"], "no_evidence"], [["William Howard Taft-107"]], ["operation"]], [[["Chris Christie-3"]], [["Chris Christie-123"], "no_evidence"], [["William Howard Taft-107"]], ["no_evidence"]]], "golden_sentence": [["Christopher James Christie (born September 6, 1962) is an American politician, political commentator, and former federal prosecutor who served as the 55th Governor of New Jersey from 2010 to 2018.", ""], ["Taft is remembered as the heaviest president; he was 5\u00a0feet 11\u00a0inches (1.80\u00a0m) tall and his weight peaked at 335\u2013340 pounds (152\u2013154\u00a0kg) toward the end of his presidency, although this later decreased, and by 1929 he weighed just 244 pounds (111\u00a0kg)."]]}, {"qid": "d0a7ec91a1cef226da90", "term": "Fever", "description": "common medical sign characterized by elevated body temperature", "question": "Will a person survive a fever of NY's highest recorded temperature?", "answer": false, "facts": ["The highest recorded temperature in NY was 108 degrees Fahrenheit.", "A temperature of 104 degrees Fahrenheit is life threatening and requires immediate medical attention."], "decomposition": ["What was NY's highest recorded temperature?", "Above what temperature will a fever become life-threatening?", "Is #1 less than #2?"], "evidence": [[[["Climate of New York-7"]], [["Fever-1"]], ["operation"]], [[["Climate of New York-7"]], [["Human body temperature-35"]], ["operation"]], [[["New York City-62"]], [["Fever-1"]], ["operation"]]], "golden_sentence": [["The record high for New York state is 108\u00a0\u00b0F (42\u00a0\u00b0C), set at Troy on July 22, 1926."], ["Fevers do not typically go higher than 41 to 42\u00a0\u00b0C (105. to 107.6\u00a0\u00b0F)."]]}, {"qid": "55bea18c414c08bbd6a1", "term": "Glucose", "description": "A simple form of sugar", "question": "4 Krispy Kreme glazed doughnuts exceed AHA  daily sugar allowance?", "answer": true, "facts": ["Glucose is a form of sugar that humans need in order to live.", "The AHA (American Heart Association) recommends no more than 38g of sugar a day.", "One Krispy Kreme glazed doughnut has 10g of sugar."], "decomposition": ["What does the AHA recommend as the maximum amount of sugar a day?", "How much sugar is in a Krispy Kreme glazed doghnut?", "What is #2 multiplied by 4?", "Is #3 greater than #1?"], "evidence": [[[["Healthy diet-8"]], [["Doughnut-1", "Junk food-6"], "no_evidence"], ["operation"], ["operation"]], [[["American Heart Association-19"], "no_evidence"], [["Doughnut-3"], "no_evidence"], ["no_evidence", "operation"], ["no_evidence", "operation"]], [[["Sugar-54"]], ["no_evidence"], ["no_evidence", "operation"], ["no_evidence", "operation"]]], "golden_sentence": [["It is recommended that children consume less than 25 grams of added sugar (100 calories) per day."], ["", ""]]}, {"qid": "7038db18730a645b6b01", "term": "Immanuel Kant", "description": "Prussian philosopher", "question": "Did Immanuel Kant ever meet the 14th president of the United States?", "answer": false, "facts": ["Immanuel Kant died on Feb 12, 1804", "Franklin Pierce was the 14th president of the United States", "Franklin PIerce was born Nov 23, 1804"], "decomposition": ["On what date did Immanuel Kant die?", "Who was the 14th president of the United States?", "On what date was #2 born?", "Is #3 before #1?"], "evidence": [[[["Immanuel Kant-1"]], [["Franklin Pierce-1"]], [["Franklin Pierce-1"]], ["operation"]], [[["Immanuel Kant-1"]], [["Franklin Pierce-1"]], [["Franklin Pierce-1"]], ["operation"]], [[["Immanuel Kant-1"]], [["Franklin Pierce-1"]], [["Franklin Pierce-1"]], ["operation"]]], "golden_sentence": [["Immanuel Kant (UK: /k\u00e6nt/, US: /k\u0251\u02d0nt/; German: [\u026a\u02c8ma\u02d0nu\u032fe\u02d0l \u02c8kant, -nu\u032f\u025bl -]; 22 April 1724\u00a0\u2013 12 February 1804) was an influential German philosopher in the Age of Enlightenment."], ["Franklin Pierce (November 23, 1804 \u2013 October 8, 1869) was the 14th president of the United States (1853\u20131857), a northern Democrat who saw the abolitionist movement as a fundamental threat to the unity of the nation."], [""]]}, {"qid": "e0ae9bd40ff4f9e173e1", "term": "Camel", "description": "Genus of mammals", "question": "Could a camel fit in a dog house?", "answer": false, "facts": ["Camels are approximately 5.5 to 6 feet tall.", "The largest dog ever was 3'8\" tall.", "Dog houses are built to fit dogs."], "decomposition": ["How large are camels?", "How large is a dog house?", "Is #1 less than or equal to #2?"], "evidence": [[[["Camel-4"]], ["no_evidence"], ["operation"]], [[["Camel-4"]], [["Dog-9"]], [["Camel-4", "Dog-9"], "operation"]], [[["Camel-4"]], [["Doghouse-1", "Great Dane-10"], "no_evidence"], ["operation"]]], "golden_sentence": [["Bactrian camels weigh 300 to 1,000\u00a0kg (660 to 2,200\u00a0lb) and dromedaries 300 to 600\u00a0kg (660 to 1,320\u00a0lb)."]]}, {"qid": "b2691e86837211ab5c81", "term": "Durian", "description": "genus of plants", "question": "Would Columbus have discovered Durian trees during his 1492 expedition?", "answer": false, "facts": ["Columbus ended up in the Americas", "Durian only exists in Southeast Asia"], "decomposition": ["Which country did Columbus discover on his 1492 experdition?", "Which countries could you find Durian on?", "Is there any overlap between #1 and #2?"], "evidence": [[[["Voyages of Christopher Columbus-1"]], [["Durian-1"]], ["operation"]], [[["Voyages of Christopher Columbus-1"]], [["Durian-1"]], [["Borneo-1", "Voyages of Christopher Columbus-27"], "operation"]], [[["Christopher Columbus-2"]], [["Borneo-2", "Durian-21", "Durian-22", "Durian-23"]], ["operation"]]], "golden_sentence": [["In 1492, a Spanish-based transatlantic maritime expedition led by Italian explorer Christopher Columbus encountered the Americas, continents which were virtually unknown in Europe, Asia and Africa and were outside the Old World political and economic system."], ["It is native to Borneo and Sumatra."]]}, {"qid": "995718aaa1355d7e537c", "term": "Chief Justice of the United States", "description": "Presiding judge of the U.S. Supreme Court", "question": "Is the current Chief Justice of the United States forbidden from buying alcohol?", "answer": false, "facts": ["The current Chief Justice of the United States is John Roberts.", "John Roberts is 65 years old.", "You have to be at least 21 years old to purchase alcohol in the United States."], "decomposition": ["How old do you have to be to buy alcohol legally in the United States?", "How old is John Roberts?", "Is #2 larger than #1?"], "evidence": [[[["Legal drinking age-6"]], [["John Roberts-1"]], ["operation"]], [[["National Minimum Drinking Age Act-6"]], [["John Roberts-1"]], ["operation"]], [[["Legal drinking age-6"]], [["John Roberts-1"]], ["operation"]]], "golden_sentence": [["In the United States, the minimum legal age to purchase any alcohol beverage from a shop, supermarket, liquor store, bar, club or any other licensed premises is 21 years of age; the two exceptions are Puerto Rico and the Virgin Islands where the age is 18."], ["John Glover Roberts Jr. (born January 27, 1955) is an American lawyer and jurist who serves as Chief Justice of the United States."]]}, {"qid": "1ee2b63c831f9f87ea1e", "term": "Mexican Revolution", "description": "major nationwide armed struggle in Mexico between 1910 and 1920", "question": "Could Barron Trump have lived through the Mexican Revolution?", "answer": true, "facts": ["The Mexican Revolution took place over a period of ten years", "Barron Trump is 14 years old"], "decomposition": ["How long did the Mexican Revolution last?", "How old has Barron Trump already lived as of 2020?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Mexican Revolution-1"]], [["Family of Donald Trump-11"]], ["operation"]], [[["Mexican Revolution-1"]], [["Family of Donald Trump-11"]], ["operation"]], [[["Mexican Revolution-1"]], [["Family of Donald Trump-11"]], ["operation"]]], "golden_sentence": [["The Mexican Revolution (Spanish: Revoluci\u00f3n Mexicana) was a major armed struggle, lasting roughly from 1910 to 1920, that transformed Mexican culture and government."], [""]]}, {"qid": "dfd8908c773b42bf1989", "term": "Martin Luther", "description": "Saxon priest, monk and theologian, seminal figure in Protestant Reformation", "question": "Did Martin Luther believe in Satan?", "answer": true, "facts": ["Martin Luther was a Protestant.", "Satan is also known as the devil.", "Protestants traditionally have believed in the devil as a being. "], "decomposition": ["What religion was Martin Luther?", "Do #1's believe in the existence of a non-human evil being (Satan, Beelzebub, the devil, etc)?"], "evidence": [[[["Martin Luther-1"]], [["Antichrist-1"], "no_evidence", "operation"]], [[["Martin Luther-12"]], [["Augustinians-1", "Devil-9"], "operation"]], [[["Martin Luther-111"]], [["Satan-32"]]]], "golden_sentence": [["He came to reject several teachings and practices of the Roman Catholic Church; in particular, he disputed the view on indulgences."], [""]]}, {"qid": "cf3ad97b02bbf8e35ea4", "term": "Snoopy", "description": "cartoon dog", "question": "Does Snoopy look like Chance from Homeward Bound?", "answer": false, "facts": ["Chance from Homeward Bound is a golden retriever. ", "Snoopy is black and white.", "Golden Retrievers are yellow in color."], "decomposition": ["What kind of animal is Chance from Homeward Bound?", "What color is Snoopy?", "What color is #1 typically?", "Is #2 the same as #3?"], "evidence": [[[["Homeward Bound: The Incredible Journey-2"]], [["Snoopy-1"], "no_evidence"], [["American Bulldog-7"]], ["no_evidence", "operation"]], [[["Homeward Bound: The Incredible Journey-2"]], [["Snoopy-1"], "no_evidence"], [["American Bulldog-7"]], ["operation"]], [[["Homeward Bound: The Incredible Journey-2"]], [["Snoopy-2"]], [["American Bulldog-7"]], ["operation"]]], "golden_sentence": [["He shares his home with Shadow (voiced by Don Ameche), a wise old Golden Retriever owned by Jamie's brother Peter (Benj Thall), and Sassy (voiced by Sally Field), a smart-mouthed Himalayan cat owned by Jamie and Peter's sister Hope (Veronica Lauren)."], [""], ["The color conformation is quite varied, but solid black or any degree of merle is considered a cosmetic fault, and a blue color is a disqualification by the National Kennel Club Breed Standard."]]}, {"qid": "8d499d96e99c1ed55d10", "term": "Canidae", "description": "family of mammals", "question": "Were any members of Canidae in Aesop's Fables?", "answer": true, "facts": ["Canidae is a family of mammals that includes dogs, foxes, and coyotes.", "Aesop's Fables was a collection of stories with animals as the main characters.", "One of the most famous stories involves a fox and a lion."], "decomposition": ["Which animals were typical characters in Aesop's Fables?", "Do any of #1 belong to the family Canidae?"], "evidence": [[[["Aesop's Fables-53"]], [["Canidae-1"], "operation"]], [[["Aesop-19", "The Boy Who Cried Wolf-1"], "no_evidence"], ["operation"]], [[["Aesop's Fables-65"]], [["Canidae-1", "Dog-1"], "operation"]]], "golden_sentence": [["In the Jewish 'fox fables' of Berechiah ha-Nakdan, the humorous account of the hares and the frogs was made the occasion to recommend trust in God, while Christian reinterpretation of animal symbolism in Mediaeval times turned The Wolf and the Crane into a parable of the rescue of the sinner's soul from Hell."], ["The Caninae are known as canines, which includes domestic dogs, wolves, foxes and other extant and extinct species."]]}, {"qid": "cd7b3ddde0e20521b43b", "term": "United States Air Force", "description": "Air and space warfare branch of the United States Armed Forces", "question": "Would United States Air Force consider Return of the Jedi's Han Solo bad hypothetical candidate?", "answer": true, "facts": ["Han Solo is an ace pilot ally in the Star Wars universe.", "The US Air Force requires candidates to be between 18 and 35 years old.", "Return of the Jedi's Han Solo is 36 years of age.", "The US Air Force requires a candidate to be an American citizen.", "Han Solo is from the planet Corellia in a galaxy far, far, away."], "decomposition": ["What requirements does the US Air Force demand of potential candidates?", "What are the characteristics of character Han Solo as featured in Return of the Jedi?", "Does #2 fail to satisfy all of #1?"], "evidence": [[[["United States Air Force-60"], "no_evidence"], [["Han Solo-12"]], ["no_evidence"]], [[["United States Air Force Basic Military Training-34"]], [["Han Solo-36"]], ["operation"]], [[["United States Air Force Fitness Assessment-1"], "no_evidence"], [["Han Solo-11", "Han Solo-12"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "07b2e82e8737c1d14ccd", "term": "Easter", "description": "Major Christian festival celebrating the resurrection of Jesus", "question": "Would Jesus understand the Easter Bunny?", "answer": false, "facts": ["During the time of Jesus, Easter was not a holiday yet.", "Rabbits were not of any profound significance to Jesus."], "decomposition": ["When did Easter become a holiday?", "In what year did Jesus die?", "Did #1 occur before #2?"], "evidence": [[[["Easter-1"]], [["Jesus-1"]], ["operation"]], [[["Easter-10"]], [["Jesus-1"]], ["operation"]], [[["Easter-1"], "no_evidence"], [["English festivals-15"]], ["no_evidence", "operation"]]], "golden_sentence": [["Easter, also called Pascha (Greek, Latin) or Resurrection Sunday, is a festival and holiday commemorating the resurrection of Jesus from the dead, described in the New Testament as having occurred on the third day after his burial following his crucifixion by the Romans at Calvary c. 30 AD."], ["Jesus (c.\u20094 BC\u00a0\u2013 c.\u2009AD 30 / 33), also referred to as Jesus of Nazareth or Jesus Christ, was a first-century Jewish preacher and religious leader."]]}, {"qid": "eb3e63185d860d15f026", "term": "Greek alphabet", "description": "script that has been used to write the Greek language", "question": "Is the Greek alphabet as common as Sumerian cuneiform?", "answer": false, "facts": ["The Greek alphabet is still commonly used", "Sumerian cuneiform is not used contemporarily "], "decomposition": ["Does the Greek Alphabet still have widespread present-day use/application?", "Does the Sumerian cuneiform still have widespread present-day use/application?", "Are #1 and #2 the same?"], "evidence": [[[["Greek alphabet-41"], "no_evidence"], [["Cuneiform-3"], "no_evidence"], ["operation"]], [[["Greek language-1", "Greek language-3"]], [["Cuneiform-5"]], ["operation"]], [[["Greek alphabet-35"]], [["Cuneiform-49"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["The Greek letter names of most fraternal organizations today are meaningless and do not stand for anything, which Caroline Winterer states is \"understandable, considering that general knowledge of the ancient Greek alphabet and language has almost entirely vanished.\""], [""]]}, {"qid": "6bfbe67916b196aa0133", "term": "Sainsbury's", "description": "chain of supermarkets in the United Kingdom", "question": "Could Sainsbury's buy Tesco?", "answer": false, "facts": ["Sainsbury is a business worth \u00a329.007 billion in 2019.", "Tesco is a business worth \u00a363.911 billion in 2019.", "63 billion is more than 29 billion.", "A business needs to have enough revenue to buy another business."], "decomposition": ["What is the total value of Sainsbury's?", "What is the total value of Tesco?", "Is #1 greater than #2?"], "evidence": [[[["Sainsbury's-3"], "no_evidence"], [["Tesco-5"], "no_evidence"], ["operation"]], [[["Sainsbury's-1", "Sainsbury's-56"], "no_evidence"], [["Tesco-5"]], ["operation"]], [[["Sainsbury's-1"], "no_evidence"], [["Tesco-5"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], ["It had a market capitalization of approximately \u00a318.1\u00a0billion as of 22 April 2015, the 28th-largest of any company with a primary listing on the London Stock Exchange."]]}, {"qid": "4a2b16831932b87ba7c4", "term": "Twin", "description": "One of two offspring produced in the same pregnancy. Use with P31 on items for one twin", "question": "Are all twins the same gender?", "answer": false, "facts": ["Identical twins are always the same gender.", "However, identical twins are very rare. Most twin cases are formed from two different fertilizations during the same conception event.", "Non-identical twins can be opposite gender or same gender."], "decomposition": ["Which kind of twins are usually of the same gender?", "Are there other kind(s) of twins apart from #1?", "Are #2 also usually of the same gender?"], "evidence": [[[["Twin-20"]], [["Twin-9"]], [["Twin-11"]]], [[["Twin-14"], "no_evidence"], [["Twin-9"]], ["no_evidence", "operation"]], [[["Twin-9"], "no_evidence"], [["Twin-14", "Twin-59"]], ["operation"]]], "golden_sentence": [["Monozygotic twins are genetically nearly identical and they are always the same sex unless there has been a mutation during development."], ["Dizygotic (DZ) or fraternal twins (also referred to as \"non-identical twins\", \"dissimilar twins\", \"biovular twins\", and, informally in the case of females, \"sororal twins\") usually occur when two fertilized eggs are implanted in the uterus wall at the same time."], ["Like any other siblings, dizygotic twins may look similar, particularly given that they are the same age."]]}, {"qid": "60fcab44e00aed744c24", "term": "Rosemary", "description": "species of plant, rosemary", "question": "Are looks the easiest way to tell rosemary from lavender? ", "answer": false, "facts": ["Before blooming, lavender and rosemary look remarkably similar.", "Rosemary has a pine-like scent.", "Lavender has a lighter, more floral scent."], "decomposition": ["What does rosemary look like?", "What does lavender look like?", "Are there significant differences between #1 and #2?"], "evidence": [[[["Rosemary-1"]], [["Lavandula-5"]], ["operation"]], [[["Rosemary-1"]], [["Lavandula-5"]], ["no_evidence", "operation"]], [[["Rosemary-1"]], [["Lavandula-26"]], ["operation"]]], "golden_sentence": [["Salvia rosmarinus, commonly known as rosemary, is a woody, perennial herb with fragrant, evergreen, needle-like leaves and white, pink, purple, or blue flowers, native to the Mediterranean region."], [""]]}, {"qid": "53087e2ace0f7508434f", "term": "Nicole Kidman", "description": "Australian-American actress and film producer", "question": "Does Nicole Kidman despise Roman Josi?", "answer": false, "facts": ["Nicole Kidman supports the Nashville Predators and has been photographed almost nightly throughout the season.", "Roman Josi is a Swiss professional ice hockey defenceman who currently serves as captain of the Nashville Predators."], "decomposition": ["Does Nicole Kidman hate the Nashville Predators players?", "Does Roman Josi play for the Nashville Predators?", "Is #2 the same answer as #1?"], "evidence": [[[["Nicole Kidman-42"], "operation"], [["Roman Josi-1"]], ["operation"]], [[["Nicole Kidman-42"]], [["Roman Josi-1"]], ["operation"]], [[["Nicole Kidman-42"]], [["Roman Josi-1"]], ["operation"]]], "golden_sentence": [[""], ["Roman Josi (born 1 June 1990) is a Swiss professional ice hockey defenceman who currently serves as captain of the Nashville Predators of the National Hockey League (NHL)."]]}, {"qid": "a9690da06b43ffc29c17", "term": "Fairy", "description": "mythical being or legendary creature", "question": "Is a fairy more prevalent in world myths than a valkyrie?", "answer": true, "facts": ["Valkyries are female figures that choose heroes to bring to Valhalla.", "Valkyries are exclusive to Norse mythology.", "A fairy is a mystical magical being that can be found in Celtic, Slavic, German, English, and French folklore."], "decomposition": ["In what myths do the Valkyries appear?", "Do fairies appear in more myths than #1?"], "evidence": [[[["Valkyrie-1"]], [["Fairy-12"]]], [[["Valkyrie-2"], "no_evidence"], [["Fairy-2"], "no_evidence", "operation"]], [[["Valkyrie-1"]], [["Fairyland-1", "Fairyland-3"]]]], "golden_sentence": [["In Norse mythology, a valkyrie (/v\u00e6l\u02c8k\u026a\u0259ri, -\u02c8ka\u026ari, v\u0251\u02d0l-, \u02c8v\u00e6lk\u0259ri/; from Old Norse valkyrja \"chooser of the slain\") is one of a host of female figures who choose those who may die in battle and those who may live."], [""]]}, {"qid": "e4f366ddf5c07296d8e2", "term": "YMCA", "description": "Worldwide organization founded in 1844 on principles of muscular Christianity", "question": "Can you get Raclette in YMCA headquarters city?", "answer": true, "facts": ["YMCA is headquartered in Geneva, Switzerland.", "Raclette is a melted cheese and potato dish.", "Raclette is one of several foods Geneva, Switzerland is famous for."], "decomposition": ["Where is the YMCA headquartered?", "What foods is #1 famous for?", "Is raclette in #2?"], "evidence": [[[["YMCA-1"]], [["Swiss Cheese Union-9"], "no_evidence"], [["Raclette-2"], "operation"]], [[["YMCA-1"]], [["Swiss cuisine-2"]], ["operation"]], [[["YMCA-53"]], [["Lincoln Park, Chicago-37"]], [["Raclette-1"], "operation"]]], "golden_sentence": [["YMCA, sometimes regionally called the Y, is a worldwide youth organization based in Geneva, Switzerland, with more than 64\u00a0million beneficiaries in 120 countries."], ["Nevertheless, Barmettler continued to produce small, soft cheeses stanser fladen, selling them directly to small vendors like hotels and small restaurants rather than larger distributors that would have caught the attention of the cartel."], [""]]}, {"qid": "6b4532ee8dc1c8996b56", "term": "Asteroid", "description": "Minor planet that is not a comet", "question": "Can I build a house on an asteroid?", "answer": false, "facts": ["Building a house requires gravity to hold the house to the surface.", "The largest asteroid in our Solar System is Ceres, 583 miles across.", "Asteroids are not large enough to create noticeable gravity."], "decomposition": ["What do you need to hold a house to the surface of an asteroid?", "Are asteroids large enough to produce #1?"], "evidence": [[[["Gravity-1"], "no_evidence"], [["Colonization of the asteroids-5"], "operation"]], [[["Gravity-1"]], [["Asteroid-48"], "no_evidence", "operation"]], [["no_evidence"], ["no_evidence"]]], "golden_sentence": [[""], ["Asteroids are not large enough to produce significant gravity, making it difficult to land a spacecraft."]]}, {"qid": "3de4186f7ee56392ce10", "term": "Maya Angelou", "description": "American poet, author, and civil rights activist", "question": "Would someone in CHE101 require a Maya Angelou book?", "answer": false, "facts": ["CHE101 is short for Chemistry 101, a basic college class.", "Maya Angelou's writings are suited for classes in history and literature. "], "decomposition": ["What class is CHE101", "Are Maya Angelou books suitable for #1?"], "evidence": [[["no_evidence"], [["Maya Angelou-1"], "no_evidence"]], [[["General chemistry-1"]], ["operation"]], [["no_evidence"], [["Maya Angelou-1"], "no_evidence", "operation"]]], "golden_sentence": [[""]]}, {"qid": "b1513685172f8ee63d36", "term": "Diarrhea", "description": "Loose or liquid bowel movements", "question": "Can too many oranges cause diarrhea?", "answer": true, "facts": ["Oranges are very high in fiber and sugar.", "Too much fiber can cause diarrhea."], "decomposition": ["What high-level nutritional values do oranges have?", "Can excess of any of #1 cause diarrhea?"], "evidence": [[[["Mandarin orange-12"]], [["Vitamin C-21"], "operation"]], [[["Orange (fruit)-40", "Orange (fruit)-41"], "no_evidence"], ["no_evidence", "operation"]], [[["Orange (fruit)-20"]], [["Vitamin C-21"], "operation"]]], "golden_sentence": [[""], ["In theory, high vitamin C intake may cause excessive absorption of iron."]]}, {"qid": "ea418da9b9c71567fdbd", "term": "Europa (moon)", "description": "The smallest of the four Galilean moons of Jupiter", "question": "Could the surface of Europa fry an egg?", "answer": false, "facts": ["Europa is known for having an icy surface.", "For an egg to become firm, the ground must be at least 158 degrees Fahrenheit. ", "Ice forms at 32 degrees Fahrenheit.", "Europa's temperatures are all in the negatives on the Fahrenheit scale."], "decomposition": ["At what temperature will an egg become fried?", "What is the temperature on the surface of Europa?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Egg as food-28"]], [["Europa (moon)-22"]], ["operation"]], [[["Frying-3"], "no_evidence"], [["Europa (moon)-22"]], ["operation"]], [[["Boiled egg-4"], "no_evidence"], [["Europa (moon)-22"]], ["operation"]]], "golden_sentence": [["Egg white gels at different temperatures: 60 to 73\u00a0\u00b0C (140 to 163\u00a0\u00b0F)."], ["Europa's surface temperature averages about 110\u00a0K (\u2212160\u00a0\u00b0C; \u2212260\u00a0\u00b0F) at the equator and only 50\u00a0K (\u2212220\u00a0\u00b0C; \u2212370\u00a0\u00b0F) at the poles, keeping Europa's icy crust as hard as granite."]]}, {"qid": "27d077bde6728a6934ea", "term": "August", "description": "eighth month in the Julian and Gregorian calendars", "question": "Can I ski in Steamboat Springs, Colorado in August?", "answer": false, "facts": ["Skiing requires snow. ", "Snow melts at temperatures higher than 0 degrees Celsius. ", "Average temperature for Steamboat Springs, Colorado in August is 27.3 degrees Celsius."], "decomposition": ["What is the average temperature in Steamboat Springs, CO in August?", "What is the melting point of snow?", "Is #1 lower than #2?"], "evidence": [[[["Steamboat Springs, Colorado-17"], "no_evidence"], [["Melting point-3"]], [["Frost (temperature)-1"], "operation"]], [[["Steamboat Springs, Colorado-17"]], [["Melting point-3"]], ["operation"]], [[["Steamboat Springs, Colorado-17"], "no_evidence"], [["Water-95"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["For example, agar melts at 85\u00a0\u00b0C (185\u00a0\u00b0F) and solidifies from 31\u00a0\u00b0C (88\u00a0\u00b0F; 304\u00a0K); such direction dependence is known as hysteresis."], [""]]}, {"qid": "67e5c6ca1a0498248f68", "term": "Dopamine", "description": "chemical compound", "question": "Is dopamine snorted nasally by drug users?", "answer": false, "facts": ["Dopamine is a hormone and a neurotransmitter.", "Neurotransmitters are produced endogenously by the body and are not consumed externally."], "decomposition": ["What kind of substance is dopamine?", "Are #1 usually taken through the nose by drug users?"], "evidence": [[[["Dopamine-6"]], [["Dopamine-15"]]], [[["Dopamine-1"]], ["operation"]], [[["Dopamine-1"]], ["operation"]]], "golden_sentence": [["Like most amines, dopamine is an organic base."], [""]]}, {"qid": "162d2b5cdc55d0675a9c", "term": "Moustache", "description": "Facial hair grown on the upper lip", "question": "Is it common for women to have moustaches?", "answer": false, "facts": ["Facial hair doesn't normally grow on women like it does on men.", "A little bit of hair can grow between the upper lip and nose but it's a very small amount and generally not enough to be noticeable."], "decomposition": ["Which gender grows sizable moustaches more commonly?", "Is #1 the same as women?"], "evidence": [[[["Moustache-9"]], ["operation"]], [[["Beard-27"]], [["Beard-27"], "operation"]], [[["Facial hair-2"]], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "c38ed1a9cb5c293d4d90", "term": "Michael", "description": "male given name", "question": "Is Michael an unpopular name in the United States?", "answer": false, "facts": ["More boys were named Michael in the United States than any other name between 1954 and 1998.", "Michael and its foreign variants were within the top 20 names in Canada, Australia, UK, and Europe in the 2010s."], "decomposition": ["What are the most popular names in the USA?", "Is Michael absent from #1?"], "evidence": [[["no_evidence"], ["no_evidence", "operation"]], [[["Michael-5"], "no_evidence"], ["operation"]], [[["John (given name)-2", "Michael-5", "Richard-2", "Robert-3"]], ["operation"]]], "golden_sentence": []}, {"qid": "26f2be23bb14c311bbfb", "term": "Central Park Zoo", "description": "Zoo in Central Park, Manhattan, New York City", "question": "Is Central Park Zoo located on an island?", "answer": true, "facts": ["Central Park Zoo is located in Manhattan.", "Manhattan is an island."], "decomposition": ["Where is the Central Park Zoo located?", "Is #1 an island?"], "evidence": [[[["Central Park Zoo-1", "Central Park-1"]], [["Manhattan-1"], "operation"]], [[["Central Park Zoo-4"]], [["Central Park-2", "Manhattan-34"]]], [[["Central Park Zoo-1", "Central Park-1"]], [["Manhattan-1"]]]], "golden_sentence": [["The Central Park Zoo is a 6.5-acre (2.6\u00a0ha) zoo located at the southeast corner of Central Park in New York City.", "Central Park is an urban park in Manhattan, New York City, located between the Upper West Side and the Upper East Side."], ["The borough consists mostly of Manhattan Island, bounded by the Hudson, East, and Harlem rivers; as well as several small adjacent islands."]]}, {"qid": "385ddeb9849a5c7c33e0", "term": "Cosmic microwave background", "description": "Universe events since the Big Bang 13.8 billion years ago", "question": "Can food be cooked in the cosmic microwave background?", "answer": false, "facts": ["The cosmic microwave background is faint electromagnetic radiation in space that is a remnant of the Big Bang.", "Food can be cooked in a microwave oven, but not in the remnants of space radiation."], "decomposition": ["What kind of radiation is used in microwave ovens?", "What kind of radiation is produced in the cosmic microwave background?", "Is #1 the same as #2?"], "evidence": [[[["Microwave oven-1"]], [["Cosmic microwave background-1"]], ["operation"]], [[["Microwave oven-1"]], [["Cosmic microwave background-1"]], ["operation"]], [[["Microwave oven-1"]], [["Cosmic microwave background-1"]], ["operation"]]], "golden_sentence": [["A microwave oven (commonly referred to as a microwave) is an electric oven that heats and cooks food by exposing it to electromagnetic radiation in the microwave frequency range."], ["The cosmic microwave background (CMB, CMBR), in Big Bang cosmology, is electromagnetic radiation as a remnant from an early stage of the universe, also known as \"relic radiation\"."]]}, {"qid": "de516e07bf2dd12fd7bc", "term": "Parachuting", "description": "action sport of exiting an aircraft and returning to Earth using a parachute", "question": "Is coal needed to practice parachuting?", "answer": true, "facts": ["Parachuting requires a parachute.", "Parachutes are made from nylon.", "Nylon is made from coal. "], "decomposition": ["What is one of the most important items that you need to go parachuting?", "What is #1 made out of?", "Is #2 originally made from coal?"], "evidence": [[[["Parachute-1"]], [["Nylon riots-3"]], [["Nylon-21"]]], [[["Parachute-1"]], [["Gerard B\u00e9rchet-2"]], [["Nylon-16"]]], [[["Parachute-1"]], [["Parachute-1"]], [["Nylon-16"], "operation"]]], "golden_sentence": [["A variety of loads are attached to parachutes, including people, food, equipment, space capsules, and bombs."], ["At first, canvas was used to create parachute canopies, and silk was much more effective (stronger and thinner)."], ["DuPont changed its campaign strategy, emphasizing that nylon was made from \"coal, air and water\", and started focusing on the personal and aesthetic aspects of nylon, rather than its intrinsic qualities."]]}, {"qid": "d0f19d991fdb4be42cc4", "term": "Cactus", "description": "Family of mostly succulent plants, adapted to dry environments", "question": "Would an oil painter avoid reds from scale insects that live on a cactus?", "answer": true, "facts": ["Carmine is the product of an insect that lives on some cacti", "Carmine is not stable in oil paints and its usage has been discontinued", "Carmine is red"], "decomposition": ["What red pigments are made from insects?", "What scale insects live on cacti?", "What pigments overlap with #1 and #2?", "Is #3 unstable in oil paint?"], "evidence": [[[["Carmine-1"]], [["Cochineal-10"]], [["Cochineal-10"]], [["Carmine-9"]]], [[["Cochineal-1"], "no_evidence"], [["Scale insect-15"], "no_evidence"], [["Carmine-1"]], [["Carmine-9"], "operation"]], [[["Red-60"]], [["Cochineal-1", "Opuntia-31"]], ["operation"], [["Oil paint-18"], "no_evidence"]]], "golden_sentence": [[""], ["Cochineal insects are soft-bodied, flat, oval-shaped scale insects."], [""], ["It is not very stable in oil paint, and its use ceased after new and better red pigments became available."]]}, {"qid": "fb2c044000126b62c2eb", "term": "New York Public Library", "description": "Public library system in New York City", "question": "Could you go to New York Public Library and the Six Flags Great Escape in the same day?", "answer": true, "facts": ["Six Flags Great Escape is located in Lake George, NY.", "New York Public Library is located in New York City.", "Lake George is 3.5 driving hours from New York City."], "decomposition": ["Where is Six Flags Great Escape located?", "Where is The New York Public Library located?", "How long does it take to drive from #1 to #2?", "Is #3 less than 24 hours?"], "evidence": [[[["The Great Escape and Hurricane Harbor-1"]], [["New York Public Library-1"]], ["no_evidence"], ["no_evidence"]], [[["The Great Escape and Hurricane Harbor-1"]], [["New York Public Library Main Branch-1"]], ["no_evidence"], ["operation"]], [[["The Great Escape and Hurricane Harbor-1"]], [["New York Public Library-1"]], ["no_evidence", "operation"], ["no_evidence", "operation"]]], "golden_sentence": [["It is located approximately 60 miles (97\u00a0km) north of Albany, in Queensbury, New York."], ["The New York Public Library (NYPL) is a public library system in New York City."]]}, {"qid": "46fcbcc8f8f96ca7e54c", "term": "Himalayas", "description": "Mountain range in Asia", "question": "Did any of religions in which Himalayas are sacred originate in 19th century?", "answer": false, "facts": ["The Himalaya mountains are sacred to three religions: Hinduism, Buddhism, and Jainism.", "Hinduism was first synthesized around 500 BC.", "Jainism began in the 6th century BC.", "Buddhism originated around the 5th century BC."], "decomposition": ["Which religions believe that the Himalayas are sacred?", "When did #1 originate?", "Are any of #2 equal to the 19th century?"], "evidence": [[[["Himalayas-36"], "no_evidence"], [["Hinduism-1", "Jainism-1"], "no_evidence"], ["operation"]], [[["Himalayas-40"]], [["Buddhism-11", "Hinduism-7", "Jainism-29", "Sikhism-6"], "no_evidence"], ["operation"]], [[["Himalayas-36"], "no_evidence"], [["Hindu art-7"], "no_evidence"], ["operation"]]], "golden_sentence": [["For the Hindus, the Himalayas are personified as Himavath, the father of the goddess Parvati."], ["This \"Hindu synthesis\" started to develop between 500 BCE and 300 CE, after the end of the Vedic period (1500 to 500 BCE), and flourished in the medieval period, with the decline of Buddhism in India.", ""]]}, {"qid": "df16df61aef0fb016d97", "term": "Goofy", "description": "Disney cartoon character", "question": "Did brother of Goofy creator's employer commit an excommunicable offense?", "answer": true, "facts": ["Goofy was created by Art Babbitt who worked for Walt Disney.", "Walt Disney's brother, Roy Disney, was a member of the Freemasons.", "The Catholic Church has a litany of offenses that can get someone excommunicated.", "Being initiated to Freemasonry, is listed as an excommunicable offense."], "decomposition": ["Who created the character Goofy?", "Who did #1 work for?", "Who are #2's siblings?", "Are any of #3 Freemasons?", "Is #4 a reason for excommunication?"], "evidence": [[[["Goofy-4"]], [["Pinto Colvig-8"]], [["Roy O. Disney-1"]], ["no_evidence"], [["Freemasonry-62"], "operation"]], [[["Goofy-2"]], [["Goofy-2", "Goofy-43"], "no_evidence"], ["no_evidence"], ["no_evidence"], ["no_evidence"]], [[["Goofy-4"], "no_evidence"], [["Walt Disney-1"]], [["Flora Call Disney-4"], "no_evidence"], ["no_evidence"], [["Papal ban of Freemasonry-1"], "no_evidence", "operation"]]], "golden_sentence": [["Animator Art Babbit is credited for developing his character."], ["For Fleischer, he worked on 1939's Gulliver's Travels, for which he voiced town crier Gabby, who was spun off into his own short-lived series."], [""], [""]]}, {"qid": "d21f31625c026121de8a", "term": "Toyota Supra", "description": "A sports car and grand tourer manufactured by Toyota Motor Corporation", "question": "Can a Toyota Supra make a vlog?", "answer": false, "facts": ["A vlog is a \"video blog\" about one's experience", "A Toyota Supra does not have consciousness to recount any experiences"], "decomposition": ["What is a vlog?", "Who makes #1?", "What is a Toyota Supra?", "Is #3 the same as #2?"], "evidence": [[[["Vlog-1"]], [["Vlog-20"]], [["Toyota Supra-1"]], ["operation"]], [[["Vlog-1"]], [["Vlog-14"]], [["Toyota Supra-1"]], ["operation"]], [[["Vlog-1"]], [["Vlog-2"]], [["Toyota Supra-1"]], ["operation"]]], "golden_sentence": [["A video blog or video log, usually shortened to vlog /vl\u0252\u0261/, is a form of blog for which the medium is video, and is a form of web television."], ["YouTube currently ranks among the top three most-visited sites on the web."], ["The Toyota Supra (Japanese: \u30c8\u30e8\u30bf\u30fb\u30b9\u30fc\u30d7\u30e9, Toyota S\u016bpura) is a sports car and grand tourer manufactured by Toyota Motor Corporation beginning in 1978."]]}, {"qid": "0f82429267fec3d167e0", "term": "New Testament", "description": "Second division of the Christian biblical canon", "question": "Was Daniel thrown into the lion's den in the New Testament?", "answer": false, "facts": ["The Book of Daniel is a book in the Old Testament of the Bible.", "The Bible is divided into the Old Testament and the New Testament.", "The New Testament focuses on four Gospels regarding the life of Jesus."], "decomposition": ["Which book of the Bible has the story of Daniel in the lions' den?", "Is #1 in the New Testament of the Bible?"], "evidence": [[[["Daniel in the lions' den-1"]], ["operation"]], [[["Book of Daniel-13"]], [["Old Testament-16"], "operation"]], [[["Daniel in the lions' den-1"]], [["Book of Daniel-2"]]]], "golden_sentence": [["Daniel in the lions' den Daniel's age Daniel in the lions' den (chapter 6 of the Book of Daniel) tells of how the legendary Daniel is saved from lions by the God of Israel \"because I was found blameless before him\" (Daniel 6:22)."]]}, {"qid": "77886536e6bc0ac1c97b", "term": "Citrus", "description": "genus of fruit-bearing plants (source of fruit such as lemons and oranges)", "question": "Would someone on antidepressants need to be cautious of some citrus fruits?", "answer": true, "facts": ["Grapefruit is a citrus fruit.", "Grapefruit can cause some medications to reach unintentionally high levels in the body. ", "SSRI's are a medication type that can be affected by grapefruit."], "decomposition": ["Which fruits can affect antidepressant medications?", "Is #1 a citrus fruit?"], "evidence": [[["no_evidence"], ["no_evidence"]], [[["Antidepressant-30"]], ["no_evidence", "operation"]], [[["Grapefruit\u2013drug interactions-1", "Grapefruit\u2013drug interactions-2"]], [["Grapefruit\u2013drug interactions-2"]]]], "golden_sentence": []}, {"qid": "111f3c776d50f8327a79", "term": "Diarrhea", "description": "Loose or liquid bowel movements", "question": "Do people take laxatives because they enjoy diarrhea?", "answer": false, "facts": ["People take laxatives to relieve constipation and associated pain.", "People with eating disorders take laxatives to lose weight."], "decomposition": ["What is the primary reason for taking laxatives?", "Is #1 to induce diarrhea?"], "evidence": [[[["Laxative-1"]], ["operation"]], [[["Laxative-1"]], [["Laxative-1"], "operation"]], [[["Laxative-1"]], [["Laxative-2"], "operation"]]], "golden_sentence": [["Laxatives, purgatives, or aperients are substances that loosen stools and increase bowel movements."]]}, {"qid": "cbd928bd3a3e7e66b54e", "term": "Durian", "description": "genus of plants", "question": "Could Durian cause someone's stomach to feel unwell?", "answer": true, "facts": ["Durian has a pungent odor that many people describe as being similar to feet and onions.", "Unpleasant smells can make people feel nauseous. "], "decomposition": ["What would some people describe durian's smell as?", "Would #1  cause some people to feel unwell?"], "evidence": [[[["Durian-3"]], [["Durian-50"]]], [[["Durian-3"]], ["operation"]], [[["Durian-29"]], [["Durian-50"]]]], "golden_sentence": [["Some people regard the durian as having a pleasantly sweet fragrance, whereas others find the aroma overpowering with an unpleasant odour."], ["The latter belief can be traced back at least to the 18th century when Rumphius stated that one should not drink alcohol after eating durians as it will cause indigestion and bad breath."]]}, {"qid": "7367375fd93e6c7a8450", "term": "Swallow", "description": "family of birds", "question": "Did the swallow play a role in a famous film about King Arthur?", "answer": true, "facts": ["Monty Python and the Holy Grail was a famous film about King Arthur", "In Monty Python and the Holy Grail, swallows are mentioned several times"], "decomposition": ["What Monty Python film is about King Arthur?", "Are swallows mentioned several times in #1?"], "evidence": [[[["Monty Python and the Holy Grail-1"]], ["no_evidence", "operation"]], [[["Monty Python and the Holy Grail-1"]], [["Monty Python and the Holy Grail-4"]]], [[["Monty Python and the Holy Grail-2", "Monty Python and the Holy Grail-4", "Monty Python and the Holy Grail-9"]], ["operation"]]], "golden_sentence": [["Monty Python and the Holy Grail is a 1975 British comedy film concerning the Arthurian legend, written and performed by the Monty Python comedy group of Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones and Michael Palin, and directed by Gilliam and Jones."]]}, {"qid": "fdf30aa20e1d3f3f76a5", "term": "Pan (god)", "description": "Ancient Greek god of the wilds, shepherds, and flocks", "question": "Does the Boy Who Cried Wolf hypothetically have reason to pray to Pan?", "answer": true, "facts": ["Pan is the ancient Greek god of the wild, shepherds and flocks.", "The Boy Who Cried Wolf, from Aesop's Fables, was a shepherd boy."], "decomposition": ["What is the profession of The Boy Who Cried Wolf?", "What profession is Pan the god of?", "Is #1 the same as #2?"], "evidence": [[[["The Boy Who Cried Wolf-2"]], [["Pan (god)-1"]], ["operation"]], [[["The Boy Who Cried Wolf-2"]], [["Pan (god)-1"]], ["operation"]], [[["The Boy Who Cried Wolf-2"]], [["Pan (god)-1"]], ["operation"]]], "golden_sentence": [["The tale concerns a shepherd boy who repeatedly tricks nearby villagers into thinking a wolf is attacking his town's flock."], ["In ancient Greek religion and mythology, Pan (/\u02c8p\u00e6n/; Ancient Greek: \u03a0\u03ac\u03bd, romanized:\u00a0P\u00e1n) is the god of the wild, shepherds and flocks, nature of mountain wilds, rustic music and impromptus, and companion of the nymphs."]]}, {"qid": "294db08890d7ec6d70f9", "term": "Balkans", "description": "Geopolitical and cultural region of southeastern Europe", "question": "Are there enough people in the Balkans to match the population of Japan?", "answer": false, "facts": ["There are approximately 55 million people in the Balkans", "There are more than 125 million people in Japan"], "decomposition": ["What is the population of the Balkans?", "What is the population of Japan?", "Is #1 more than #2?"], "evidence": [[[["Balkans-16"]], [["Demographics of Japan-5"]], ["operation"]], [[["Balkans-16"]], [["Japan-2"]], ["operation"]], [[["Balkans-16"]], [["Japan-2"]], ["operation"]]], "golden_sentence": [["Its total area is usually given as 666,700\u00a0km2 (257,400\u00a0sq\u00a0mi) and the population as 59,297,000 (est."], ["Using the annual estimate for October of each year, the population peaked in 2008 at 128,083,960 and had fallen 1,818,960 by October 2019."]]}, {"qid": "55be0929b20fb64f088c", "term": "Surveying", "description": "The technique, profession, and science of determining the positions of points and the distances and angles between them", "question": "Would you hire someone with dyscalculia to do surveying work?", "answer": false, "facts": ["Dyscalculia is a learning disability in math. People with dyscalculia have trouble with math at many levels. ", "Surveyors work with elements of geometry, trigonometry, regression analysis, physics, engineering, metrology, programming languages, and the law. ", "Geometry and trigonometry are types of advanced mathematics."], "decomposition": ["What do people with dyscalculia struggle with?", "What skills are necessary to be a competent surveyor?", "Is #1 not listed in #2?"], "evidence": [[[["Dyscalculia-1"]], [["Surveying-2"]], ["operation"]], [[["Dyscalculia-1"]], [["Surveying-2"]], ["operation"]], [[["Dyscalculia-1"]], [["Surveying-2"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "72613b26b78d9dd613c9", "term": "Conducting", "description": "Directing a musical performance by way of visible gestures", "question": "Is a paraplegic suitable for conducting an orchestra?", "answer": true, "facts": ["Musical conductors use their hands to wave a baton and guide the orchestra.", "Paraplegics are people that are paralyzed from the waist down.", "Paraplegics are able to play sports that involve their upper body, such as basketball."], "decomposition": ["What part(s) of the body is/are needed to conduct an orchestra?", "Which portion of a paraplegic's body is paralyzed?", "Are #1 separate from #2?"], "evidence": [[[["Conducting-1"]], [["Paraplegia-1"]], ["operation"]], [[["Conducting-1"]], [["Paraplegia-2"]], [["Conducting-1", "Paraplegia-2"]]], [[["Conducting-1"]], [["Paraplegia-1"]], ["operation"]]], "golden_sentence": [[""], ["Paraplegia is an impairment in motor or sensory function of the lower extremities."]]}, {"qid": "11ef3d21851aa22bb8f8", "term": "Satanism", "description": "group of ideological and philosophical beliefs based on Satan", "question": "Is Capricorn the hypothetical zodiac sign of Satanism?", "answer": true, "facts": ["Satanism is a group of beliefs based on Satan and has numerous symbols.", "Satan (The Devil) is often depicted as the deity Baphomet, a Sabbatic Goat with a human body and goat head.", "The Capricorn is the zodiac sign symbolized by the goat.", "The Tarot card attributed to the Zodiac sign Capricorn is The Devil."], "decomposition": ["What animal represents the zodiac sign Capricorn?", "What are some symbols in Satanism?", "Is #1 among #2?"], "evidence": [[[["Capricorn (astrology)-1"]], [["Baphomet-1", "Baphomet-3"]], ["operation"]], [[["Capricorn (astrology)-1"]], [["Sigil of Baphomet-2"]], ["operation"]], [[["Capricorn (astrology)-2"]], [["Baphomet-3", "LaVeyan Satanism-40"]], ["operation"]]], "golden_sentence": [["Capricorn (\u2651\ufe0e) is the tenth astrological sign in the zodiac out of twelve total zodiac signs, originating from the constellation of Capricornus, the horned goat."], ["", "half-human and half-animal, male and female, good and evil, on and off, etc.)."]]}, {"qid": "81ee1bd09a8897cbfd53", "term": "Sesame", "description": "species of plant", "question": "Would a sesame seed be mistaken for a wood frog egg?", "answer": false, "facts": ["A sesame seed is a flat 3 to 4 mm size seed.", "Wood frog eggs are globe looking masses about 2 to 5 inches in diameter."], "decomposition": ["What shape and size is a sesame seed?", "What is the shape and size of a wood frog egg?", "Are #1 and #2 the same?"], "evidence": [[[["Sesame-11"]], [["Wood frog-14"]], [["Sesame-11", "Wood frog-14"]]], [[["Sesame-11"]], [["Wood frog-14"], "no_evidence"], ["operation"]], [[["Sesame-11"]], [["Wood frog-14", "Wood frog-3"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Sesame seeds are small."], [""], ["", ""]]}, {"qid": "e52cf2df1ff85744d805", "term": "Easter", "description": "Major Christian festival celebrating the resurrection of Jesus", "question": "Does Adam Sandler skip celebrating Easter?", "answer": true, "facts": ["Adam Sandler is Jewish.", "Jewish religious people do not celebrate Easter."], "decomposition": ["Easter is usually celebrated by people of which religion?", "What is Adam Sandler's religion?", "Is #1 different from #2?"], "evidence": [[[["Easter-6"]], [["Adam Sandler-26"]], ["operation"]], [[["Easter-58"]], [["Adam Sandler-26"]], ["operation"]], [[["Easter-2"]], [["Adam Sandler-5"]], ["operation"]]], "golden_sentence": [["In Latin and Greek, the Christian celebration was, and still is, called Pascha (Greek: \u03a0\u03ac\u03c3\u03c7\u03b1), a word derived from Aramaic \u05e4\u05e1\u05d7\u05d0 (Paskha), cognate to Hebrew \u05e4\u05b6\u05bc\u05e1\u05b7\u05d7 (Pesach)."], ["Titone had converted to Sandler's religion, Judaism."]]}, {"qid": "3a551fbc4397830d804c", "term": "Alcatraz Island", "description": "United States historic place", "question": "Is there historic graffiti on Alcatraz?", "answer": true, "facts": ["Native Americans occupied the island of Alcatraz in 1969.", "Wall writings from the Native American occupation has been preserved and in some cases, restored completely."], "decomposition": ["Who were the occupants of the island of Alcatraz in 1969?", "Did #1 make wall writings?"], "evidence": [[[["Alcatraz Island-1"]], [["Alcatraz Island-23"], "operation"]], [[["Alcatraz Island-1"]], ["no_evidence", "operation"]], [[["Alcatraz Island-1"]], [["Alcatraz Island-23"], "operation"]]], "golden_sentence": [["Beginning in November 1969, the island was occupied for more than 19 months by a group of Native Americans from San Francisco, who were part of a wave of Native American activism across the U.S., with public protests through the 1970s."], [""]]}, {"qid": "19a3ccb85657661af3ff", "term": "Jean-Paul Sartre", "description": "French existentialist philosopher, playwright, novelist, screenwriter, political activist, biographer, and literary critic", "question": "Did Queen Elizabeth I read the works of Jean-Paul Sartre?", "answer": false, "facts": ["Jean-Paul Sartre was born in 1905.", "Queen Elizabeth I died in 1603."], "decomposition": ["When did Queen Elizabeth I die?", "When was Jean-Paul Sartre bron?", "Is #2 before #1?"], "evidence": [[[["Elizabeth I of England-1"]], [["Jean-Paul Sartre-1"]], ["operation"]], [[["Elizabeth I of England-1"]], [["Jean-Paul Sartre-4"]], ["operation"]], [[["Elizabeth I of England-1"]], [["Jean-Paul Sartre-1"]], ["operation"]]], "golden_sentence": [["Elizabeth I (7 September 1533 \u2013 24 March 1603) was Queen of England and Ireland from 17 November 1558 until her death on 24 March 1603."], ["Jean-Paul Charles Aymard Sartre (/\u02c8s\u0251\u02d0rtr\u0259/, US also /\u02c8s\u0251\u02d0rt/; French:\u00a0[sa\u0281t\u0281]; 21 June 1905 \u2013 15 April 1980) was a French philosopher, playwright, novelist, screenwriter, political activist, biographer, and literary critic."]]}, {"qid": "488e7b2ed07943863d17", "term": "Ancient Greece", "description": "Civilization belonging to an early period of Greek history", "question": "Did Polar Bears roam around in Ancient Greece?", "answer": false, "facts": ["Polar Bears live in the Arctic, with temperatures that can get as low as -35 degrees celsius.", "Ancient Greece had an average temperature of 24 degrees celsius."], "decomposition": ["Where do polar bears live?", "What is the average temperature of #1?", "What was the average temperature of Ancient Greece?", "Is #3 the same as #2?"], "evidence": [[[["Polar bear-9"]], [["Arctic Circle-12"]], [["Ancient Greece-42"], "no_evidence"], ["operation"]], [[["Polar bear-1"]], [["Climate of the Arctic-40"]], [["Climate of Greece-7"]], [["Climate of Greece-7"], "operation"]], [[["Polar bear-1"]], [["Arctic Circle-12"]], [["Greece-60"]], ["operation"]]], "golden_sentence": [["The polar bear is found in the Arctic Circle and adjacent land masses as far south as Newfoundland."], [""], [""]]}, {"qid": "b085fda8e3badcfbe88e", "term": "United Airlines", "description": "Airline in the United States", "question": "Was United Airlines blameless in worst crash in history?", "answer": true, "facts": ["The Tenerife Airport disaster is the deadliest crash in aviation history.", "The Tenerife Airport disaster involved a Boeing plane and a Pan Am plane.", "Pan Am airlines competed with United Airlines and other US companies.", "Boeing is an American multinational corporation that designs and sells airplanes, rockets, satellites,and missiles."], "decomposition": ["Which aviation accident is considered the worst in aviation history?", "Is United Airlines excluded from #1?"], "evidence": [[[["Tenerife airport disaster-1"]], ["operation"]], [[["Tenerife airport disaster-1"]], [["United Airlines-1"], "operation"]], [[["Tenerife-36"]], [["Tenerife airport disaster-3"]]]], "golden_sentence": [[""]]}, {"qid": "2158d41bc3da221e9ee0", "term": "New York Harbor", "description": "harbor in the New York City, U.S.A. metropolitan area", "question": "Does New York Harbor sit on a craton without volcanic activity?", "answer": false, "facts": ["New York Harbor is located on Laurentia craton. ", "The southwestern portion of Laurentia contains numerous large volcanic eruptions."], "decomposition": ["What craton is New York Harbor on?", "Is #1 devoid of volcanic activity?"], "evidence": [[[["Laurentia-3"]], [["Laurentia-6"], "operation"]], [[["Staten Island-42"], "no_evidence"], [["The Palisades (Hudson River)-5"], "no_evidence"]], [[["New York Harbor-1"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "10e1c2f6e6d16a0ec196", "term": "Bohai Sea", "description": "The innermost gulf of the Yellow Sea and Korea Bay on the coast of Northeastern and North China", "question": "Would Statue of Liberty be visible if submerged in Bohai Sea?", "answer": true, "facts": ["The Bohai Sea is 230 feet deep.", "The Statue of Liberty is 305 feet tall."], "decomposition": ["How deep is the Bohai Sea?", "How tall is the Statue of Liberty?", "Is #2 greater than #1?"], "evidence": [[["no_evidence"], [["Statue of Liberty-18"]], ["operation"]], [["no_evidence"], [["Statue of Liberty-18"]], ["no_evidence"]], [["no_evidence"], [["Statue of Liberty-18"]], ["no_evidence", "operation"]]], "golden_sentence": [["Bartholdi had decided on a height of just over 151 feet (46\u00a0m) for the statue, double that of Italy's Sancarlone and the German statue of Arminius, both made with the same method."]]}, {"qid": "7d966811bff1567e355c", "term": "Fake news", "description": "Hoax or deliberate spread of misinformation", "question": "Have Jamie Lee Curtis been the subject of fake news?", "answer": true, "facts": ["Fake news is a hoax that is circulated and spreads to get people to believe a falsehood.", "Jamie Lee Curtis is an American actress known for the Halloween series.", "Rumors have been spread about Jamie Lee Curtis that she was born a hermaphrodite."], "decomposition": ["What is fake news?", "Has Jamie Lee Curtis ever been the victim of #1?"], "evidence": [[[["Fake news in India-1"]], ["no_evidence", "operation"]], [[["Fake news-7"]], [["Jamie Lee Curtis-25"], "no_evidence"]], [[["Fake news-1"]], ["no_evidence"]]], "golden_sentence": [["Fake news in India refers to misinformation, disinformation or mal-information in the country which is spread through word of mouth and traditional media and more recently through digital forms of communication such as edited videos, memes, unverified advertisements and social media propagated rumours."]]}, {"qid": "a78ea1c8d106b33c5935", "term": "Golden Gate Bridge", "description": "suspension bridge on the San Francisco Bay", "question": "Will the Stanford Linear Accelerator fit on the Golden Gate Bridge?", "answer": false, "facts": ["The Golden Gate Bridge is a famous suspension bridge in the San Francisco bay area.", "The Golden Gate Bridge is 1.7miles long.", "The Stanford Linear Accelerator is part of a particle physics lab in Menlo Park, California. ", "The Sanford Linear Accelerator is 2miles long."], "decomposition": ["What is the length of the Golden Gate Bridge?", "How long is the Stanford Linear Accelerator?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Golden Gate Bridge-1"]], [["SLAC National Accelerator Laboratory-1"]], ["operation"]], [[["Golden Gate Bridge-25"]], [["Mile-1", "SLAC National Accelerator Laboratory-1"]], ["operation"]], [[["SLAC National Accelerator Laboratory-1"]], [["Golden Gate Bridge-1"]], ["operation"]]], "golden_sentence": [["The Golden Gate Bridge is a suspension bridge spanning the Golden Gate, the one-mile-wide (1.6\u00a0km) strait connecting San Francisco Bay and the Pacific Ocean."], ["It is the site of the Stanford Linear Accelerator, a 3.2 kilometer (2 mile) linear accelerator constructed in 1966 and shut down in the 2000s, which could accelerate electrons to energies of 50\u00a0GeV."]]}, {"qid": "48049ad139fba6eeaf13", "term": "JPMorgan Chase", "description": "American multinational banking and financial services holding company", "question": "Could JPMorgan Chase give every American $10?", "answer": true, "facts": ["JPMorgan Chase has total assets of US$2.687 trillion.", "As of November 8, 2018, the United States is estimated to have a population of 328,953,020.", "One trillion is equal to 1,000 billions.", "One billion is equal to 1,000 millions."], "decomposition": ["How much are the total assets of JPMorgan Chase?", "What is the population of the United States?", "Is #2 times $10 less than #1?"], "evidence": [[[["JPMorgan Chase-1"]], [["Demographics of the United States-1"]], ["operation"]], [[["JPMorgan Chase-1"]], [["United States-1"]], ["operation"]], [[["JPMorgan Chase-1"]], [["United States-1"]], ["operation"]]], "golden_sentence": [["JPMorgan Chase is ranked by S&P Global as the largest bank in the United States and the sixth largest bank in the world by total assets, with total assets of US$2.687 trillion."], ["The United States is the third-most populous country in the world, with an estimated population of 329,227,746 as of January\u00a028, 2020[update]."]]}, {"qid": "537aa3429137cb795f1d", "term": "The Matrix", "description": "1999 science fiction action film directed by the Wachowskis", "question": "Is the Matrix a standalone movie?", "answer": false, "facts": ["The Matrix ends in a cliffhanger.", "The story is then resolved in two sequels, making a trilogy.", "There are also supplemental works adding to the story, such as a video game and the Animatrix."], "decomposition": ["How many movies are in The Matrix franchise?", "Is #1 equal to one?"], "evidence": [[[["The Matrix (franchise)-9"]], [["The Matrix (franchise)-9"], "operation"]], [[["The Matrix (franchise)-1"]], ["operation"]], [[["The Matrix-49"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "dfa111eb347dcac149b8", "term": "Noah's Ark", "description": "the vessel in the Genesis flood narrative", "question": "Is Noah's Ark an upgrade for Golden Age of Piracy pirates?", "answer": true, "facts": ["The Golden Age of Piracy took place between 1650 and 1720 and had many famous pirates including Blackbeard.", "Noah's Ark had dimensions in feet of 450 x 75 x 45.", "Blackbeard's ship, Queen Anne's Revenge, had a length of 103 feet.", "William Kidd's ship, Adventure Galley, was 124 feet long."], "decomposition": ["Which pirates were famously known during the Golden Age of Piracy?", "What were the dimensions of Noah's Ark?", "Is #2 greater than the dimensions of the ships of most of #1?"], "evidence": [[[["William Kidd-1"]], [["Noah's Ark-3"]], [["Noah's Ark-3", "William Kidd-9"], "no_evidence"]], [[["Whydah Gally-6"], "no_evidence"], [["Noah's Ark-3"]], ["no_evidence"]], [[["Blackbeard-1", "Henry Morgan-1", "Samuel Bellamy-1"], "no_evidence"], [["Noah's Ark-3"]], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["Accordingly, Noah's instructions are given to him by God (Genesis 6:14\u201316): the ark is to be 300 cubits long, 50 cubits wide, and 30 cubits high."], ["", ""]]}, {"qid": "0fcd8e25577ed693f145", "term": "Nikola Tesla", "description": "Serbian American inventor", "question": "Has Nikola Tesla's name inspired multiple brands?", "answer": true, "facts": ["Nikola Tesla was a famous inventor born in 1856.", "The electric car company Tesla was named after Nikola Tesla.", "The hard rock band Tesla is named after Nikola Tesla."], "decomposition": ["How many brands are named after Nikola Tesla?", "Is #1 much more than one?"], "evidence": [[[["Tesla, Inc.-4"]], ["operation"]], [[["Nikola Motor Company-1", "Tesla, Inc.-4"]], ["operation"]], [[["Nikola Motor Company-1", "Tesla Electric Light and Manufacturing-2", "Tesla, Inc.-1", "Tesla-2"]], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "25643acabd893e88cc6c", "term": "Wednesday", "description": "Day of the week", "question": "Are all Wednesdays in a year enough to read Bible 15 times?", "answer": true, "facts": ["There are 52 Wednesdays in a year.", "There are 1,248 hours over all the Wednesdays in a year.", "The Old Testament of the Bible takes an average of 52 hours to read.", "The New Testament of the Bible takes an average of 18 hours to read."], "decomposition": ["How many Wednesdays are there in a year?", "What is #1 multiplied by 24?", "How long does it take to read the old testament?", "How long does it take to read the new testament?", "Is #2 greater than or equal to: #3 plus #4?"], "evidence": [[[["Year-57"]], ["operation"], [["Old Testament-2"], "no_evidence"], [["New Testament-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Week-10"]], ["operation"], [["New Testament-11", "Old Testament-2"], "no_evidence"], [["New Testament-9"], "no_evidence"], ["operation"]], [[["Year-66"], "no_evidence"], ["operation"], [["Protestant Bible-15"], "no_evidence"], [["Protestant Bible-15"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], [""], ["While the Old Testament canon varies somewhat between different Christian denominations, the 27-book canon of the New Testament has been almost universally recognized within Christianity since at least Late Antiquity."]]}, {"qid": "a2ebf0e70d68872843ad", "term": "Bicycle", "description": "Pedal-driven two-wheel vehicle", "question": "Do children's bicycles often have extra wheels?", "answer": true, "facts": ["Training wheels are a set of two wheels to attach to bicycles of new bike riders for additional support.", "Training wheels are marketed primarily at children."], "decomposition": ["What types of bicycles have more than two wheels?", "Are any of #1 customarily bought for children?"], "evidence": [[[["Training wheels-1"]], [["Training wheels-1"]]], [[["Tricycle-3"]], ["operation"]], [[["Tricycle-1"]], [["Tricycle-3"]]]], "golden_sentence": [[""], ["Typically they are used in teaching very young children to ride a bike, although versions for adults exist."]]}, {"qid": "cee9f1e6ad41261832b1", "term": "Groundhog Day", "description": "Traditional method of weather prediction", "question": "Would most school children in New York be wearing jackets on groundhog day?", "answer": true, "facts": ["Groundhog day takes place on February second.", "New York is typically very cold in February."], "decomposition": ["What month does Groundhog day occur?", "What is the season in #1?", "Do people typically wear jackets during #2?"], "evidence": [[[["Groundhog Day-1"]], [["Groundhog Day-1"]], [["Winter clothing-2"]]], [[["Groundhog Day-1"]], [["February-2"], "no_evidence"], ["operation"]], [[["Groundhog Day-1"]], [["Groundhog Day-1"]], [["Jacket-1"]]]], "golden_sentence": [["Groundhog Day (Pennsylvania German: Grund'sau d\u00e5k, Grundsaudaag, Grundsow Dawg, Murmeltiertag; Nova Scotia: Daks Day) is a popular American tradition observed in the United States and Canada on February 2nd."], [""], [""]]}, {"qid": "f8405174e89d891b461e", "term": "Thirty Years' War", "description": "War between 1618 and 1648; with over 8 million fatalities", "question": "Could a white cockatoo have lived through the entire Thirty Years' War?", "answer": true, "facts": ["The Thirty Years' War lasted 30 years", "White cockatoos have been reported to live between 40-60 years in captivity"], "decomposition": ["How long did the Thirty Years' War last?", "How long can white cockatoos live?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Thirty Years' War-1"]], [["White cockatoo-9"]], ["operation"]], [[["Thirty Years' War-1"]], [["White cockatoo-9"]], ["operation"]], [[["Germany in the early modern period-9"]], [["White cockatoo-9"]], ["operation"]]], "golden_sentence": [["The Thirty Years' War was a war fought primarily in Central Europe between 1618 and 1648."], ["Whilst the maximum lifespan of the white cockatoo is poorly documented; a few zoos report that they live 40\u201360 years in captivity."]]}, {"qid": "58fb76d7b21327460ff8", "term": "Railroad engineer", "description": "person who operates a train on a railroad or railway", "question": "Did Jesus go to school to study railroad engineering?", "answer": false, "facts": ["The steam locomotive to drive a train was invented in the 19th century.", "Jesus lived around 0 AD. "], "decomposition": ["When was the steam locomotive invented?", "When did Jesus die?", "Was #1 before #2?"], "evidence": [[[["Steam locomotive-2"]], [["Jesus-1"]], ["operation"]], [[["Steam locomotive-2"]], [["Jesus-1"]], ["operation"]], [[["Steam locomotive-6"]], [["Crucifixion of Jesus-1"]], ["operation"]]], "golden_sentence": [["Richard Trevithick built the first steam locomotive in 1802."], ["Jesus (c.\u20094 BC\u00a0\u2013 c.\u2009AD 30 / 33), also referred to as Jesus of Nazareth or Jesus Christ, was a first-century Jewish preacher and religious leader."]]}, {"qid": "0646aa8409b9dd4ac719", "term": "Arnold Schwarzenegger", "description": "Austrian-American actor, businessman, bodybuilder and politician", "question": "Can Arnold Schwarzenegger deadlift an adult Black rhinoceros?", "answer": false, "facts": ["Arnold Schwarzenegger deadlifted 710 pounds in a competition.", "The world deadlift record is 1,104 pounds, set by Game of Thrones actor Hafthor Bjornsson.", "The weight of an adult Black rhinoceros is between 1,800 \u2013 3,100 pounds."], "decomposition": ["How much can Arnold Schwarzenegger deadlift?", "How much does an adult Black rhino weigh?", "Is #1 greater than #2?"], "evidence": [[[["Arnold Schwarzenegger-24"]], [["Black rhinoceros-8"]], ["operation"]], [[["Arnold Schwarzenegger-24"]], [["Black rhinoceros-8"]], ["operation"]], [[["Arnold Schwarzenegger-24"]], [["Black rhinoceros-8"]], ["operation"]]], "golden_sentence": [["Clean and press \u2013 264\u00a0lb (120\u00a0kg) Snatch \u2013 243\u00a0lb (110\u00a0kg) Clean and jerk \u2013 298\u00a0lb (135\u00a0kg) Squat \u2013 545\u00a0lb (247\u00a0kg) Bench press \u2013 520\u00a0lb (240\u00a0kg) Deadlift \u2013 710\u00a0lb (320\u00a0kg) Schwarzenegger's goal was to become the greatest bodybuilder in the world, which meant becoming Mr. Olympia."], ["An adult typically weighs from 800 to 1,400\u00a0kg (1,760 to 3,090\u00a0lb), however unusually large male specimens have been reported at up to 2,896\u00a0kg (6,385\u00a0lb)."]]}, {"qid": "6dee0162457d5e7f7eba", "term": "Palace of Westminster", "description": "Meeting place of the Parliament of the United Kingdom,", "question": "Can the Palace of Westminster tell time in the dark?", "answer": true, "facts": ["The Palace of Westminster has Big Ben, a striking clock tower", "Big Ben communicates the time via bells"], "decomposition": ["What is the clock tower of the Palace of Westminster?", "What does #1 use to communicate time?", "Can #2 work without light?"], "evidence": [[[["Palace of Westminster-29"]], [["Palace of Westminster-30"]], ["operation"]], [[["Big Ben-1"]], [["Big Ben-50"]], ["no_evidence", "operation"]], [[["Palace of Westminster-29"]], [["Palace of Westminster-30"]], [["Palace of Westminster-30"]]]], "golden_sentence": [["Originally known simply as the Clock Tower (the name Elizabeth Tower was conferred on it in 2012 to celebrate the Diamond Jubilee of Elizabeth II), it houses the Great Clock of Westminster, built by Edward John Dent on designs by amateur horologist Edmund Beckett Denison."], [""]]}, {"qid": "09e2646ecfc549e7f419", "term": "Prime number", "description": "Integer greater than 1 that has no positive integer divisors other than itself and 1", "question": "Can a prime number be represented by the number of days in a week?", "answer": true, "facts": ["There are seven days in a week.", "Seven is a prime number."], "decomposition": ["How many days are there in a week?", "Is #1 a prime number?"], "evidence": [[[["Week-1"]], [["Prime number-1", "Prime number-13"], "operation"]], [[["Week-1"]], [["7-1"]]], [[["Week-8"]], [["Prime number-7"]]]], "golden_sentence": [["A week is a time unit equal to seven days."], ["A prime number (or a prime) is a natural number greater than 1 that is not a product of two smaller natural numbers.", "Therefore, every prime number other than 2 is an odd number, and is called an odd prime."]]}, {"qid": "2bdd153cb709ff3681f1", "term": "Reformation", "description": "Schism within the Christian Church in the 16th century", "question": "Would a tool used for Martin Luther's Reformation opening salvo aid in a crucifixion?", "answer": true, "facts": ["Martin Luther began the Reformation with the defiant act of nailing 95 grievances to the door of the Wittenberg church.", "Roman crucifixions required several tools including nails and wooden beams."], "decomposition": ["What did Martin Luther begin his Reformation with?", "What tools were used in #1?", "What are the tools required to preform Roman crucifixions?", "Is there any overlap between #2 and #3?"], "evidence": [[[["Ninety-five Theses-1"]], [["Wittenberg-7"]], [["Crucifixion of Jesus-44"]], ["operation"]], [[["Martin Luther-19"]], [["Nail (fastener)-1", "Nail (fastener)-2"]], [["Crucifixion-1"]], ["operation"]], [[["Martin Luther-19"]], [["Nail (fastener)-2"]], [["Crucifixion-1", "Nail (fastener)-2"]], ["operation"]]], "golden_sentence": [["These indulgences, according to Luther, discouraged Christians from giving to the poor and performing other acts of mercy, believing that indulgence certificates were more spiritually valuable."], [""], [""]]}, {"qid": "e56d13eafe51c0cf2833", "term": "Horror fiction", "description": "genre of fiction", "question": "Is Edgar Allan Poe obscure in the world of horror fiction?", "answer": false, "facts": ["Edgar Allan Poe's writing has endured for over 150 years. ", "Edgar Allan Poe's horror writing has been included in classroom curriculum for decades.  "], "decomposition": ["How long have Edgar Allan Poe's writings remained in common use?", "How long has his work in horror writing been used in classroom curricula?", "Is #1 or #2 less than a decade?"], "evidence": [[[["Edgar Allan Poe-1"], "no_evidence"], ["no_evidence"], ["operation"]], [[["Edgar Allan Poe-1", "Edgar Allan Poe-35"], "no_evidence"], [["The Masque of the Red Death (1964 film)-1", "The Pit and the Pendulum (1991 film)-1"], "no_evidence"], ["operation"]], [[["Edgar Allan Poe-3"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""]]}, {"qid": "b1f666945415163e98a4", "term": "Shooting sports", "description": "sports involving firearms used to hit targets", "question": "Do all shooting sports involve bullets?", "answer": false, "facts": ["Paintball is a shooting sport that uses paint pellets in lieu of bullets. ", "Crossbow is a shooting sport that uses 'bolts' or arrows instead of bullets."], "decomposition": ["What are some common projectiles used in shooting sports?", "Are all of #1 bullets?"], "evidence": [[[["Bow and arrow-1", "Bullet-1", "Shooting sports-1"]], ["operation"]], [[["Archery-1"]], [["Arrow-1", "Bullet-1"], "operation"]], [[["Shooting sports-1"]], [["Crossbow bolt-1", "Shooting sports-1"]]]], "golden_sentence": [["The bow and arrow is a ranged weapon system consisting of an elastic launching device (bow) and long-shafted projectiles (arrows).", "", "Shooting sports is a collective group of competitive and recreational sporting activities involving proficiency tests of accuracy, precision and speed in shooting \u2014 the art of using various types of ranged weapons, mainly referring to man-portable guns (firearms and airguns, in forms such as handguns, rifles and shotguns) and bows/crossbows."]]}, {"qid": "0a4b84cda3c9118e32ff", "term": "Ahura Mazda", "description": "highest deity of Zoroastrianism", "question": "Will Ahura Mazda have to look down to see Abaddon's dwelling??", "answer": true, "facts": ["Abaddon is a demon that is said to dwell in a bottomless pit below the earth that is a realm of the dead.", "Ahura Mazda is the chief deity of Zoroastrianism.", "Ahura Mazda is the Supreme Being located in heaven, high above the sky."], "decomposition": ["Where does Abaddon dwell?", "Where is Ahura Mazda known to reside?", "Is #2 located physically above #1?"], "evidence": [[[["Abaddon-1"]], [["Ahura Mazda-1"], "no_evidence"], ["operation"]], [[["Abaddon-1"]], [["Ahura Mazda-1"], "no_evidence"], ["operation"]], [[["Abaddon in popular culture-1"]], [["Ahura Mazda-1"], "no_evidence"], ["no_evidence"]]], "golden_sentence": [["The Hebrew term \"Abaddon\" (Hebrew: \u05d0\u05b2\u05d1\u05b7\u05d3\u05bc\u05d5\u05b9\u05df\u200e Avaddon, meaning \"doom\"), and its Greek equivalent \"Apollyon\" (Greek: \u1f08\u03c0\u03bf\u03bb\u03bb\u03cd\u03c9\u03bd, Apoll\u00fd\u014dn) appear in the Bible as both a place of destruction and an angel of the abyss."], [""]]}, {"qid": "1f3c11b28c2484b4d6c7", "term": "Alaska Purchase", "description": "1867 sale of Alaska to the USA by Russia", "question": "Was Alaska part of the Northern Army during the Civil War?", "answer": false, "facts": ["The Civil War was from 1861 - 1865.", "The Northern Army consisted of soldiers from states north of the Mason-Dixon line.", "Alaska did not become part of the United States until 1867."], "decomposition": ["When did the American Civil War take place?", "When did Alaska become part of the United States?", "Is #2 before #1?"], "evidence": [[[["American Civil War-1"]], [["Alaska-42"]], ["operation"]], [[["American Civil War-1"]], [["Alaska Purchase-1"]], ["operation"]], [[["American Civil War-1"]], [["Alaska Statehood Act-3"]], ["operation"]]], "golden_sentence": [["The American Civil War (also known by other names) was a civil war in the United States from 1861 to 1865, fought between the northern United States (loyal to the Union) and the southern United States (that had seceded from the Union and formed the Confederacy)."], ["Statehood was approved by Congress on July 7, 1958."]]}, {"qid": "22b8f88d820a7b003d91", "term": "Bing (search engine)", "description": "Web search engine from Microsoft", "question": "Do Bing (search engine) searches earn the searcher more than competitors do?", "answer": true, "facts": ["Bing (search engine) has a search rewards program that gives the user points, from conducting searches, to redeem for prizes.", "Bing (search engine) has several competitors such as Google, and DuckDuckGo.", "Google and DuckDuckGo do not have search rewards programs."], "decomposition": ["What does Bing give to people who use the search engine?", "Who are Bing's major competitors?", "What do the companies in #2 give people for using their service?", "Is #1 of greater value than #3?"], "evidence": [[[["Bing (search engine)-10", "Bing (search engine)-26"]], [["Bing (search engine)-52"]], ["no_evidence"], ["operation"]], [[["Bing (search engine)-57"]], [["Bing (search engine)-67"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Bing (search engine)-57"]], [["Bing (search engine)-54"]], ["no_evidence"], ["operation"]]], "golden_sentence": [["", "This inspired Bing to use its search data to infer outcomes of certain events, such as winners of reality shows."], ["As of October 2018, Bing is the third largest search engine in the US, with a query volume of 4.58%, behind Google (77%) and Baidu(14.45%)."]]}, {"qid": "caa295b81f0e4c09115c", "term": "Loudspeaker", "description": "a microphone which contains the Bluetooth as at the moment as the microphone is contained the energy; expected energy cooling expect that early in the alphabet (early out of it contains one of these earlier)", "question": "Would a loudspeaker be useful for most Gallaudet students?", "answer": false, "facts": ["Gallaudet is a school for the deaf in the USA.", "Most deaf students would not be able to accurately use or rely on information conveyed via loudspeaker."], "decomposition": ["What disability do Gallaudet students suffer from?", "What does a loudspeaker do?", "Would #2 be beneficial for people who have #1?"], "evidence": [[[["Gallaudet University-1"]], [["Loudspeaker-1"]], ["operation"]], [[["Gallaudet University-1"]], [["Loudspeaker-1"]], ["operation"]], [[["Gallaudet University-1"]], [["Loudspeaker-1"]], ["operation"]]], "golden_sentence": [["Gallaudet University /\u02cc\u0261\u00e6l\u0259\u02c8d\u025bt/ is a federally chartered private university for the education of the deaf and hard of hearing."], ["A loudspeaker is an electroacoustic transducer; a device which converts an electrical audio signal into a corresponding sound."]]}, {"qid": "28aa968b35362d3ae68d", "term": "Amtrak", "description": "Intercity rail operator in the United States", "question": "Would three newborn kittens fit on a standard Amtrak coach seat?", "answer": true, "facts": ["Newborn kittens are small enough to fit in an average human hand.", "The average human hand is 7 inches.", "An Amtrak coach seat is 39\" x 23\"."], "decomposition": ["What is the size of a newborn kitten?", "How big would #1 times three kittens be?", "How large is an Amtrak coach seat?", "Is #2 smaller than #3?"], "evidence": [[[["Cat-28"], "no_evidence"], ["operation"], [["Airline seat-29"], "no_evidence"], ["operation"]], [[["Kitten-4"], "no_evidence"], ["no_evidence"], [["Amtrak-43"], "no_evidence"], ["no_evidence", "operation"]], [[["Cat-28"], "no_evidence"], ["no_evidence", "operation"], [["Amtrak-48"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["It averages about 46\u00a0cm (18\u00a0in) in head-to-body length and 23\u201325\u00a0cm (9.1\u20139.8\u00a0in) in height, with about 30\u00a0cm (12\u00a0in) long tails."], ["In 1985 none of the main four US carriers offered a seat less than 19 inches wide."]]}, {"qid": "d7cff70a52ae5816b161", "term": "Horseradish", "description": "species of plant", "question": "Could a newborn look over the top of a fully grown horseradish plant?", "answer": false, "facts": ["A fully grown horseradish plant can reach a height of 4.9 feet.", "Newborn children are typically between 14-20 inches tall in first world countries."], "decomposition": ["How tall are newborn babies on average?", "How tall is the average horseradish plant?", "Is #1 greater than #2?"], "evidence": [[[["Infant-5"]], [["Horseradish-2"]], ["operation"]], [[["Infant-5"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Infant-9"]], [["Horseradish-2"]], ["operation"]]], "golden_sentence": [["In first world nations, the average total body length of a newborn is 35.6\u201350.8\u00a0cm (14.0\u201320.0\u00a0in), although premature newborns may be much smaller."], ["It grows up to 1.5 meters (4.9 feet) tall, and is cultivated primarily for its large, white, tapered root."]]}, {"qid": "a13f77c47dfb3e2254a6", "term": "Red Sea", "description": "Arm of the Indian Ocean between Arabia and Africa", "question": "Does the Red Sea have biblical significance? ", "answer": true, "facts": ["During the biblical Exodus, the Israelite had to cross the Red Sea.", "Moses parted the Red Sea to allow the Israelite group to escape from the Egyptians. "], "decomposition": ["What bodies of water are important to Biblical stories?", "Is the Red Sea among #1?"], "evidence": [[[["Red Sea-8"]], [["Red Sea-8"], "operation"]], [[["Jordan River-2", "Mediterranean Sea-11", "Red Sea-8", "Sea of Galilee-4"]], ["operation"]], [[["Crossing the Red Sea-1"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "eb1343cc4601ddc719eb", "term": "Chinatown, Manhattan", "description": "Neighborhood of Manhattan in New York City", "question": "Would moon cakes be easy to find in Chinatown, Manhattan?", "answer": true, "facts": ["Moon cakes are a Chinese traditional desert.", "Chinatown offers many 'tastes of home' to Chinese locals and travelers. "], "decomposition": ["What cuisine are moon cakes from?", "Does Chinatown, Manhattan sell food products of #1?"], "evidence": [[[["Mooncake-1"]], [["Chinatown-14"], "no_evidence", "operation"]], [[["Moon shrimp cake-1"]], [["Moon shrimp cake-1"]]], [[["Mooncake-1"]], [["Chinatown-1"], "operation"]]], "golden_sentence": [["A mooncake (simplified Chinese: \u6708\u997c; traditional Chinese: \u6708\u9905; pinyin: yu\u00e8bing, yu\u00e8b\u01d0ng; Jyutping: jyut6 beng2; Yale: yuht b\u00e9ng) is a Chinese bakery product traditionally eaten during the Mid-Autumn Festival (\u4e2d\u79cb\u7bc0)."], [""]]}, {"qid": "928b0a4379979ea819d3", "term": "Bugs Bunny", "description": "Warner Bros. cartoon character", "question": "Is Bugs Bunny known for carrying a root vegetable around with him?", "answer": true, "facts": ["Bugs Bunny carries a carrot around with him.", "Carrots are considered root vegetables."], "decomposition": ["What does Bugs Bunny carry around with him?", "Is #1 a root vegetable?"], "evidence": [[[["Bugs Bunny-20"]], [["Carrot-31"]]], [[["Bugs Bunny-20"]], [["Carrot-1"]]], [[["Bugs Bunny-37"]], [["Carrot-1"]]]], "golden_sentence": [[""], [""]]}, {"qid": "14468aaf17395d527774", "term": "Cricket (insect)", "description": "small insects of the family Gryllidae", "question": "Would someone buying crickets be likely to own pets?", "answer": true, "facts": ["Reptiles are a popular pet for people.", "Reptiles enjoy eating crickets. ", "Crickets are sold at many pet stores."], "decomposition": ["What are some common animal classes that people keep as pets?", "Do any of #1 usually eat crickets?"], "evidence": [[[["Pet-22"]], [["Crickets as pets-4"]]], [[["Crickets as pets-26", "Pet-2"], "no_evidence"], [["Cricket (insect)-3"], "no_evidence", "operation"]], [[["Pet-2"]], [["Lizard-27"]]]], "golden_sentence": [["Alpacas Camels Cats Cattle Dogs Donkeys Ferrets Goats Hedgehogs Horses Llamas Pigs Rabbits Red foxes Rodents as pets, including rats, mice, hamsters, guinea pigs, gerbils, and chinchillas Sheep Sugar gliders Companion parrots, like the budgie and cockatiel."], ["This makes crickets less appealing as pets in Western countries."]]}, {"qid": "1f6eb529d30617a2fb4c", "term": "Christianity in China", "description": "Religious community", "question": "Are some adherents to Christianity in China historic enemies of Catholic Church?", "answer": true, "facts": ["Christianity in China is comprised of several different groups including: Protestants, Catholics, Evangelicals, and Orthodox Christians.", "Catholics have been at war with Protestants throughout history.", "The 1572 St. Bartholomew's Day Massacre saw thousands of Protestants killed by Catholic mobs.", "English Protestant rulers killed many Irish Catholics during the Reformation."], "decomposition": ["Which Christian denominations are historic enemies of the Catholic Church?", "Do any of the denominations in #1 currently have adherents in China?"], "evidence": [[[["Catholic Church-119"]], [["Protestantism by country-3"]]], [[["St. Bartholomew's Day massacre-1"]], [["Christianity in China-35", "Robert Morrison (missionary)-50"], "operation"]], [[["Protestantism-1"]], [["Protestantism by country-3"]]]], "golden_sentence": [[""], [""]]}, {"qid": "f85ec3654836efb357a2", "term": "Oval Office", "description": "office of the U.S. President", "question": "Can a Kia Rio fit inside the Oval Office?", "answer": true, "facts": ["The Oval Office is 35' long and 29' wide.", "A Kia Rio is 14.3' long and 5.6' wide. "], "decomposition": ["How large is the Oval Office?", "How large is a Kia Rio?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Oval Office-2"], "no_evidence"], [["Kia Rio-28"], "no_evidence"], ["operation"]], [[["Eric Gugler-9"], "no_evidence"], [["Kia Rio-1"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "87553625dfa61ddc55cf", "term": "War in Vietnam (1945\u201346)", "description": "Prelude to the Indochina Wars", "question": "Were veterans of the War in Vietnam (1945\u201346) given free education by the Soviet Union?", "answer": false, "facts": ["The Soviet Union provided free education to children of those who died in the Vietnam War with America.", "The War in Vietnam (1945\u201346) was twenty years before the Vietnam War with America."], "decomposition": ["The Soviet Union gave free education to children of people who died in which war?", "When did #1 end?", "When did the War in Vietnam (1945-46) end?", "Was #3 before #2?"], "evidence": [[[["Orphans in the Soviet Union-14"], "no_evidence"], [["World War II-1"]], [["Vietnam War-1"]], ["operation"]], [[["Orphans in the Soviet Union-2", "Orphans in the Soviet Union-6"], "no_evidence"], [["Russian Civil War-2", "World War I-1"], "no_evidence"], [["War in Vietnam (1945\u20131946)-1", "War in Vietnam (1945\u20131946)-2"]], ["no_evidence", "operation"]], [["no_evidence"], ["no_evidence"], [["Vietnam War-1"]], ["operation"]]], "golden_sentence": [["With World War II came a new wave of orphans."], [""], ["The war, considered a Cold War-era proxy war by some, lasted 19 years, with direct U.S. involvement ending in 1973, and included the Laotian Civil War and the Cambodian Civil War, which ended with all three countries becoming communist in 1975."]]}, {"qid": "a1b0f2f0c5b313a6e539", "term": "Easter Bunny", "description": "Folkloric figure and symbol of Easter", "question": "Is the Easter Bunny popular in September?", "answer": false, "facts": ["The Easter Bunny is a symbol of the Christian holiday of Easter", "Easter occurs in March or April each year"], "decomposition": ["What holiday does the Easter Bunny symbolize?", "Is #1 celebrated in September?"], "evidence": [[[["Easter Bunny-1"]], [["Easter-14", "Easter-15"]]], [[["Easter Bunny-1"]], [["Easter-14", "Easter-15"], "operation"]], [[["Easter Bunny-1"]], [["Easter-17"]]]], "golden_sentence": [["The Easter Bunny (also called the Easter Rabbit or Easter Hare) is a folkloric figure and symbol of Easter, depicted as a rabbit bringing Easter eggs."], ["incomplete short citation] In Western Christianity, using the Gregorian calendar, Easter always falls on a Sunday between 22 March and 25 April, within about seven days after the astronomical full moon.", "Easter therefore varies between 4 April and 8 May in the Gregorian calendar (the Julian calendar is no longer used as the civil calendar of the countries where Eastern Christian traditions predominate)."]]}, {"qid": "5154b3df25abc81b15fa", "term": "United States Army Rangers", "description": "Elite military formation of the United States Army", "question": "Is Mozambique Drill an easy shot for United States Army Ranger?", "answer": true, "facts": ["The Mozambique Drill is a close quarters combat technique involving firing two shots to the body and one to the head.", "United States Army Rangers are equipped with M4A1 guns.", "M4A1 guns can fire up to 600 meters."], "decomposition": ["What is the The Mozambique Drill?", "What guns are United States Army Rangers equipped with?", "What is the shooting range of #2?", "Would a gun with the range of #3 be helpful in #1?"], "evidence": [[[["Mozambique Drill-1"]], [["75th Ranger Regiment-2", "United States Army-68"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Mozambique Drill-1"]], ["no_evidence"], ["no_evidence"], ["operation"]], [[["Mozambique Drill-1"]], [["M4 carbine-31"]], [["M4 carbine-31"]], [["Close-quarters combat-1"], "operation"]]], "golden_sentence": [["The Mozambique Drill, also known as the Failure Drill, or Failure to Stop drill, informally, \"two to the body, one to the head,\" is a close-quarters shooting technique that requires the shooter to fire twice into the torso of a target (known as a double tap or hammered pair to the center of mass), and follow up with a more difficult head shot that, if properly placed, will instantly stop the target if the previous shots failed to do so."], ["", ""]]}, {"qid": "eeded89e3ca99b842475", "term": "Rainbow", "description": "meteorological phenomenon", "question": "Is lunch on the beach a good activity to spot the full circle of a rainbow?", "answer": false, "facts": ["The full circle of a rainbow cannot usually be seen from ground level", "Sometimes the full circle of a rainbow can be seen from a high building or aircraft", "You can see more of a rainbow the closer to the horizon the sun is", "Lunch occurs at midday when the sun is likely high in the sky"], "decomposition": ["At what point in the sky is the sun most likely to create a full circle rainbow?", "At what altitudes are full rainbows more likely to be seen?", "Is lunchtime at the beach relatively close to conditions #1 and #2?"], "evidence": [[[["Halo (optical phenomenon)-1"]], [["Halo (optical phenomenon)-2"]], [["Atmospheric optics-17", "Beach-16"]]], [[["Rainbow-2"], "no_evidence"], [["Rainbow-11"]], ["operation"]], [["no_evidence"], [["Rainbow-11"], "no_evidence"], [["Sea level-1"], "no_evidence", "operation"]]], "golden_sentence": [[""], [""], ["", ""]]}, {"qid": "51c0854f19645ca756bb", "term": "Mona Lisa", "description": "Painting by Leonardo da Vinci", "question": "Is the Mona Lisa based on a real person?", "answer": true, "facts": ["There are two main theories about the origin of the Mona Lisa.", "The first is that a wealthy noblewoman, the wife of one of Leonardo's friends, sat as the model.", "Another popular theory is that Leonardo painted her as a cross-gendered self portrait."], "decomposition": ["Who was the Mona Lisa painting based on?", "Is #1 a real person?"], "evidence": [[[["Mona Lisa-2"]], [["Lisa del Giocondo-1"]]], [[["Mona Lisa-2"]], [["Lisa del Giocondo-1"], "operation"]], [[["Mona Lisa-12"]], [["Isabella of Aragon, Queen of Germany-1"]]]], "golden_sentence": [["The painting is likely of the Italian noblewoman Lisa Gherardini, the wife of Francesco del Giocondo, and is in oil on a white Lombardy poplar panel."], [""]]}, {"qid": "bbdbe34fa02b4ee88588", "term": "Johann Sebastian Bach", "description": "German composer", "question": "Did Johann Sebastian Bach leave his first wife for his second wife?", "answer": false, "facts": ["Johann Sebastian Bach was married to Maria Barbara Bach from 1707\u20131720.", "Johann Sebastian Bach was married to Anna Magdalena Bach from 1721\u20131750.", "Maria Barbara Bach died suddenly in 1720."], "decomposition": ["What dates was Johann Sebastian Bach married to Anna Magdalena Bach?", "When did Maria Barbara Bach die?", "Is #2 included in #1?"], "evidence": [[[["Anna Magdalena Bach-10", "Anna Magdalena Bach-5"]], [["Maria Barbara Bach-1"]], ["operation"]], [[["Johann Sebastian Bach-20"]], [["Johann Sebastian Bach-12", "Maria Barbara Bach-1"]], ["operation"]], [[["Anna Magdalena Bach-5"]], [["Maria Barbara Bach-4"]], ["operation"]]], "golden_sentence": [["Christiana Sophia Henrietta (1723\u20131726) Gottfried Heinrich (1724\u20131763) Christian Gottlieb (1725\u20131728) Elisabeth Juliana Friederica, called \"Liesgen\" (1726\u20131781), married to Bach's pupil, Johann Christoph Altnickol Ernestus Andreas (1727\u20131727) Regina Johanna (1728\u20131733) Christiana Benedicta (1730\u20131730) Christiana Dorothea (1731\u20131732) Johann Christoph Friedrich, the 'B\u00fcckeburg' Bach (1732\u20131795) Johann August Abraham (1733\u20131733) Johann Christian, the 'London' Bach (1735\u20131782) Johanna Carolina (1737\u20131781) Regina Susanna (1742\u20131809) After Bach's death in 1750, his sons came into conflict and moved on in separate directions, going to live with other family members.", "Bach married Anna on December 3, 1721, 17 months after the death of his first wife, Maria Barbara Bach."], ["20 October]\u00a01684 \u2013 buried 7 July 1720) was the first wife of composer Johann Sebastian Bach."]]}, {"qid": "f155677481682015623e", "term": "Cheeseburger", "description": "hamburger topped with cheese", "question": "Could Eddie Hall hypothetically deadlift the world's largest cheeseburger?", "answer": false, "facts": ["The largest cheeseburger ever made weighed 2,014 pounds.", "Eddie Hall is the former world record deadlift holder, lifting 1,102 pounds under strongman rules."], "decomposition": ["What is Eddie Hall's record deadlist?", "What is the weight of the world largest cheeseburger?", "Is #1 larger than #2?"], "evidence": [[[["Eddie Hall-1"]], [["Cheeseburger-8"]], ["operation"]], [[["Eddie Hall-1"]], [["Cheeseburger-8"]], ["operation"]], [[["Eddie Hall-1"]], [["Cheeseburger-8"]], ["operation"]]], "golden_sentence": [["Edward Stephen Hall (born 15 January 1988) is an English former professional strongman, notable for winning the World's Strongest Man 2017 competition and for being the world record deadlift holder, lifting 500\u00a0kg (1,102\u00a0lb; 79\u00a0st) under strongman rules, which he achieved in 2016."], ["The largest cheeseburger ever made weighed 2,014 pounds (914\u00a0kg), including \"60 pounds (27\u00a0kg) of bacon, 50 pounds (23\u00a0kg) of lettuce, 50 pounds (23\u00a0kg) of sliced onions, 40 pounds (18\u00a0kg) of pickles, and 40 pounds (18\u00a0kg) of cheese.\""]]}, {"qid": "bf1530ed3a5a20ebac08", "term": "Justin Timberlake", "description": "American singer, record producer, and actor", "question": "Has Justin Timberlake ever written a song about Britney Spears?", "answer": true, "facts": ["Justin Timberlake and Britney Spears dated in 1999.", "In 2002, Justin Timberlake released a music video for his breakup song 'Cry Me A River' and featured an actress who resembled his then ex Britney Spears."], "decomposition": ["Who did Justin Timberlake date in 1999?", "Who was the song 'Cry Me A River' by Justin timberlake about?", "Is #1 and #2 the same?"], "evidence": [[[["Justin Timberlake-32"]], [["Cry Me a River (Justin Timberlake song)-1"]], ["operation"]], [[["Justin Timberlake-32"]], [["Cry Me a River (Justin Timberlake song)-1"]], ["operation"]], [[["Justin Timberlake-32"]], [["Cry Me a River (Justin Timberlake song)-1"]], ["operation"]]], "golden_sentence": [["In early 1999, Timberlake began dating fellow former The All-New Mickey Mouse Club cast member and singer Britney Spears."], ["Accompanied by an electric piano, beatbox, guitars, synthesizers, Arabian-inspired riffs and Gregorian chants, \"Cry Me a River\" is an R&B song about a brokenhearted man who moves on from his last girlfriend, who had cheated on him."]]}, {"qid": "5e50a47cde27fdc09ede", "term": "Head coach", "description": "Senior coach or manager of a sports team", "question": "Do most high school head coaches make as much as the Head Coach at NCSU?", "answer": false, "facts": ["The average high school makes about $41,000.", "The head coach for NCSU makes about $1.8 million dollars."], "decomposition": ["What is the average salary for a high school head coach?", "What is the salary of the head football coach at NCSU?", "Is #1 within 5% of #2?"], "evidence": [[["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Head coach-4"], "no_evidence"], [["NC State Wolfpack-1"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], [["NC State Wolfpack football-34"]], ["no_evidence", "operation"]]], "golden_sentence": []}, {"qid": "1d33f1d899f3cbc6ba6f", "term": "Christopher Columbus", "description": "Italian explorer, navigator, and colonizer", "question": "Did Christopher Columbus break the fifth commandment in Christianity?", "answer": true, "facts": ["The fifth commandment states that Thou Shalt Not Kill.", "Christopher Columbus ordered a brutal crackdown in which many natives were killed, and then paraded their dismembered bodies through the streets."], "decomposition": ["What is the fifth commandment in Christianity?", "What activities did Christopher Columbus subject Native Americans too?", "Is the activity in #1 also in #2?"], "evidence": [[[["Thou shalt not kill-29"]], [["Christopher Columbus-48"]], [["Christopher Columbus-48"], "operation"]], [[["Ten Commandments-16"]], [["Christopher Columbus-93"], "no_evidence"], ["no_evidence", "operation"]], [[["Thou shalt not kill-36"], "no_evidence"], [["Christopher Columbus-48"]], ["operation"]]], "golden_sentence": [["The basis of all Catholic teaching about the fifth commandment is the \"sanctity of life\", which is often contrasted with the \"quality of life\" to some extent."], [""], [""]]}, {"qid": "4888a59ff88395329a03", "term": "Transport", "description": "Human-directed movement of things or people between locations", "question": "Can you transport a primate in a backpack?", "answer": true, "facts": ["Primates include lemurs, monkeys, apes, and humans.", "A Capuchin is a type of monkey that are an average height of twelve inches and weighs nine pounds.", "A school ruler is twelve inches.", "One school textbook can weigh up to six pounds."], "decomposition": ["What is the average size and weight capacity of a backpack?", "What are some common primates?", "Is any of #2 such that its size and weight is less than or equal to #1?"], "evidence": [[[["Backpack-3"]], [["Primate-1"]], [["Madame Berthe's mouse lemur-1"]]], [[["Backpack-3"]], [["Primate-1"], "no_evidence"], ["operation"]], [[["Backpack-18"], "no_evidence"], [["Primates (journal)-1"], "no_evidence"], ["no_evidence"]]], "golden_sentence": [["Large backpacks, used to carry loads over 10 kilograms (22\u00a0lb), as well as smaller sports backpacks (e.g."], [""], [""]]}, {"qid": "97d4feb109af8dc3d64b", "term": "King Arthur", "description": "legendary British leader of the late 5th and early 6th centuries", "question": "Was King Arthur at the beheading of Anne Boleyn?", "answer": false, "facts": ["King Arthur was a legendary British leader who, according to medieval histories and romances, led the defence of Britain against Saxon invaders in the late 5th and early 6th centuries.", "Anne Boleyn was beheaded May 19, 1536."], "decomposition": ["When is King Arthur thought to have died?", "When was Anne Boleyn born?", "Is #2 before #1?"], "evidence": [[[["King Arthur-1"], "no_evidence"], [["Anne Boleyn-1"]], ["operation"]], [[["Battle of Camlann-1"]], [["Anne Boleyn-1"]], ["operation"]], [[["King Arthur-6"]], [["Anne Boleyn-6"]], ["operation"]]], "golden_sentence": [[""], ["Anne Boleyn (/\u02c8b\u028al\u026an, b\u028a\u02c8l\u026an/; c. 1501 \u2013 19 May 1536) was Queen of England from 1533 to 1536 as the second wife of King Henry VIII."]]}, {"qid": "efe44afdf54069124386", "term": "Uppsala", "description": "Place in Uppland, Sweden", "question": "Can the city of Miami fit inside Uppsala?", "answer": false, "facts": ["Miami measures 55.25 mi\u00b2.", "Uppsala has an area of 18.83 mi\u00b2. "], "decomposition": ["What is the area of Miami?", "What is the area of Uppsala?", "Is #1 less than or equal to #2?"], "evidence": [[[["Miami-15"]], ["no_evidence"], ["operation"]], [[["Miami-1"]], [["Uppsala-1"], "no_evidence"], ["operation"]], [[["Miami-1"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["According to the U.S. Census Bureau, the city encompasses a total area of 56.06\u00a0sq\u00a0mi (145.2\u00a0km2), of which 35.99\u00a0sq\u00a0mi (93.2\u00a0km2) is land and 20.08\u00a0sq\u00a0mi (52.0\u00a0km2) is water."]]}, {"qid": "235c768731cb5b3ae515", "term": "Samsung Galaxy", "description": "series of Android mobile computing devices", "question": "Does Iphone have more iterations than Samsung Galaxy?", "answer": false, "facts": ["As of 2020 the latest Iphone is Iphone 11.", "As of 2020 the latest Samsung Galaxy phone is the Samsung Galaxy S20."], "decomposition": ["How many models of the iPhone have been released as of 2020?", "How many models of the Samsung Galaxy have been released as of 2020?", "Is #1 greater than #2?"], "evidence": [[[["IPhone-178"]], [["Samsung Galaxy-1"], "no_evidence"], ["operation"]], [[["IPhone-178"]], [["Samsung Galaxy-1"], "no_evidence"], ["no_evidence", "operation"]], [[["IPhone SE (2nd generation)-1"]], [["Samsung Galaxy S20-1"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "bf5615183cdadb73585d", "term": "Hanuman", "description": "The divine monkey companion of Rama in Hindu mythology", "question": "Is Hanuman associated with a Norse god?", "answer": false, "facts": ["Hanuman is a companion of the god Rama.", "Rama is an avatar of the god Vishnu in Hindu mythology."], "decomposition": ["Which god is Hanuman associated with?", "Is #1 in Norse mythology?"], "evidence": [[[["Hanuman-1"]], [["Norse mythology-2"], "no_evidence", "operation"]], [[["Hanuman-1"]], ["operation"]], [[["Hanuman-1"]], [["Norse mythology-1", "Rama-1"]]]], "golden_sentence": [["Hanuman (/\u02c8h\u028cn\u028a\u02ccm\u0251\u02d0n/; Sanskrit: \u0939\u0928\u0941\u092e\u093e\u0928\u094d, IAST: Hanum\u0101n) is a Hindu god and divine monkey (vanara) companion of the god Rama."], ["The source texts mention numerous gods, such as the hammer-wielding, humanity-protecting thunder-god Thor, who relentlessly fights his foes; the one-eyed, raven-flanked god Odin, who craftily pursues knowledge throughout the worlds and bestowed among humanity the runic alphabet; the beautiful, sei\u00f0r-working, feathered cloak-clad goddess Freyja who rides to battle to choose among the slain; the vengeful, skiing goddess Ska\u00f0i, who prefers the wolf howls of the winter mountains to the seashore; the powerful god Nj\u00f6r\u00f0r, who may calm both sea and fire and grant wealth and land; the god Freyr, whose weather and farming associations bring peace and pleasure to humanity; the goddess I\u00f0unn, who keeps apples that grant eternal youthfulness; the mysterious god Heimdallr, who is born of nine mothers, can hear grass grow, has gold teeth, and possesses a resounding horn; the j\u00f6tunn Loki, who brings tragedy to the gods by engineering the death of the goddess Frigg's beautiful son Baldr; and numerous other deities."]]}, {"qid": "d1a228ab9c5d96c47a87", "term": "Sonnet", "description": "form of poetry with fourteen lines; by the thirteenth century it signified a poem of fourteen lines that follows a strict rhyme scheme and specific structure", "question": "Did a Polish poet write sonnets about Islamic religion?", "answer": true, "facts": ["Adam Mickiewicz was a Polish poet. ", "Adam Mickiewicz 's sonnet sequence focuses heavily on the culture and Islamic religion of the Crimean Tatars."], "decomposition": ["What were the major focus of Adam Mickiewicz's sonnets?", "Is #1 about Islamic religion?", "Was Adam Mickiewicz a Polish poet?", "Are #2 and #3 positive?"], "evidence": [[[["The Crimean Sonnets-1"]], [["Orientalism-1", "The Crimean Sonnets-2"]], [["Adam Mickiewicz-1"]], ["operation"]], [[["The Crimean Sonnets-2"]], ["no_evidence"], [["Adam Mickiewicz-1"]], ["operation"]], [[["The Crimean Sonnets-1"], "no_evidence"], [["Adam Mickiewicz-23"], "no_evidence", "operation"], [["Adam Mickiewicz-1"]], ["no_evidence", "operation"]]], "golden_sentence": [["The Crimean Sonnets (Sonety krymskie) are a series of 18 Polish sonnets by Adam Mickiewicz, constituting an artistic telling of a journey through the Crimea."], ["", ""], ["Adam Bernard Mickiewicz ([mit\u0361s\u02c8k\u02b2\u025bvit\u0361\u0282] (listen); 24 December 1798\u00a0\u2013 26 November 1855) was a Polish poet, dramatist, essayist, publicist, translator, professor of Slavic literature, and political activist."]]}, {"qid": "372e26f1e3f11846bea0", "term": "Bugs Bunny", "description": "Warner Bros. cartoon character", "question": "Can you find Bugs Bunny at Space Mountain?", "answer": false, "facts": ["Space Mountain is an attraction at Disney theme parks", "Bugs Bunny is a Warner Bros. character", "Warner Bros. characters appear at Six Flags theme parks"], "decomposition": ["Where is Space Mountain located?", "Which animation studio created Bugs Bunny?", "Which entertainment company is #1 related to?", "Is #2 part of #3?"], "evidence": [[[["Space Mountain-1"]], [["Bugs Bunny-1"]], [["Space Mountain (Disneyland)-1"]], ["operation"]], [[["Space Mountain-1"]], [["Bugs Bunny-1"]], [["Disney Parks, Experiences and Products-1"]], ["operation"]], [[["Space Mountain (Disneyland)-1"]], [["Bugs Bunny-1"]], [["Disneyland-1"]], ["operation"]]], "golden_sentence": [["Space Mountain is a space-themed indoor roller coaster attraction located at five of the six Disneyland-style Disney Parks."], ["Bugs Bunny is an animated cartoon character, created in the late 1930s by Leon Schlesinger Productions (later Warner Bros. Cartoons) and voiced originally by Mel Blanc."], ["Opened on May 27, 1977, it was the second roller coaster built at Disneyland, and was the second of the five versions of Space Mountain built by The Walt Disney Company."]]}, {"qid": "5665c559a8e6954297cf", "term": "Brazilian Navy", "description": "Naval warfare branch of Brazil's military forces", "question": "Could modern Brazilian Navy have hypothetically turned the tide in Battle of Actium?", "answer": true, "facts": ["The Battle of Actium saw Mark Antony's army lose to Octavian.", "Octavian's army had 400 ships, 16000 infantry, and 3,000 archers.", "The Brazilian Navy has over 80,000 personnel, including 16,000 marines.", "Several Brazilian Navy ships are armed with explosive torpedoes. "], "decomposition": ["What was the result of the Battle of Actium?", "In #1, how many resources did the Octavian's army have?", " How many resources does the Brazilian Navy have? ", "Is #3 significantly more than #2?"], "evidence": [[[["Battle of Actium-26"]], [["Battle of Actium-14"]], [["Brazilian Navy-55", "Brazilian Navy-56"]], ["operation"]], [[["Battle of Actium-2"]], [["Battle of Actium-12"]], [["Brazilian Navy-55"]], ["operation"]], [[["Battle of Actium-2"]], [["Battle of Actium-12"]], [["Brazilian Navy-56"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["It is estimated that Antony had around 230 ships,(plus 30-50 armed transports)as opposed to the 300 ships of Octavian's fleet."], ["As of 2011, the Brazilian Navy has a reported strength of 60,000 active personnel, of which approximately 15,000 are naval infantry.", "As of 2012, the Brazilian Navy had about 100 commissioned ships, with others undergoing construction, acquisition and modernization."]]}, {"qid": "2660a1aed74b3fd438d2", "term": "Jason", "description": "Greek mythological hero", "question": "Could the children of Greek hero Jason hypothetically fill a polo team?", "answer": true, "facts": ["The Greek mythological hero is known for his quest to obtain the Golden Fleece.", "The Greek mythological hero had four children: Euneus, Nebrophonus, Mermerus, and Pheres.", "Polo is a sport played between two teams of 4 players."], "decomposition": ["How many children did Greek mythological hero Jason have?", "How many people are needed to make a polo team?", "Is #1 equal to or more than #2?"], "evidence": [[["no_evidence"], [["Polo-51"]], ["operation"]], [[["Medea-10"]], [["Polo-4"]], ["operation"]], [[["Medea-10"]], [["Water polo-1"]], ["operation"]]], "golden_sentence": [["Each team consists of four mounted players, which can be mixed teams of both men and women."]]}, {"qid": "8dc6310683d2f3f98ac0", "term": "Miami", "description": "", "question": "Can you swim to Miami from New York?", "answer": false, "facts": ["The longest distance swam by a person is 139.8 miles.", "It is over 1,000 miles from New York to Miami."], "decomposition": ["What is the longest distance that a human has ever swum?", "How far does one need to swim to get from New York to Miami?", "Is #1 more than #2?"], "evidence": [[[["Veljko Rogo\u0161i\u0107-1"]], [["Silver Meteor-19"]], ["operation"]], [[["Veljko Rogo\u0161i\u0107-1"]], [["Miami River (New York)-1"], "no_evidence"], ["operation"]], [[["Long-distance swimming-1"]], [["Miami-1", "New York City-1"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["He swum a total of 225 km (139.8 miles) across the Adriatic sea in a time of 50 hours and 10 minutes."], ["The Silver Meteor's route covers 1,389 miles (2,235\u00a0km) between New York City and Miami, Florida."]]}, {"qid": "21783bdedfc9f64749ea", "term": "ABBA", "description": "Swedish pop group", "question": "Could ABBA play a mixed doubles tennis game against each other?", "answer": true, "facts": ["ABBA contained two male and two female members.", "Mixed doubles tennis games consist of two teams of one man and one woman on each."], "decomposition": ["How many men and women are required to participate in a mixed doubles tennis game?", "How many men and women are members of the ABBA group?", "Is #2 at least equal to #1?"], "evidence": [[[["Mixed doubles-1", "Mixed-sex sports-12"]], [["ABBA-1", "ABBA-2"]], ["operation"]], [[["Types of tennis match-4"]], [["ABBA-2"]], ["operation"]], [[["Mixed doubles-1"]], [["ABBA-1", "Agnetha F\u00e4ltskog-11"]], ["operation"]]], "golden_sentence": [["Mixed doubles or mixed pairs is a form of mixed-sex sports that consists of teams of one man and one woman.", "Mixed doubles are events where two mixed-sex pairs directly compete (that is, all four competitors are in open play as two teams)."], ["", "During the band's main active years, it was composed of two married couples: F\u00e4ltskog and Ulvaeus, and Lyngstad and Andersson."]]}, {"qid": "9da0a21d85c8736644c3", "term": "Aloe", "description": "genus of plants", "question": "Is material from an aloe plant sometimes enclosed in petroleum-derived products?", "answer": true, "facts": ["Aloe vera gel is sometimes kept in plastic packaging.", "Plastic packaging is derived from petroleum."], "decomposition": ["What kind of products are derived from petroleum?", "What products are made from aloe plants?", "Is #2 ever be packaged inside #1?"], "evidence": [[[["Petroleum product-4"]], [["Aloe vera-19"]], ["no_evidence", "operation"]], [[["Plastic-3"], "no_evidence"], [["Aloe vera-15"], "no_evidence"], ["operation"]], [[["Petroleum-2"]], [["Petroleum-2"]], [["Petroleum-2"], "operation"]]], "golden_sentence": [["Over 6,000 items are made from petroleum waste by-products, including: fertilizer, flooring (floor covering), perfume, insecticide, petroleum jelly, soap, vitamins and some essential amino acids."], ["Cosmetic companies commonly add sap or other derivatives from Aloe vera to products such as makeup, tissues, moisturizers, soaps, sunscreens, incense, shaving cream, or shampoos."]]}, {"qid": "e876aede34f0fdeece1f", "term": "Hurricane Harvey", "description": "Category 4 Atlantic hurricane in 2017", "question": "Could Hurricane Harvey catch a Peregrine falcon?", "answer": false, "facts": ["Hurricane Harvey had maximum winds of 130 MPH.", "The Peregrine falcon is the fastest animal on Earth.", "A Peregrine falcon can reach a maximum speed of 240 MPH."], "decomposition": ["What was the top speed of Hurricane Harvey?", "What is the top speed of a Peregrine falcon?", "Is #1 greater than #2?"], "evidence": [[[["Hurricane Harvey-24"], "no_evidence"], [["Peregrine falcon-1"], "no_evidence"], ["no_evidence"]], [[["Hurricane Harvey-5"]], [["Peregrine falcon-15"]], ["operation"]], [[["Hurricane Harvey-6"]], [["Peregrine falcon-1"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["According to a National Geographic TV program, the highest measured speed of a peregrine falcon is 389\u00a0km/h (242\u00a0mph)."]]}, {"qid": "a5638836d3447c8113df", "term": "Hanging", "description": "execution or suicide method involving suspension of a person by a ligature", "question": "Do bodies movie during hanging?", "answer": true, "facts": ["Electrochemical nerve signals are fired after death that can cause a body to twitch.", "If death by hanging is accomplished due to asphyxia, the victim may attempt to free themselves or may appear to struggle."], "decomposition": ["What does death by hanging usually induce in victims?", "What processes could occur in the nervous system immediately after death?", "Do #1 or #2 result in body movement?"], "evidence": [[[["Hanging-25"]], [["Hanging-26"]], ["operation"]], [[["Hanging-19"]], ["no_evidence"], ["operation"]], [[["Hanging-1"]], [["Hanging-25"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "e78394a86e4dbebeca0a", "term": "Benito Mussolini", "description": "Fascist leader of Italy", "question": "Would Benito Mussolini hypothetically play well in the NBA?", "answer": false, "facts": ["Height is an important factor in playing basketball at a high level.", "The average NBA player is 6 feet 7 inches tall.", "Benito Mussolini was 5 feet 6.5 inches tall."], "decomposition": ["What is the height of Benito Mussolini?", "On average, what is the height of an NBA player?", "Is #1 comparable to #2?"], "evidence": [[["no_evidence"], [["Basketball-85"]], ["operation"]], [["no_evidence"], [["Basketball-85"]], ["no_evidence"]], [[["Benito Mussolini-1"], "no_evidence"], [["Basketball-85"]], ["operation"]]], "golden_sentence": [["the average height of all NBA players is just under 6\u00a0feet 7\u00a0inches (2.01\u00a0m), with the average weight being close to 222 pounds (101\u00a0kg)."]]}, {"qid": "7b7faaab5b0ebe86c2d4", "term": "Western honey bee", "description": "Species of insect", "question": "Does US brand Nice depend on Western honey bee?", "answer": true, "facts": ["US brand Nice creates a number of products including honey.", "The Western honey bee can be found on every continent except Antarctica.", "The Western honey bee is the most common pollinator in US, making it the most important bee to domestic agriculture."], "decomposition": ["Which insect does US brand Nice need to obtain the honey they sell from?", "Is the Western honey bee a common example of #1 that can be found in the US?"], "evidence": [[[["Honey-1"], "no_evidence"], [["Honey bee-20", "Western honey bee-1"], "operation"]], [[["Honey-5"]], [["Western honey bee-5"], "operation"]], [[["Walgreens-13"], "no_evidence"], [["Western honey bee-5"]]]], "golden_sentence": [["Bees produce honey from the sugary secretions of plants (floral nectar) or from secretions of other insects (such as honeydew), by regurgitation, enzymatic activity, and water evaporation."], ["", "The western honey bee or European honey bee (Apis mellifera) is the most common of the 7\u201312 species of honey bees worldwide."]]}, {"qid": "2160f9e1a4cb0e7f2c9d", "term": "Hulk", "description": "Superhero appearing in Marvel Comics publications and related media", "question": "Can Hulk's alter ego explain atomic events?", "answer": true, "facts": ["Hulk's alter ego is Dr. Robert Bruce Banner", "Dr. Robert Bruce Banner is a nuclear physicist. ", "Nuclear physics is the field of physics that studies atomic nuclei and their constituents and interactions. "], "decomposition": ["Who is the Hulk's alter ego?", "What is the profession of #1?", "What do people in #2 have a knowledge of?", "Is atomic events included in #3?"], "evidence": [[[["Hulk-1"]], [["Hulk-45"]], [["Physicist-1"]], [["Elementary event-1"]]], [[["Hulk-9"]], [["Hulk-1"]], [["Scientist-1"]], [["Atomic Age (design)-1"], "operation"]], [[["Hulk-1"]], [["Hulk-57"]], ["operation"], ["operation"]]], "golden_sentence": [["In his comic book appearances, the character is both the Hulk, a green-skinned, hulking and muscular humanoid possessing a vast degree of physical strength, and his alter ego Dr. Robert Bruce Banner, a physically weak, socially withdrawn, and emotionally reserved physicist, the two existing as independent personalities and resenting of the other."], ["Banner, a physicist, is sarcastic and seemingly very self-assured when he first appears in Incredible Hulk #1, but is also emotionally withdrawn in most fashions."], ["A physicist is a scientist who specializes in the field of physics, which encompasses the interactions of matter and energy at all length and time scales in the physical universe."], [""]]}, {"qid": "5cec7a2077bb82bde4c5", "term": "Firewall (computing)", "description": "Software or hardware-based network security system", "question": "Can a firewall protect against a short circuit?", "answer": false, "facts": ["A firewall is a computer program that protects unwanted attacks from penetrating a computer.", "Firewalls are installed on computers and conduct routine background maintenance.", "A short circuit is an electrical failure resulting from wires unable to conduct currents.", "Short circuits, especially during updates can lead to the dreaded Windows Blue Screen of Death in which a computer is unable to restart."], "decomposition": ["What kind of threats does a firewall protect a computer system against?", "What are the possible causes and results of a short circuit as concerning computers?", "Is any of #2 included in #1?"], "evidence": [[[["Firewall (computing)-13"]], [["Short circuit-7", "Short circuit-9"]], ["operation"]], [[["Windows Firewall-2"]], [["Short circuit-7"]], [["Short circuit-7"], "operation"]], [[["Firewall (computing)-1"]], [["Short circuit-1", "Short circuit-10", "Short circuit-7"]], ["operation"]]], "golden_sentence": [[""], ["", ""]]}, {"qid": "70a675c346a13651ece6", "term": "Swiss Guard", "description": "Military of Vatican City", "question": "Would Swiss Guard defeat the Marines?", "answer": false, "facts": ["The Swiss Guard is the military of Vatican City and consists of 135 members.", "There are 186,000 active duty Marines as of 2017."], "decomposition": ["How many people are in the Swiss Guard?", "How many people are in the US Marine Corp?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Swiss Guard-31"]], [["United States Marine Corps-3"]], ["operation"]], [[["Military in Vatican City-14"]], [["United States Marine Corps-3"]], ["operation"]], [[["Swiss Guards-18", "Swiss Guards-3"], "no_evidence"], [["United States Marine Corps-3"]], ["operation"]]], "golden_sentence": [["As of 2010 the guard numbered 107 halberdiers divided into three squads, plus five officers."], ["As of 2017, the USMC has around 186,000 active duty members and some 38,500 personnel in reserve."]]}, {"qid": "631d18219373648cc982", "term": "Conservatism", "description": "Political philosophy focused on retaining traditional social institutions", "question": "Does conservatism repulse Blaire White?", "answer": false, "facts": ["Blaire White is a Youtuber.", "Blaire White posts content that leans towards conservative politics."], "decomposition": ["What is Blaire White's political orientation as evidenced by her expressions?", "Is #1 completely devoid of conservatism?"], "evidence": [[[["Blaire White-1"]], [["Liberal conservatism-9"], "operation"]], [[["Blaire White-1"]], ["operation"]], [[["Blaire White-1"]], [["Liberal conservatism-9"]]]], "golden_sentence": [["Describing her beliefs as center-right, she is a critic of third-wave feminism, activists she views as social justice warriors and activist movements like Black Lives Matter."], [""]]}, {"qid": "56a9a0f0d74012ccf57e", "term": "Hair", "description": "protein filament that grows from follicles found in the dermis, or skin", "question": "Do skeletons have hair?", "answer": false, "facts": ["Hair grows from the skin.", "Skeletons are a structure of multiple bones.", "Bones do not grow hair. "], "decomposition": ["Where does hair grow from?", "What are skeletons made out of?", "Does #2 have #1?"], "evidence": [[[["Dermis-1", "Hair-1"]], [["Skeleton-1", "Skeleton-19"]], ["operation"]], [[["Hair-6"]], [["Skeleton-19"], "no_evidence"], ["operation"]], [[["Hair-1"]], [["Skeleton-14"]], ["operation"]]], "golden_sentence": [["", "Hair is a protein filament that grows from follicles found in the dermis."], ["", "The human skeleton consists of both fused and individual bones supported and supplemented by ligaments, tendons, muscles and cartilage."]]}, {"qid": "32302070b6a4ddad7cad", "term": "Cosmic ray", "description": "High-energy particle, mainly originating outside the Solar system", "question": "Did H.G. Wells' \"War of the Worlds\" include cosmic rays?", "answer": false, "facts": ["The book \"War of the Worlds\" was published in 1898.", "Cosmic rays were not discovered until 1912."], "decomposition": ["When was the War of the Worlds published?", "When were cosmic rays discovered?", "Did #2 come before #1?"], "evidence": [[[["The War of the Worlds-1"]], [["Cosmic ray-7"]], ["operation"]], [[["The War of the Worlds-1"]], [["Cosmic ray-7"]], ["operation"]], [[["The War of the Worlds-1"]], [["Cosmic ray-7"]], ["operation"]]], "golden_sentence": [["The War of the Worlds is a science fiction novel by English author H. G. Wells, first serialised in 1897 by Pearson's Magazine in the UK and by Cosmopolitan magazine in the US."], [""]]}, {"qid": "f2d4f4c65684babcb44e", "term": "50 Cent", "description": "American rapper, singer, songwriter, actor, television producer, entrepreneur and investor", "question": "Does 50 Cent get along with Jeffrey Atkins?", "answer": false, "facts": ["Jeffrey Atkins is a rapper better known as Ja Rule.", "Ja Rule released the diss track \"Loose Change\" in April 2003, where he attacks 50 Cent.", "In 2018 50 cent responded to a Ja Rule diss by purchasing and vacating the first 4 rows of Ja Rule's concert."], "decomposition": ["What artists was Jeffrey Atkins critical of in his song Loose Change?", "Is 50 Cent outside the group of #1?"], "evidence": [[[["Ja Rule-1", "Ja Rule-12"]], ["operation"]], [[["Ja Rule-12"]], ["operation"]], [[["Ja Rule-12"]], ["operation"]]], "golden_sentence": [["", "Ja Rule released the diss track \"Loose Change\" in April 2003, where he attacks 50 Cent, as well as Eminem, Busta Rhymes and Dr. Dre."]]}, {"qid": "e5e5d825ae2a11ea01b6", "term": "YMCA", "description": "Worldwide organization founded in 1844 on principles of muscular Christianity", "question": "Is it normal for people to sing when the YMCA is mentioned?", "answer": true, "facts": ["The YMCA was written about in a widely popular song by \"The Village People\"", "The Village People's song \"YMCA\" had an easy to do and very popular dance routine that went with it. ", "The song \"YMCA\" is extremely well known."], "decomposition": ["In what song was the YMCA written about?", "Is #1 a very popular song?"], "evidence": [[[["Y.M.C.A. (song)-1"]], [["Y.M.C.A. (song)-2"]]], [[["Y.M.C.A. (song)-1"]], [["Y.M.C.A. (song)-4"], "no_evidence"]], [[["Y.M.C.A. (song)-1"]], [["Y.M.C.A. (song)-1", "Y.M.C.A. (song)-2"]]]], "golden_sentence": [["is a song by the American disco group Village People."], [""]]}, {"qid": "8fbc0228453f7b0936a5", "term": "Infantry", "description": "military personnel who travel and fight on foot", "question": "Do members of NFL teams receive infantry training?", "answer": false, "facts": ["Members of NFL teams play football", "Infantry training is provided to members of the US armed forces"], "decomposition": ["Which group(s) are entitled to infantry training?", "Are members of the NFL team one of #1?"], "evidence": [[[["Infantry-47"]], [["Infantry-47"]]], [[["Infantry-47"]], [["National Football League-1"], "operation"]], [[["United States Marine Corps School of Infantry-1"]], [["National Football League-1"]]]], "golden_sentence": [[""], [""]]}, {"qid": "edc975a00a21e20afad9", "term": "Quran", "description": "The central religious text of Islam", "question": "Do most religious people in Quebec refer to the Quran?", "answer": false, "facts": ["Christianity is the major religion in Quebec.", "Christians refer to the Bible as their book of reference."], "decomposition": ["What is the dominant religion in Quebec?", "Do adherents of #1 commonly refer to the Quran?"], "evidence": [[[["Demographics of Quebec-21"]], [["Catholic Bible-4", "Quran-1"], "operation"]], [[["Quebec-76"]], [["Bible-1", "Quran-1"]]], [[["Demographics of Quebec-21"]], [["Catholic Bible-25"]]]], "golden_sentence": [["Quebec is unique among the provinces in its overwhelmingly Roman Catholic population, though now has a low church attendance."], ["", ""]]}, {"qid": "f7017719c6c4e5580545", "term": "Gulf of Mexico", "description": "An Atlantic Ocean basin extending into southern North America", "question": "Is a Halloween cruise in the Gulf of Mexico likely to be safe from storms?", "answer": false, "facts": ["Hurricanes often strike the Gulf of Mexico", "Hurricane season in the gulf lasts until the end of November", "Halloween is October 31"], "decomposition": ["Which storms are a common occurrence in the Gulf of Mexico?", "What time of the year is Halloween celebrated?", "According to known patterns, are any of #1 likely to happen during #2?"], "evidence": [[[["Atlantic hurricane-35"]], [["Halloween-1"]], [["Tropical cyclone-46"], "operation"]], [[["Gulf of Mexico-26"]], [["Halloween-1"]], [["Atlantic hurricane season-2"], "operation"]], [[["Tropical cyclone-46", "Tropical cyclone-60", "Tropical cyclone-86"]], [["All Hallows' Eve (disambiguation)-1"]], ["operation"]]], "golden_sentence": [["These quiescent intervals were separated by a hyperactive period during 1400 BC and 1000 AD, when the Gulf coast was struck frequently by catastrophic hurricanes and their landfall probabilities increased by 3\u20135 times."], ["Halloween or Hallowe'en (a contraction of Hallows' Even or Hallows' Evening), also known as Allhalloween, All Hallows' Eve, or All Saints' Eve, is a celebration observed in many countries on 31 October, the eve of the Western Christian feast of All Hallows' Day."], [""]]}, {"qid": "3ca4dc54d01d0a8d8246", "term": "Porsche", "description": "automotive brand manufacturing subsidiary of Volkswagen", "question": "Can Billie Eilish afford a Porsche?", "answer": true, "facts": ["Billie Eilish is a famous female singer.", "Billie Eilish is 18 years old and has a net worth of $25 Million. ", "A Porsche Boxster is a car that starts at $59,000.", "$25,000,000 is greater than $59,000."], "decomposition": ["What is Billie Eilish's net worth?", "How much does a Porsche cost?", "Is #1 greater than #2?"], "evidence": [[[["Billie Eilish-3"], "no_evidence"], [["Porsche Carrera GT-5", "Porsche-16"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], [["Porsche-16"]], ["no_evidence", "operation"]], [[["Billie Eilish-23"], "no_evidence"], [["Porsche Panamera-19"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["Porsche started a production run of the Carrera GT in 2004, shipping the units with an MSRP of US$448,000.", "In 2004, production of the 456 kilowatts (620\u00a0PS; 612\u00a0bhp) Carrera GT commenced in Leipzig, and at EUR 450,000 ($440,000 in the United States) it was the most expensive production model Porsche ever built."]]}, {"qid": "7881f49044ce87ca0e42", "term": "Formula One", "description": "Auto racing championship held worldwide", "question": "Did Secretariat win a Formula One championship?", "answer": false, "facts": ["Secretariat is a famous race horse", "Formula One is an auto racing championship"], "decomposition": ["What is Secretariat?", "What is #1's top speed?", "What is the top speed for a Formula One car?", "Is #2 greater than #3?"], "evidence": [[[["Secretariat (horse)-1"]], [["Secretariat (horse)-3"], "no_evidence"], [["Formula One car-60"]], ["operation"]], [[["Secretariat (horse)-1"]], [["Secretariat (horse)-3"], "no_evidence"], [["Formula One-3"]], ["operation"]], [[["Secretariat (horse)-1"]], [["Secretariat (horse)-27"]], [["Formula One-3"]], ["operation"]]], "golden_sentence": [["Secretariat (March 30, 1970 \u2013 October 4, 1989) was an American Thoroughbred racehorse who, in 1973, became the first Triple Crown winner in 25 years."], [""], ["The car, badged as a Honda following their takeover of BAR at the end of 2005, set an FIA ratified record of 400\u00a0km/h (249\u00a0mph) on a one way run on 21 July 2006 at Bonneville Speedway."]]}, {"qid": "8c322dbd3fe0a4605e9a", "term": "Robert Downey Jr.", "description": "American actor", "question": "Does Robert Downey Jr's Marvel Cinematic Universe character survive the Infinity War?", "answer": false, "facts": ["Robert Downey Jr. plays Iron Man in the MCU.", "At the end of Avengers: Endgame, Iron Man uses the Infinity Stones against Thanos.", "The stones are too powerful for his human body to handle, so he dies after using them."], "decomposition": ["What is Robert Downey Jr's character in Infinity War?", "Does #1 survive to the end of Infinity War?"], "evidence": [[[["Robert Downey Jr.-38"]], [["Avengers: Infinity War-10"], "no_evidence"]], [[["Tony Stark (Marvel Cinematic Universe)-36"]], [["Avengers: Infinity War-9", "Tony Stark (Marvel Cinematic Universe)-26"], "operation"]], [[["Avengers: Infinity War-5", "Tony Stark (Marvel Cinematic Universe)-1"]], [["Tony Stark (Marvel Cinematic Universe)-26"]]]], "golden_sentence": [[""], [""]]}, {"qid": "39bf3d4008d353280065", "term": "Astronomer", "description": "Scientist who studies celestial bodies", "question": "Does James Webb Space Telescope fail astronomer in locating planet Krypton?", "answer": true, "facts": ["The James Webb Space Telescope is the most powerful telescope created.", "Krypton is a planet in the fictional Superman comic book series."], "decomposition": ["Which universe does the planet Krypton exist in?", "Does the James Webb Space Telescope as we know it exist in a universe different from #1?"], "evidence": [[[["Krypton (comics)-1"]], [["James Webb Space Telescope-1"], "no_evidence"]], [[["Krypton (comics)-1"]], [["James Webb Space Telescope-1"]]], [[["Krypton (comics)-1"]], [["James Webb Space Telescope-1"], "operation"]]], "golden_sentence": [[""], ["The James Webb Space Telescope (JWST or \"Webb\") is a space telescope that is planned to be one of the successors to the Hubble Space Telescope."]]}, {"qid": "ae86f27d5b9cc01d5e1b", "term": "Crustacean", "description": "subphylum of arthropods", "question": "Do all crustaceans live in the ocean?", "answer": false, "facts": ["The woodlice family of crustaceans is terrestrial.", "There are also many crustacean species living in fresh water rivers and lakes."], "decomposition": ["What are some common families of crustaceans?", "Is there any of #1 that lives in a terrestrial habitat?", "Do any of #1 that are aquatic also live in freshwater?", "Are #2 and #3 negative?"], "evidence": [[[["Crab-1", "Crustacean-4"]], [["Woodlouse-2"]], [["Potamon fluviatile-1"]], ["operation"]], [[["Crustacean-2"]], [["Crustacean-4"]], [["Crustacean-10"]], ["no_evidence"]], [[["Crustacean-1"]], [["Crustacean-10", "Woodlouse-2"], "no_evidence"], [["Shrimp-11", "Shrimp-2"]], ["operation"]]], "golden_sentence": [["", ""], ["They have many common names and although often referred to as 'terrestrial Isopods' some species live semiterrestrially or have recolonised aquatic environments."], ["They inhabit burrows and are aggressive, apparently outcompeting native crayfish."]]}, {"qid": "9361ab063d02ec7f1c83", "term": "Warsaw Ghetto", "description": "Ghetto in Nazi occupied Poland", "question": "Did the population of the Warsaw Ghetto record secret police on cell phones?", "answer": false, "facts": ["The Warsaw Ghetto existed during the second world war.", "Cell phones with video recording capability did not exist until the 2000s."], "decomposition": ["When was the Warsaw Ghetto in existence?", "When was the first cell phone capable of recording developed?", "Is #2 before the end of #1?"], "evidence": [[[["Warsaw Ghetto-6"]], [["Digital electronics-11"]], [["Digital electronics-11", "Warsaw Ghetto-6"], "operation"]], [[["Warsaw Ghetto-1"]], [["Camera phone-22"]], ["operation"]], [[["Warsaw Ghetto-3"]], [["Mobile phone-4"]], [["Mobile phone-4"], "operation"]]], "golden_sentence": [["The Germans closed the Warsaw Ghetto to the outside world on November 15, 1940."], [""], ["", ""]]}, {"qid": "0fbd40c627f7bab418c1", "term": "John Lennon", "description": "English singer and songwriter, founding member of the Beatles", "question": "Was John Lennon known to be a good friend to Sasha Obama?", "answer": false, "facts": ["John Lennon died in 1980.", "Sasha Obama was born in 2001."], "decomposition": ["When was Sasha Obama born?", "When did John Lennon die?", "Is #1 before #2?"], "evidence": [[[["Family of Barack Obama-5"]], [["John Lennon-1"]], ["operation"]], [[["Family of Barack Obama-5"]], [["John Lennon-36"]], ["operation"]], [[["Michelle Obama-21"]], [["John Lennon-1"]], ["operation"]]], "golden_sentence": [["Barack and Michelle Obama have two daughters: Malia Ann (/m\u0259\u02c8li\u02d0\u0259/), born July 4, 1998, and Natasha (known as Sasha /\u02c8s\u0251\u02d0\u0283\u0259/), born on June 10, 2001."], ["John Winston Ono Lennon MBE (born John Winston Lennon, 9 October 1940\u00a0\u2013\u00a08 December 1980) was an English singer, songwriter and peace activist who gained worldwide fame as the founder, co-lead vocalist, and rhythm guitarist of the Beatles."]]}, {"qid": "8079e8ef53bf5f202073", "term": "Eggplant", "description": "plant species Solanum melongena", "question": "Can spiders help eggplant farmers control parasites?", "answer": true, "facts": ["The potato tuber moth is a parasite that targets the plant family Solanaceae, including eggplant ", "Selenops radiatus is a spider genus in South Africa that effectively controls the potato tuber moth"], "decomposition": ["Which major parasite insect are eggplants host plants to?", "What are the natural enemies of #1 that farmers can use to control them?", "Is any of #2 a spider?"], "evidence": [[[["Eggplant-50"]], [["Spider behavior-2"]], ["operation"]], [[["Eggplant-46"]], [["Spider-4"]], ["operation"]], [[["Eggplant-46", "Eggplant-47"]], [["Aphid-36", "Spider-4"], "no_evidence"], ["operation"]]], "golden_sentence": [["The potato tuber moth (Phthorimaea operculella) is an oligophagous insect that prefers to feed on plants of the family Solanaceae such as eggplants."], [""]]}, {"qid": "d8521ac08c91c05c8099", "term": "Darth Vader", "description": "fictional character in the Star Wars franchise", "question": "Does Darth Vader's character resemble Severus Snape?", "answer": false, "facts": ["Darth Vader is portrayed as a man who always appears in black full-body armor and a mask.", "Severus Snape is portrayed as a white man with long, greasy black hair who often wears a cloak. "], "decomposition": ["What type of clothing does Darth Vader wear?", "What type of clothing does Severus Snape wear?", "Are there any significant similarities between #1 and #2?"], "evidence": [[[["Darth Vader-16"]], [["Severus Snape-35"]], ["operation"]], [[["Darth Vader-15", "Darth Vader-33"]], [["Severus Snape-35"]], ["operation"]], [[["Darth Vader-15"]], [["Severus Snape-35"]], ["operation"]]], "golden_sentence": [["Darth Vader designers Working from McQuarrie's designs, the costume designer John Mollo devised a costume that could be worn by an actor on-screen using a combination of clerical robes, a motorcycle suit, a German military helmet and a gas mask."], ["He wears black, flowing robes which give him the appearance of \"an overgrown bat\"."]]}, {"qid": "573bcd3151f139795c09", "term": "Organ transplantation", "description": "moving of an organ from one body or body region to another", "question": "Can a carrot receive an organ transplant?", "answer": false, "facts": ["Organs are groups of tissues that perform a similar function.", "The whole of a carrot is a root.", "A root is a plant organ.", "You cannot transplant the entire carrot into another carrot."], "decomposition": ["What part of the plant is the carrot?", "Does #1 have organs?"], "evidence": [[[["Carrot-1"]], [["Organ (anatomy)-1", "Taproot-1"]]], [[["Carrot-1"]], ["operation"]], [[["Carrot-1"]], ["operation"]]], "golden_sentence": [["They are a domesticated form of the wild carrot, Daucus carota, native to Europe and Southwestern Asia."], ["Plant life and animal life rely on many organs that coexist in organ systems.", ""]]}, {"qid": "84bd6bd36db2590204d6", "term": "Memory", "description": "information stored in the mind, or the mental processes involved in receiving, storing, and retrieving this information", "question": "Do people with DID have a good memory?", "answer": false, "facts": ["DID is an abbreviation for 'Dissociative Identity Disorder.'", "DID is characterized by gaps in memory, as well as altered states or 'personalities' during these dissociative moments."], "decomposition": ["What does DID stand for?", "What is #1 characterized by?", "Would be with #2 have good memory?"], "evidence": [[[["Dissociative identity disorder-1"]], [["Dissociative identity disorder-1"]], [["Dissociative identity disorder-1"]]], [[["Dissociative identity disorder-1"]], [["Dissociative identity disorder-1"]], [["Dissociative identity disorder-1", "Psychogenic amnesia-1"]]], [[["Dissociative identity disorder-1"]], [["Dissociative identity disorder-7"]], ["operation"]]], "golden_sentence": [["Dissociative identity disorder (DID), previously known as multiple personality disorder (MPD), is a mental disorder characterized by the maintenance of at least two distinct and relatively enduring personality states."], ["Dissociative identity disorder (DID), previously known as multiple personality disorder (MPD), is a mental disorder characterized by the maintenance of at least two distinct and relatively enduring personality states."], [""]]}, {"qid": "4e56cb387c3899cee42c", "term": "Pandora", "description": "Mythological figure", "question": "Were items released from Pandora's box at least two of the names of Four Horsemen?", "answer": true, "facts": ["Pandora was a mythical figure that opened a box and released several ills on the world including famine, sickness, and death.", "The Four Horsemen of the Apocalypse are: Pestilence, War, Famine, and Death."], "decomposition": ["What items were released from Pandora's box?", "What were the names of the Four Horsemen", "Is there any overlap between #1 and #2?"], "evidence": [[[["Pandora's box-2"]], [["Four Horsemen of the Apocalypse-3"]], [["Four Horsemen of the Apocalypse-3", "Pandora's box-2"]]], [[["Pandora's box-2", "Pandora's box-31"]], [["Four Horsemen of the Apocalypse-5", "Horsemen of Apocalypse-7"], "no_evidence"], ["no_evidence", "operation"]], [[["Pandora's box-2"]], [["Four Horsemen of the Apocalypse-3"]], ["operation"]]], "golden_sentence": [["Pandora opened a jar left in her care containing sickness, death and many other unspecified evils which were then released into the world."], [""], ["", ""]]}, {"qid": "9e1f2e3b60eb6c10433a", "term": "Scientific Revolution", "description": "Beginnings of modern science that occured in Europe towards the end of the Renaissance", "question": "Did the iPhone usher in the scientific revolution?", "answer": false, "facts": ["The scientific revolution took place in the 16th and 17th centuries.", "The iPhone came out in the 21st century."], "decomposition": ["When did the Scientific Revolution begin?", "When did the iPhone come out?", "Did #2 occur before #1?"], "evidence": [[[["Scientific Revolution-1"]], [["IPhone-1"]], ["operation"]], [[["Scientific Revolution-4"]], [["IPhone-1"]], ["operation"]], [[["Scientific Revolution-1"]], [["IPhone-1"]], ["operation"]]], "golden_sentence": [["The Scientific Revolution was a series of events that marked the emergence of modern science during the early modern period, when developments in mathematics, physics, astronomy, biology (including human anatomy) and chemistry transformed the views of society about nature."], ["The first-generation iPhone was released on June 29, 2007, and multiple new hardware iterations with new iOS releases have been released since."]]}, {"qid": "a95a77649bb1e2655c00", "term": "Nature", "description": "Hominin events for the last 10 million years", "question": "Would someone go to San Francisco for a nature escape?", "answer": false, "facts": ["San Francisco is a major US city with over 800,000 people.", "San Francisco is known for mass transit and being a metropolitan area."], "decomposition": ["What kind of developed human settlement is San Francisco?", "Are #1's known for nature?"], "evidence": [[[["San Francisco-1"]], [["San Francisco-1"]]], [[["San Francisco-1"]], ["no_evidence", "operation"]], [[["San Francisco-95"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "a80e3e895cbc931c8e5d", "term": "Parsley", "description": "species of plant, herb", "question": "Does parsley sink in milk?", "answer": false, "facts": ["Items sink if they are denser than the surrounding material.", "Parsley has a density of 0.26 g/cm^3 when fresh.", "Milk has a density of 1.026 g/cm^3."], "decomposition": ["What is the density of parsley?", "What is the density of milk?", "Is #1 greater than #2?"], "evidence": [[["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Parsley-1"], "no_evidence"], [["Milk-1"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], ["no_evidence"], ["operation"]]], "golden_sentence": []}, {"qid": "a9a83dd44482e22e9107", "term": "1912 United States presidential election", "description": "Election of 1912", "question": "Did anyone in the 1912 election take a majority of the popular vote?", "answer": false, "facts": ["Woodrow Wilson took 41% of the vote.", "Theodore Roosevelt took 27% of the vote.", "William Howard Taft took 23% of the vote.", "Eugene Debs took 6% of the vote.", "A majority is more than 50%."], "decomposition": ["What percentage of votes would constitute a majority?", "What percentage of votes did the winner of the 1912 presidential election receive?", "Does #2 fall within the range of #1?"], "evidence": [[[["Majority-1"]], [["1912 United States presidential election-4"]], ["operation"]], [[["Double majority-11"], "no_evidence"], [["1912 United States presidential election-4"]], ["operation"]], [[["Majority-1"]], [["1912 United States presidential election-4"]], ["operation"]]], "golden_sentence": [["\"Majority\" can be used to specify the voting requirement, as in a \"majority vote\", which means more than half of the votes cast."], ["Wilson won 41.8% of the national popular vote, while Roosevelt won 27%, Taft 23%, and Debs 6%."]]}, {"qid": "6760a6c275b094fe8ae9", "term": "Amoeba", "description": "polyphyletic group including different eucariot taxons", "question": "Could amoebas have played a part in the Black Death?", "answer": true, "facts": ["The Black Death is a bacterial disease called bubonic plague.", "Yersinia pestis has been found to grow and thrive inside amoebas.", "Bubonic plague is caused by Yersinia pestis."], "decomposition": ["Which disease is referred to as the Black Death?", "Which specific organism is responsible for #1?", "Could #2 be hosted by amoeba?"], "evidence": [[[["Black Death-1"]], [["Black Death-3"]], [["Amoeba-1"]]], [[["Black Death-1"]], [["Yersinia pestis-1"]], [["Amoeba-10"], "operation"]], [[["Black Death-1"]], [["Black Death-3"]], [["Amoeba-1"], "no_evidence", "operation"]]], "golden_sentence": [["The Black Death, also known as the Pestilence and the Plague, was the most fatal pandemic recorded in human history, resulting in the deaths of up to 75\u2013200 million people in Eurasia and North Africa, peaking in Europe from 1347 to 1351."], ["Current evidence indicates that once it came onshore, the Black Death was in large part spread by human fleas \u2013 which cause pneumonic plague \u2013 and the person-to-person contact via aerosols which pneumonic plague enables, thus explaining the very fast inland spread of the epidemic, which was faster than would be expected if the primary vector was rat fleas causing bubonic plague."], [""]]}, {"qid": "c3ccd57263bf29603701", "term": "P. G. Wodehouse", "description": "English author", "question": "Did P. G. Wodehouse like the internet as a child?", "answer": false, "facts": ["P. G. Wodehouse was born in 1881.", "The internet was not conceived of until 1965. "], "decomposition": ["When was P. G. Wodehouse born?", "When was the internet invented?", "Did #1 come before or during #2?"], "evidence": [[[["P. G. Wodehouse-1"]], [["Internet-2"]], ["operation"]], [[["P. G. Wodehouse-5"], "operation"], [["When Radio Was-6"], "no_evidence"], ["no_evidence"]], [[["P. G. Wodehouse-5"], "operation"], [["Internet-13"], "operation"], ["operation"]]], "golden_sentence": [["Sir Pelham Grenville Wodehouse KBE (/\u02c8w\u028adha\u028as/, WOOD-howss; 15 October 1881\u00a0\u2013 14 February 1975) was an English author and one of the most widely read humorists of the 20th century."], ["The origins of the Internet date back to the development of packet switching and research commissioned by the United States Department of Defense in the 1960s to enable time-sharing of computers."]]}, {"qid": "054fdca9f04b0f3903a2", "term": "The Invisible Man", "description": "1897 science fiction novella by H. G. Wells", "question": " Is The Invisible Man more prevalent in films than Picnic at Hanging Rock?", "answer": true, "facts": ["H.G. Wells's book The Invisible Man has been adapted into more than 7 films.", "Joan Lindsay's book Picnic at Hanging Rock was adapted into one film and one TV series."], "decomposition": ["How many films have been made of \"The Invisible Man\"", "How many films have been made of Picnic at Hanging Rock?", "Is #1 larger than #2?"], "evidence": [[[["Griffin (The Invisible Man)-11"]], [["Picnic at Hanging Rock (novel)-25"]], ["operation"]], [[["The Invisible Man-2"]], [["Picnic at Hanging Rock (film)-11"]], ["operation"]], [[["The Invisible Man-13"], "no_evidence"], [["Picnic at Hanging Rock (film)-1"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "c4bb6d6d7f5fb1dc0cdb", "term": "Disco", "description": "music genre", "question": "Is ABBA's 1970's genre still relevant today?", "answer": true, "facts": ["ABBA was a 1970's music group that specialized in Disco music.", "Pop artist Dua Lipa's 2020 album, Future Nostalgia, was described by Rolling Stone as , \"The Disco Liberation We Need.\"", "Magnetic Magazine released an article in 2020 entitled, \"The Best Disco and Funk Tracks of 2020.\""], "decomposition": ["What genre of music did music group ABBA specialize in in the 1970's?", "Are #1 still relevant today?"], "evidence": [[[["ABBA-1"]], [["Mamma Mia! Here We Go Again-1", "Mamma Mia! Here We Go Again-21"]]], [[["ABBA-1"]], [["Popular music-1"], "operation"]], [[["ABBA-43"]], [["Disco-5"], "operation"]]], "golden_sentence": [["ABBA (/\u00e6\u02c8b\u0259/, Swedish pronunciation:\u00a0[\u00b2ab\u02d0a]) are a Swedish pop supergroup formed in Stockholm in 1972 by Agnetha F\u00e4ltskog, Bj\u00f6rn Ulvaeus, Benny Andersson, and Anni-Frid Lyngstad."], ["", ""]]}, {"qid": "fa33257166d977ecdbe1", "term": "Cactus", "description": "Family of mostly succulent plants, adapted to dry environments", "question": " Is cactus fruit an important menu item for a restaurant based on Cuauht\u00e9moc?", "answer": true, "facts": ["The Aztecs cultivated cacti for the fruit", "Tenochtitlan was the capital of the Aztec empire", "Cuauht\u00e9moc was the last king of Tenochtitlan "], "decomposition": ["Where city was Cuauht\u00e9moc the king of?", "What empire was #1 the capital of?", "Did people in #2 eat cacti?"], "evidence": [[[["Cuauht\u00e9moc-1"]], [["Tenochtitlan-1"]], [["Aztec Empire-8"], "no_evidence"]], [[["Cuauht\u00e9moc-1"]], [["Tenochtitlan-1"]], [["Cactus-61"], "operation"]], [[["Cuauht\u00e9moc-1"]], [["Cuauht\u00e9moc-1"]], [["Aztec cuisine-4"]]]], "golden_sentence": [["Cuauht\u00e9moc (Nahuatl pronunciation:\u00a0[k\u02b7a\u02d0\u028d\u02c8temo\u02d0k] (listen), Spanish pronunciation:\u00a0[kwaw\u02c8temok] (listen) also known as Cuauhtemotz\u00edn, Guatimoz\u00edn or Guat\u00e9moc; c. 1495) was the Aztec ruler (tlatoani) of Tenochtitlan from 1520 to 1521, making him the last Aztec Emperor."], ["The city was the capital of the expanding Aztec Empire in the 15th century until it was captured by the Spanish in 1521."], [""]]}, {"qid": "90ba0501cf53c5b21289", "term": "Euphoria", "description": "mental and emotional condition in which a person experiences intense feelings of well-being, elation, happiness and excitement", "question": "Did Rumi spend his time in a state of euphoria?", "answer": true, "facts": ["Euphoria is a state in which people experience intense feelings that overwhelm their body.", "Rumi was a 13th century Persian poet that was also a dervish.", "Dervishes participated in ceremonies in which they experienced religious ecstasy.", "Religious ecstasy is an altered state of consciousness characterized by visions and emotional (and sometimes physical) euphoria."], "decomposition": ["What religious practices did Rumi engage in?", "What emotional experiences are associated with #1?", "Is euphoria among #2?"], "evidence": [[[["Rumi-1", "Sufi whirling-1"]], [["Sufi whirling-5"]], ["operation"]], [[["Rumi-1"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Rumi-1"]], [["Sufism-75"]], [["Ecstasy (emotion)-8"], "operation"]]], "golden_sentence": [["Jal\u0101l ad-D\u012bn Muhammad R\u016bm\u012b (Persian: \u062c\u0644\u0627\u0644\u200c\u0627\u0644\u062f\u06cc\u0646 \u0645\u062d\u0645\u062f \u0631\u0648\u0645\u06cc\u200e), also known as Jal\u0101l ad-D\u012bn Muhammad Balkh\u012b (\u062c\u0644\u0627\u0644\u200c\u0627\u0644\u062f\u06cc\u0646 \u0645\u062d\u0645\u062f \u0628\u0644\u062e\u0649), Mevl\u00e2n\u00e2/Mawl\u0101n\u0101 (\u0645\u0648\u0644\u0627\u0646\u0627, \"our master\"), Mevlev\u00ee/Mawlaw\u012b (\u0645\u0648\u0644\u0648\u06cc, \"my master\"), and more popularly simply as Rumi (30 September 1207\u00a0\u2013 17 December 1273), was a 13th-century Persian poet, faqih, Islamic scholar, theologian, and Sufi mystic originally from Greater Khorasan.", "It is a customary meditation practice performed within the Sema, or worship ceremony, through which dervishes (also called semazens, from Persian \u0633\u0645\u0627\u0639\u0632\u0646) aim to reach the source of all perfection, or karma."], ["This dhikr is coupled with physical exertions of movement, specifically dancing and whirling, in order to reach a state assumed by outsiders to be one of \"ecstatic trances\"."]]}, {"qid": "df148f3fab3997a86ef2", "term": "Silicon", "description": "Chemical element with atomic number 14", "question": "Is silicon important in California?", "answer": true, "facts": ["There is a region in California called the Silicon Valley.", "Silicon Valley is home to a large number of technology corporations.", "Silicon Valley was originally named after the large number of corporations there that manufactured silicon-based circuit chips."], "decomposition": ["Which industrial area in California is named after silicon?", "What kind of companies are prevalent in #1?", "What kind of products do #2 make?", "Is silicon an important raw material for #3?"], "evidence": [[[["Silicon Valley-1"]], [["Silicon Valley-38"]], [["Silicon Valley-29"]], [["Silicon Valley-2"]]], [[["Silicon Valley-1"]], [["Silicon Valley-2"]], [["Silicon Valley-2"]], [["Integrated circuit-1", "Transistor-48"]]], [[["Silicon Valley-1"]], [["Silicon Valley-2"]], [["Transistor-48"]], [["Semiconductor-1"], "operation"]]], "golden_sentence": [["Silicon Valley is a region in the southern part of the San Francisco Bay Area in Northern California that serves as a global center for high technology, innovation, venture capital, and social media."], [""], [""], [""]]}, {"qid": "579420d3f4a381ba10f6", "term": "Lord Voldemort", "description": "Fictional character of Harry Potter series", "question": "Would Lord Voldemort have been barred from Hogwarts under his own rules?", "answer": true, "facts": ["Lord Voldemort wanted to rid the wizarding world of half blood wizards.", "Lord Volemort was born a half blood, part muggle part wizard."], "decomposition": ["What kinds of people did Lord Voldemort want to prohibit from Hogwarts?", "What was Lord Voldemort born as?", "Is #1 the same as #2?"], "evidence": [[[["Lord Voldemort-4"]], [["Lord Voldemort-4"]], ["operation"]], [[["Lord Voldemort-2"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Lord Voldemort-2"]], [["Lord Voldemort-33"]], ["operation"]]], "golden_sentence": [[""], ["She began to link him to real-life tyrants, describing him as \"a raging psychopath, devoid of the normal human responses to other people's suffering\"."]]}, {"qid": "d61aede0997947263d39", "term": "Central processing unit", "description": "Central component of any computer system which executes input/output, arithmetical, and logical operations", "question": "Would a modern central processing unit circuit chip fit on a housekey?", "answer": false, "facts": ["A CPU circuit chip is about an inch across.", "A housekey is generally less than a half-inch across."], "decomposition": ["What is the size of a CPU Circuit chip across?", "How long is an average house key?", "Is #1 less than #2?"], "evidence": [[[["Central processing unit-20"], "no_evidence"], [["Lock and key-15"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], ["no_evidence"], ["operation"]], [[["Integrated circuit-2"], "no_evidence"], [["Lock and key-7"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "3c2f026c1648085b9dac", "term": "Pancake", "description": "Thin, round cake made of eggs, milk and flour", "question": "Are pancakes a bad snack for cats?", "answer": true, "facts": ["Pancakes contain the dairy product milk as one of the main ingredients.", "After 6 months cats lose the enzyme lactase that breaks down lactose, which makes them lactose intolerant.", "Cats that drink milk can suffer from upset stomach and vomiting."], "decomposition": ["What are the three major ingredients of pancake?", "Which substance do cats lose the ability to break down after six months?", "Does any of #1 contain #2?"], "evidence": [[[["Pancake-1"]], [["Cat-59"]], ["operation"]], [[["Pancake-64"]], [["Cat-59"]], [["Cat-59"], "operation"]], [[["Pancake-1"]], [["Cat-59"]], [["Cat-59"], "operation"]]], "golden_sentence": [["A pancake (or hotcake, griddlecake, or flapjack, not to be confused with oat bar flapjacks) is a flat cake, often thin and round, prepared from a starch-based batter that may contain eggs, milk and butter and cooked on a hot surface such as a griddle or frying pan, often frying with oil or butter."], ["Most adult cats are lactose intolerant; the sugar in milk is not easily digested and may cause soft stools or diarrhea."]]}, {"qid": "536d9b05510fb932a27f", "term": "Lunch", "description": "meal, usually served at midday", "question": "Are all students guaranteed lunch at school in the US?", "answer": false, "facts": ["Schools across the US have been struggling with school lunch debts.", "News articles have been published about students being turned away from the cafeteria due to outstanding debts for lunches."], "decomposition": ["How effectively have schools across the US been managing lunch debts?", "According to news articles, how are students with lunch debts treated at the cafeteria?", "Does 'excellently' describe #1 and #2?"], "evidence": [[[["National School Lunch Act-35"], "no_evidence"], [["Poverty-56"], "no_evidence"], ["operation"]], [[["School meal programs in the United States-36"], "no_evidence"], ["no_evidence"], ["operation"]], [[["School meal-72"], "no_evidence"], [["Shooting of Philando Castile-48"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "39287f73cb6a1a9d328b", "term": "Methane", "description": "Simplest organic molecule with one carbon atom and four hydrogen", "question": "Can methane be seen by the naked eye?", "answer": false, "facts": ["Methane is a gas.", "Methane is colorless.", "Methane is odorless."], "decomposition": ["What is the color of methane?", "Can #1 gases be seen by the naked eye?"], "evidence": [[[["Methane-4"]], ["operation"]], [[["Methane-4"]], ["operation"]], [[["Methane-4"]], ["operation"]]], "golden_sentence": [["At room temperature and standard pressure, methane is a colorless, odorless gas."]]}, {"qid": "1ac04892a2ac06073394", "term": "Alcatraz Island", "description": "United States historic place", "question": "Could an escapee swim nonstop from Alcatraz island to Siberia?", "answer": false, "facts": ["Alcatraz Island was a San Francisco Bay prison.", "Siberia is over 5,217 miles away from San Francisco.", "The longest continuous swim record was 139 miles."], "decomposition": ["How far is Alcatraz from Siberia?", "How far is the record longest swim?", "Is #2 equal to or greater than #1?"], "evidence": [[[["Alcatraz Island-1", "Siberia-1"], "no_evidence"], [["Veljko Rogo\u0161i\u0107-1"]], ["operation"]], [[["Alcatraz Island-1", "Pacific Ocean-1", "Siberia-37"], "no_evidence"], [["Long-distance swimming-1"]], ["operation"]], [["no_evidence"], [["Veljko Rogo\u0161i\u0107-1"]], ["operation"]]], "golden_sentence": [["", ""], ["He swum a total of 225 km (139.8 miles) across the Adriatic sea in a time of 50 hours and 10 minutes."]]}, {"qid": "93e3248045b7068073a6", "term": "Ice", "description": "water frozen into the solid state", "question": "Is there a popular Disney character made from living ice?", "answer": true, "facts": ["Olaf is a popular character in Disney's Frozen series.", "Olaf is a snowman, accidentally enchanted to life by Elsa while she magically builds her ice tower."], "decomposition": ["Which popular Disney character did Elsa accidentally enchant to life while building her ice tower?", "Was #1 made of snow/ice?"], "evidence": [[[["Frozen (2013 film)-7"]], [["Snowman-4"], "operation"]], [[["Olaf (Frozen)-1"]], [["Snowman-1"]]], [[["Olaf (Frozen)-5"]], [["Olaf (Frozen)-1"], "operation"]]], "golden_sentence": [["On foot, they meet Olaf, a cheerful snowman brought to life unknowingly by Elsa, who offers to lead them to her."], ["These other types range from snow columns to elaborate snow sculptures similar to ice sculptures."]]}, {"qid": "9485398c24d6a74e566a", "term": "Onion", "description": "vegetable", "question": "Would a blooming onion be possible with a shallot?", "answer": false, "facts": ["A blooming onion is a dish for sharing, featuring a sliced and deep fried onion made to resemble petals.", "A shallot is very small and would only make a few \"petals\""], "decomposition": ["What characteristics of onions are important when making blooming onion dish?", "Do the characteristics of shallot match with all of #1?"], "evidence": [[[["Blooming onion-1"]], [["Shallot-1"], "no_evidence", "operation"]], [[["Blooming onion-1"], "no_evidence"], [["Shallot-7"], "no_evidence", "operation"]], [[["Blooming onion-1"]], [["Shallot-14"], "no_evidence", "operation"]]], "golden_sentence": [["A blooming onion, onion bloom, onion blossom, onion flower, bloomin' onion, or onion mum is a dish consisting of one large onion which is cut to resemble a flower, battered, and deep-fried."], [""]]}, {"qid": "f77d772145ffcd3393de", "term": "Friday", "description": "day of the week", "question": "Did goddess Friday is named after despise felines?", "answer": false, "facts": ["Felines are a species of animals that include lions, tigers, and domestic cats.", "Friday is named after the Norse goddess Freya. ", "Freya is often depicted in art with cats.", "Freya had two cats that pulled her magical chariot."], "decomposition": ["Which goddess is Friday named after?", "Which animals pulled #1's chariots?", "Are felines excluded from #2?"], "evidence": [[[["Friday-3"]], [["Frigg-27"]], ["operation"]], [[["Friday-3"]], [["Venus (mythology)-1"]], [["Venus (mythology)-1"]]], [[["Friday-3"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["The name Friday comes from the Old English Fr\u012b\u0121ed\u00e6\u0121, meaning the \"day of Frige\", a result of an old convention associating the Germanic goddess Frigg with the Roman goddess Venus, with whom the day is associated in many different cultures."], [""]]}, {"qid": "f83abc53241fb9f7c9fd", "term": "Gunpowder", "description": "explosive most commonly used as propellant in firearms", "question": "Did the Gunpowder plot eliminate Mary, Queen of Scots bloodline?", "answer": false, "facts": ["Mary, Queen of Scots was the queen of Scotland whose rivalry with Elizabeth I led to her beheading.", "Mary, Queen of Scots son became King of England as James I.", "The Gunpowder plot was a 1605 plot to blow up Parliament and King James I.", "The Gunpowder plot failed and the conspirators were executed.", "King James I was succeeded by his son, Charles I of England."], "decomposition": ["In what year did the Gunpowder Plot happen?", "Of the descendants of Mary, Queen of Scots, what is the birth year of the person who died most recently?", "Is #1 within the range of #2?"], "evidence": [[[["Gunpowder Plot-1"]], [["Arthur Stuart, 7th Earl Castle Stewart-1", "House of Stuart-1", "House of Stuart-4"]], ["operation"]], [[["Gunpowder Plot-1"]], [["James VI and I-1", "James VI and I-2"], "no_evidence"], ["operation"]], [[["Gunpowder Plot-2"]], [["Mary, Queen of Scots-2"], "no_evidence"], ["operation"]]], "golden_sentence": [["The Gunpowder Plot of 1605, in earlier centuries often called the Gunpowder Treason Plot or the Jesuit Treason, was a failed assassination attempt against King James I by a group of provincial English Catholics led by Robert Catesby."], ["", "The first monarch of the Stewart line was Robert II whose descendants were kings and queens of Scotland from 1371 until the union with England in 1707.", ""]]}, {"qid": "53ecb0326e9aeaf52844", "term": "Onion", "description": "vegetable", "question": "Does Sockington enjoy onions?", "answer": false, "facts": ["Sockington is a domestic cat", "Onions can cause toxicity in cats by breaking down their red blood cells"], "decomposition": ["What kind of creature is Sockington?", "Are onions harmless to #1?"], "evidence": [[[["Sockington-1"]], [["Hemolytic anemia-23"], "operation"]], [[["Sockington-1"]], [["Onion-25"], "operation"]], [[["Sockington-1"]], [["Onion-25"]]]], "golden_sentence": [["Sockington (also known as \"Sockamillion\" or \"Socks\") is a domestic cat who lives in Waltham, Massachusetts."], ["Garlic is less toxic to dogs than onion."]]}, {"qid": "d0c51880f66827e2fd66", "term": "Monarch", "description": "Person at the head of a monarchy", "question": "Would Hapshetsut be considered a monarch?", "answer": true, "facts": ["A monarch is the head of a monarchy and was appointed for life.", "Hapshetsut was the second known female pharaoh.", "Pharaohs ruled for life and their wealth was even buried with them to take into the afterlife."], "decomposition": ["What kind of leader was Hatshepsut?", "What was the nature of the leadership of a #1?", "What is the nature of a monarch's rule?", "Is #2 very similar to #3?"], "evidence": [[[["Hatshepsut-1"]], [["Pharaoh-1"]], [["Monarch-1"]], [["Monarch-1", "Pharaoh-1"]]], [[["Hatshepsut-1"]], [["Pharaoh-1", "Pharaoh-2"]], [["Monarch-1"]], ["operation"]], [[["Hatshepsut-1"]], [["Pharaoh-5"]], [["Monarch-1"]], ["operation"]]], "golden_sentence": [["Hatshepsut (/h\u00e6t\u02c8\u0283\u025bps\u028at/; also Hatchepsut; Egyptian: \u1e25\ua723t-\u0161ps.wt \"Foremost of Noble Ladies\"; 1507\u20131458\u00a0BC) was the fifth pharaoh of the Eighteenth Dynasty of Egypt."], [""], ["A monarch usually reigns for life or until abdication."], ["", ""]]}, {"qid": "e02f95b4aca44ba9217e", "term": "Rand Paul", "description": "American politician, ophthalmologist, and United States Senator from Kentucky", "question": "Can a New Yorker get their eyes checked by Rand Paul legally?", "answer": false, "facts": ["Rand Paul is a senator from Kentucky.", "Rand Paul was an ophthalmologist in Kentucky with ABO certification.", "The National Board of Ophthalmology does not recognize ABO certification.", "Kentucky does not require ophthalmologists to be certified.", "NY ophthalmologists must have approved application for licensure certifications."], "decomposition": ["What certifications does NY require of ophthalmologists?", "Does Rand Paul have #1?"], "evidence": [[[["Ophthalmology-42"], "no_evidence"], [["Rand Paul-12", "Rand Paul-13"], "operation"]], [[["Ophthalmology-1"]], [["Rand Paul-10"]]], [[["Ophthalmology-41"]], ["operation"]]], "golden_sentence": [["Physicians must complete the requirements of continuing medical education to maintain licensure and for recertification."], ["", ""]]}, {"qid": "f3374d7f4ed4affb7ec9", "term": "Hulk", "description": "Superhero appearing in Marvel Comics publications and related media", "question": "Hypothetically, will an African elephant be crushed by Hulk on its back?", "answer": false, "facts": ["The Hulk is a Marvel comics character.", "The Hulk is said to be around 8 feet tall and weigh around 1400 pounds.", "An African elephant can carry up to 9,000 kg, or 19,841 pounds."], "decomposition": ["How big is the Hulk?", "How much can an African elephant carry?", "Is #1 greater than #2?"], "evidence": [[[["Hulk-1"], "no_evidence"], [["African elephant-16"], "no_evidence"], ["no_evidence", "operation"]], [[["Hulk-44"]], [["African elephant-18"], "no_evidence"], ["operation"]], [[["Hulk-44"]], [["African elephant-16"]], ["operation"]]], "golden_sentence": [["In his comic book appearances, the character is both the Hulk, a green-skinned, hulking and muscular humanoid possessing a vast degree of physical strength, and his alter ego Dr. Robert Bruce Banner, a physically weak, socially withdrawn, and emotionally reserved physicist, the two existing as independent personalities and resenting of the other."], ["Cows are 2.2\u20132.6\u00a0m (7.2\u20138.5\u00a0ft) tall at the shoulder and weigh 2,160\u20133,232\u00a0kg (4,762\u20137,125\u00a0lb), while bulls are 3.2\u20134\u00a0m (10\u201313\u00a0ft) tall and weigh 4,700\u20136,048\u00a0kg (10,362\u201313,334\u00a0lb)."]]}, {"qid": "50ff44a79275484ba2f0", "term": "Suicide", "description": "Intentional act of causing one's own death", "question": "Would Modafinil be effective in completing a suicide?", "answer": false, "facts": ["Modafinil is a powerful wakefulness drug, typically prescribed at 100mg or 200mg per day doses.", "Suicide attempts with up to 5,000mg of Modafinil have been unsuccessful. "], "decomposition": ["What is Modafinil?", "What are the effects of taking too much #1?", "Would someone who wanted to commit suicide want to #2?"], "evidence": [[[["Modafinil-1"]], [["Modafinil-12"]], ["operation"]], [[["Modafinil-1"]], [["Modafinil-19"]], ["operation"]], [[["Modafinil-1"]], [["Modafinil-19"]], ["no_evidence", "operation"]]], "golden_sentence": [["Modafinil, sold under the brand name Provigil among others, is a medication to treat sleepiness due to narcolepsy, shift work sleep disorder, or obstructive sleep apnea."], [""]]}, {"qid": "871df24f31f49000d78b", "term": "Ten Commandments", "description": "Set of biblical principles relating to ethics and worship, which play a fundamental role in the Abrahamic religions", "question": "Were the Ten commandments the part of the bible that Jewish people do not believe in?", "answer": false, "facts": ["The Jewish religion regards the Old Testament as their holy book.", "The New Testament of the bible is not acknowledged by Jewish religious people.", "The Ten Commandments are in the Old Testamanet."], "decomposition": ["What parts of the Bible do Jews not accept?", "What part of the Bible are the Ten Commandments in?", "Is #2 also listed in #1?"], "evidence": [[[["Old Testament-32"]], [["Ten Commandments-50"]], ["operation"]], [[["Christianity and Judaism-13"]], [["Ten Commandments-1"]], ["operation"]], [[["Old Testament-1"]], [["Book of Exodus-12"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "eac2440d3f3b9baa09e1", "term": "Dr. Seuss", "description": "American children's writer and illustrator", "question": "Did the death of Helen Palmer have a significant effect on Dr. Seuss?", "answer": true, "facts": ["Dr. Seuss's real name was Theodor Geisel.", "Theodor Geisel was married to Helen Palmer at the time of her suicide.", "Theodor Geisel is quoted having said he considered suicide after the death of his wife."], "decomposition": ["What relatives did Helen Palmer have when she died?", "What is Dr. Suess's real name?", "Is #2 one of #1?"], "evidence": [[[["Helen Palmer (author)-1"]], [["Dr. Seuss-1"]], ["operation"]], [[["Helen Palmer (author)-1", "Helen Palmer (author)-8"], "no_evidence"], [["Dr. Seuss-1"]], ["operation"]], [[["Helen Palmer (author)-1", "Helen Palmer (author)-8"]], [["Helen Palmer (author)-1"]], ["operation"]]], "golden_sentence": [[""], ["He is known for his work writing and illustrating more than 60 books under the pen name Dr. Seuss (/su\u02d0s, zu\u02d0s/,)."]]}, {"qid": "674f841a0971417c813f", "term": "Federal Reserve", "description": "Central banking system of the United States", "question": "Is the Federal Reserve a quick walk from Space Needle?", "answer": false, "facts": ["The Federal Reserve building is headquartered in Washington, D.C.", "The Space Needle is located in Seattle, Washington.", "There are over 2700 miles from Seattle, Washington to Washington, D.C."], "decomposition": ["Where is the Space Needle located?", "Where is the headquarters of Federal Reserve located?", "Can the distance between #1 and #2 be covered quickly by walking?"], "evidence": [[[["Space Needle-1"]], [["Marriner S. Eccles-8"]], ["operation"]], [[["Space Needle-1"]], [["Eccles Building-4"]], ["no_evidence"]], [[["Space Needle-1"]], [["Eccles Building-1"]], ["operation"]]], "golden_sentence": [["The Space Needle is an observation tower in Seattle, Washington, United States."], ["citation needed] The Eccles Building that houses the headquarters of the Federal Reserve in Washington, D.C. was named after Eccles in 1982."]]}, {"qid": "2f4ace380f8caaf66206", "term": "Eric Clapton", "description": "English musician, singer, songwriter, and guitarist", "question": "Would Eric Clapton's mother hypothetically be unable to legally purchase cigarettes in the USA at his birth?", "answer": true, "facts": ["Eric Clapton's mother was 16 years old at the time of his birth.", "As of 2020, federal law required states comply with a minimum age of 21 years for sale/purchase of tobacco products."], "decomposition": ["How old was Eric Clapton's mom when he was born?", "How old must you be to legally buy cigarettes in the USA?", "Is #2 greater than #1?"], "evidence": [[[["Eric Clapton-4"]], [["Cigarette-19"]], [["Cigarette-19", "Eric Clapton-4"], "operation"]], [[["Eric Clapton-4"]], [["Tobacco 21-16"]], ["operation"]], [[["Eric Clapton-4"]], [["Cigarette-22"]], ["operation"]]], "golden_sentence": [["Clapton was born on 30 March 1945 in Ripley, Surrey, England, to 16-year-old Patricia Molly Clapton (7 January 1929 \u2013 March 1999) and Edward Walter Fryer (21 March 1920 \u2013 15 May 1985), a 25-year-old soldier from Montreal, Quebec."], ["In the United States, the age to buy tobacco products is 21 in all states as of 2020."], ["", ""]]}, {"qid": "680d37294e2ec9f9d943", "term": "Casio", "description": "Japanese electronics company", "question": "Could Casio's first invention be worn around the ankle?", "answer": false, "facts": ["Casio's first invention was the yubiwa pipe.", "The yubiwa pipe was a ring worn that held a cigarette in place worn on the finger.", "Ankles are several inches thicker than fingers."], "decomposition": ["What was Casio's first invention?", "Where was #1 worn?", "What is the largest diameter of #2?", "What is the smallest diameter of ankle?", "Is #4 less than or equal to #3?"], "evidence": [[[["Casio-2"]], [["Casio-2"]], [["Ring (jewellery)-17"], "no_evidence"], [["Ankle-1"], "no_evidence"], ["operation"]], [[["Casio-2"]], [["Casio-2"]], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Casio-2"]], [["Casio-2"]], [["Hand-3"], "no_evidence"], [["Ankle-4"], "no_evidence"], ["operation"]]], "golden_sentence": [["Kashio's first major product was the yubiwa pipe, a finger ring that would hold a cigarette, allowing the wearer to smoke the cigarette down to its nub while also leaving the wearer's hands-free."], ["Kashio's first major product was the yubiwa pipe, a finger ring that would hold a cigarette, allowing the wearer to smoke the cigarette down to its nub while also leaving the wearer's hands-free."], [""], [""]]}, {"qid": "78b22527c270564e2a9d", "term": "Cousin", "description": "any descendant of an ancestor's sibling", "question": "Does Zelda Williams have any cousins on her father's side?", "answer": false, "facts": ["Robin Williams was the father of Zelda Williams. ", "Robin Williams was an only child.", "A cousin is the child of a parent's siblings. ", "Only children do not have siblings."], "decomposition": ["Who was Zelda Williams's father?", "Does #1 have any siblings?"], "evidence": [[[["Zelda Williams-1"]], [["Robin Williams-4"], "no_evidence", "operation"]], [[["Zelda Williams-2"]], [["Robin Williams-4"]]], [[["Marsha Garces Williams-1"]], [["Robin Williams-39"]]]], "golden_sentence": [["She is the daughter of late actor and comedian Robin Williams and film producer Marsha Garces Williams."], ["Williams had two elder half-brothers: paternal half-brother Robert (also known as Todd) and maternal half-brother McLaurin."]]}, {"qid": "5d56b9c9784b039864ca", "term": "Toronto Star", "description": "Newspaper in Toronto, Ontario, Canada", "question": "Can someone sell their time through the Toronto Star?", "answer": true, "facts": ["The Toronto Star has a classifieds section", "Readers can advertise their own labor or services and thus their time "], "decomposition": ["What section of the Toronto Star lists things for sale?", "Can someone's services or labor be sold in #1?"], "evidence": [[[["Toronto Star-31"]], [["Classified advertising-1"]]], [[["Toronto Star-31"]], [["Classified advertising-1"], "operation"]], [[["Toronto Star-31"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "e4bc742bd89506f3356b", "term": "Snickers", "description": "brand name chocolate bar made by Mars, Incorporated", "question": "Is it wise to feed a Snickers bar to a poodle?", "answer": false, "facts": ["A Snickers bar contains chocolate.", "Chocolate is harmful or even toxic to dogs.", "Poodles are a breed of dog."], "decomposition": ["What are poodles a breed of?", "What substances are harmful to #1?", "What is a Snickers make out of?", "Is there no overlap between #2 and #3?"], "evidence": [[[["Poodle-1"], "no_evidence"], [["Dog food-60"], "no_evidence"], [["Snickers-1"], "no_evidence"], ["operation"]], [[["Poodle-1"]], [["Dog-19"]], [["Snickers-1"]], ["operation"]], [[["Poodle-18"], "operation"], ["no_evidence"], [["Snickers-3"], "operation"], ["no_evidence"]]], "golden_sentence": [["The Poodle is a formal dog breed that comes in three varieties: Standard Poodle, Miniature Poodle, and Toy Poodle."], ["A number of common human foods and household ingestibles are toxic to dogs, including chocolate solids (theobromine poisoning), onion and garlic (thiosulfate, sulfoxide or disulfide poisoning), grapes and raisins (cause kidney failure in dogs), milk (some dogs are lactose intolerant and suffer diarrhea; goats' milk can be beneficial), nutmeg (neurotoxic to dogs), mushrooms, fatty foods, rhubarb, xylitol, macadamia nuts, as well as various plants and other potentially ingested materials."], ["Snickers is a brand name chocolate bar made by the American company Mars, Incorporated, consisting of nougat topped with caramel and peanuts that has been enrobed in milk chocolate."]]}, {"qid": "3d9427efd53476127c68", "term": "United Airlines", "description": "Airline in the United States", "question": "Does United Airlines have a perfect operation record?", "answer": false, "facts": ["An airline with a perfect operation record has no crashes or other damaging incidents.", "United Airlines has had over 30 crash incidents over several decades."], "decomposition": ["What must an airline avoid if they want a perfect operation record?", "Is United Airlines free of #1?"], "evidence": [[[["Freedoms of the air-3"], "no_evidence"], [["United Airlines-62"], "no_evidence"]], [["no_evidence"], [["United Airlines-62"], "no_evidence", "operation"]], [[["Incident management (ITSM)-1"], "no_evidence"], [["United Airlines-62"], "operation"]]], "golden_sentence": [["Even when such services are allowed by countries, airlines may still face restrictions to accessing them by the terms of treaties or for other reasons."], [""]]}, {"qid": "ffb99a17067ce629f7c6", "term": "Sea otter", "description": "A species of marine mammal from the northern and eastern coasts of the North Pacific Ocean", "question": "Does Long John Silver's serve sea otter?", "answer": false, "facts": ["Sea Otters are endangered marine mammals.", "Long John Silver's is an american fast food chain that serves seafood.", "Long John Silver's does not serve meat from mammals."], "decomposition": ["What classes of animals does Long John Silver's serve food from?", "To which class of animals does the sea otter belong?", "Is #2 listed in #1?"], "evidence": [[[["Long John Silver's-1"]], [["Sea otter-10"]], ["operation"]], [[["Long John Silver's-4"]], [["Sea otter-10"]], [["Sea otter-10"]]], [[["Long John Silver's-1"]], [["Sea otter-1"]], [["Seafood-1"], "operation"]]], "golden_sentence": [[""], ["Unlike most other marine mammals, the sea otter has no blubber and relies on its exceptionally thick fur to keep warm."]]}, {"qid": "ffa272e19803c85cb4bf", "term": "Parachuting", "description": "action sport of exiting an aircraft and returning to Earth using a parachute", "question": "Would Matt Damon be afraid of parachuting?", "answer": true, "facts": ["Parachuting involves jumping from high places or airplanes.", "Matt Damon is afraid of heights. "], "decomposition": ["What is Matt Damon afraid of?", "Does parachuting involve #1?"], "evidence": [[[["Matt Damon-1"], "no_evidence"], [["Parachuting-1"], "no_evidence", "operation"]], [["no_evidence"], ["no_evidence"]], [["no_evidence"], ["no_evidence"]]], "golden_sentence": [[""], ["It may involve more or less free-falling (the skydiving segment) which is a period when the parachute has not yet been deployed and the body gradually accelerates to terminal velocity."]]}, {"qid": "655a994ac5184c6fe707", "term": "Marlboro (cigarette)", "description": "cigarette brand", "question": "Are the colors on Marlboro package found on French flag?", "answer": false, "facts": ["The colors of the Marlboro package are red, white, and black.", "The French flag has the colors red, white, and blue."], "decomposition": ["What are the colors of a Marlboro package?", "What are the colors of the French flag?", "Is #1 identical to #2?"], "evidence": [[[["Marlboro (cigarette)-12", "Marlboro (cigarette)-15", "Marlboro (cigarette)-17"]], [["Flag of France-1"]], ["operation"]], [[["Marlboro (cigarette)-17"]], [["Flag of France-1"]], ["operation"]], [[["Marlboro (cigarette)-17"]], [["Flag of France-1"]], ["operation"]]], "golden_sentence": [["", "In 2015, Philip Morris announced they would introduce a \"Firm Filter\" to their Marlboro Red, Gold, Silver Blue, Ice Blast and White Menthol variants.", "The red and white package was designed by the designer Frank Gianninoto."], ["The flag of France (French: drapeau fran\u00e7ais) is a tricolour flag featuring three vertical bands coloured blue (hoist side), white, and red."]]}, {"qid": "b48a84b57d888b5d573a", "term": "Richard Wagner", "description": "German composer", "question": "Did Richard Wagner compose the theme songs for two television series?", "answer": false, "facts": ["Richard Wagner died in 1883.", "Televisions started to be developed in the 1920s."], "decomposition": ["When did the television first become available?", "When did Richard Wagner die?", "Is #1 before #2?"], "evidence": [[[["Television-2"]], [["Richard Wagner-1"]], ["operation"]], [[["Television-2"]], [["Richard Wagner-45"]], ["operation"]], [[["History of television-141"]], [["Ca' Vendramin Calergi-9"]], ["operation"]]], "golden_sentence": [["Television became available in crude experimental forms in the late 1920s, but it would still be several years before the new technology would be marketed to consumers."], ["Wilhelm Richard Wagner (/\u02c8v\u0251\u02d0\u0261n\u0259r/ VAHG-n\u0259r, German: [\u02c8\u0281\u026a\u00e7a\u0281t \u02c8va\u02d0\u0261n\u0250] (listen); 22 May 1813\u00a0\u2013 13 February 1883) was a German composer, theatre director, polemicist, and conductor who is chiefly known for his operas (or, as some of his mature works were later known, \"music dramas\")."]]}, {"qid": "ddf0f4d62062dbdd914b", "term": "B\u00f6rek", "description": "Stuffed phyllo pastry", "question": "Would Recep Tayyip Erdo\u011fan be unfamiliar with b\u00f6rek?", "answer": false, "facts": ["Turkey enjoys a wide variety of regional variations of b\u00f6rek among the different cultures and ethnicities composing it.", "B\u00f6rek is very popular in the cuisines of the former Ottoman Empire, especially in North Africa and throughout the Balkans.", "Recep Tayyip Erdo\u011fan is the current president of Turkey and he was born and raised there."], "decomposition": ["Where was Recep Tayyip Erdo\u011fan born?", "In which regions is b\u00f6rek part of the normal cuisine?", "Is #1 not part of #2?"], "evidence": [[[["Istanbul-1", "Recep Tayyip Erdo\u011fan-7"]], [["B\u00f6rek-1"]], ["operation"]], [[["Recep Tayyip Erdo\u011fan-7"]], [["B\u00f6rek-9"]], ["operation"]], [[["Istanbul-1", "Recep Tayyip Erdo\u011fan-7"]], [["B\u00f6rek-1"]], [["Turkey-1"], "operation"]]], "golden_sentence": [["Istanbul (/\u02cc\u026ast\u00e6n\u02c8b\u028al/, also US: /\u02c8\u026ast\u00e6nb\u028al/; Turkish: \u0130stanbul [is\u02c8tanbu\u026b] (listen)), formerly known as Byzantium and Constantinople, is the most populous city in Turkey and the country's economic, cultural and historic center.", "Erdo\u011fan was born in the Kas\u0131mpa\u015fa, a poor neighborhood of Istanbul, to which his family had moved from Rize Province in the 1930s."], ["B\u00f6rek (Turkish pronunciation:\u00a0[b\u0153\u02c8\u027eec]; also burek and other variants) is a family of baked filled pastries made of a thin flaky dough such as phyllo or yufka, of Turkish origins and also found in the cuisines of the Balkans, the South Caucasus, Levant, Mediterranean, and other countries in Eastern Europe and Western Asia."]]}, {"qid": "4a8f9bc8cf92c63fe5db", "term": "Naruhito", "description": "Emperor of Japan", "question": "Are Naruhito's ancestors the focus of Romance of the Three Kingdoms?", "answer": false, "facts": ["Naruhito is the Emperor of Japan.", "Romance of the Three Kingdoms was a 14th century historical novel about the Three Kingdoms Period.", "The Three Kingdoms Period was the division of China among the states of Wei, Shu, and Wu."], "decomposition": ["Where are the ancestors of Naruhito from?", "What country is the novel Romance of the Three Kingdoms set in?", "Are #1 and #2 the same?"], "evidence": [[[["Naruhito-3"]], [["Romance of the Three Kingdoms-1"]], ["operation"]], [[["Naruhito-1"]], [["Romance of the Three Kingdoms-1"]], ["operation"]], [[["Naruhito-3"], "no_evidence"], [["Romance of the Three Kingdoms-1"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "7589d5c75bf43bd78534", "term": "Felicity Huffman", "description": "American actress", "question": "Would Felicity Huffman vote for Mike DeWine?", "answer": false, "facts": ["Mike DeWine is Governor of Ohio", "Felicity Huffman is a resident of California"], "decomposition": ["What elected office is held by Mike DeWine?", "What state is Mike DeWine #1 of?", "What state does Felicity Huffman live in?", "Are #2 and #3 the same state?"], "evidence": [[[["Mike DeWine-24"]], [["Mike DeWine-24"]], [["Felicity Huffman-24"]], [["Felicity Huffman-24"]]], [[["Mike DeWine-1"]], [["Mike DeWine-1"]], [["Felicity Huffman-24"]], ["operation"]], [[["Mike DeWine-1"]], [["Mike DeWine-1"]], [["Felicity Huffman-24"]], ["operation"]]], "golden_sentence": [["On February 22, 2019, President Trump appointed Governor DeWine to the bipartisan Council of Governors."], [""], ["Huffman was arrested at her California home on March 12 and charged with conspiracy to commit mail fraud and honest services fraud."], [""]]}, {"qid": "ee97eb87b656190755b0", "term": "The Colbert Report", "description": "US satirical news commentary TV program", "question": "Would the host of The Colbert Report be likely to vote for Trump?", "answer": false, "facts": ["The host of the Colbert report is Stephen Colbert.", "Stephen Colbert has spoken against Donald Trump multiple times on various platforms."], "decomposition": ["Who is the host of The Colbert Report show?", "Has #1 said more positive things than negative things about Trump?"], "evidence": [[[["The Colbert Report-1"], "no_evidence"], [["Stephen Colbert-30"], "no_evidence"]], [[["The Colbert Report-1"]], [["Stephen Colbert-56"], "no_evidence"]], [[["The Colbert Report-17"]], [["Stephen Colbert-56"]]]], "golden_sentence": [["The Colbert Report (/ko\u028al\u02c8b\u025b\u0259r r\u026a\u02ccp\u0254\u02d0r/ kohl-BAIR rih-por) is an American late-night talk and news satire television program hosted by Stephen Colbert that aired four days a week on Comedy Central from October 17, 2005, to December 18, 2014, for 1,447 episodes."], [""]]}, {"qid": "9889be1602496d2c14a5", "term": "Panic of 1907", "description": "three-week financial crisis in the United States", "question": "Was the father of social security system serving in the white house during the Panic of 1907?", "answer": false, "facts": ["The father of social security system is Franklin D. Roosevelt. ", "Franklin D. Roosevelt was in Columbia Law School in 1907. "], "decomposition": ["Who is the father of the social security system?", "What position serves in the White House?", "When did #1 serve as #2?", "Is 1907 in the range of #3?"], "evidence": [[[["Franklin D. Roosevelt-3"]], [["White House-1"]], [["Franklin D. Roosevelt-1"]], ["operation"]], [[["Edwin E. Witte-1"]], [["Edwin E. Witte-12"]], [["Edwin E. Witte-12"], "no_evidence"], ["operation"]], [[["Social Security (United States)-1"]], [["White House-1"]], [["Franklin D. Roosevelt-1"]], ["operation"]]], "golden_sentence": [["Major surviving programs and legislation implemented under Roosevelt include the Securities and Exchange Commission, the National Labor Relations Act, the Federal Deposit Insurance Corporation, Social Security, and the Fair Labor Standards Act of 1938."], [""], [""]]}, {"qid": "da0705ae5cf9f0d91899", "term": "Cancer", "description": "group of diseases", "question": "Do all cancer patients get disability?", "answer": false, "facts": ["All forms of cancer qualify as diagnoses that can result in disability.", "Disability is not determined by diagnosis, but by degree of impairment.", "Some cancer patients do not experience major impairment."], "decomposition": ["What is disability determined by?", "Do all patients of cancer have the same degree of #1?"], "evidence": [[[["Disability-1"]], [["Disability-4"]]], [[["Disability-2"], "no_evidence"], [["Cancer-99"], "no_evidence", "operation"]], [[["Disability Determination Services-18"]], [["Cancer rehabilitation-3"], "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "6a567739d55ba24cc9b3", "term": "Harry Potter and the Philosopher's Stone", "description": "1997 fantasy novel by J. K. Rowling", "question": "Did children read Harry Potter and the Philosopher's Stone during the Albanian Civil War?", "answer": true, "facts": ["Harry Potter and the Philosopher's Stone was a 1997 children's fantasy book.", "The Albanian Civil War, also called the Albanian Civil Unrest, happened in 1997."], "decomposition": ["What year was Harry Potter and the Philosopher's Stone published?", "What year was the Albanian Civil War?", "Did #1 not after #2?"], "evidence": [[[["Harry Potter and the Philosopher's Stone-35"]], [["Albanian Civil War-6"], "no_evidence"], [["Albanian Civil War-6", "Harry Potter and the Philosopher's Stone-35"], "operation"]], [[["Harry Potter and the Philosopher's Stone-2"]], [["Albanian Civil War-1"]], ["operation"]], [[["Harry Potter and the Philosopher's Stone-2"]], [["Albanian Civil War-1"]], ["operation"]]], "golden_sentence": [["It was published worldwide in English on June 21, 2003."], [""], ["", ""]]}, {"qid": "3702a090d336fd0a34e9", "term": "Arnold Schwarzenegger", "description": "Austrian-American actor, businessman, bodybuilder and politician", "question": "Would Arnold Schwarzenegger have a hard time picking up a red fox in 1967?", "answer": false, "facts": ["In 1967, Schwarzenegger won the Munich stone-lifting contest, in which a stone weighing 508 German pounds (254 kg / 560 lb) is lifted between the legs while standing on two footrests.", "Red foxes weigh between 2.2\u201314 kg (5\u201331 lb)."], "decomposition": ["How much could Arnold Schwarzenegger life in 1967?", "What is the typical weight of a Red Fox?", "Is #2 more than #1?"], "evidence": [[[["Arnold Schwarzenegger-23"]], [["Red fox-18"]], ["operation"]], [[["Arnold Schwarzenegger-23"]], [["Red fox-18"]], ["operation"]], [[["Arnold Schwarzenegger-23"]], [["Vulpes-1"]], ["operation"]]], "golden_sentence": [[""], ["The largest red fox on record in Great Britain was a 17.2\u00a0kg (38\u00a0lb), 1.4-metre (4\u00a0ft 7\u00a0in) long male, killed in Aberdeenshire, Scotland, in early 2012."]]}, {"qid": "906a8ced8ca185607438", "term": "Six-Day War", "description": "1967 war between Israel and Egypt, Jordan, and Syria", "question": "Could an NBA game be completed within the span of the Six-Day War?", "answer": true, "facts": ["The Six-Day War took place between June 5th-June 10th, 1967.", "There are 24 hours in a day.", "An NBA game consists of four quarters that are 12 minutes long and a 15 minute long halftime.", "There are 60 minutes in an hour."], "decomposition": ["How long did the Six-day War last?", "How long does a basketball game last?", "Is #1 longer than #2?"], "evidence": [[[["Six-Day War-8"]], [["Rules of basketball-3"], "no_evidence"], ["operation"]], [[["Six-Day War-1"]], [["Basketball-32"]], ["operation"]], [[["Six-Day War-1"]], [["Basketball-32"]], ["operation"]]], "golden_sentence": [[""], ["The time shall be two fifteen-minute halves, with five minutes rest between."]]}, {"qid": "b035f958cbd5ba1a3a27", "term": "Mount Emei", "description": "mountain", "question": "Would it be difficult for Kami Rita to climb Mount Emei?", "answer": false, "facts": ["Kami Rita has climbed Mount Everest 24 times.", "Mount Everest has an elevation of 8,848 m (29,029 ft).", "Mount Emei has an elevation of 3,099 metres (10,167 ft)."], "decomposition": ["What is the highest mountain Kami Rita has climbed?", "What is the height of #1?", "What is the height of Mount Emei?", "Is #3 greater or equal to #2?"], "evidence": [[[["Kami Rita-1"]], [["Mount Everest-2"]], [["Mount Emei-2"]], ["operation"]], [[["Kami Rita-1"]], [["Mount Everest-2"]], [["Mount Emei-2"]], ["operation"]], [[["Kami Rita-1"]], [["Mount Everest-2"], "no_evidence"], [["Mount Emei-2"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Kami Rita (born 1970) is a Nepali Sherpa guide who, since May 2018, has held the record for most ascents to the summit of Mount Everest."], ["The current official elevation of 8,848\u00a0m (29,029\u00a0ft), recognised by China and Nepal, was established by a 1955 Indian survey and subsequently confirmed by a Chinese survey in 1975."], ["At 3,099 metres (10,167\u00a0ft), Mt."]]}, {"qid": "870c0d4acdb480edbd23", "term": "Scottish independence", "description": "political aim for Scotland to be independent from the UK", "question": "Is Alistair Darling in favor of Scottish independence?", "answer": false, "facts": ["Alistair Darling was the chair of the Better Together Campaign.", "Better Together was the principal campaign for a No vote in the 2014 Scottish independence referendum, advocating Scotland continuing to be part of the United Kingdom. "], "decomposition": ["What was the main purpose of the Better Together Campaign?", "Was #1 against independence of Scotland?", "Was Alistair Darling the chair of the campaign?", "Is #2 or #3 negative?"], "evidence": [[[["Better Together (campaign)-1"]], [["Better Together (campaign)-2"]], [["Better Together (campaign)-2"]], ["operation"]], [[["Better Together (campaign)-1"]], ["operation"], [["Alistair Darling-3"]], ["operation"]], [[["Better Together (campaign)-1"]], ["operation"], [["Better Together (campaign)-2"]], ["operation"]]], "golden_sentence": [["Better Together was the principal campaign for a No vote in the 2014 Scottish independence referendum, advocating Scotland continuing to be part of the United Kingdom."], [""], ["From its commencement, it was chaired by former UK Chancellor of the Exchequer Alistair Darling and the Campaign Director was the Scottish Labour adviser and activist Blair McDougall."]]}, {"qid": "0a0259cb52b97155c169", "term": "Maya Angelou", "description": "American poet, author, and civil rights activist", "question": "Did any of Maya Angelou's children follow in her footsteps?", "answer": true, "facts": ["Maya Angelou was a civil rights activist and author.", "Maya Angelou had a son named Guy Johnson in 1945.", "Guy Johnson is an author that has written over twenty books and essays.", "Guy Johnson's books explore many civil rights themes."], "decomposition": ["What was Maya angelou's profession?", "Who is Maya Angelou's son?", "Did #2 do #1?"], "evidence": [[[["Maya Angelou-1"]], [["Maya Angelou-25"]], [["Maya Angelou-1", "Maya Angelou-25"], "no_evidence"]], [[["Maya Angelou-1"]], [["Maya Angelou-8"]], ["no_evidence", "operation"]], [[["Maya Angelou-1"]], [["Maya Angelou-8"]], ["no_evidence", "operation"]]], "golden_sentence": [["Maya Angelou (/\u02c8\u00e6nd\u0292\u0259lo\u028a/ (listen); born Marguerite Annie Johnson; April 4, 1928\u00a0\u2013 May 28, 2014) was an American poet, singer, memoirist, and civil rights activist."], ["Angelou had one son, Guy, whose birth she described in her first autobiography; one grandson, two great-grandchildren, and, according to Gillespie, a large group of friends and extended family."], ["", ""]]}, {"qid": "2d5c4eb75a668db82417", "term": "Sarah", "description": "Biblical character", "question": "Did Methuselah live at least 800 years as long as Sarah?", "answer": true, "facts": ["The biblical Sarah lived to the age of 127.", "The biblical Methuselah lived to 969 years of age."], "decomposition": ["At what age did Methuselah die?", "At what age did Sarah die?", "What is the difference between #1 and #2?", "Is #3 at least 800?"], "evidence": [[[["Methuselah-1"]], [["Sarah-11"]], ["operation"], ["operation"]], [[["Methuselah-1"]], [["Sarah-11"]], ["operation"], ["operation"]], [[["Methuselah-1"]], [["Sarah-11"]], ["operation"], ["operation"]]], "golden_sentence": [["Said to have died at the age of 969, he lived the longest of all figures mentioned in the Bible."], ["Sarah dies at the age of 127, and Abraham buys a piece of land with a cave near Hebron from Ephron the Hittite in which to bury her, which is the first land owned by the Israelites in Canaan according to the biblical narrative."]]}, {"qid": "be156628b0e76202c5cb", "term": "Rosemary", "description": "species of plant, rosemary", "question": "Is Rosemary outclassed as plant found in most song titles?", "answer": true, "facts": ["Rosemary appears in a few popular song titles such as Love Grows (Where My Rosemary Goes) and Randy Newman's Rosemary.", "Rose appears in many song titles including: Kiss From a Rose, The Rose, Desert Rose, Beauty of the Rose, and I Never Promised You a Rose Garden."], "decomposition": ["How many songs have \"rosemary\" in the title?", "How many songs have the plant \"rose\" in the title?", "Is #1 fewer than #2?"], "evidence": [[[["Love Grows (Where My Rosemary Goes)-3"], "no_evidence"], [["Blue Rose (song)-4", "Kiss from a Rose-1", "Lady Rose (song)-1", "The Rose (song)-2"], "no_evidence"], ["operation"]], [[["Love Grows (Where My Rosemary Goes)-1", "Rosemary Lane (song)-1"]], [["Desert Rose (Sting song)-1", "Every Rose Has Its Thorn-1", "Kiss from a Rose-1"]], ["operation"]], [[["Rosemary-23"], "no_evidence"], [["Rose-25"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Burrows sang the song on the programme during his third appearance on the same show with three different groups."], ["", "", "", ""]]}, {"qid": "45a9b00bd623941eeb1c", "term": "Spanish\u2013American War", "description": "Conflict in 1898 between Spain and the United States", "question": "Did Switzerland support the United States in the Spanish\u2013American War?", "answer": false, "facts": ["The Spanish\u2013American War was an armed conflict between Spain and the United States in 1898.", "Switzerland avoids alliances that might entail military, political, or direct economic action and has been neutral since the end of its expansion in 1515.", "Its policy of neutrality was internationally recognised at the Congress of Vienna in 1815."], "decomposition": ["What is Switzerland's major policy in terms of foreign relations and international institutions?", "Considering #1, is it likely that Switzerland would have supported the US in the Spanish\u2013American War?"], "evidence": [[[["Switzerland-2"]], [["Neutral country-1"], "operation"]], [[["Foreign relations of Switzerland-23"]], [["Spanish\u2013American War-1"]]], [[["Swiss neutrality-4"]], ["operation"]]], "golden_sentence": [["Since the Reformation of the 16th century, Switzerland has maintained a strong policy of armed neutrality; it has not fought an international war since 1815 and did not join the United Nations until 2002."], [""]]}, {"qid": "ddc468520bab44fd6f70", "term": "Bill Gates", "description": "American business magnate and philanthropist", "question": "Is Bill Gates the wealthiest of the Baby Boomers?", "answer": false, "facts": ["The Baby Boomers are the generation born between the years 1946-1964.", "Bill Gates was born on October 28, 1955 and has a net worth of 108 billion as of 2020.", "Jeff Bezos was born on January 12, 1964 and has a net worth of 160 billion as of 2020."], "decomposition": ["Which of the present billionaires are baby boomers?", "Who is the wealthiest among #1?", "Is #2 the same as Bill Gates?"], "evidence": [[[["Baby boomers-1", "Bill Gates-1", "Jeff Bezos-1", "Jim Walton-1"], "no_evidence"], [["Jeff Bezos-1"]], ["operation"]], [[["Baby boomers-1", "Billionaire-3"], "no_evidence"], [["Jeff Bezos-1"]], ["operation"]], [[["Baby boomers-1", "Bill Gates-1", "Bill Gates-3", "Jeff Bezos-1"], "no_evidence"], [["Jeff Bezos-1"], "operation"], ["operation"]]], "golden_sentence": [["", "", "", ""], ["Jeffrey Preston Bezos (/\u02c8be\u026azo\u028as/; n\u00e9 Jorgensen; born January 12, 1964) is an American industrialist, media proprietor, and investor."]]}, {"qid": "e1750e5f31ee45d02f33", "term": "Ammonia", "description": "Chemical compound of nitrogen and hydrogen", "question": "Is it safe to use Ammonia with Clorox?", "answer": false, "facts": ["Clorox is a brand name of a line of bleach products.", "Ammonia and bleach react together to produce toxic gas."], "decomposition": ["What is the main ingredient in Clorox?", "What happens when you mix ammonia and #1 together?", "Is #2 dangerous?"], "evidence": [[[["Sodium hypochlorite-46"]], [["Sodium hypochlorite-3"]], ["no_evidence"]], [[["Bleach-4", "Clorox-2"]], [["Ammonia-72"]], ["operation"]], [[["Sodium hypochlorite-2"]], [["Sodium hypochlorite-3"]], ["operation"]]], "golden_sentence": [["Household bleach is, in general, a solution containing 3\u20138% sodium hypochlorite, by weight, and 0.01\u20130.05% sodium hydroxide; the sodium hydroxide is used to slow the decomposition of sodium hypochlorite into sodium chloride and sodium chlorate."], ["In particular, mixing liquid bleach with other cleaning products, such as acids or ammonia, may produce toxic fumes."]]}, {"qid": "6cbaa8d24575e8f837f6", "term": "Tony Bennett", "description": "American singer", "question": "Did Tony Bennett have more children than he had wives?", "answer": true, "facts": ["Tony Bennett had four children.", "Tony Bennet has had three wives."], "decomposition": ["How many children has Tony Bennett had?", "How many wives has Tony Bennett had?", "Is #1 greater than #2?"], "evidence": [[[["Tony Bennett-13", "Tony Bennett-27"]], [["Tony Bennett-13", "Tony Bennett-27", "Tony Bennett-43"]], ["operation"]], [[["Tony Bennett-13", "Tony Bennett-27"]], [["Tony Bennett-29", "Tony Bennett-43"]], ["operation"]], [[["Sandra Grant Bennett-2"], "no_evidence"], [["Tony Bennett-27"], "no_evidence"], ["operation"]]], "golden_sentence": [["The couple had two sons, D'Andrea (Danny, born 1954) and Daegal (Dae, born 1955).", "They had two daughters, Joanna (born 1970) and Antonia (born 1974), and moved to Los Angeles."], ["The couple had two sons, D'Andrea (Danny, born 1954) and Daegal (Dae, born 1955).", "They had two daughters, Joanna (born 1970) and Antonia (born 1974), and moved to Los Angeles.", ""]]}, {"qid": "e6d0e7deead88356bbd2", "term": "Greyhound", "description": "Dog breed used in dog racing", "question": "Would a greyhound be able to outrun a greyhound bus?", "answer": false, "facts": ["A greyhound bus can travel speeds upward of 60 mph.", "A greyhound dog can run at speeds up to 45 mph."], "decomposition": ["What is the top speed of a Greyhound bus?", "What is the top speed of a greyhound dog?", "Is #2 greater than #1?"], "evidence": [[[["Greyhound Lines-83", "Speed limits in the United States-1"], "no_evidence"], [["Greyhound-3"]], ["operation"]], [["no_evidence"], [["Greyhound-3"]], ["no_evidence", "operation"]], [[["Greyhound Lines-17", "MCI 102DL3 & D4500-4"], "no_evidence"], [["Greyhound-3"]], ["operation"]]], "golden_sentence": [["", "Highway speed limits can range from an urban low of 20\u00a0mph (32\u00a0km/h) to a rural high of 85\u00a0mph (137\u00a0km/h)."], ["better\u00a0source\u00a0needed] It is a gentle and intelligent breed whose combination of long, powerful legs, deep chest, flexible spine and slim build allows it to reach average race speeds exceeding 64 kilometres per hour (40\u00a0mph)."]]}, {"qid": "6268a4c17702398b091e", "term": "Bonanza", "description": "1959-1973 American western/cowboy television series", "question": "Would Bonanza marathon end before WWE Heat marathon?", "answer": false, "facts": ["Bonanza had a total of 431 episodes.", "WWE Heat had a total of 513 episodes.", "The average run time of WWE Heat was 45 minutes.", "The average run time of Bonanza was 49 minutes."], "decomposition": ["How many episodes exist of Bonanza?", "How many episodes exist of WWE Heat?", "How long is each episode of Bonanza?", "How long is each episode of WWE Heat?", "Is #1 times #3 less than #2 times #4?"], "evidence": [[[["Bonanza-30"]], [["WWE Heat-11"]], ["no_evidence"], ["no_evidence"], ["operation"]], [[["Bonanza-30"]], [["WWE Heat-3"]], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Bonanza-1"]], [["WWE Heat-11"]], [["Bonanza-19", "Bonanza-33"], "no_evidence"], [["WWE Heat-4"]], ["operation"]]], "golden_sentence": [["(Of 431 total episodes) Lorne Greene \u2013 Ben Cartwright \u2013 417 episodes (Season 1\u201314) Michael Landon \u2013 Joseph \"Little Joe\" Cartwright \u2013 416 episodes (Season 1\u201314) Dan Blocker \u2013 Eric \"Hoss\" Cartwright \u2013 401 episodes (Season 1\u201313) Pernell Roberts \u2013 Adam Cartwright \u2013 173 episodes (Season 1\u20136) Victor Sen Yung \u2013 Hop Sing \u2013 107 episodes (Season 1\u201314) Ray Teal \u2013 Sheriff Coffee \u2013 98 episodes (Season 2\u201313) David Canary \u2013 \"Candy\" Canaday \u2013 93 episodes (Season 9\u201311, 14) Bing Russell \u2013 Deputy Clem Foster \u2013 57 episodes (Season 4\u20135, 7\u201314) Mitch Vogel \u2013 Jamie Hunter Cartwright \u2013 47 episodes (Season 12\u201314) Tim Matheson \u2013 Griff King \u2013 12 episodes (Season 14) Lou Frizzell \u2013 Dusty Rhodes \u2013 11 episodes (Season 11\u201313) Guy Williams \u2013 Will Cartwright \u2013 5 episodes (Season 5) Initially, Bonanza aired on Saturdays at 7:30\u00a0p.m. Eastern, opposite \"Dick Clark's Saturday Night Beech-Nut Show\" and \"John Gunther's High Road\" on ABC, and Perry Mason on CBS."], ["After 10 years of programming and 513 episodes, the final episode of WWE Heat was uploaded to WWE.com on May 30, 2008."]]}, {"qid": "5de69d0563a7df34136b", "term": "Futurama", "description": "American animated sitcom for the Fox Broadcasting Company and Comedy Central", "question": "Will Futurama surpass the number of episodes of The Simpsons by the end of 2020?", "answer": false, "facts": ["Futurama was cancelled in 2013.", "The Simpsons is still creating new episodes as of May 2020.", "Futurama aired 140 total episodes.", "The Simpsons has aired over 600 episodes."], "decomposition": ["How many episodes of Futurama have been produced to date?", "How many episodes of the Simpsons has been produced to date?", "Is #1 greater than #2?"], "evidence": [[[["Meanwhile (Futurama)-1"]], [["The Simpsons-3"]], ["operation"]], [[["Futurama-2", "Futurama-3"], "no_evidence"], [["History of The Simpsons-4"]], ["operation"]], [[["Futurama (season 1)-2"], "no_evidence"], [["History of The Simpsons-4"]], ["operation"]]], "golden_sentence": [["It serves as the 26th episode of the seventh season, and the 140th episode of the series overall."], ["Since its debut on December 17, 1989, 683 episodes of The Simpsons have been broadcast."]]}, {"qid": "aafd1651752696bdfb6e", "term": "Ariana Grande", "description": "American singer, songwriter, and actress", "question": "Was Ariana Grande inspired by Imogen Heap?", "answer": true, "facts": ["Ariana Grande's song 'Goodnight And Go' uses a sample from a track of the same name.", "\"Goodnight and Go\" is originally an Imogen Heap song."], "decomposition": ["Who was the original singer of Ariana Grande's cover 'Goodnight and Go'", "Is #1 Imogen Heap?"], "evidence": [[[["Goodnight and Go-1", "Sweetener (song)-1"]], ["operation"]], [[["Goodnight and Go-1"]], ["operation"]], [[["Goodnight and Go-1"]], ["operation"]]], "golden_sentence": [["\"Goodnight and Go\" is a song by British singer-songwriter Imogen Heap, the second single from her 2005 album Speak for Yourself.", ""]]}, {"qid": "324c99eb1a799e498f16", "term": "Communist Party of the Soviet Union", "description": "Ruling political party of the Soviet Union", "question": "Can the Communist Party of the Soviet Union get a perfect all kill?", "answer": false, "facts": ["The Communist Party of the Soviet Union is a political party", "A perfect all kill occurs when a South Korean recording artist hits number one simultaneously on every music chart"], "decomposition": ["Who can get a perfect all kill?", "Is the Communist Party of the Soviet Union a kind of #1?"], "evidence": [[[["Lisa (rapper)-5"]], [["Communist Party of the Soviet Union-1"], "operation"]], [[["Whistle (Blackpink song)-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Love Scenario-4", "Musical ensemble-1", "Song-1"]], [["Communist Party of the Soviet Union-1"]]]], "golden_sentence": [["\"Whistle\" achieved a perfect \"all-kill\", topping all South Korean charts upon debut."], [""]]}, {"qid": "b569ad5f83d45ac7f4e1", "term": "Cantonese", "description": "Standard dialect of Yue language that originated in the vicinity of Guangzhou (Canton) in southern China", "question": "Did George W. Bush grow up speaking Cantonese?", "answer": false, "facts": ["George Bush grew up primarily in Texas.", "English is the primary language spoken in Texas.", "Cantonese is the primary language spoken in certain parts of China."], "decomposition": ["Where did George Bush spend the first few years of his life?", "Which language is primarily spoken in #1?", "Is #2 the same as Cantonese?"], "evidence": [[[["George W. Bush-5"]], [["United States-80"]], ["operation"]], [[["George W. Bush Childhood Home-1"]], [["Texas-146"]], ["operation"]], [[["George W. Bush-5"]], [["Texas-61"]], ["operation"]]], "golden_sentence": [["He was raised in Midland and Houston, Texas, with four siblings, Jeb, Neil, Marvin and Dorothy."], ["English (specifically, American English) is the de facto national language of the United States."]]}, {"qid": "98f4b292403c5d92c4dc", "term": "Thesis", "description": "document submitted in support of candidature for an academic degree", "question": "Would a thesis paper be unusual to assign to kindergartners? ", "answer": true, "facts": ["Kindergartners are usually between 4 and 6 years of age.", "Kindergartners are tasked with learning the alphabet and how to write their own names."], "decomposition": ["What skill set is required to create a thesis paper?", "What skill set do kindergartners possess?", "Are all the skills in #1 also found in #2?"], "evidence": [[[["Thesis-1"], "no_evidence"], [["Kindergarten-89"], "no_evidence"], ["operation"]], [[["Thesis-15"]], [["Kindergarten-29"]], [["Kindergarten-29"], "operation"]], [[["Thesis-17"]], [["Cognitive development-23", "Kindergarten-29"]], [["Cognitive development-23"], "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "239a645e819aadf4509c", "term": "Henry Ford", "description": "American businessperson", "question": "Do people still see Henry Ford's last name often?", "answer": true, "facts": ["Henry Ford is the founder of Ford Motor Company.", "Every Ford vehicle still bears Henry's last name on the brand logo."], "decomposition": ["What company did Henry Ford create?", "What does #1 produce? ", "Does #2 have Henry's name on it?"], "evidence": [[[["Henry Ford-1"]], [["Ford Motor Company-1"]], ["operation"]], [[["Henry Ford Company-1"]], [["Henry Ford Company-1"]], [["Henry Ford Company-1"]]], [[["Henry Ford-14"]], [["Henry Ford-14"]], [["Henry Ford-14"], "operation"]]], "golden_sentence": [["Henry Ford (July 30, 1863 \u2013 April 7, 1947) was an American industrialist and business magnate, founder of the Ford Motor Company and chief developer of the assembly line technique of mass production."], ["The company sells automobiles and commercial vehicles under the Ford brand, and most luxury cars under the Lincoln brand."]]}, {"qid": "0cac8d2c4541a64764b5", "term": "Harvey Milk", "description": "American politician who became a martyr in the gay community", "question": "Could a cow produce Harvey Milk?", "answer": false, "facts": ["Harvey Milk was a human being.", "Cows are not human beings.", "Only human beings can produce offspring which are also human beings."], "decomposition": ["What products can be derived from cows?", "Is Harvey Milk a kind of any of #1?"], "evidence": [[[["Cattle-2"]], [["Harvey Milk-1"], "operation"]], [[["Milk-32"]], [["Harvey Milk-1"], "operation"]], [[["Cattle-2"]], ["operation"]]], "golden_sentence": [["Another product of cattle is dung, which can be used to create manure or fuel."], ["Although he was the most pro-LGBT politician in the United States at the time, politics and activism were not his early interests; he was neither open about his sexuality nor civically active until he was 40, after his experiences in the counterculture movement of the 1960s."]]}, {"qid": "ba26006fc0168dc6907b", "term": "Funeral", "description": "ceremony for a person who has died", "question": "Is it unusual to play Happy hardcore music at a funeral?", "answer": true, "facts": ["Happy hardcore is a music genre of hard dance.", "Happy hardcore emerged both from the UK breakbeat hardcore rave scene, and Belgian, German and Dutch hardcore techno scenes.", "A funeral is traditionally a somber event.", "Funerals typically do not involve dancing.", "Raves are typically energetic and upbeat places and are not somber like a funeral."], "decomposition": ["What type of music is usually played at funerals?", "What are the characteristics of Happy Hardcore music?", "Do any of #1 have the characteristics of #2?"], "evidence": [[[["Funeral-8"]], [["Happy hardcore-1"]], ["operation"]], [[["Dirge-1"]], [["Happy hardcore-1", "Happy hardcore-7"], "no_evidence"], ["no_evidence", "operation"]], [[["Funeral march-1"]], [["Happy hardcore-2"]], [["Funeral march-1"]]]], "golden_sentence": [["One issue of concern as the 21st century began was with the use of secular music at Christian funerals, a custom generally forbidden by the Roman Catholic Church."], ["Happy hardcore, also known as 4-beat or happycore, is a music genre of hard dance."]]}, {"qid": "3309b285e44f053269cd", "term": "Communist Party of the Soviet Union", "description": "Ruling political party of the Soviet Union", "question": "Would Communist Party of the Soviet Union hypothetically support Trickle Down Economics?", "answer": false, "facts": ["The Communist Party of the Soviet Union believed in the main aspects of Communism.", "Communism believes that no private ownership of property should be allowed. ", "Trickle Down Economics, popularized by Ronald Reagan, involved rich businesses getting tax breaks so they could supposedly pass the wealth to the poor.", "Trickle Down Economics required rich business owners to have control over the means of production and property."], "decomposition": ["Under Leninism, which class led the Communist Party of the Soviet Union?", "Under Trickle-down economics, which economic class gains wealth and power?", "Are #1 and #2 the same?"], "evidence": [[[["Leninism-1"]], [["Trickle-down economics-1"]], ["operation"]], [[["Dictatorship of the proletariat-24"]], [["Trickle-down economics-7"]], ["operation"]], [[["Leninism-1"]], [["Trickle-down economics-1"]], ["operation"]]], "golden_sentence": [["Leninist revolutionary leadership is based upon The Communist Manifesto (1848) identifying the communist party as \"the most advanced and resolute section of the working class parties of every country; that section which pushes forward all others.\""], [""]]}, {"qid": "79d2da2167c853d97841", "term": "Brazilian jiu-jitsu", "description": "martial art focusing on grappling and ground fighting, originally based on Kodokan judo newaza taught by Japanese judoka, that developed independently in Brazil from experimentation and adaptation by Carlos and H\u00e9lio Gracie, Luiz Fran\u00e7a, et al.", "question": "Could a white belt defeat Jon Jones in a Brazilian jiu-jitsu match?", "answer": false, "facts": ["A white belt is the lowest ranking in Brazilian jiu-jitsu.", "Jon Jones has a purple belt in Brazilian jiu-jitsu under Roberto Alencar.", "A purple belt is the second highest ranking in Brazilian jiu-jitsu.", "Jon Jones is one of the greatest combat sports athletes to ever live."], "decomposition": ["What color belt does Jon Jones have in Brazilian jiu-jitsu?", "In belt color ranking in Brazilian jiu-jitsu, where is #1? ", "In belt color ranking in Brazilian jiu-jitsu, where is white belt?", "Is #2 higher than #3?"], "evidence": [[[["Jon Jones-2"], "no_evidence"], [["Brazilian jiu-jitsu-39"]], [["Brazilian jiu-jitsu-39"]], ["operation"]], [["no_evidence"], ["no_evidence"], [["Brazilian jiu-jitsu ranking system-5"]], ["no_evidence", "operation"]], [[["Jon Jones-1"], "no_evidence"], [["Black belt (martial arts)-9"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["Leglocks are allowed in varying degrees depending on skill level, with the most prominent BJJ tournaments typically allowing only the straight ankle lock and muscle stretching submissions such as the \"banana split\" from white through purple belt, with the kneebar, toehold, and calf slicer submissions being permitted at brown and black belt."], ["Leglocks are allowed in varying degrees depending on skill level, with the most prominent BJJ tournaments typically allowing only the straight ankle lock and muscle stretching submissions such as the \"banana split\" from white through purple belt, with the kneebar, toehold, and calf slicer submissions being permitted at brown and black belt."]]}, {"qid": "a3d530ca4fd886da5ec8", "term": "Bull shark", "description": "Species of fish", "question": "Does bull shark bite hurt worse than crocodile bite?", "answer": false, "facts": ["The bull shark has the highest weight for weight bite of all cartilaginous fish at 5,914 newtons.", "Crocodile slam their jaws shut with 3,700 pounds per square inch (psi), or 16,460 newtons, of bite force."], "decomposition": ["What is the weight bite of a bull shark?", "What is the weight bite of a Crocodile?", "Is #1 less than #2?"], "evidence": [[[["Bull shark-9"]], [["Crocodile-24"]], ["operation"]], [[["Bull shark-9"]], [["Crocodile-24"]], ["operation"]], [[["Bull shark-9"], "operation"], [["Crocodile-24"]], ["operation"]]], "golden_sentence": [["Bull sharks have a bite force up to 5,914 newtons (1,330\u00a0lbf), weight for weight the highest among all investigated cartilaginous fishes."], ["The force of a large crocodile's bite is more than 5,000\u00a0lbf (22,000\u00a0N), which was measured in a 5.5\u00a0m (18\u00a0ft) Nile crocodile, in the field; comparing to 335\u00a0lbf (1,490\u00a0N) for a Rottweiler, 800\u00a0lbf (3,600\u00a0N) for a hyena, 2,200\u00a0lbf (9,800\u00a0N) for an American alligator,[failed verification] and 4,095\u00a0lbf (18,220\u00a0N) for the largest confirmed great white shark."]]}, {"qid": "87dfba946e7f6e2d7926", "term": "1936 Summer Olympics", "description": "games of the XI Olympiad, celebrated in Berlin in 1936", "question": "Is 1936 Summer Olympics venue too small for a Superbowl crowd?", "answer": false, "facts": ["The 1936 Summer Olympics was held at the Olympiastadion Berlin.", "The Olympiastadion Berlin has a capacity of over 74,000 people.", "The 2020 Superbowl was attended by 62,417 fans."], "decomposition": ["At which venue did the 1936 Summer Olympics take place?", "What is the spectator capacity of #1?", "How many people attended the 2020 Superbowl?", "Is #3 greater than #2?"], "evidence": [[[["1936 Summer Olympics-21"]], [["1936 Summer Olympics-2"]], [["Super Bowl LIV-53"]], ["operation"]], [[["Olympiastadion (Berlin)-1"]], [["Olympiastadion (Berlin)-1"]], [["Super Bowl LIV-4"]], [["Hard Rock Stadium-22"], "operation"]], [[["Olympiastadion (Berlin)-1"]], [["Olympiastadion (Berlin)-2"]], [["Hard Rock Stadium-22", "Super Bowl LIV-2"]], ["operation"]]], "golden_sentence": [["During World War II, Deutschlandhalle suffered heavy aerial bombing damage."], [""], ["San Francisco 49ers vs. Kansas City Chiefs\u2014Game summary at Hard Rock Stadium, Miami Gardens, Florida Date: February 2, 2020 Game time: 6:30\u00a0p.m. EST/5:30\u00a0p.m. CST/3:30\u00a0p.m. PST Game weather: 64\u00a0\u00b0F (18\u00a0\u00b0C) Game attendance: 62,417 Referee: Bill Vinovich (52) TV announcers\u00a0(Fox): Joe Buck, Troy Aikman, Erin Andrews, Chris Myers and Mike Pereira Recap, Game Book 1Completions/attempts 2Carries 3Long gain 4Receptions 5Times targeted Super Bowl LIV had seven officials."]]}, {"qid": "08c173b2bf6386bed826", "term": "Dr. Seuss", "description": "American children's writer and illustrator", "question": "Was Dr. Seuss a liar?", "answer": true, "facts": ["Dr. Seuss was a writer and illustrator of children's books", "Dr. Seuss first published a children's book under the name of Dr. Seuss in 1937", "Dr. Seuss did not actually have a doctorate or equivalent degree until 1956"], "decomposition": ["When did Dr. Seuss first use the title \"Dr.\"?", "When did he get his doctorate (or equivalent)?", "Is #1 before #2?"], "evidence": [[[["Dr. Seuss-2"]], [["Dr. Seuss-18"]], ["operation"]], [[["Dr. Seuss-8"]], [["Dr. Seuss-2"]], ["operation"]], [[["Dr. Seuss-2"]], [["Dr. Seuss-18"]], ["operation"]]], "golden_sentence": [["Geisel adopted the name \"Dr. Seuss\" as an undergraduate at Dartmouth College and as a graduate student at Lincoln College, Oxford."], ["In 1956, Dartmouth awarded Geisel with an honorary doctorate, finally legitimizing the \"Dr.\" in his pen name."]]}, {"qid": "51fe75635991e5a7fe12", "term": "Christopher Columbus", "description": "Italian explorer, navigator, and colonizer", "question": "Did Christopher Columbus go to Antarctica? ", "answer": false, "facts": ["Between 1492 and 1503, Columbus completed four round-trip voyages between Spain and the Americas.", " His expeditions, sponsored by the Catholic Monarchs of Spain, were the first European contact with the Caribbean, Central America, and South America.", "Antarctica is Earth's southernmost continent."], "decomposition": ["Which areas did Christopher Columbus visit in his voyages?", "Is Antarctica one of #1?"], "evidence": [[[["Christopher Columbus-1"]], [["Antarctica-3"], "operation"]], [[["Christopher Columbus-1"]], ["operation"]], [[["Christopher Columbus-1"]], ["operation"]]], "golden_sentence": [["His expeditions, sponsored by the Catholic Monarchs of Spain, were the first European contact with the Caribbean, Central America, and South America."], [""]]}, {"qid": "150b54e9d71ff0ef4cac", "term": "Breakdancing", "description": "Style of street dance", "question": "Is breakdancing safe for people with tendonitis?", "answer": false, "facts": ["Tendonitis is a condition where the joints are inflamed.", "Strong motions in joints suffering from tendonitis can result in damage to nerves.", "Breakdancing is a style of dance that involves many vigorous motions.", "The downrock breakdancing maneuver involves balancing the body weight on the floor using one arm."], "decomposition": ["What are the symptoms of tendonitis?", "Which kind of movements are involved in breakdancing?", "Are #2 safe when experiencing #1?"], "evidence": [[[["Tendinopathy-1"], "no_evidence"], [["Breakdancing-27"], "no_evidence"], ["no_evidence", "operation"]], [[["Tendinopathy-1"]], [["Breakdancing-1", "Power move-3"], "no_evidence"], ["operation"]], [[["Tendinopathy-4"]], [["Breakdancing-27"]], [["Tendinopathy-5"], "operation"]]], "golden_sentence": [["Tendinopathy, also known as tendinitis or tendonitis, is a type of tendon disorder that results in pain, swelling, and impaired function."], ["Power moves are acrobatic moves that require momentum, speed, endurance, strength, flexibility, and control to execute."]]}, {"qid": "7141ba10ac6e55d05c0e", "term": "Louvre", "description": "Art museum and Historic site in Paris, France", "question": "Is the Louvre's pyramid known for being unbreakable? ", "answer": false, "facts": ["The Pyramid at the Louvre is made of glass and metal.", "The Louvre Pyramid glass is 10mm thick.", "10mm thick glass is not unbreakable."], "decomposition": ["What materials is the Louvre made of?", "Are all the materials in listed in #1 unbreakable?"], "evidence": [[[["Louvre Pyramid-1"]], [["Glass-19"]]], [[["Louvre Pyramid-1"]], [["Glass-2"], "operation"]], [[["Louvre Pyramid-3"]], [["Mohs scale of mineral hardness-4", "Strength of glass-5"], "no_evidence"]]], "golden_sentence": [["The Louvre Pyramid (Pyramide du Louvre) is a large glass and metal pyramid designed by Chinese-American architect I. M. Pei, surrounded by three smaller pyramids, in the main courtyard (Cour Napol\u00e9on) of the Louvre Palace (Palais du Louvre) in Paris."], [""]]}, {"qid": "5afad7881c77c27b7d80", "term": "Naruto", "description": "Japanese manga and anime series", "question": "Did Naruto escape the Temple of Doom?", "answer": false, "facts": ["Naruto is a character in a Japanese anime and manga about ninjas", "The Temple of Doom is a setting from an Indiana Jones movie"], "decomposition": ["Which country was the movie Indiana Jones and the Temple of Doom set in?", "What is the setting of manga that features Naruto?", "Are #1 and #2 the same?"], "evidence": [[[["Indiana Jones and the Temple of Doom-4"]], [["Manga-1"]], ["operation"]], [[["Indiana Jones and the Temple of Doom-4"]], [["Naruto-1"]], ["operation"]], [[["Indiana Jones-8"]], [["Naruto-16"]], ["operation"]]], "golden_sentence": [["They ride down the mountain slopes and fall into a raging river, eventually arriving at the village of Mayapore in northern India."], ["The term manga (kanji: \u6f2b\u753b; hiragana: \u307e\u3093\u304c; katakana: \u30de\u30f3\u30ac; listen\u00a0(help\u00b7info); English: /\u02c8m\u00e6\u014b\u0261\u0259/ or /\u02c8m\u0251\u02d0\u014b\u0261\u0259/) is used in Japan to refer to both comics and cartooning."]]}, {"qid": "b0d70309d8bb297f6631", "term": "August", "description": "eighth month in the Julian and Gregorian calendars", "question": "Is August a winter month for part of the world?", "answer": true, "facts": ["August is a summer month in the northern hemisphere.", "However, the seasons are opposite south of the Equator.", "August is in the middle of winter for Australia, Antarctica, and parts of Africa and South America."], "decomposition": ["What season is August a part of in the northern hemisphere?", "Does #1 correspond to winter south of the Equator?"], "evidence": [[[["Summer-2"]], [["Summer-1"]]], [[["Summer-1"]], [["Summer-1"]]], [[["August-3"]], [["Winter-1"], "operation"]]], "golden_sentence": [["The meteorological convention is to define summer as comprising the months of June, July, and August in the northern hemisphere and the months of December, January, and February in the southern hemisphere."], ["When it is summer in the Northern Hemisphere, it is winter in the Southern Hemisphere, and vice versa."]]}, {"qid": "2d7ed6ffd73085cc5990", "term": "Saddam Hussein", "description": "Iraqi politician and President", "question": "Would Saddam Hussein hypothetically choose Saladin as ally over Idris I?", "answer": true, "facts": ["Saddam Hussein, President of Iraq, was a Sunni Muslim that brutalized many Shiite Muslims.", "Saladin was the first Sultan of Egypt and was a Sunni Muslim.", "Idris I was called the founder of Morocco and was a Shiite Muslim."], "decomposition": ["Which denomination of Islam did Saddam Hussein identify with?", "Which Islamic denomination did Saladin belong to?", "Which Islamic denomination did Idris I belong to?", "Does #1 match #2 and contrast with #3?"], "evidence": [[[["Saddam Hussein-2"]], [["Saladin-1"]], [["Idris I of Morocco-1"], "no_evidence"], ["no_evidence"]], [[["Saddam Hussein-1", "Saddam Hussein-54"]], [["Saladin-1"]], [["Idris I of Morocco-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Saddam Hussein-2"]], [["Saladin-1"]], [["Zaidiyyah-1", "Zaidiyyah-22"]], ["operation"]]], "golden_sentence": [["Positions of power in the country were mostly filled with Sunni Arabs, a minority that made up only a fifth of the population."], ["A Sunni Muslim of Kurdish ethnicity, Saladin led the Muslim military campaign against the Crusader states in the Levant."], ["Idris I (Arabic: \u0625\u062f\u0631\u064a\u0633 \u0627\u0644\u0623\u0648\u0644\u200e), also known as Idris ibn Abdillah, was the founder of the Arab Idrisid dynasty in part of northern Morocco in alliance with the Berber tribe of Awraba."]]}, {"qid": "6290f75c97ff9cfd0d4f", "term": "Apollo", "description": "God in Greek mythology", "question": "Do Apollo and Baldur share similar interests?", "answer": true, "facts": ["Apollo is a Greek god of light.", "Baldur is a Norse god of light.", "They are both interested in light."], "decomposition": ["Apollo is the Greek god of what object?", "What is Baldur the Norse god of?", "Is the item in #2 the same as #1?"], "evidence": [[[["Apollo-1"]], ["no_evidence"], ["operation"]], [[["Apollo-1"]], [["Baldr-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Apollo-1"]], [["Baldr-7"]], ["operation"]]], "golden_sentence": [["The national divinity of the Greeks, Apollo has been recognized as a god of archery, music and dance, truth and prophecy, healing and diseases, the Sun and light, poetry, and more."]]}, {"qid": "57c19e82dc3c33abf7ce", "term": "Minor League Baseball", "description": "hierarchy of professional baseball leagues affiliated with Major League Baseball", "question": "Are any minor league baseball teams named after felines?", "answer": true, "facts": ["Felines include cats, tigers, and lions.", "The Sacramento River Cats are a minor league baseball affiliate of the San Francisco Giants.", "The Tri-City Valley Cats are a minor league baseball affiliate of the Houston Astros.", "The Lakeland Flying Tigers are a minor league baseball affiliate of the Detroit Tigers."], "decomposition": ["What are the names of teams in Minor League Baseball?", "Which animals are regarded as felines?", "Does any of #1 include any of #2?"], "evidence": [[[["Lynchburg Hillcats-1"], "no_evidence"], [["Felinae-1"]], ["operation"]], [[["Minor League Baseball-1"], "no_evidence"], [["Felidae-1"]], [["New Hampshire Fisher Cats-1"], "operation"]], [[["New Hampshire Fisher Cats-1", "Sacramento River Cats-1"], "no_evidence"], [["Felidae-1"]], ["operation"]]], "golden_sentence": [["The Lynchburg Hillcats are a Minor League Baseball team in Lynchburg, Virginia."], ["This subfamily comprises the small cats having a bony hyoid, because of which they are able to purr but not roar."]]}, {"qid": "2bdee465b22ffa730c29", "term": "Art Deco", "description": "Influential visual arts design style which first appeared in France during the 1920s", "question": "Did Andy Warhol influence Art Deco style?", "answer": false, "facts": ["Art Deco is a visual style that first appeared in the 1920s.", "Andy Warhol was born in 1928.", "Andy Warhol started drawing when he was in third grade."], "decomposition": ["When did Art Deco first appear as a visual style?", "When was Andy Warhol born?", "Is #2 before #1?"], "evidence": [[[["Art Deco-9"]], [["Andy Warhol-4"]], ["operation"]], [[["Art Deco-1"]], [["Andy Warhol-1"]], ["operation"]], [[["Art Deco-9"]], [["Andy Warhol-1"]], ["operation"]]], "golden_sentence": [[""], ["Warhol was born on August 6, 1928, in Pittsburgh, Pennsylvania."]]}, {"qid": "1c7f1c4aa3f308716e6a", "term": "Rupert Murdoch", "description": "Australian-born American media mogul", "question": "Would Dante Alighieri hypothetically place Rupert Murdoch in 8th Circle of Hell?", "answer": true, "facts": ["Dante Alighieri was an Italian poet that wrote Inferno.", "Inferno depicts several layers of Hell.", "The 8th Circle of Hell is reserved for liars, bribers, flatterers, and false prophets.", "Rupert Murdoch was involved in a News International scandal in which police were bribed and phones were hacked.", "Rupert Murdoch refused to take any responsibility for the actions of his employees in the News International scandal."], "decomposition": ["Which of Dante Alighieri's works describes hell?", "According to #1, which sins would cause one to be placed in the 8th circle of hell?", "Has Rupert Murdoch come under allegations of any of #2?"], "evidence": [[[["Divine Comedy-2"]], [["Malebolge-2"]], [["Rupert Murdoch-31"], "no_evidence"]], [[["Inferno (Dante)-1"]], [["Inferno (Dante)-57"]], [["News International phone hacking scandal-1"], "no_evidence", "operation"]], [[["Inferno (Dante)-45"], "operation"], ["no_evidence"], ["no_evidence"]]], "golden_sentence": [["The narrative takes as its literal subject the state of souls after death and presents an image of divine justice meted out as due punishment or reward, and describes Dante's travels through Hell, Purgatory, and Paradise or Heaven, while allegorically the poem represents the soul's journey towards God, beginning with the recognition and rejection of sin (Inferno), followed by the penitent Christian life (Purgatorio), which is then followed by the soul's ascent to God (Paradiso)."], ["In it, sinners guilty of \"simple\" fraud are punished (that is, fraud that is committed without particularly malicious intent, whereas malicious or \"compound\" fraud\u2014fraud which goes against bond of love, blood, honor, or the bond of hospitality\u2014would be punished in the ninth circle)."], ["In the UK, his media empire remains under fire, as investigators continue to probe reports of other phone hacking."]]}, {"qid": "3732e512c2763ae7affd", "term": "Giant squid", "description": "Deep-ocean dwelling squid in the family Architeuthidae", "question": "Is capturing giant squid in natural habitat impossible with no gear?", "answer": true, "facts": ["Giant squids live between 1,000 and 3,800 feet in the ocean.", "With a dry dive suit, a scuba tank, gloves, and so on, divers can reach depths of around 1000 feet.", "Without scuba gear people can safely dive a maximum of 60 feet without feeling the negative risks associated with diving beyond the limit."], "decomposition": ["At what depths do giant squid live?", "What is the max depth a person can safely dive without gear?", "Is #2 less than #1?"], "evidence": [[[["Giant squid-5"]], [["Freediving-47"]], ["operation"]], [[["Giant squid-5"]], [["Scuba diving-65"]], ["operation"]], [[["Giant squid-5"]], [["Underwater diving-5"]], ["operation"]]], "golden_sentence": [["The vertical distribution of giant squid is incompletely known, but data from trawled specimens and sperm whale diving behaviour suggest it spans a large range of depths, possibly 300\u20131,000 metres (980\u20133,280\u00a0ft)."], ["clarification needed][citation needed] Nicholas Mevoli, a diver from New York died on 17 November 2013 after losing consciousness on surfacing from a 3-minute 38 second dive to a depth of 72 metres during an official record attempt in the \"constant weight without fins\" event."]]}, {"qid": "450e6310469420c7d1c6", "term": "Harry Potter and the Philosopher's Stone", "description": "1997 fantasy novel by J. K. Rowling", "question": "Would characters in Harry Potter and the Philosopher's Stone be persecuted as pagans?", "answer": true, "facts": ["Pagans are defined as people that hold beliefs other than those of the major world religions (Christianity, Islam, and Judaism).", "The characters in Harry Potter and the Philosopher's Stone practice magic.", "Islam explicitly forbid the practice of magic and has harsh consequences for it.", "Jezebel in Hebrew scripture was a worshiper of pagan Baal and was thrown from a window for her beliefs.", "Women accused of being witches were burned alive by Christians during the Salem Witch Trials."], "decomposition": ["What are the characters in Harry Potter and the Philosopher's Stone known to perform?", "What would performers of #1 be considered by Christians?", "What have Christians done to #2 in the past?", "Are #2 pagans and #3 a form of persecution?"], "evidence": [[[["Harry Potter and the Philosopher's Stone-1"]], [["Witchcraft-1"]], [["Witchcraft-5"]], [["Persecution-1"]]], [[["Harry Potter and the Philosopher's Stone-1"]], [["Magic (supernatural)-2", "Paganism-24"]], [["Witch trials in the early modern period-1"], "no_evidence"], ["operation"]], [[["Harry Potter-1"]], [["Witchcraft-2"]], [["Death by burning-18"]], ["operation"]]], "golden_sentence": [[""], [""], [""], ["The most common forms are religious persecution, racism and political persecution, though there is naturally some overlap between these terms."]]}, {"qid": "fbf4b7e4de60b95c5427", "term": "Aretha Franklin", "description": "American singer, songwriter, and pianist", "question": "Has Aretha Franklin ever collaborated with a suicidal person?", "answer": true, "facts": ["Donny Hathaway was a singer and session musician that worked with Staple Singers, Jerry Butler, Aretha Franklin, the Impressions  and Curtis Mayfield.", "Donny Hathaway jumped from his 15th floor room and his death was ruled a suicide."], "decomposition": ["What music artists has Aretha Franklin done collaborations with?", "Did any of the artists listed in #1 commit suicide?"], "evidence": [[[["Donny Hathaway-4"]], [["Donny Hathaway-17"]]], [[["Donny Hathaway-1"], "no_evidence"], [["Donny Hathaway-16"], "operation"]], [[["Donny Hathaway-4"], "no_evidence"], [["Donny Hathaway-17"]]]], "golden_sentence": [["He did the arrangements for hits by the Unifics (\"Court of Love\" and \"The Beginning of My End\") and took part in projects by the Staple Singers, Jerry Butler, Aretha Franklin, the Impressions and Curtis Mayfield himself."], ["The glass had been neatly removed from the window and there were no signs of struggle, leading investigators to rule that Hathaway's death was a suicide."]]}, {"qid": "a1efc9a0567bb66afea8", "term": "Paramount leader", "description": "The highest leader of China, usually the General Secretary or Chairman of Chinese Communist Party.", "question": "Did the Paramount leader produce Titanic?", "answer": false, "facts": ["The Paramount leader is the highest leader of China", "Titanic was produced by Paramount Pictures", "Paramount Pictures is an American film studio"], "decomposition": ["Who is the Paramount leader?", "Who produced Titanic?", "Is #1 the same as #2?"], "evidence": [[[["Jim Gianopulos-1"]], [["Titanic (1997 film)-1", "Titanic (1997 film)-12"]], ["operation"]], [[["Paramount leader-1"]], [["Titanic (1997 film)-1"]], ["operation"]], [[["Paramount leader-1"]], [["Titanic (1997 film)-43"]], ["operation"]]], "golden_sentence": [[""], ["Titanic is a 1997 American epic romance and disaster film directed, written, co-produced, and co-edited by James Cameron.", "\"It wasn't until after the movie came out that we found out that there was a J. Dawson gravestone,\" said the film's producer, Jon Landau, in an interview."]]}, {"qid": "66fde88b0993411caea5", "term": "Muslims", "description": "Adherents of Islam", "question": "Do Muslims have a different idea of Seraphim than Christians?", "answer": false, "facts": ["Seraphim are celestial of heavenly beings.", "Christians refer to Seraphims as the highest choir of the angelic hierarchy and caretakers of God's throne.", "Muslims refer to Seraphim as beings created from celestial fire that are part of an exalted assembly."], "decomposition": ["How is the Seraphim regarded in Christianity?", "How is the Seraphim regarded in Islam?", "Does #1 clearly contrast with #2?"], "evidence": [[[["Seraph-12"]], [["Seraph-20"]], ["operation"]], [[["Seraph-2"]], [["Seraph-20"]], ["operation"]], [[["Christianity in Zambia-47"], "no_evidence"], [["Antisemitism in Islam-33"], "no_evidence"], ["operation"]]], "golden_sentence": [["Medieval Christian theology places seraphim in the highest choir of the angelic hierarchy."], ["Apart from that, Seraphim rarely appear in Islam."]]}, {"qid": "b63e6ad57ae06b66de3b", "term": "Julia Roberts", "description": "American actress and producer", "question": "Did Julia Roberts practice blast beats as a child?", "answer": false, "facts": ["Julia Roberts played the clarinet in her school band.", "Blast beats are a drum beat that originated in hardcore punk and grindcore, and is often associated with certain styles of extreme metal, namely black metal and death metal."], "decomposition": ["What instrument did Julia Roberts play as a child?", "What instrument does Blast Beats simulate?", "Is #1 the same as #2?"], "evidence": [[[["Julia Roberts-7"]], [["Blast beat-1"]], ["operation"]], [[["Julia Roberts-7"]], [["Blast beat-1"]], ["operation"]], [[["Julia Roberts-7"]], [["Blast beat-6"]], ["operation"]]], "golden_sentence": [["She also played the clarinet in her school band."], ["A blast beat is a drum beat that originated in hardcore punk and grindcore, and is often associated with certain styles of extreme metal, namely black metal and death metal, and occasionally in metalcore."]]}, {"qid": "2f35f40d755cf9a0e5e7", "term": "Muslims", "description": "Adherents of Islam", "question": "Can a Muslim eat a McRib sandwich?", "answer": false, "facts": ["Pork products are haram, or forbidden in Islam.", "The McRib is a pork-based sandwich."], "decomposition": ["What foods are Muslims forbidden to eat?", "What is a McRib made of?", "Are #1 and #2 different from each other?"], "evidence": [[[["Islamic dietary laws-14"]], [["McRib-3"]], ["operation"]], [[["Islamic culture-45"]], [["McRib-1"]], ["operation"]], [[["Islamic dietary laws-2"]], [["McRib-3"]], ["operation"]]], "golden_sentence": [[""], ["McRib consists of a restructured boneless pork patty shaped like a miniature rack of ribs, barbecue sauce, onions, and pickles served on a 5\u00a01\u20442 inches (14\u00a0cm) roll."]]}, {"qid": "3790dd60a5c7ba3a5602", "term": "Mental disorder", "description": "Distressing thought or behavior pattern", "question": "Are there mental disorders you can hide?", "answer": true, "facts": ["Many people do not notice depression in their friends or loved ones. ", "\"Masking\" is a phrase used to describe concealing the effects of one's personality, including mental disorder."], "decomposition": ["Do any mental disorders have symptoms/effects that can be hidden?"], "evidence": [[[["Mental disorder-39", "Mental disorder-4"], "no_evidence", "operation"]], [[["Major depressive disorder-2"]]], [[["Mental disorder-6"], "no_evidence"]]], "golden_sentence": [["Alternatively, functioning may be affected by the stress of having to hide a condition in work or school etc., by adverse effects of medications or other substances, or by mismatches between illness-related variations and demands for regularity.", ""]]}, {"qid": "890bc5bf0d6830be931c", "term": "Underworld", "description": "The mythic Relm of the Dead, located far underground (aka, Hades; Underworld)", "question": "Does Hades have a loose grip on the Underworld?", "answer": false, "facts": ["Hades alone can allow passage out of the Underworld.", "Hades created a terribly difficult task for Orpheus to complete to bring Eurydice from the Underworld. ", "The subjects of Hades in the Underworld are under his complete control."], "decomposition": ["In whose power is it solely within to allow passage out of the underworld?", "Whose rule are the subjects in the Underworld under?", "Is #1 or #2 not Hades?"], "evidence": [[[["Hades-11"]], [["Hades-1"]], ["operation"]], [[["Hades-13"]], [["Hades-11"]], ["operation"]], [[["Hades-11"]], [["Hades-11"]], ["operation"]], [[["Hades-1"]], [["Hades-11"]], ["operation"]]], "golden_sentence": [["Hades ruled the dead, assisted by others over whom he had complete authority."], ["Hades received the underworld, Zeus the sky, and Poseidon the sea, with the solid earth, long the province of Gaia, available to all three concurrently."]]}, {"qid": "c39a2ecb21d92c0bf124", "term": "Surfing", "description": "sport that consists of riding a wave", "question": "Was Surfing popular when pogs came out?", "answer": true, "facts": ["Pogs came out in the 1990's.", "The 90's saw a rise in 'Big Wave Culture', a practice involving finding the largest possible waves to surf on."], "decomposition": ["When were Pogs released?", "Did surfing experience a growth in popularity in #1?"], "evidence": [[[["Milk caps (game)-8"]], [["Surf culture-68"]]], [[["Milk caps (game)-3"]], [["Surf culture-68"], "operation"]], [[["Milk caps (game)-1"]], [["Surf culture-28"]]]], "golden_sentence": [["In October 1994, a lawsuit was settled between World Pog and Universal Pogs Association."], [""]]}, {"qid": "49cc24d576a91592b98b", "term": "Upload", "description": "sending of data from a local system to a remote system", "question": "Can Centurylink max internet plan upload 1000GB in a fortnight?", "answer": true, "facts": ["A fortnight refers to a period of two weeks.", "Centurylink's max internet plan speed is 1,000MB per second.", "1000GB takes 2.5 hours to upload for every 1000MB speed."], "decomposition": ["How long is a fortnight?", "What is Centurylink's max internet plan speed?", "With #2, how long would it take to upload 1000GB?", "Is #3 less than #1?"], "evidence": [[[["Counting-9"]], [["CenturyLink-12"]], [["CenturyLink-12"]], ["operation"]], [["no_evidence"], [["CenturyLink-12"], "no_evidence"], ["operation"], ["operation"]], [[["Fortnight (disambiguation)-1"]], [["CenturyLink-12"]], [["Bit-15", "Megabyte-5"], "operation"], ["operation"]]], "golden_sentence": [["For example, the French phrase for \"fortnight\" is quinzaine (15\u00a0[days]), and similar words are present in Greek (\u03b4\u03b5\u03ba\u03b1\u03c0\u03b5\u03bd\u03b8\u03ae\u03bc\u03b5\u03c1\u03bf, dekapenth\u00edmero), Spanish (quincena) and Portuguese (quinzena)."], ["Unlike the company's existing high speed Internet deployments, which utilize fiber-to the node/neighborhood to increase the speed of ADSL2+ speeds up to 20/2 Mbit/s, Vectored VDSL2+ speeds up to 140/10Mbit/s, in these markets CenturyLink now installs their fiber optic cable all the way to the home or business with speeds up to 1,000 Mbit/s download and 1,000 Mbit/s upload using Calix Optical Network Terminals."], ["Unlike the company's existing high speed Internet deployments, which utilize fiber-to the node/neighborhood to increase the speed of ADSL2+ speeds up to 20/2 Mbit/s, Vectored VDSL2+ speeds up to 140/10Mbit/s, in these markets CenturyLink now installs their fiber optic cable all the way to the home or business with speeds up to 1,000 Mbit/s download and 1,000 Mbit/s upload using Calix Optical Network Terminals."]]}, {"qid": "1a2f8e9a0ea655b506c9", "term": "Solomon", "description": "king of Israel and the son of David", "question": "Did Solomon make up bigger percentage of Islamic prophets than Kings of Judah?", "answer": false, "facts": ["According to The Quran, Solomon was one of 25 prophets.", "According to some Islamic hadiths, there have been as many as 124,000 prophets.", "Solomon was one of 20 Kings of Judah."], "decomposition": ["According to the Quran, how many prophets were there?", "How many Kings of Judah were there?", "What is 1 divided by #1?", "What is 1 divided by #2?", "Is #3 greater than #4?"], "evidence": [[[["Prophet-23"]], [["Kings of Judah-14"]], ["operation"], ["operation"], ["operation"]], [[["Prophet-23"]], [["David-1", "Zedekiah-1"], "no_evidence"], ["operation"], ["operation"], ["operation"]], [[["Quran-47"], "no_evidence"], [["Kingdom of Judah-1"]], ["no_evidence", "operation"], ["no_evidence", "operation"], ["no_evidence", "operation"]]], "golden_sentence": [["Although only twenty-five prophets are mentioned by name in the Quran, a hadith (no."], ["By Albright's calculations, Jehu's initial year was 842 BC; and between it and Samaria's destruction the Books of Kings give the total number of the years the kings of Israel ruled as 143 7/12, while for the kings of Judah the number is 165."]]}, {"qid": "f52b8b4d95fc237e698d", "term": "Billy Joel", "description": "American singer-songwriter and pianist", "question": "Has Billy Joel sold out Astana Arena?", "answer": false, "facts": ["Astana Arena is a 30,000 seat football stadium in Kazakhstan.", "Billy Joel, who has performed concerts all over the world, has never been to Kazakhstan."], "decomposition": ["Which country is the Astana Arena located in?", "Has Billy Joel ever been to #1?"], "evidence": [[[["Astana Arena-1"]], ["no_evidence"]], [[["Astana Arena-1"]], ["no_evidence", "operation"]], [[["Astana Arena-1"]], ["no_evidence"]]], "golden_sentence": [["The Astana Arena (Kazakh: \u0410\u0441\u0442\u0430\u043d\u0430 \u0410\u0440\u0435\u043d\u0430) is a football stadium in Nur-Sultan (formerly Astana), Kazakhstan."]]}, {"qid": "528a4ff0ab9fb0f65015", "term": "Good", "description": "Term in religion, ethics, and philosophy", "question": "In star rating systems, is 5 stars considered good?", "answer": true, "facts": ["Most star rating systems are composed of 5 stars.", "In star rating, most people want to avoid a 1 star review."], "decomposition": ["What is the highest rating possible in most star rating systems?", "Is #1 equal to five stars?"], "evidence": [[[["Star (classification)-1"]], ["operation"]], [[["Nutritional rating systems-4"], "no_evidence"], ["operation"]], [[["Star (classification)-1"]], ["operation"]]], "golden_sentence": [["For example, a system of one to five stars is commonly employed to rate hotels, with five stars being the highest quality."]]}, {"qid": "3bab9dbb44bc96425871", "term": "Geometry", "description": "Branch of mathematics that studies the shape, size and position of objects", "question": "Does Siri know geometry?", "answer": true, "facts": ["Geometry is the study of size, shape and distance of objects.", "Determining location requires geometry.", "Siri can determine your location."], "decomposition": ["What are some basic user information that Siri can determine?", "What mathematical concepts must be known in order to determine the location of a point?", "Is location included in #1 and geometry in #2?"], "evidence": [[[["Siri-3"]], [["Location-5"], "no_evidence"], ["operation"]], [[["Siri-3"]], [["Geometry-15"], "no_evidence"], ["operation"]], [[["Global Positioning System-1", "Siri-1", "Siri-11"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Siri supports a wide range of user commands, including performing phone actions, checking basic information, scheduling events and reminders, handling device settings, searching the Internet, navigating areas, finding information on entertainment, and is able to engage with iOS-integrated apps."], ["Because latitude and longitude are expressed relative to these lines, a position expressed in latitude and longitude is also a relative location."]]}, {"qid": "65a23583a6c2764708e0", "term": "Coca", "description": "group of plant varieties cultivated for coca production", "question": "Is a Coca plant farm likely to be found in Yakutsk?", "answer": false, "facts": ["Coca is a plant originating in South America and used as a cash crop.", "Yakutsk is a city in east Siberia.", "The Coca plant grows in a humid tropical climate.", "Siberia has an extremely cold subarctic climate."], "decomposition": ["What kind of climate does the Coca plant grow in?", "What country is Yakutsk located on?", "What is the climate of #2?", "Is #1 the same as #3?"], "evidence": [[[["Coca-16"], "no_evidence"], [["Yakutsk-1"]], [["Yakutsk-2"]], ["operation"]], [[["Coca-16", "Yungas-1"]], [["Yakutsk-1"]], [["Climate of Russia-1", "Russia-84", "Russia-85"]], ["operation"]], [[["Coca-25"]], [["Yakutsk-1"]], [["Yakutsk-2"]], ["operation"]]], "golden_sentence": [[""], ["Yakutsk (Russian: \u042f\u043a\u0443\u0442\u0441\u043a, IPA:\u00a0[j\u026a\u02c8kutsk]; Yakut: \u0414\u044c\u043e\u043a\u0443\u0443\u0441\u043a\u0430\u0439, D'okuuskay, pronounced\u00a0[\u025foku\u02d0skaj]) is the capital city of the Sakha Republic, Russia, located about 450 kilometers (280\u00a0mi) south of the Arctic Circle."], [""]]}, {"qid": "17d69358e10370a4dc42", "term": "Butter", "description": "dairy product", "question": "Does butter industry survive cow extinction?", "answer": true, "facts": ["Butter is a dairy product made from milk.", "Cows are the predominant source of milk in the US.", "Goats, sheep, buffalo, and other mammals produce milk.", "Goat butter, made from 100% Goat's Milk, is an excellent source of Vitamin A."], "decomposition": ["What animal product is butter made from?", "What common livestock animals produce #1?", "Are animals other than cows included in #2?"], "evidence": [[[["Butter-2"]], [["Butter-2"]], [["Butter-2"]]], [[["Butter-10"], "no_evidence"], [["Butter-2"], "no_evidence"], ["operation"]], [[["Butter-1"]], [["Milk-26"]], ["operation"]]], "golden_sentence": [["Most frequently made from cow's milk, butter can also be manufactured from the milk of other mammals, including sheep, goats, buffalo, and yaks."], ["Most frequently made from cow's milk, butter can also be manufactured from the milk of other mammals, including sheep, goats, buffalo, and yaks."], ["Most frequently made from cow's milk, butter can also be manufactured from the milk of other mammals, including sheep, goats, buffalo, and yaks."]]}, {"qid": "23ca2c817dc3d01b8e3b", "term": "Chrome OS", "description": "Linux-based operating system developed by Google", "question": "Is an internet connection essential for someone using Chrome OS?", "answer": true, "facts": ["Most Chromebook apps require internet access to function properly.", "There are apps for the Chromebook that can function properly without internet access.", "To download any apps onto the Chromebook, including offline apps, one must connect to the internet."], "decomposition": ["What are the applications needed for essential functions on devices running Chrome OS?", "Is an internet connection needed for any of #1 to be downloaded or to work properly?"], "evidence": [[[["Chrome OS-1"]], [["Chrome OS-5"], "operation"]], [[["Chrome OS-2"]], ["operation"]], [[["Chrome OS-2", "Chrome OS-3"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "f8c839fb1cc42e1ce821", "term": "The Godfather", "description": "1972 film directed by Francis Ford Coppola", "question": "Was a USB flash drive used in The Godfather?", "answer": false, "facts": ["USB flash drives first appeared on the market in late 2000.", "The Godfather was released in 1972."], "decomposition": ["When did USB flash drives become available?", "When was The Godfather released?", "What time-span was setting of The Godfather based on?", "Is #1 before or within #2 or #3?"], "evidence": [[[["USB flash drive-6"]], [["The Godfather-3"]], [["The Godfather-12"]], [["USB flash drive-7"], "operation"]], [[["USB flash drive-1"]], [["The Godfather-1"]], [["The Godfather (novel)-1"]], ["operation"]], [[["USB flash drive-1"]], [["The Godfather-1"]], [["The Godfather-5"]], ["operation"]]], "golden_sentence": [["IBM became the first to sell USB flash drives in the United States in 2000."], ["The Godfather premiered at the Loew's State Theatre on March 14, 1972, and was released wider in the United States on March 24, 1972."], [""], [""]]}, {"qid": "cac25dbdba899c32fa54", "term": "Goofy", "description": "Disney cartoon character", "question": "Would Goofy hypothetically enjoy Nylabone?", "answer": true, "facts": ["Goofy is a popular Disney cartoon character that is a dog.", "Nylabone is a popular dog bone brand.", "Dogs chew bones for the taste, and to exercise the muscles of the jaw."], "decomposition": ["What kind of animal does Goofy portray?", "What kind of animal are Nylabones made for?", "Are #1 and #2 both dog?"], "evidence": [[[["Goofy-1"]], [["Dog toy-7"]], ["operation"]], [[["Goofy-1"]], [["Dog toy-7"]], ["operation"]], [[["Goofy-1"]], [["Dog toy-7"]], ["operation"]]], "golden_sentence": [["Goofy is a funny animal cartoon character created in 1932 at Walt Disney Productions."], ["The term \"bones\" can include animal bones as well as manufactured bones such as Nylabones and dental bones."]]}, {"qid": "0fb8cba3bada6760132b", "term": "Software engineer", "description": "Practitioner of software engineering", "question": "Can a software engineer work during a power outage?", "answer": false, "facts": ["Software engineers require computers to do their work.", "Computers do not work without electricity.", "A power outage is the temporary lack of electrical power."], "decomposition": ["What is the main equipment software engineers need to do their job?", "What is a power outage?", "Can #1 work without #2?"], "evidence": [[[["Software engineering-2"]], [["Power outage-1"]], [["Computer-46"], "no_evidence"]], [[["Software engineer-1"]], [["Power outage-1"]], ["operation"]], [[["Software engineer-6"]], [["Power outage-5"]], [["Power outage-5"]]]], "golden_sentence": [[""], ["A power outage (also called a power cut, a power out, a power blackout, power failure or a blackout) is the loss of the electrical power network supply to an end user."], [""]]}, {"qid": "cba2ca6ca6d4cbc6c551", "term": "Gallic Wars", "description": "Wars in which the Roman Republic conquered Gaul", "question": "Would Roman Gallic Wars army struggle to build the pyramids faster?", "answer": false, "facts": ["The pyramids were built by an estimated 30,000 workers.", "The Roman Gallic war army had around 75,000 soldiers."], "decomposition": ["How many people worked on the pyramids?", "How many soldiers were in the Roman Gallic war army?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Giza pyramid complex-19"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Egyptian pyramid construction techniques-2", "Egyptian pyramid construction techniques-28"], "no_evidence"], [["Roman legion-24", "Size of the Roman army-7"]], ["operation"]], [[["Giza pyramid complex-31"]], [["Gallic Wars-4", "Size of the Roman army-3"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["When Greek historian Herodotus visited Giza in 450\u00a0BC, he was told by Egyptian priests that \"the Great Pyramid had taken 400,000 men 20 years to build, working in three-month shifts 100,000 men at a time.\""]]}, {"qid": "22873b04573479c9aabf", "term": "Telescope", "description": "Optical instrument that makes distant objects appear magnified", "question": "Can telescopes hear noise?", "answer": false, "facts": ["Telescopes are used to view things far away.", "Telescopes are an optical instrument. "], "decomposition": ["What are the uses of a telescope?", "Does #1 include detecting noise?"], "evidence": [[[["Telescope-1"]], ["operation"]], [[["Telescope-5"]], [["Telescope-5"]]], [[["Telescope-1"]], [["Telescope-1"], "operation"]]], "golden_sentence": [["A telescope is an optical instrument that makes distant objects appear magnified by using an arrangement of lenses or curved mirrors and lenses, or various devices used to observe distant objects by their emission, absorption, or reflection of electromagnetic radiation."]]}, {"qid": "b6a493f0af59a288af81", "term": "Palace of Westminster", "description": "Meeting place of the Parliament of the United Kingdom,", "question": "Are Big Ben's bells currently rung on their normal schedule at the Palace of Westminster?", "answer": false, "facts": ["Big Ben is currently under a four year renovation project.", "The bells have been silenced for the duration of the work due to safety.", "They are only rung on certain holidays, until the construction finishes in 2021."], "decomposition": ["What is the status of Big Ben right now?", "When will #1 be completed?", "Until #2, is it safe for the bells to ring at Big Ben?"], "evidence": [[[["Big Ben-61"]], [["Big Ben-61"]], [["Big Ben-61"]]], [[["Big Ben-61"]], [["Big Ben-61"]], [["Big Ben-61"], "no_evidence"]], [[["Big Ben-5"]], [["Big Ben-5"]], [["Big Ben-46"]]]], "golden_sentence": [["The additional renovation work was not set to derail the completion of the project: Big Ben will resume its usual striking and tolling in 2021."], ["The additional renovation work was not set to derail the completion of the project: Big Ben will resume its usual striking and tolling in 2021."], [""]]}, {"qid": "8b72dad37edadb40f15e", "term": "The Hobbit", "description": "Fantasy novel by J. R. R. Tolkien", "question": "Would a Drow tower over The Hobbit's hero?", "answer": true, "facts": ["The hero of the Hobbit is Bilbo Baggins.", "Bilbo Baggins is a hobbit, which is a race resembling very short humans with furry feet.", "Halfling is another term for hobbits, and halflings are described as being half the size of a human.", "The Drow are a race of dark elves described as being around five feet in height."], "decomposition": ["Who is the hero of The Hobbit", "What is a Drow?", "Is #2 taller than #1?"], "evidence": [[[["Hobbit-12", "Hobbit-2"]], [["Drow-28"]], ["operation"]], [[["The Hobbit-7"]], [["Drow-28"]], ["no_evidence", "operation"]], [[["The Hobbit-2"]], [["Drow-1"]], [["Drow-28", "Hobbit-1"], "operation"]]], "golden_sentence": [["Two hobbits, Bilbo Baggins and the Old Took, are described as living to the age of 130 or beyond, though Bilbo's long lifespan owes much to his possession of the One Ring.", "Hobbits first appeared in the 1937 children's novel The Hobbit, whose titular hobbit is the protagonist Bilbo Baggins, who is thrown into an unexpected adventure involving a dragon."], ["These creatures, later known as the \"dark elvenfolk\" or drow, grew strong in the arcane arts over the centuries and content with their gloomy fairyland beneath the earth, though they still bear enmity towards and seek revenge against their distant kin, the elves and faeries who drove them down."]]}, {"qid": "2e7e17ab9dbf059322f7", "term": "March", "description": "third month in the Julian and Gregorian calendars", "question": "Does March begin on the same day of the week as February during leap years?", "answer": false, "facts": ["During normal years, February has exactly 28 days, so March begins on the same day of the week as February.", "However, on leap years, February has an extra day, so March begins the next day of the week from whichever day started February."], "decomposition": ["How many days are in February in a non-leap year?", "How many days are in February in a leap year?", "Does #1 mean that March will begin on the same day as February?", "Given that #3 is positive, will #2 make no difference to this outcome?"], "evidence": [[[["Leap year-2"]], [["Leap year-3"]], ["no_evidence"], ["operation"]], [[["February-10"]], [["Leap year-2"]], [["Determination of the day of the week-11"], "operation"], [["Determination of the day of the week-11"], "operation"]], [[["February-1"]], [["February-1"]], ["no_evidence", "operation"], ["no_evidence", "operation"]]], "golden_sentence": [["For example, in the Gregorian calendar, each leap year has 366 days instead of 365, by extending February to 29 days rather than the common 28."], ["The term leap year probably comes from the fact that a fixed date in the Gregorian calendar normally advances one day of the week from one year to the next, but the day of the week in the 12 months following the leap day (from March 1 through February 28 of the following year) will advance two days due to the extra day, thus leaping over one day in the week."]]}, {"qid": "e99bfdaf001bb895e375", "term": "QWERTY", "description": "keyboard layout where the first line is \"QWERTYUIOP\"", "question": "Can first letter row of QWERTY keyboard spell a palindrome?", "answer": true, "facts": ["The first letter row of the QWERTY keyboard contains: QWERTYUIOP", "A palindrome is a word that can be spelled the same backwords and forwards such as racecar and rotor."], "decomposition": ["What letters are on the first line of a QWERTY keyboard?", "What vowels are listed in #!?", "What consonants are listed in #1?", "Could a word be formed by repeating the following process: Pick a consonant from #3 then select a vowel from #2. Construct a string of letters from those 2 elements such that the first and third letters are the selected consonant and the second is the vowel.", "If #4 is \"no\" then using the process in Step 4 insert a second occurrence of the same vowel adjacent to the first then choose a consonant from #3 and insert it between the repeated vowel. Is a word formed?"], "evidence": [[[["QWERTY-1"]], ["operation"], ["operation"], ["operation"], ["operation"]], [[["QWERTY-1"], "no_evidence"], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"], ["no_evidence", "operation"]], [[["QWERTY-1"], "no_evidence"], ["operation"], ["operation"], ["operation"], ["operation"]]], "golden_sentence": [["The name comes from the order of the first six keys on the top left letter row of the keyboard (Q W E R T Y)."]]}, {"qid": "353c639b716192c036bc", "term": "British royal family", "description": "Family consisting of close relatives of the monarch of the United Kingdom", "question": "Have any members of the 2020 British royal family allegedly committed a felony?", "answer": true, "facts": ["The 2020 British royal family includes Queen Elizabeth II and her children.", "Prince Andrew is the son of Queen Elizabeth II.", "Prince Andrew was accused of sexual abuse in 2019.", "Sexual assault is classified as a felony."], "decomposition": ["Which royal family does Prince Andrew belong to?", "What is Prince Andrew accused of?", "What type of crime is #2?", "Does #1 have a member accused of #3?"], "evidence": [[[["Prince Andrew, Duke of York-1"]], [["Prince Andrew, Duke of York-22"]], [["Adolescent sexuality in the United States-53"]], [["Prince Andrew, Duke of York-25"]]], [[["Prince Andrew, Duke of York-6"]], [["Prince Andrew, Duke of York-22", "Prince Andrew, Duke of York-25"]], [["Prince Andrew, Duke of York-22"]], [["Prince Andrew, Duke of York-25"], "operation"]], [[["Prince Andrew, Duke of York-1"]], [["Prince Andrew, Duke of York-3"]], [["Felony-6"]], ["operation"]]], "golden_sentence": [["Prince Andrew, Duke of York, KG, GCVO, CD, ADC(P) (Andrew Albert Christian Edward, born 19 February 1960) is a member of the British royal family."], [""], [""], ["The allegations have, as of early 2015, not been tested in any court."]]}, {"qid": "2c27f5d1f55b81ce8f28", "term": "John Kerry", "description": "68th United States Secretary of State", "question": "Did any Golden Globe winners attend John Kerry's alma mater?", "answer": true, "facts": ["John Kerry graduated from Yale University.", "Jennifer Connelly attended Yale University in the late 1980s.", "Jennifer Connelly won a Golden Globe award for the film A Beautiful Mind,"], "decomposition": ["What is John Kerry's alma mater?", "Who has won a Golden Globe?", "Have any of #2 attended #1?"], "evidence": [[[["John Kerry-2"]], [["Meryl Streep-19"], "no_evidence"], [["Meryl Streep-9"], "no_evidence", "operation"]], [[["John Kerry-2"]], [["Claire Danes-2"]], [["Claire Danes-3"]]], [[["John Kerry-9"]], [["Meryl Streep-1"]], [["Meryl Streep-9"], "operation"]]], "golden_sentence": [["He graduated from Yale University in 1966 with a major in political science."], [""], [""]]}, {"qid": "b5d9ba96356d164cb292", "term": "Breathing", "description": "Process of moving air into and out of the lungs", "question": "Is snoring a sign of good breathing while sleeping?", "answer": false, "facts": ["Snoring can be a sign of sleep apnea.", "Snoring can cause a variety of symptoms ranging from low energy to high blood pressure."], "decomposition": ["What medical condition can snoring be a sign of?", "Is #1 considered good?"], "evidence": [[[["Snoring-1"]], ["operation"]], [[["Snoring-1"]], [["Sleep apnea-1"], "operation"]], [[["Snoring-1"]], [["Obstructive sleep apnea-1", "Obstructive sleep apnea-2"]]]], "golden_sentence": [["Snoring during sleep may be a sign, or first alarm, of obstructive sleep apnea (OSA)."]]}, {"qid": "1ae65a585994643f3e32", "term": "Clown", "description": "A comic performer often for children's entertainment", "question": "Would Stephen King fans be likely to own an image of a clown?", "answer": true, "facts": ["Stephen King wrote a popular book called \"It\" about an evil clown.", "\"It\" has been made into two major films and has been merchandised. "], "decomposition": ["Who is the antagonist of popular Stephen King's book 'It'?", "Is #1 a clown?"], "evidence": [[[["It (novel)-1"]], ["operation"]], [[["It (character)-1"]], ["operation"]], [[["It (character)-1"]], [["It (character)-4"]]]], "golden_sentence": [["\"It\" primarily appears in the form of Pennywise the Dancing Clown to attract its preferred prey of young children."]]}, {"qid": "6b4a798cea5eb2305236", "term": "Secretary", "description": "occupation", "question": "Is the US Secretary of State similar to an administrative secretary of an office?", "answer": false, "facts": ["An administrative secretary of an office is hired to handle routine and calendar scheduling for a superior.", "The US Secretary of State is the head of the Department of State.", "The US Secretary of State is analogous to a foreign minister of other countries. ", "The US secretary of state can have administrative assistants. ", "Another name for administrative secretary is administrative assistant. "], "decomposition": ["What kind of duties are assigned to an administrative secretary?", "What are the duties and nature of the position of the US Secretary of State?", "Is #2 in accordance with #1?"], "evidence": [[[["Secretary-3"]], [["United States Secretary of State-4"]], ["operation"]], [[["Secretary-1", "Secretary-3"]], [["Secretary of state-14"]], ["operation"]], [[["Secretary-1"]], [["Secretary of state-14"]], ["operation"]]], "golden_sentence": [["A secretary, also known as a personal assistant (PA) or administrative assistant, has many administrative duties."], ["The stated duties of the secretary of state are to supervise the United States foreign service, immigration policy, and administer the Department of State."]]}, {"qid": "f1026caeb6713448b49c", "term": "Burning Man", "description": "annual experimental festival based in Nevada, United States", "question": "Would it be impossible to get to Burning Man on the Mayflower?", "answer": true, "facts": ["The Mayflower was a seafaring vessel", "Burning Man is held in Black Rock Desert", "There are no bodies of water flowing into the Black Rock Desert"], "decomposition": ["What was the Mayflower?", "What environment does #1 work in?", "What kind of environment is The Burning Man?", "Is #2 different from #3?"], "evidence": [[[["Mayflower-1"]], [["Ship-1"]], [["Burning Man-1"]], ["operation"]], [[["Mayflower-1"]], [["Fluyt-1"]], [["Burning Man-1"]], ["operation"]], [[["Mayflower-1"]], [["Ship-1"]], [["Burning Man-1"]], ["operation"]]], "golden_sentence": [["Mayflower was an English ship that transported the first English Puritans, known today as the Pilgrims, from Plymouth, England to the New World in 1620."], [""], ["Burning Man is an event held annually since 1986 in the western United States at Black Rock City, a temporary city erected in the Black Rock Desert of northwest Nevada, approximately 100 miles (160\u00a0km) north-northeast of Reno and a thriving year-round culture generated by a global community of participants."]]}, {"qid": "62f07dc96e27133bf987", "term": "Hippopotamus", "description": "A large, mostly herbivorous, semiaquatic mammal native to sub-Saharan Africa", "question": "Are hippos dangerous to humans?", "answer": true, "facts": ["Hippos are large and have large teeth.", "If threatened, they aggressively defend themselves."], "decomposition": ["How do hippopotami respond to perceived threats?", "Considering #1 and their prominent physical features, could they hurt humans?"], "evidence": [[[["Hippopotamus-42"]], ["operation"]], [[["Hippopotamus-35", "Hippopotamus-36"]], [["Hippopotamus-42"], "operation"]], [[["Hippopotamus-42"]], [["Hippopotamus-42"]]]], "golden_sentence": [[""]]}, {"qid": "90d173971bb5d3c4ff77", "term": "Sniper", "description": "Highly trained marksman", "question": "Can a sniper shoot a fish past Bathypelagic Zone in ocean?", "answer": false, "facts": ["The Bathypelagic Zone extends 4000 meters down in the ocean.", "The longest recorded sniper kill is 3,540 meters."], "decomposition": ["How deep is the bathypelagic zone?", "How far can snipers shoot?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Bathyal zone-1"]], [["Sniper rifle-9"], "no_evidence"], ["operation"]], [[["Bathyal zone-1"]], [["Longest recorded sniper kills-1"]], ["operation"]], [[["Bathyal zone-1"]], [["Sniper-16"]], ["operation"]]], "golden_sentence": [["The bathyal zone or bathypelagic \u2013 from Greek \u03b2\u03b1\u03b8\u03cd\u03c2 (bath\u00fds), deep \u2013 (also known as midnight zone) is the part of the pelagic zone that extends from a depth of 1,000 to 4,000\u00a0m (3,300 to 13,100\u00a0ft) below the ocean surface."], ["Sniper rifles continue to be adapted and improved upon with the effective range of modern sniper rifles exceeding 1,000\u00a0m (1,094\u00a0yd), which make it one of the most accurate, deadly and efficient weapons in use now."]]}, {"qid": "d7db8a3dfdaa8b856cdb", "term": "Holy Grail", "description": "Cup, dish or stone with miraculous powers, important motif in Arthurian literature", "question": "Has the Holy Grail been featured in at least five films?", "answer": true, "facts": ["1981's Excalibur film features King Arthur and his knights looking for the Holy Grail.", "Monty Python and the Holy Grail spoofs Arthurian legend.", "Indiana Jones and the Last Crusade features a search for the Holy Grail.", "Prince Killian and the Holy Grail focuses on retrieval of the grail.", "The Silver Chalice focuses on a man that has to sculpt the Holy Grail."], "decomposition": ["What movies have featured the Holy Grail?", "Are at least 5 movies listed in #1?"], "evidence": [[[["Indiana Jones and the Last Crusade-1", "Lancelot du Lac (film)-3", "Monty Python and the Holy Grail-2", "The Fisher King-4", "The Light in the Dark-4"]], ["operation"]], [[["Holy Grail-32"]], [["Holy Grail-32"], "operation"]], [[["Holy Grail-32"]], ["operation"]]], "golden_sentence": [["", "", "", "", ""]]}, {"qid": "3bde95cc70b72c7b4dd4", "term": "Autumn", "description": "one of the Earth's four temperate seasons, occurring between summer and winter", "question": "Does American Independence Day occur during autumn?", "answer": false, "facts": ["Autumn runs from about September 20 to about December 20.", "American Independence Day is July 4, over two months before autumn begins."], "decomposition": ["When does autumn occur in North America?", "When is American Independence Day celebrated?", "Is #2 within the range of #1?"], "evidence": [[[["Autumn-1"], "no_evidence"], [["Independence Day (United States)-3"], "no_evidence"], ["operation"]], [[["Autumn-3"]], [["Independence Day (United States)-1"]], ["operation"]], [[["Autumn-1"]], [["Independence Day (United States)-1"]], ["operation"]]], "golden_sentence": [["Autumn marks the transition from summer to winter, in September (Northern Hemisphere) or March (Southern Hemisphere), when the duration of daylight becomes noticeably shorter and the temperature cools considerably."], [""]]}, {"qid": "08491200f493414e090d", "term": "Communist Party of China", "description": "Political party of the People's Republic of China", "question": "Did Karl Marx influence the communist party of China?", "answer": true, "facts": ["Communist ideology is the foundation of communist party of China.", "Marx produced a political pamphlet that has since come to be commonly known as the communist manifesto. "], "decomposition": ["What does the communist party of China stand to represent?", "What were the political activities of Karl Max?", "Does any of #1 have a source/origin in #2?"], "evidence": [[[["Communist Party of China-37"]], [["Das Kapital-3"]], [["Communist Party of China-37", "Das Kapital-3"]]], [[["Communist Party of China-2"]], [["Marxism-1"]], ["operation"]], [[["Communist Party of China-2"]], [["Karl Marx-3"]], ["operation"]]], "golden_sentence": [[""], [""], ["", ""]]}, {"qid": "3bb39c19a8d5916efae1", "term": "Great Lakes", "description": "System of interconnected, large lakes in North America", "question": "Are the Great Lakes part of an international border?", "answer": true, "facts": ["The lakes are bordered on the north by Canada.", "The lakes are bordered on the south by United States.", "Canada and United States are two different countries."], "decomposition": ["What borders the great lakes to the north?", "What borders the great lakes to the south?", "Are #1 and #2 different countries? "], "evidence": [[[["Great Lakes-1", "Great Lakes-19"]], [["Great Lakes-5"]], ["operation"]], [[["Great Lakes-5"]], [["Great Lakes-5"]], ["no_evidence"]], [[["Great Lakes-1"]], [["Great Lakes-1"]], ["operation"]]], "golden_sentence": [["The Great Lakes (French: les Grands Lacs), or the Great Lakes of North America, are a series of interconnected freshwater lakes in the upper mid-east part of North America, on the Canada\u2013United States border, which connect to the Atlantic Ocean through the Saint Lawrence River.", "Of the total 10,500 miles (16,900\u00a0km) of shoreline, Canada borders approximately 5,200 miles (8,400\u00a0km), while the remaining 5,300 miles (8,500\u00a0km) are bordered by the United States."], ["Both the province of Ontario and the state of Michigan include in their boundaries portions of four of the lakes: The province of Ontario does not border Lake Michigan, and the state of Michigan does not border Lake Ontario."]]}, {"qid": "b915f3106524fca790b9", "term": "Curiosity (rover)", "description": "American robotic rover exploring the crater Gale on Mars", "question": "Can Curiosity (rover) kill a cat?", "answer": true, "facts": ["Cats weigh on average between 7 to 10 pounds.", "Curiosity (rover), a space vehicle that explores Mars, weighs 1,982 pounds.", "As mass falls, it picks up acceleration and adds to the force of impact."], "decomposition": ["How much does a cat weigh?", "How much does Curiosity (rover) weigh?", "Is #2 more than #1?"], "evidence": [[[["Cat-28"], "no_evidence"], [["Curiosity (rover)-33"]], ["operation"]], [[["Cat-29"]], [["Curiosity (rover)-33"]], ["operation"]], [[["Meow (cat)-2"], "no_evidence"], [["Curiosity (rover)-6"], "no_evidence"], ["operation"]]], "golden_sentence": [["It averages about 46\u00a0cm (18\u00a0in) in head-to-body length and 23\u201325\u00a0cm (9.1\u20139.8\u00a0in) in height, with about 30\u00a0cm (12\u00a0in) long tails."], ["Curiosity is 2.9\u00a0m (9.5\u00a0ft) long by 2.7\u00a0m (8.9\u00a0ft) wide by 2.2\u00a0m (7.2\u00a0ft) in height, larger than Mars Exploration Rovers, which are 1.5\u00a0m (4.9\u00a0ft) long and have a mass of 174\u00a0kg (384\u00a0lb) including 6.8\u00a0kg (15\u00a0lb) of scientific instruments."]]}, {"qid": "4bb2809c391f61f90403", "term": "Albany, Georgia", "description": "City in Georgia, United States", "question": "Is Albany, Georgia the most populous US Albany?", "answer": false, "facts": ["Albany, Georgia had a population of 75,249 in 2018.", "Albany, New York had a population of 97,279 in 2018."], "decomposition": ["Which places are known as Albany in the United States?", "What are the respective populations of #1?", "Is the population of Albany, Georgia the greatest of #2?"], "evidence": [[[["Albany, Georgia-1", "Albany, New York-2"]], [["Albany, Georgia-1", "Albany, New York-2"]], ["operation"]], [[["Albany, New York-1"]], [["Albany, New York-2"]], [["Albany, Georgia-1"]]], [[["Albany, Georgia-1", "Albany, New York-2"]], [["Albany, Georgia-1", "Albany, New York-2"]], ["operation"]]], "golden_sentence": [["", "Albany constitutes the economic and cultural core of the Capital District of New York State, which comprises the Albany\u2013Schenectady\u2013Troy, NY Metropolitan Statistical Area, including the nearby cities and suburbs of Troy, Schenectady, and Saratoga Springs."], ["The population was 77,434 at the 2010 U.S. Census, making it the eighth-largest city in the state.", ""]]}, {"qid": "7877c644f5f961670805", "term": "Eiffel Tower", "description": "Tower located on the Champ de Mars in Paris, France", "question": "Was King Kong climbing at a higher altitude than Eiffel Tower visitors?", "answer": true, "facts": ["The Eiffel Tower is 984 ft high, and the visitor platform is 906 ft high.", "King Kong climbed up to the top of the Empire State Building.", "The Empire State Building is 1230 ft high."], "decomposition": ["How high is the visitor platform at the Eiffel Tower?", "What is the height of the Empire State Building?", "Is #2 higher than #1?"], "evidence": [[[["Eiffel Tower-4"]], [["Empire State Building-1"]], ["operation"]], [[["Eiffel Tower-4"]], [["Empire State Building-1", "Empire State Building-28"]], ["operation"]], [[["Eiffel Tower-4"]], [["Empire State Building-1"]], ["operation"]]], "golden_sentence": [["The top level's upper platform is 276\u00a0m (906\u00a0ft) above the ground \u2013 the highest observation deck accessible to the public in the European Union."], ["The Empire State Building is a 102-story Art Deco skyscraper in Midtown Manhattan in New York City."]]}, {"qid": "0c8c2ed4dd54beef5062", "term": "Constitution of the United States", "description": "Supreme law of the United States of America", "question": "Would Constitution of the United States paper offend PETA?", "answer": true, "facts": ["The Constitution of the United States is written on parchment.", "Parchment is  writing material made from specially prepared untanned skins of animals.", "PETA is an organization that advocates for the ethical treatment of animals."], "decomposition": ["What is the US Constitution written on?", "What is #1 made of?", "What does PETA hate?", "How is #2 acquired?", "Is there an overlap between #3 and #4?"], "evidence": [[[["Vellum-1"], "no_evidence"], [["Vellum-1"]], [["People for the Ethical Treatment of Animals-1"]], [["Vellum-7"]], ["operation"]], [[["Constitution of the United States-2"]], [["Parchment-1"]], [["Killing of animals-28"]], [["Parchment-1"]], ["operation"]], [[["U.S. Constitution hemp paper hoax-1"]], ["operation"], [["Lauren Anderson (model)-3"], "no_evidence"], [["Hot dog-12"]], ["operation"]]], "golden_sentence": [["Parchment is another term for this material, and if vellum is distinguished from this, it is by vellum being made from calfskin, as opposed to that from other animals, or otherwise being of higher quality."], ["Parchment is another term for this material, and if vellum is distinguished from this, it is by vellum being made from calfskin, as opposed to that from other animals, or otherwise being of higher quality."], ["Its slogan is \"Animals are not ours to eat, wear, experiment on, use for entertainment, or abuse in any other way.\""], [""]]}, {"qid": "ae9c385f3ae82da8bebc", "term": "Cream", "description": "Dairy product", "question": "If you bottle your own milk, would there be cream on top of it?", "answer": true, "facts": ["Milk that has been bottled straight from a cow has not been homogenized. ", "Homogenization causes the fats in milk to become emulsified.", "Non-homogenized milk will feature fats that separate and float to the top.", "The fats in non-homogenized milk are cream."], "decomposition": ["When milk is taken directly from a cow, what appearance and position do the fats assume?", "Is #1 cream and at the top?"], "evidence": [[[["Cream-1"]], ["operation"]], [[["Milk-59"]], ["operation"]], [[["Milk-53", "Milk-59"]], [["Milk-59"]]]], "golden_sentence": [[""]]}, {"qid": "6de03c355801dc7e101f", "term": "Nine Inch Nails", "description": "American industrial rock band", "question": "Is Nine Inch Nails's lead singer associated with David Lynch?", "answer": true, "facts": ["David Lynch is a director that created the television show Twin Peaks.", "Trent Reznor is the lead singer of Nine Inch Nails.", "Trent Reznor appeared on Twin Peaks: The Return in 2017.", "David Lynch directed the music video for Nine Inch Nail's Came Back Haunted."], "decomposition": ["Who is the lead singer of Nine Inch Nails?", "What works has #1 appeared in?", "What are the works of David Lynch?", "Is there overlap between #2 and #3?"], "evidence": [[[["Trent Reznor-1"]], [["Trent Reznor-29"]], [["Trent Reznor-29"]], ["operation"]], [[["Trent Reznor-1"]], [["Trent Reznor-29"], "no_evidence"], [["David Lynch-1"], "no_evidence"], ["operation"]], [[["Nine Inch Nails-1"]], [["Part 8 (Twin Peaks)-13", "Trent Reznor-3"], "no_evidence"], [["David Lynch-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["He is the founder, lead vocalist, and principal songwriter of the industrial rock band Nine Inch Nails, which he founded in 1988 and of which he was the sole official member until adding long-time collaborator Atticus Ross as a permanent member in 2016."], [""], ["He produced two pieces of the film's score, \"Driver Down\" and \"Videodrones; Questions\", with Peter Christopherson."]]}, {"qid": "7d6d8d0a26f0446ab2bb", "term": "Pan (god)", "description": "Ancient Greek god of the wilds, shepherds, and flocks", "question": "Is Pan a symbol of virtue and virginity in women?", "answer": false, "facts": ["Pan is famous for his sexual powers.", "Women who had had sexual relations with several men were referred to as \"Pan girls.\""], "decomposition": ["What was the nature of Pan's relation with women?", "Is #1 not sexual?"], "evidence": [[[["Pan (god)-17"]], [["Pan (god)-17", "Pan (god)-19"]]], [[["Pan (god)-17", "Pan (god)-18"]], ["operation"]], [[["Pan (god)-17", "Pan (god)-18"], "no_evidence"], [["Virginity-1"], "operation"]]], "golden_sentence": [[""], ["", ""]]}, {"qid": "54379f9d60b6ca52528a", "term": "Black Sea", "description": "Marginal sea of the Atlantic Ocean between Europe and Asia", "question": "Do people put creatures from the Black Sea on their pizza?", "answer": true, "facts": ["Pizza toppings include pepperoni, sausage, bacon, meatball, and anchovies.", "The Black Sea is home to many animals including dogfish, jellyfish, and anchovies."], "decomposition": ["What creatures are native to the Black Sea?", "What are common pizza toppings?", "Do any of #1 appear in #2?"], "evidence": [[[["Zebra mussel-14"]], [["Pizza-1"]], ["operation"]], [[["Black Sea-38", "Black Sea-41", "Black Sea-64"]], [["Pizza-1"]], ["operation"]], [[["Anchovy-2"], "no_evidence"], [["Anchovies as food-3"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], ["Pizza (Italian:\u00a0[\u02c8pittsa], Neapolitan:\u00a0[\u02c8pitts\u0259]) is a savory dish of Italian origin, consisting of a usually round, flattened base of leavened wheat-based dough topped with tomatoes, cheese, and often various other ingredients (anchovies, olives, meat, etc.)"]]}, {"qid": "96a25a2697740040d67d", "term": "Wool", "description": "Textile fibre from the hair of sheep or other mammals", "question": "Should wool be hand washed only?", "answer": true, "facts": ["Felting is a process through which wool is shaped and shrunken through agitation in soapy water. ", "Felting will often occur if you put a wool item in the washer."], "decomposition": ["What is felting?", "Will washing wool cause #1?"], "evidence": [[[["Felt-8"]], [["Felt-9"], "operation"]], [[["Felt-8", "Wool-5"], "no_evidence"], ["operation"]], [[["Sense-45"]], [["Wool-3"]]]], "golden_sentence": [["In the wet felting process, hot water is applied to layers of animal hairs, while repeated agitation and compression causes the fibers to hook together or weave together into a single piece of fabric."], [""]]}, {"qid": "f097f75155f952cb9ce7", "term": "Paratrooper", "description": "Military parachutists functioning as part of an airborne force", "question": "Are paratroopers good at mountain rescue?", "answer": true, "facts": ["A paratrooper is a member of a military unit that deploys parachutes. ", "A PJ is the acronym name for a military parachute jumper.", "PJs are an elite mountain rescue unit. "], "decomposition": ["What military unit do paratroopers belong to?", "Do #1 use equipment that makes them suitable for mountain rescue?"], "evidence": [[[["Paratrooper-1"]], [["Paratrooper-2"], "no_evidence"]], [[["Paratrooper-1"]], ["no_evidence", "operation"]], [[["Paratrooper-74"]], [["Paratrooper-1"], "no_evidence"]]], "golden_sentence": [["A paratrooper is a military parachutist\u2014someone trained to parachute into an operation, and usually functioning as part of an airborne force."], [""]]}, {"qid": "83b2bd78e0724d738a33", "term": "Berlin University of the Arts", "description": "public art school in Berlin, Germany", "question": "Is the Berlin University of the Arts a Baroque period relic?", "answer": true, "facts": ["The Berlin University of the Arts is a German school created in 1696.", "The Baroque period lasted in Europe from 1600 to 1750."], "decomposition": ["In which year was the Berlin University of the Arts established?", "When the Baroque period start and end?", "Does #1 fall within #2?"], "evidence": [[[["Berlin University of the Arts-5", "Prussian Academy of Arts-1"]], [["17th century-1", "Baroque-1"]], ["operation"]], [[["Berlin University of the Arts-5"], "no_evidence"], [["Baroque music-1", "Baroque-1"]], ["no_evidence", "operation"]], [[["Berlin University of the Arts-5"]], [["Baroque-1"]], ["operation"]]], "golden_sentence": [["", "The Prussian Academy of Arts (German: Preu\u00dfische Akademie der K\u00fcnste) was a state arts academy first established in Berlin, Brandenburg, in 1694/1696 by prince-elector Frederick III, in personal union Duke Frederick I of Prussia, and later king in Prussia."], ["It falls into the Early Modern period of Europe and in that continent (whose impact on the world was increasing) was characterized by the Baroque cultural movement, the latter part of the Spanish Golden Age, the Dutch Golden Age, the French Grand Si\u00e8cle dominated by Louis XIV, the Scientific Revolution, the world's first public company and megacorporation known as the Dutch East India Company, and according to some historians, the General Crisis.", "The Baroque (UK: /b\u0259\u02c8r\u0252k/, US: /b\u0259\u02c8ro\u028ak/; French:\u00a0[ba\u0281\u0254k]) is a style of architecture, music, dance, painting, sculpture and other arts that flourished in Europe from the early 17th century until the 1740s."]]}, {"qid": "6a230baa54942d85e76d", "term": "French people", "description": "People from France", "question": "Can a student from Smithtown's Cleary School understand the speech of a French person?", "answer": false, "facts": ["French is a romance language that originated in France.", "The Cleary School in Smithtown New York is a school for the deaf."], "decomposition": ["The Cleary School in Smithtown, New York is for students with which disability?", "Can a person with #1 perceive or understand speech?"], "evidence": [[[["Deaf education-25"]], ["operation"]], [[["Deaf education-25", "Hearing loss-1"]], ["operation"]], [[["Deaf education-25"]], [["Physical disability-7"], "operation"]]], "golden_sentence": [["Examples of auditory/verbal K-12 programs in the United States include Clarke Schools for Hearing and Speech, Cleary School for the Deaf, and Memphis Oral School for the Deaf."]]}, {"qid": "c89afeed030f407e9615", "term": "Nostradamus", "description": "16th-century French apothecary and reputed seer", "question": "Would Dante have hypothetically placed Nostradamus in 3rd Circle of Hell?", "answer": false, "facts": ["Nostradamus was a famous seer and court astrologer.", "Dante's 3rd Circle of Hell is reserved for gluttons.", "The 8th Circle of Hell is reserved for frauds.", "Astrology was seen as a valuable skill during Dante's lifetime.", "Dante places the mystic and prophet Joachim of Flora in the heaven of the sun."], "decomposition": ["What type of people did Dante put in the 3rd Circle of Hell?", "Is there evidence Nostradamus was #1?"], "evidence": [[[["Inferno (Dante)-33"]], ["no_evidence", "operation"]], [[["Inferno (Dante)-32"]], ["no_evidence", "operation"]], [[["Inferno (Dante)-32"]], [["Nostradamus-13"]]]], "golden_sentence": [[""]]}, {"qid": "726f8235b2247b0de5f1", "term": "Rabbi", "description": "teacher of Torah in Judaism", "question": "Can a rabbi save the soul of a Christian?", "answer": true, "facts": ["Rabbis are spiritual leaders of the Jewish community.", "Baptism is a necessary Christian ritual for salvation.", "Any non-Christian can perform an emergency baptism if someone's life is in danger."], "decomposition": ["What are Rabbis's role in the Jewish community?", "What is a Christian ritual that is needed for salvation?", "In an emergency, can #1 perform #2?"], "evidence": [[[["Rabbi-1"]], [["Salvation in Christianity-5"]], [["Salvation in Christianity-2"]]], [[["Rabbi-1"]], [["Baptism-2", "Last rites-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Rabbi-1"]], [["Baptism-1"]], [["Baptism-59"], "no_evidence"]]], "golden_sentence": [["A rabbi is a spiritual leader or religious teacher in Judaism."], ["Salvation in Christianity, or deliverance or redemption, is the \"saving [of] human beings from death and separation from God\" by Christ's death and resurrection."], [""]]}, {"qid": "6eede6e7b43a3316cc3c", "term": "Seven Years' War", "description": "Global conflict between 1756 and 1763", "question": "Could the Austrian casualties from Seven Years' War fit in Indianapolis Motor Speedway?", "answer": true, "facts": ["There were 373,588 Austrian casualties during the Seven Years' War.", "The infield seating at the Indianapolis Motor Speedway raises capacity to an approximate 400,000 people."], "decomposition": ["How many casualties did the Austrian's have in the Seven Year War?", "What is the seating capacity for the Indianapolis Motor Speedway?", "Is #1 less than #2?"], "evidence": [[[["Battle of Prague (1757)-1"], "no_evidence"], [["Indianapolis Motor Speedway-3"]], ["operation"]], [["no_evidence"], [["Indianapolis Motor Speedway-3"]], ["operation"]], [[["Seven Years' War-36"], "no_evidence"], [["Indianapolis Motor Speedway-3"]], ["no_evidence", "operation"]]], "golden_sentence": [["In the Battle of Prague or Battle of \u0160t\u011brboholy, fought on 6 May 1757 during the Third Silesian War (Seven Years' War), Frederick the Great's 67,000 Prussians forced 60,000 Austrians to retreat, but having lost 14,300 men, decided he was not strong enough to attack Prague."], ["With a permanent seating capacity of 257,325, it is the highest-capacity sports venue in the world."]]}, {"qid": "0d4011e886512c423b9f", "term": "Butler", "description": "male domestic worker in charge of all the male household staff", "question": "Did the butler Eugene Allen retire the same year a centuries-old war ended?", "answer": true, "facts": ["Eugene Allen was a butler at the White House for 34 years until 1986", "The United Kingdom and the Kingdom of the Netherlands ended the Three Hundred and Thirty Five Years' Warnin 1986"], "decomposition": ["In what year did Eugene Allen retire?", "Which war ended in #1?", "How many years did #2 last?", "Is #3 greater than 100?"], "evidence": [[[["Eugene Allen-6"]], [["Three Hundred and Thirty Five Years' War-1"]], [["Three Hundred and Thirty Five Years' War-1"]], ["operation"]], [[["Eugene Allen-1"]], [["Three Hundred and Thirty Five Years' War-1"]], [["Three Hundred and Thirty Five Years' War-1"]], ["operation"]], [[["Eugene Allen-1"]], [["Three Hundred and Thirty Five Years' War-1"]], [["Three Hundred and Thirty Five Years' War-1"]], ["operation"]]], "golden_sentence": [["He retired in 1986."], ["The Three Hundred and Thirty Five Years' War (Dutch: Driehonderdvijfendertigjarige Oorlog, Cornish: Bell a dri hans pymthek warn ugens) was an alleged state of war between the Netherlands and the Isles of Scilly (located off the southwest coast of Great Britain), and its existence is disputed."], ["It is said to have been extended by the lack of a peace treaty for 335 years without a single shot being fired, which would make it one of the world's longest wars, and a bloodless war."]]}, {"qid": "b3e2d71e0ce45508cbdf", "term": "Van Morrison", "description": "Northern Irish singer-songwriter and musician", "question": "Does title of Van Morrison's most played song apply to a minority of women worldwide?", "answer": false, "facts": ["Van Morrison's most played song was the hit Brown Eyed Girl.", "Between 55 to 79 percent of people worldwide have brown eyes.", "Brown is the most common eye color."], "decomposition": ["What is Van Morrison's most played song?", "What percentage of women worldwide meet the description in #1?", "Is #2 less than 50%?"], "evidence": [[[["Van Morrison-1"]], [["Eye color-11"], "no_evidence"], ["no_evidence", "operation"]], [[["Brown Eyed Girl-7"]], [["Brown-20"], "no_evidence"], ["operation"]], [[["Brown Eyed Girl-7"]], [["Eye color-2"], "no_evidence"], ["no_evidence"]]], "golden_sentence": [["Van Morrison rose to prominence in the mid-1960s as the lead singer of the Northern Irish R&B band, Them, with whom he recorded the garage band classic \"Gloria\"."], [""]]}, {"qid": "48bbd1567bd6bb68ec3c", "term": "Hanuman", "description": "The divine monkey companion of Rama in Hindu mythology", "question": "Does Hanuman have some of the same duties as Athena?", "answer": true, "facts": ["Hanuman, the divine Hindu monkey, is a god of strength, knowledge, and victory.", "Athena was the Greek goddess of war, and wisdom."], "decomposition": [" Hanuman said to be the deity of what subjects?", "Athena said to be the deity of what subjects?", "Is at least one subject listed in both #1 and #2?"], "evidence": [[[["Hanuman-14"]], [["Athena-1"]], ["operation"]], [[["Hanuman-2"], "no_evidence"], [["Athena-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Hanuman-2"]], [["Athena-1"]], ["operation"]]], "golden_sentence": [["Hanuman evolved and emerged in this era as the ideal combination of shakti and bhakti."], ["Athena or Athene, often given the epithet Pallas, is an ancient Greek goddess associated with wisdom, handicraft, and warfare who was later syncretized with the Roman goddess Minerva."]]}, {"qid": "61953d73cfc340f6d6ee", "term": "Nissan", "description": "Japanese automobile manufacturer", "question": "Do workers at Nissan's headquarters eat with chopsticks?", "answer": true, "facts": ["Nissan's headquarters are located in Yokohama, Japan.", "It is customary to eat with chopsticks in East Asian countries.", "Japan is a country in East Asia."], "decomposition": ["Where is Nissan's headquarters located?", "Do people living in #1 usually eat with chopsticks?"], "evidence": [[[["Nissan-1", "Yokohama-1"]], [["Chopsticks-1"]]], [[["Nissan-1"]], [["Chopsticks-1", "Etiquette in Japan-21"]]], [[["Nissan-3"]], [["Chopsticks-1"], "operation"]]], "golden_sentence": [["Nissan Motor Co., Ltd. (Japanese: \u65e5\u7523\u81ea\u52d5\u8eca\u682a\u5f0f\u4f1a\u793e, Hepburn: Nissan Jid\u014dsha Kabushiki-gaisha), usually shortened to Nissan (US: /\u02c8ni\u02d0s\u0251\u02d0n/, UK: /\u02c8n\u026as\u00e6n/ or AU/NZ: /\u02c8n\u026as\u0259n/; Japanese pronunciation: [\u0272issa\u0274]), is a Japanese multinational automobile manufacturer headquartered in Nishi-ku, Yokohama.", "It lies on Tokyo Bay, south of Tokyo, in the Kant\u014d region of the main island of Honshu."], ["Chopsticks are shaped pairs of equal-length sticks that have been used as kitchen and eating utensils in virtually all of East Asia for over three millennia."]]}, {"qid": "ddada84580fb2edcff1e", "term": "Edward II of England", "description": "14th-century King of England and Duke of Aquitaine", "question": "Would Edward II of England have been born without Vikings?", "answer": false, "facts": ["Edward II was King of England from 1307-1327.", "Rollo was a Viking who became the first ruler of Normandy.", "William the Conqueror was the first Norman King of England and the great-great-great-grandson of Rollo.", "Edward II is related to William the Conqueror through Rollo's granddaughter's line (Empress Matilda)."], "decomposition": ["What descendant of Vikings conquered England?", "What realm did #1 hail from?", "Who was the first ruler of #2?", "Is Edward II of England descended from #3?"], "evidence": [[[["Normandy-10"]], [["William the Conqueror-1"]], [["House of Normandy-1"]], [["Edward II of England-1"]]], [[["Harold Godwinson-1"], "no_evidence"], [["Normans-1"], "no_evidence"], ["no_evidence"], [["Edward II of England-1"], "no_evidence", "operation"]], [[["William the Conqueror-1"]], [["William the Conqueror-8"]], [["Rollo-1"]], [["Rollo-23"], "no_evidence", "operation"]]], "golden_sentence": [["Rollo's descendant William became king of England in 1066 after defeating Harold Godwinson, the last of the Anglo-Saxon kings, at the Battle of Hastings, while retaining the fiefdom of Normandy for himself and his descendants."], [""], ["The house emerged from the union between the Viking Rollo (first ruler of the Duchy of Normandy) and Poppa of Bayeux, a West Frankish noblewoman."], [""]]}, {"qid": "91bf5337fc2016124b93", "term": "Black swan", "description": "species of bird", "question": "Do black swan cygnets typically know both of their genetic parents?", "answer": false, "facts": ["Up to one-quarter of monogamous black swan pairs are same-sex, with males taking over a female's nest to raise her young without her", "One-third of monogamous black swan pairs show extra-pair paternity, with the male aiding the female in raising another male's offspring ", "A cygnet is a juvenile swan"], "decomposition": ["What kinds of adults take on the responsibility of raising black swan cygnets?", "Is there an insignificant chance of #1 not being both genetic parents of the cygnets they raise?"], "evidence": [[[["Black swan-1"], "no_evidence"], [["Black swan-11"], "operation"]], [[["Black swan-22", "Black swan-23"]], ["operation"]], [[["Black swan-22", "Black swan-23"]], [["Black swan-22"], "no_evidence"]]], "golden_sentence": [[""], [""]]}, {"qid": "ef2f44bf4222ba552ce6", "term": "Painting", "description": "Practice of applying paint, pigment, color or other medium to a surface", "question": "Is the most recent Democrat President in the US known for his painting practice?", "answer": false, "facts": ["George W. Bush is a former Republican President of the US.", "George W. Bush posts photos of his paintings online.", "Barack Obama succeeded George W. Bush as a Democrat President.", "Barack Obama doesn't post photos of paintings he has made."], "decomposition": ["Who is the most recent Democrat President in the US?", "Was #1 well known for painting?"], "evidence": [[[["Democratic Party (United States)-4"]], ["no_evidence", "operation"]], [[["Barack Obama-65"]], [["Barack Obama-128", "George W. Bush-157"]]], [[["Barack Obama-1"]], ["no_evidence"]]], "golden_sentence": [["The most recent was Barack Obama, who was the 44th and held office from 2009 to 2017."]]}, {"qid": "40498c70a962ec8ca95f", "term": "Pickled cucumber", "description": "Cucumber pickled in brine, vinegar, or other solution", "question": "Could pickled cucumbers from 1,000 years ago be good still?", "answer": false, "facts": ["Pickled foods disintegrate over time.", "Pickling and preserving technology from 1,000 years ago was not airtight or made for longevity."], "decomposition": ["What happens to picked foods over time?", "Was technology available 1000 years ago to prevent #1?"], "evidence": [[[["Pickling-2", "Pickling-35"]], [["Refrigeration-1"]]], [[["Pickling-35"], "no_evidence"], [["Refrigeration-26"], "operation"]], [[["Pickling-2"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Pickling can preserve perishable foods for months.", ""], [""]]}, {"qid": "a58989dc59eeb11d38af", "term": "Royal Society", "description": "National academy of science in the United Kingdom", "question": "Can numerologists become members of Royal Society?", "answer": false, "facts": ["The royal society fulfills a number of roles: promoting science and its benefits, recognizing excellence in science, supporting outstanding science, providing scientific advice for policy.", "Numerology is a superstition and a pseudoscience that uses numbers to give the subject a veneer of scientific authority."], "decomposition": ["What is the primary basis for being selected as a member of the Royal Society?", "What do numerologists do?", "Is #2 included in #1?"], "evidence": [[[["Royal Society-24"]], [["Numerology-3"]], ["operation"]], [[["Royal Society-24"]], [["Numerology-1"]], ["operation"]], [[["Royal Society-17"]], [["Numerology-3"]], ["operation"]]], "golden_sentence": [["The society's core members are the fellows: scientists and engineers from the United Kingdom and the Commonwealth selected based on having made \"a substantial contribution to the improvement of natural knowledge, including mathematics, engineering science and medical science\"."], ["For example, in his 1997 book Numerology: Or What Pythagoras Wrought, mathematician Underwood Dudley uses the term to discuss practitioners of the Elliott wave principle of stock market analysis."]]}, {"qid": "a78f89cd5ebc6f97b11e", "term": "Cream", "description": "Dairy product", "question": "If someone is lactose intolerant, do they have to avoid cream?", "answer": true, "facts": ["People with lactose intolerance are unable to fully digest the sugar (lactose) in milk.", "Cream is a dairy product composed of the higher-fat layer skimmed from the top of milk before homogenization", "Cream contains milk."], "decomposition": ["What do people who are lactose intolerant have to avoid?", "Does cream contain #1?"], "evidence": [[[["Lactose intolerance-1"]], [["Cream-1", "Milk-51"]]], [[["Lactose intolerance-1"]], [["Cream-1"], "operation"]], [[["Lactose intolerance-1"]], [["Cream-1"], "operation"]]], "golden_sentence": [[""], ["", ""]]}, {"qid": "39ed61da370858c64395", "term": "United States Naval Academy", "description": "The U.S. Navy's federal service academy", "question": "Could Jamie Brewer have attended the United States Naval Academy?", "answer": false, "facts": ["Jamie Brewer is a famous actress with down syndrome.", "Individuals with down syndrome are disqualified from military service."], "decomposition": ["What genetic disorder does Jamie Brewer have?", "Are individuals with #1 allowed to be in the US Naval Academy?"], "evidence": [[[["New York Fashion Week-26"]], ["no_evidence"]], [[["Jamie Brewer-7"]], [["Basic Military Qualification-3"], "no_evidence", "operation"]], [[["Jamie Brewer-3"]], [["United States Naval Academy-99"]]]], "golden_sentence": [["In that week, actress Jamie Brewer became the first woman with Down syndrome to walk the red carpet at New York Fashion Week, which she did for designer Carrie Hammer."]]}, {"qid": "bb12b05bbe4a76f6039f", "term": "Apartheid", "description": "System of institutionalised racial segregation that existed in South Africa and South West Africa (Namibia) from 1948 until the early 1990s", "question": "Did Elle Fanning play an essential part in ending apartheid?", "answer": false, "facts": ["Apartheid lasted from 1948 until the early 1990s.", "Actress Elle Fanning was born on April 9, 1998."], "decomposition": ["When was Actress Elle Fanning born?", "Through which period did the Apartheid last?", "Is #1 before or within #2?"], "evidence": [[[["Elle Fanning-1"]], [["Apartheid-1"]], ["operation"]], [[["Elle Fanning-5"]], [["Apartheid-153"]], ["operation"]], [[["Elle Fanning-1"]], [["Apartheid-1"]], ["operation"]]], "golden_sentence": [["Mary Elle Fanning (born April 9, 1998) is an American actress."], ["\"aparthood\") was a system of institutionalised racial segregation that existed in South Africa and South West Africa (now Namibia) from 1948 until the early 1990s."]]}, {"qid": "039b9b5991e111ec5c00", "term": "Twenty-third Amendment to the United States Constitution", "description": "Grants residents of Washington, D.C. the right to vote in U.S. presidential elections", "question": "Did the 23rd amendment give Puerto Ricans the right to vote for president?", "answer": false, "facts": ["The 23rd Amendment to the Constitution gave residents of Washington D.C. the right to vote in presidential elections.", "Puerto Rico is an American territory, not a state, and does not have the right to vote for president.", "Puerto Rico is not Washington D.C."], "decomposition": ["Which US state was given the right to vote by the 23rd Amendment?", "Is Is Puerto Rico a US state or the same as #1?"], "evidence": [[[["Twenty-third Amendment to the United States Constitution-1"]], [["Puerto Rico-1"], "operation"]], [[["Twenty-third Amendment to the United States Constitution-1"]], [["Puerto Rico-1"], "operation"]], [[["Twenty-third Amendment to the United States Constitution-1"]], [["Puerto Rico-1"]]]], "golden_sentence": [[""], ["'Free Associated State of Puerto Rico') and in previous centuries called Porto Rico in English, is an unincorporated territory of the United States located in the northeast Caribbean Sea, approximately 1,000 miles (1,600\u00a0km) southeast of Miami, Florida."]]}, {"qid": "6ed3fb051bb34ac3e0d0", "term": "Edward Snowden", "description": "American whistleblower and former National Security Agency contractor", "question": "Could Edward Snowden join MENSA?", "answer": true, "facts": ["Snowden scored above 145 on two separate IQ tests.", "The minimum accepted IQ score for MENSA on the Stanford\u2013Binet is 132, while for the Cattell it is 148."], "decomposition": ["What is the minimum accepted IQ score to be admitted to MENSA?", "What is Edward Snowden's IQ?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Mensa International-5"]], [["Edward Snowden-5"]], ["operation"]], [[["Mensa International-5"]], [["Edward Snowden-5"]], [["Edward Snowden-5", "IQ Award-1"], "operation"]], [[["Mensa International-5"]], [["Edward Snowden-5"]], ["operation"]]], "golden_sentence": [["Mensa's requirement for membership is a score at or above the 98th percentile on certain standardised IQ or other approved intelligence tests, such as the Stanford\u2013Binet Intelligence Scales."], [""]]}, {"qid": "f2cf2ae28db97b8c8d9e", "term": "French people", "description": "People from France", "question": "Were French people involved in the American Civil War?", "answer": true, "facts": ["The French General Lafayette allied with the American Revolutionaries.", "Lafayette's army scored several key victories for the rebels."], "decomposition": ["Which allies did the American Revolutionaries have during the war?", "Which of #1 scored many important victories for them?", "Are any of #2 French?"], "evidence": [[[["France in the American Revolutionary War-16"]], [["Major General Comte Jean de Rochambeau-1"]], [["Jean-Baptiste Donatien de Vimeur, comte de Rochambeau-1"], "operation"]], [[["George Washington in the American Revolution-10"], "no_evidence"], [["France in the American Revolutionary War-5"], "no_evidence"], ["operation"]], [[["American Revolutionary War-42"], "no_evidence"], [["Battle of the Chesapeake-1"]], ["operation"]]], "golden_sentence": [[""], [""], [""]]}, {"qid": "48e1a9389ca20f7864e2", "term": "Helium", "description": "Chemical element with atomic number 2", "question": "Is helium the cause of the Hindenburg explosion?", "answer": false, "facts": ["The Hindenburgh was filled with hydrogen.", "Helium is considered a noble, inert gas that doesn't react.", "Hydrogen is highly flammable. "], "decomposition": ["What gas was the Hindenburg filled with?", "Is #1 helium?"], "evidence": [[[["Hindenburg disaster-46"]], ["operation"]], [[["Hindenburg disaster-52"]], [["Hindenburg disaster-52"], "operation"]], [[["Hindenburg disaster-58"]], ["operation"]]], "golden_sentence": [["In his book LZ-129 Hindenburg (1964), Zeppelin historian Dr. Douglas Robinson commented that although ignition of free hydrogen by static discharge had become a favored hypothesis, no such discharge was seen by any of the witnesses who testified at the official investigation into the accident in 1937."]]}, {"qid": "f1a8a693dc3046800a2d", "term": "Bulk carrier", "description": "merchant ship specially designed to transport unpackaged bulk cargo", "question": "Does Southwest Airlines use bulk carriers?", "answer": true, "facts": ["Southwest Airlines requires jet fuel, which is brought in by bulk carrier transport.", "Southwest Airlines requires glycol for de-icing their planes, which is brought in by bulk transport."], "decomposition": ["Which vehicles does Southwest Airlines use to provide their services?", "What are some common products needed for the running and maintenance of #1?", "Would Southwest Airlines require #2 in quantities large enough for bulk carriers?"], "evidence": [[[["Southwest Airlines fleet-1"]], [["Aircraft maintenance-16", "Aviation fuel-20"]], [["Boeing 737 Next Generation-41"], "operation"]], [[["Southwest Airlines fleet-1", "Southwest Airlines-2"], "no_evidence"], ["no_evidence"], [["Bulk carrier-1"], "no_evidence", "operation"]], [[["Southwest Airlines fleet-1"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Since its inception Southwest Airlines has almost exclusively operated Boeing 737 aircraft (except for a brief period when it leased and flew some Boeing 727-200 aircraft)."], ["In 2017, of the $70 billion spent by airlines on maintenance, repair and overhaul (MRO), 31% were for engines, 27% for components, 24% for line maintenance, 10% for modifications and 8% for the airframe; 70% were for mature airliners (Airbus A320 and A330, Boeing 777 and 737NG), 23% were for \u201csunset\u201d aircraft (MD-80, Boeing 737 Classic, B747 or B757) and 7% was spent on modern models (Boeing 787, Embraer E-Jet, Airbus A350XWB and A380).", ""], [""]]}, {"qid": "385a8adfe0f7a4875689", "term": "Pelvis", "description": "lower part of the trunk of the human body between the abdomen and the thighs (sometimes also called pelvic region of the trunk", "question": "Is dysphoria around one's pelvis treatable without surgery?", "answer": true, "facts": ["For individuals experiencing dysphoria around having a vagina, packers can relieve symptoms.", "For people experiencing dysphoria about having a penis, there are tucking underwear and comfort gaffs available."], "decomposition": ["Which dysphoria could be experienced around the pelvis/genitals?", "What are the ways of treating #1?", "Are there others apart from surgery included in #2?"], "evidence": [[[["Gender dysphoria-7"]], [["Gender dysphoria-16"]], [["Gender dysphoria-16", "Gender dysphoria-18", "Gender dysphoria-20"]]], [[["Dysphoria-1"], "no_evidence"], ["no_evidence"], ["operation"]], [[["Dysphoria-5"]], [["Gender dysphoria-16"]], ["operation"]]], "golden_sentence": [[""], ["This may include psychological counseling, resulting in lifestyle changes, or physical changes, resulting from medical interventions such as hormonal treatment, genital surgery, electrolysis or laser hair removal, chest/breast surgery, or other reconstructive surgeries."], ["This may include psychological counseling, resulting in lifestyle changes, or physical changes, resulting from medical interventions such as hormonal treatment, genital surgery, electrolysis or laser hair removal, chest/breast surgery, or other reconstructive surgeries.", "", ""]]}, {"qid": "22f2af73032b63ba0f50", "term": "Keyboard layout", "description": "any specific mechanical, visual, or functional arrangement of the keys of a keyboard or typewriter", "question": "Is the QWERTY keyboard layout meant to be slow?", "answer": true, "facts": ["The QWERTY keyboard layout was made for use on typewriters.", "Typewriters could not be used too quickly, or they would jam. "], "decomposition": ["What machine was QWERTY keyboard layout created for?", "Why was QWERTY keyboard layout created for #1?", "Will typing slow solve the problem of #2?"], "evidence": [[[["QWERTY-1"]], [["QWERTY-17"]], ["operation"]], [[["QWERTY-1"]], [["QWERTY-4"], "no_evidence"], [["QWERTY-6", "QWERTY-7"], "operation"]], [[["QWERTY-1"]], [["QWERTY-5"]], [["QWERTY-17"]]]], "golden_sentence": [["The QWERTY design is based on a layout created for the Sholes and Glidden typewriter and sold to E. Remington and Sons in 1873."], ["Contrary to popular belief, the QWERTY layout was not designed to slow the typist down, but rather to speed up typing by preventing jams."]]}, {"qid": "c6c6ec2f407f8def9ffa", "term": "Longitude", "description": "A geographic coordinate that specifies the east-west position of a point on the Earth's surface", "question": "Can I find my home with latitude and longitude?", "answer": true, "facts": ["My home is a location on earth where I live. ", "Latitude and Longitude are geographic coordinate systems that identify east/west and north/south locations.", "Specific Latitude and Longitude coordinates can be used to pinpoint specific locations. ", "Every point on the earth has a corresponding latitude and longitude coordinate. "], "decomposition": ["What are the uses of latitude and longitude?", "What is the range of latitude and longitude?", "Does #1 and #2 make it possible to locate most people's homes?"], "evidence": [[[["Geographic coordinate system-4"], "no_evidence"], [["Geographic coordinate system-16"], "no_evidence"], ["no_evidence", "operation"]], [[["Geographic coordinate system-15"]], [["Geographic coordinate system-16"], "no_evidence"], ["operation"]], [[["Geographic coordinate system-15"]], [["Geographic coordinate system-15"], "no_evidence"], [["Geographic coordinate system-15"]]]], "golden_sentence": [["Ptolemy credited him with the full adoption of longitude and latitude, rather than measuring latitude in terms of the length of the midsummer day."], [""]]}, {"qid": "2662795b34c72e481d24", "term": "Robotics", "description": "Design, construction, operation, and application of robots", "question": "Did the Wall Street Crash of 1929 hurt the stocks of robotics companies?", "answer": false, "facts": ["The first robotics company was formed in the 1950s", "The crash of 1929 was a single event, not one that lasted decades"], "decomposition": ["When did the first robotic company form?", "When did the crash of 1929 last till?", "Is there any overlap between #1 and #2?"], "evidence": [[[["History of robots-41"]], [["Wall Street Crash of 1929-1"]], ["operation"]], [[["Unimate-2"], "no_evidence"], [["Great Depression-1"]], ["operation"]], [[["Robotics-7"]], [["Wall Street Crash of 1929-1"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "5e76d6ce27d322b9a53a", "term": "Snoopy", "description": "cartoon dog", "question": "Could Snoopy transmit rabies?", "answer": false, "facts": ["Snoopy is a fictional dog.", "Fictional animals cannot transmit diseases to real people."], "decomposition": ["What can transmit rabies?", "What is Snoopy?", "Is #2 included in #1?"], "evidence": [[[["Rabies-3"]], [["Snoopy-2"]], ["operation"]], [[["Rabies-2"]], [["Snoopy-1"]], ["operation"]], [[["Rabies-15"]], [["Snoopy-1"]], ["operation"]]], "golden_sentence": [[""], ["Snoopy is a loyal, imaginative and good-natured beagle who is prone to imagining fantasy lives, including being an author, a college student known as \"Joe Cool\", an attorney and a British World War I flying ace."]]}, {"qid": "b6d86d6ecc6bbdc6738e", "term": "Messiah (Handel)", "description": "Oratorio by Handel", "question": "Would Bruce Gandy be an odd choice for Messiah (Handel)?", "answer": true, "facts": ["Messiah (Handel) is a 1741 Oratorio by George Frideric Handel.", "Messiah (Handel) requires the following instruments: 2 trumpets; timpani; 2 oboes; 2 violins; and a viola.", "Bruce Gandy is a world renowned bagpipe player."], "decomposition": ["What instruments are used in Messiah (Handel)?", "What instrument is played by Bruce Gandy?", "Is #2 listed in #1?"], "evidence": [[[["Messiah (Handel)-3", "Orchestra-1"], "no_evidence"], [["Bruce Gandy-1"]], ["operation"]], [[["Structure of Handel's Messiah-9"], "no_evidence"], [["Bruce Gandy-1"]], ["operation"]], [[["Structure of Handel's Messiah-7"]], [["Bruce Gandy-5"]], ["operation"]]], "golden_sentence": [["Handel wrote Messiah for modest vocal and instrumental forces, with optional settings for many of the individual numbers.", "An orchestra (/\u02c8\u0254\u02d0rk\u026astr\u0259/; Italian:\u00a0[or\u02c8k\u025bstra]) is a large instrumental ensemble typical of classical music, which combines instruments from different families, including bowed string instruments such as the violin, viola, cello, and double bass, brass instruments such as the horn, trumpet, trombone and tuba, woodwinds such as the flute, oboe, clarinet and bassoon, and percussion instruments such as the timpani, bass drum, triangle, snare drum, cymbals, and mallet percussion instruments each grouped in sections."], ["Bruce Gandy (born August 10, 1962) is a Canadian bagpipe player and composer."]]}, {"qid": "0cbb069a5d62d9e5c5b5", "term": "Popular science", "description": "Interpretation of science intended for a general audience", "question": "Is popular science used to peer review papers?", "answer": false, "facts": ["Popular science is a simplified version of scientific work.", "Peer review uses detailed scientific information to verify papers. "], "decomposition": ["What is popular science?", "What types of documents does peer review use to verify papers?", "Is #1 the same as #2?"], "evidence": [[[["Popular Science-1"]], [["Peer review-1"]], ["operation"]], [[["Popular science-1"]], [["Peer review-2"], "no_evidence"], ["operation"]], [[["Popular science-1"]], [["Scholarly peer review-2"], "no_evidence"], ["operation"]]], "golden_sentence": [["Popular Science (also known as PopSci) is an American quarterly magazine carrying popular science content, which refers to articles for the general reader on science and technology subjects."], [""]]}, {"qid": "29fc39dcf922471a057a", "term": "Egyptian pyramids", "description": "Ancient pyramid-shaped masonry structures located in Egypt", "question": "Do the Egyptian pyramids look the same from outside as they did when new?", "answer": false, "facts": ["When originally built, the Great Pyramids had a thin surface of limestone covering the bricks, making them shine in the sun.", "Over the centuries, the limestone layer has been removed by thieves and erosion, exposing the more common stone bricks underneath."], "decomposition": ["When originally built, what was the outer layer of the Great Pyramids covered in?", "Is #1 able to withstand erosion over time?"], "evidence": [[[["Great Pyramid of Giza-2"]], [["Limestone-18"], "operation"]], [[["Great Pyramid of Giza-2"]], [["Limestone-32"]]], [[["Egyptian pyramids-9"]], [["Limestone-2"], "operation"]]], "golden_sentence": [["Originally, the Great Pyramid was covered by limestone casing stones that formed a smooth outer surface; what is seen today is the underlying core structure."], [""]]}, {"qid": "8888454c6307fdb077c6", "term": "Ku Klux Klan", "description": "American white supremacy group", "question": "Would an average American Public University be welcoming to Ku Klux Klan members?", "answer": false, "facts": ["American Universities are known for being liberal in their demographics.", "Groups like the Ku Klux Klan are condemned by liberal groups, as they advocate for human equality."], "decomposition": ["What political party is the majority in American Universities?", "Is the Ku Klux Klan welcomed by #1?"], "evidence": [[[["Political views of American academics-1"]], [["Ku Klux Klan-74"], "no_evidence"]], [[["University-1"], "no_evidence"], [["Ku Klux Klan-1"], "no_evidence", "operation"]], [[["Liberal arts college-1", "Liberalism-8"], "no_evidence"], [["Ku Klux Klan-74"], "no_evidence", "operation"]]], "golden_sentence": [["Demographic surveys of faculty that began in the 1950s and continue to the present have found higher percentages of liberals than of conservatives, particularly among those who work in the humanities and social sciences."], [""]]}, {"qid": "921d3f15518552306050", "term": "Lullaby", "description": "soothing song, usually sung to young children before they go to sleep", "question": "Would a lullaby be enough to wake Hellen Keller up?", "answer": false, "facts": ["Lullabies can be sung or played via instrument.", "Hellen Keller was deaf. "], "decomposition": ["How are lullabies played?", "What did Hellen Keller suffer from?", "Would a person with #2 be able to hear #1?"], "evidence": [[[["Lullaby-1"]], [["Helen Keller Day-5"]], ["operation"]], [[["Lullaby-1"]], [["Helen Keller-1"]], [["Deafblindness-1"]]], [[["Lullaby-11"]], [["Helen Keller-6"]], [["Deaf hearing-2"]]]], "golden_sentence": [["As a result, the music is often simple and repetitive."], [""]]}, {"qid": "ea91b765dc1ed1024ab4", "term": "Accountant", "description": "practitioner of accountancy or accounting", "question": "Is accountant a difficult profession for a person suffering from Dyscalculia?", "answer": true, "facts": ["Accounting is a math intensive profession in which a person keeps or inspects financial accounts.", "Dyscalculia is a math learning disability that impairs an individual's ability to represent and process numerical magnitude in a typical way. ", "Common symptoms of Dyscalculia include: difficulty with number sense. difficulty with fact and calculation", "Dyscalculia is sometimes called \u201cnumber dyslexia\u201d or \u201cmath dyslexia.\u201d"], "decomposition": ["What skills does dyscalculia impair?", "What skills are necessary to be an accountant?", "Are some parts of #2 also in #1?"], "evidence": [[[["Dyscalculia-1"]], ["no_evidence"], ["no_evidence"]], [[["Dyscalculia-1"]], [["Accountant-2"]], ["operation"]], [[["Dyscalculia-4"]], [["Accounting-12"]], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "fcbbbef417509ba32513", "term": "German Shepherd", "description": "Dog breed", "question": "Would Robert Wadlow tower over a German Shepherd?", "answer": true, "facts": ["German Shepherds have a height between 22 and 26 inches.", "Robert Wadlow was the tallest man ever, reaching a height of 8 ft 11.1 inches at his death."], "decomposition": ["What is the typical height range of German Shepherds?", "How tall was Robert Wadlow?", "Is #2 greater than #1?"], "evidence": [[[["German Shepherd-3"]], [["Robert Wadlow-2"]], ["operation"]], [[["German Shepherd-3"]], [["Robert Wadlow-2"]], ["operation"]], [[["German Shepherd-3"], "no_evidence"], [["Robert Wadlow-2"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["German Shepherds are longer than they are tall, with an ideal proportion of 10 to \u200b8\u00a01\u20442."], ["Wadlow reached 8\u00a0ft 11.1\u00a0in (2.72\u00a0m) in height and weighed 439\u00a0lb (199\u00a0kg) at his death at age 22."]]}, {"qid": "8ad52f9c88fa703a8f6a", "term": "Cherokee", "description": "Native American people indigenous to the Southeastern United States", "question": "Did the Cherokee people send a delegation to oppose allotment?", "answer": true, "facts": ["The Four Mothers Society or Four Mothers Nation is a religious, political, and traditionalist organization of Muscogee Creek, Cherokee, Choctaw and Chickasaw people.", "The Four Mothers Society sent a delegation to Congress in 1906 to oppose the Curtis Act and the Dawes Act.", "With the passage of the Curtis Act in 1898 and Dawes Act, allotment became US policy and the various tribal governments were forced to allot land."], "decomposition": ["Which acts cause allotment to become US policy?", "Who sent as a delegate to congress to oppose #1?", "Is #2 related to the Cherokee people?"], "evidence": [[[["Dawes Act-3", "Dawes Act-4"]], [["Redbird Smith-3"]], ["operation"]], [[["Dawes Act-1"]], [["Dawes Act-3"]], ["operation"]], [[["Dawes Act-1"]], [["Dawes Act-3"]], ["operation"]]], "golden_sentence": [["", ""], ["Redbird Smith led a political resistance movement to the Dawes Allotment Act and sought to return to traditional Cherokee religion and values."]]}, {"qid": "8131131f2a25a8281954", "term": "Moscow Kremlin", "description": "fortified complex in Moscow, Russia", "question": "Can the Moscow Kremlin fit inside Disney Land?", "answer": true, "facts": ["The Moscow Kremlin is a fortified complex in the middle of Moscow Russia.", "The Kremlin takes up sixty eight acres.", "Disney Land is an amusement park in California. ", "Disney Land occupies eighty five acres."], "decomposition": ["What is the area of Moscow Kremlin?", "What is the size of Disney Land?", "Is #1 smaller than #2?"], "evidence": [[["no_evidence"], [["Disneyland-22"]], ["no_evidence", "operation"]], [[["Moscow Kremlin-1"], "no_evidence"], [["Disneyland-2"], "no_evidence"], ["operation"]], [[["Moscow Kremlin-18"]], [["Disneyland-22"]], ["operation"]]], "golden_sentence": [["Disneyland Park consists of nine themed \"lands\" and a number of concealed backstage areas, and occupies over 100 acres (40\u00a0ha) with the new addition of Mickey and Minnie's Runaway Railway that's coming to Mickeys Toontown in 2022."]]}, {"qid": "d969de539a2cc1c69d33", "term": "Euphoria", "description": "mental and emotional condition in which a person experiences intense feelings of well-being, elation, happiness and excitement", "question": "Is euphoria associated with drug addiction?", "answer": true, "facts": ["Euphoria is a state of unusually extreme happiness.", "Several drugs are known to artificially induce this reaction including cannabis and opium."], "decomposition": ["What is euphoria?", "Do some drugs create the feeling of #1?"], "evidence": [[[["Euphoria-1"]], [["Euphoria-15"]]], [[["Euphoria-19"]], [["Euphoria-19"]]], [[["Euphoria-1"]], [["Euphoria-15", "Euphoria-16"], "operation"]]], "golden_sentence": [["Euphoria ( /ju\u02d0\u02c8f\u0254\u02d0ri\u0259/ (listen)) is the experience (or affect) of pleasure or excitement and intense feelings of well-being and happiness."], [""]]}, {"qid": "58b44d156ab779d8c3b9", "term": "Agnosticism", "description": "view that the existence of any deity is unknown or unknowable", "question": "Does Billy Graham support agnosticism?", "answer": false, "facts": ["Bill Graham was a prominent American evangelical leader.", "In Christianity, evangelism is the commitment to or act of publicly preaching (ministry) of the Gospel with the intention to share the message and teachings of Jesus Christ. ", "Agnosticism is the belief that humanity is unsure if God exists.", "Evangelical Christians share the belief that God exists."], "decomposition": ["What religion does Billy Graham subscribe to?", "Does #1 allow for the uncertainty of God's existence?"], "evidence": [[[["Billy Graham-1"]], [["Evangelicalism-1"], "no_evidence", "operation"]], [[["Billy Graham-1"]], [["Evangelicalism-1"], "operation"]], [[["Billy Graham-1"]], [["Evangelicalism-1"]]]], "golden_sentence": [["William Franklin Graham Jr. KBE (November 7, 1918\u00a0\u2013 February 21, 2018) was an American evangelist, a prominent evangelical Christian figure, and an ordained Southern Baptist minister who became well-known internationally in the late 1940s."], [""]]}, {"qid": "20987eb4faa643955e4b", "term": "Milky Way", "description": "Spiral galaxy containing our Solar System", "question": "Is Ganymede in the Milky Way galaxy?", "answer": true, "facts": ["Ganymede is a moon of Jupiter.", "Jupiter is the largest planet in our solar system.", "The solar system is part of the Milky Way galaxy."], "decomposition": ["What does Ganymede orbit?", "What larger astronomical system is #1 part of?", "Is #2 located in the Milky Way?"], "evidence": [[[["Ganymede (moon)-1"]], [["Ganymede (moon)-1"]], [["Solar System-6"]]], [[["Ganymede (moon)-1"]], [["Milky Way-1"]], ["operation"]], [[["Ganymede (moon)-1"]], [["Jupiter-1"]], [["Milky Way-1"]]]], "golden_sentence": [["Ganymede orbits Jupiter in roughly seven days and is in a 1:2:4 orbital resonance with the moons Europa and Io, respectively."], ["Ganymede /\u02c8\u0261\u00e6n\u026ami\u02d0d/, a satellite of Jupiter (Jupiter III), is the largest and most massive of the Solar System's moons."], ["The Solar System is located in the Orion Arm, 26,000 light-years from the center of the Milky Way galaxy."]]}, {"qid": "838bff0971934642d53d", "term": "Internet troll", "description": "Person who sows discord on the Internet", "question": "Can you avoid internet trolls on reddit?", "answer": false, "facts": ["Internet Trolls flock to any popular platform on the internet.", "Reddit is the 19th most popular website online."], "decomposition": ["What types of sites do internet trolls go to?", "What is the popularity ranking of Reddit compared to other websites", "Would #1's be likely to go to a site ranked #2 in popularity? "], "evidence": [[[["Internet troll-1"]], [["Reddit-2"]], [["Internet troll-37"]]], [[["Internet troll-37"]], [["Reddit-2"]], ["operation"]], [[["Internet troll-1"]], [["Reddit-2"]], ["operation"]]], "golden_sentence": [["In internet slang, a troll is a person who starts Flame wars or upsets people on the Internet by posting inflammatory and digressive, extraneous, or off-topic messages in an online community (such as a newsgroup, forum, chat room, or blog) with the intent of provoking readers into displaying emotional responses and normalizing tangential discussion, either for the troll's amusement or a specific gain."], [""], [""]]}, {"qid": "77bc4b027b94d74ca345", "term": "Giraffe", "description": "Tall African ungulate", "question": "Could Javier Sotomayor jump over the head of the average giraffe?", "answer": false, "facts": ["Fully grown giraffes stand 4.3\u20135.7 m (14.1\u201318.7 ft) tall.", "Javier Sotomayor is the current world record holder in the long jump, with a personal best of 2.45 m (8 ft 1/2 in)."], "decomposition": ["How tall are giraffes?", "What is Javier Sotomayor's personal record in the high jump?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Giraffe-16"]], [["Javier Sotomayor-1"]], ["operation"]], [[["Giraffe-16"]], [["Javier Sotomayor-1"]], ["operation"]], [[["Giraffe-16"]], [["Javier Sotomayor-1"]], ["operation"]]], "golden_sentence": [["Fully grown giraffes stand 4.3\u20135.7\u00a0m (14.1\u201318.7\u00a0ft) tall, with males taller than females."], ["The 1992 Olympic champion, he was the dominant high jumper of the 1990s; his personal best of 2.45\u00a0m (8\u00a0ft 1/2 in) makes him the only person to have ever cleared eight feet."]]}, {"qid": "f194d5fc91f560510c51", "term": "Goofy", "description": "Disney cartoon character", "question": "Could Goofy have counted nine planets in his first year?", "answer": true, "facts": ["Goofy was created in 1932", "Pluto (the celestial object) was discovered in 1930 and labeled a planet"], "decomposition": ["When was Goofy first created?", "What year was the ninth planet found?", "Was #1 after #2?"], "evidence": [[[["Goofy-1"]], [["Pluto-2"]], [["Goofy-1", "Pluto-2"]]], [[["Goofy-1"]], [["Pluto-2"]], ["operation"]], [[["Goofy-1"]], [["Pluto-2"]], ["operation"]]], "golden_sentence": [["Goofy is a funny animal cartoon character created in 1932 at Walt Disney Productions."], ["Pluto was discovered by Clyde Tombaugh in 1930 and declared to be the ninth planet from the Sun."], ["", ""]]}, {"qid": "f89d98946e166e3b2029", "term": "Douglas Adams", "description": "British author and humorist", "question": "Did Douglas Adams use email as a child?", "answer": false, "facts": ["Douglas Adams was born in 1952.", "Modern email did not emerge until 1977."], "decomposition": ["When was Douglas Adams born?", "What year did email begin?", "Is #2 before #1?"], "evidence": [[[["Douglas Adams-1"]], [["Email-1"]], ["operation"]], [[["Douglas Adams-1"]], [["Email-6"]], ["operation"]], [[["Douglas Adams-1"]], [["Email-1"]], ["operation"]]], "golden_sentence": [["Douglas Noel Adams (11 March 1952 \u2013 11 May 2001) was an English author, screenwriter, essayist, humorist, satirist and dramatist."], ["Ray Tomlinson is credited as the inventor of email; in 1971, he developed the first system able to send mail between users on different hosts across the ARPANET, using the @ sign to link the user name with a destination server."]]}, {"qid": "c5c24c8e746ae4dae4c3", "term": "Kurt Cobain", "description": "American singer, composer, and musician", "question": "Would Kurt Cobain have benefited from Project Semicolon?", "answer": true, "facts": ["Project Semicolon is an American nonprofit organization known for its advocacy of mental health wellness and its focus as an anti-suicide initiative.", "During the last years of his life, Cobain struggled with heroin addiction and chronic health problems such as depression.", "Cobain died at the age of 27 from apparent suicide by gunshot."], "decomposition": ["What problems does Project Semicolon work to solve?", "Did Kurt Cobain have any of the problems listed in #1?"], "evidence": [[[["Project Semicolon-1"]], [["Suicide of Kurt Cobain-2"]]], [[["Project Semicolon-1"]], [["Kurt Cobain-3"]]], [[["Project Semicolon-5"]], [["Kurt Cobain-47", "Kurt Cobain-49", "Project Semicolon-5"]]]], "golden_sentence": [["Founded in 2013, the movement's aim is \"presenting hope and love to those who are struggling with depression, suicide, addiction, and self-injury\"."], ["According to The Telegraph, Cobain had depression."]]}, {"qid": "9da847e69e989e97e203", "term": "Arctic Ocean", "description": "The smallest and shallowest of the world's five major oceans, located in the north polar regions", "question": "Could the Eiffel Tower be completely submerged at the Arctic Ocean's deepest point?", "answer": true, "facts": ["The deepest point in the Arctic Ocean is 18,210 feet below the surface.", "The Eiffel Tower is 1,063 feet tall."], "decomposition": ["How deep is the deepest point in the Arctic Ocean?", "How tall is the Eiffel Tower?", "Is #2 smaller than #1?"], "evidence": [[[["Molloy Deep-2"]], [["Eiffel Tower-3"]], ["operation"]], [[["Molloy Deep-2"]], [["Eiffel Tower-3"]], [["Eiffel Tower-3", "Molloy Deep-2"], "operation"]], [[["Fram Strait-4"]], [["Eiffel Tower-3"]], ["operation"]]], "golden_sentence": [["The basin floor measures about 220 km2, and is the deepest point in the Arctic Ocean."], ["The tower is 324 metres (1,063\u00a0ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris."]]}, {"qid": "32c5e6f9c9c24826535e", "term": "Goblin shark", "description": "Deep-sea shark", "question": "Would a goblin shark eat at Crossroads Kitchen?", "answer": false, "facts": ["Goblin sharks are carnivores that subsist on other fish, cephalopods and crustaceans", "Crossroads Kitchen is a vegan restaurant in Los Angeles", "Vegans do not consume any animal products"], "decomposition": ["What is the goblin shark's diet-based classification?", "What kind of food is served at Crossroads Kitchen?", "Would a #1 typically consume #2?"], "evidence": [[[["Goblin shark-12"]], [["Crossroads Kitchen-1"]], ["operation"]], [[["Goblin shark-12"]], [["Crossroads Kitchen-6"]], [["Goblin shark-12"], "operation"]], [[["Goblin shark-12"]], [["Crossroads Kitchen-1"]], ["operation"]]], "golden_sentence": [[""], ["Crossroads Kitchen is a vegan fine-dining restaurant in the Beverly Grove neighborhood of Los Angeles."]]}, {"qid": "b4c2c3cfc28de662cc34", "term": "Jalape\u00f1o", "description": "Hot pepper", "question": "Is jalapeno heat outclassed by Bhut jolokia?", "answer": true, "facts": ["The Scoville scale measures how hot peppers are.", "The jalapeno pepper has a Scoville scale rating of between 3,500 and 3,600 SHU (Scoville Heat Units).", "The Bhut jolokia (ghost pepper) has a Scoville scale rating of 1 million SHU (Scoville Heat Units)."], "decomposition": ["How many Scoville units does a Jalapeno have?", "How many Scoville units does a  Bhut jolokia have?", "Is #2 greater than #1?"], "evidence": [[[["Jalape\u00f1o-16"]], [["Bhut jolokia-4"]], ["operation"]], [[["Jalape\u00f1o-1"]], [["Bhut jolokia-2"]], ["operation"]], [[["Jalape\u00f1o-16"]], [["Race to grow the hottest pepper-1"]], ["operation"]]], "golden_sentence": [["Compared to other chillies, the jalape\u00f1o heat level varies from mild to hot depending on cultivation and preparation and can have from a few thousand to over 10,000 Scoville heat units."], ["In 2005, New Mexico State University's Chile Pepper Institute in Las Cruces, New Mexico, found Bhut jolokia grown from seed in southern New Mexico to have a Scoville rating of 1,001,304 SHUs by HPLC."]]}, {"qid": "db6cafa0f47890f64b12", "term": "Linux", "description": "Family of free and open-source software operating systems based on the Linux kernel", "question": "If you're running focal fossa, are you using linux?", "answer": true, "facts": ["Focal Fossa is the most recent Ubuntu release.", "Ubuntu is a Linux distribution. "], "decomposition": ["Which operating system was codenamed focal fossa?", "Is #1 a Linux distribution?"], "evidence": [[[["Ubuntu version history-146"]], [["Ubuntu-1"], "operation"]], [[["Ubuntu version history-146"]], [["Ubuntu-1"]]], [[["Ubuntu-2"]], ["operation"]]], "golden_sentence": [["Ubuntu 20.04 LTS, codenamed Focal Fossa, is the latest Long Term Support release and was released on 23 April 2020."], ["Ubuntu (/\u028a\u02c8b\u028antu\u02d0/ (listen) uu-BUUN-too) is a free and open-source Linux distribution based on Debian."]]}, {"qid": "bc64e3d437b7cde6d306", "term": "Badminton", "description": "racquet sport", "question": "Are birds important to badminton?", "answer": true, "facts": ["Badminton is played with racquets and a shuttlecock.", "A shuttlecock is a projectile made of feathers attached to a cork base.", "Birds have feathers covering their body."], "decomposition": ["What are the equipment needed to play badminton?", "Is any of #1 made with a bird product?"], "evidence": [[[["Badminton-1"]], [["Badminton-3"], "operation"]], [[["Badminton-35"], "no_evidence"], [["Shuttlecock-4"]]], [[["Badminton-28"], "no_evidence"], [["Badminton-28"], "no_evidence"]]], "golden_sentence": [[""], [""]]}, {"qid": "ec5d5e32d94ab6e84d43", "term": "Banana", "description": "edible fruit", "question": "Would you buy bananas for tostones?", "answer": false, "facts": ["Tostones are a Latin American twice fried dish made of plantains.", "Plantains resemble bananas, but are not bananas."], "decomposition": ["What ingredients are used to make tostones?", "Are bananas included in #1?"], "evidence": [[[["Tostones-2"]], ["operation"]], [[["Tostones-2"]], ["operation"]], [[["Tostones-1"]], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "809927a9757ab4ec167f", "term": "Dungeons & Dragons", "description": "Fantasy role-playing game", "question": "Is Dungeons and Dragons a game well suited for solo play?", "answer": false, "facts": ["Dungeons and Dragons requires one person to act as the \"Dungeon Master\" to construct the world for the other players to roleplay in.", "Dungeons and Dragons cannot work without at least one DM and two players."], "decomposition": ["How many basic roles must be accounted for in order to play a game of Dungeons and Dragons?", "Is #1 equal to one?"], "evidence": [[[["Dungeons & Dragons-5"], "no_evidence"], ["operation"]], [[["Dungeons & Dragons-2"]], ["operation"]], [[["Dungeons & Dragons-2"], "no_evidence"], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "d7bd360a9cba847d6714", "term": "Chickpea", "description": "species of plant", "question": "Would vegans consider chickpeas for a tuna substitute?", "answer": true, "facts": ["Vegans do not eat any animal products, including milk, eggs, meat, and cheese. ", "Vegan alternatives for things like mayo exist. ", "There are vegan recipes for \"tuna\" online that include chickpeas as the main component."], "decomposition": ["What kinds of foods do vegans avoid?", "What are replacements for #1 in vegan diets called?", "What are some #2 for tuna?", "Are chickpeas included in the recipes of any of #3?"], "evidence": [[[["Veganism-1"]], [["Meat analogue-16"]], ["no_evidence", "operation"], ["operation"]], [[["Veganism-1"]], [["Veganism-34"]], [["Chickpea-14"], "no_evidence"], ["operation"]], [[["Veganism-1"]], [["Chickpea-1", "Meat analogue-1"]], [["Chickpea-17"], "no_evidence"], ["operation"]]], "golden_sentence": [["Dietary vegans (also known as \"strict vegetarians\") refrain from consuming meat, eggs, dairy products, and any other animal-derived substances."], ["Meat analog products are currently made by two basic processes, through either thermoplastic extrusion or fiber spinning."]]}, {"qid": "0bc78791ed2b7783da79", "term": "Giant panda", "description": "species of mammal", "question": "Does giant panda have colors that differ from yin yang?", "answer": false, "facts": ["The giant panda is a mammal that is black and white in color.", "Yin yang is an ancient Chinese concept represented by the colors black and white."], "decomposition": ["What colors does the concept \" yin and yang\" represent?", "What are colors of adult giant pandas?", "Is any of #2 excluded from #1?"], "evidence": [[[["Yin and yang-40"]], [["Giant panda-12"]], ["operation"]], [[["Yin and yang-40"]], [["Giant panda-11"]], ["operation"]], [[["Yin and yang-1"]], [["Giant panda-13"]], ["operation"]]], "golden_sentence": [["Yin is the black side, and yang is the white side."], ["The giant panda has luxuriant black-and-white fur."]]}, {"qid": "68ad1b54fd85727f2e78", "term": "Nine Inch Nails", "description": "American industrial rock band", "question": "Did Nine Inch Nails inspire Aretha Franklin's sound?", "answer": false, "facts": ["Nine Inch Nails is a industrial heavy rock band.", "Aretha Franklin was a soul and R&B singer.", "Aretha Franklin began singing in a gospel choir.", "Nine Inch Nails lyrics have been described as profane and anti-God."], "decomposition": ["What genre are Nine Inch Nails' music?", "What genre of songs does Aretha Franklin sing?", "Is #1 the same as #2?"], "evidence": [[[["Nine Inch Nails-1"]], [["Aretha Franklin-27"]], ["operation"]], [[["Nine Inch Nails-1"]], [["Aretha Franklin-27"]], ["operation"]], [[["Nine Inch Nails-1"]], [["Aretha Franklin-1"]], ["operation"]]], "golden_sentence": [["Nine Inch Nails, commonly abbreviated as NIN (stylized as NI\u0418), is an American industrial rock band formed in 1988 in Cleveland, Ohio."], ["According to Richie Unterberger, Franklin was \"one of the giants of soul music, and indeed of American pop as a whole."]]}, {"qid": "cfa9c92e1cb2c9991341", "term": "Hundred Years' War", "description": "Series of conflicts and wars between England and France during the 14th and 15th-century", "question": "Was Hundred Years' War a misnomer?", "answer": true, "facts": ["A misnomer is a wrong or inaccurate name.", "The Hundred Years' War lasted for longer than one hundred years.", "The Hundred Years' War lasted from 1337-1453."], "decomposition": ["How many years the the Hundred Years' War actually last?", "Is #1 greater or less than 100?"], "evidence": [[[["Hundred Years' War-1"]], ["operation"]], [[["Hundred Years' War-1"]], ["operation"]], [[["Hundred Years' War-1"]], ["operation"]]], "golden_sentence": [["The Hundred Years War was a series of conflicts from 1337 to 1453, waged between the House of Plantagenet, rulers of England and the French House of Valois, over the right to rule the Kingdom of France."]]}, {"qid": "88399a86400ab4c6a9cb", "term": "Sicilian Defence", "description": "Chess opening", "question": "Would Lee Sedol understand the complexities of the Sicilian Defence?", "answer": false, "facts": ["Lee Sedol is a former South Korean professional Go player of 9 dan rank.", "Go has a different rule set than chess.", "It would not be worthwhile to spend time understanding the complexities of a game that you don't play professionally."], "decomposition": ["What is Lee Sedol's profession?", "The Sicilian defense is a tactic of which game?", "Would #1 typically invest time in learning about #2?"], "evidence": [[[["Lee Sedol-1"]], [["Sicilian Defence-1"]], ["operation"]], [[["Lee Sedol-1"]], [["Sicilian Defence, Accelerated Dragon-1"]], ["operation"]], [[["Lee Sedol-3"], "operation"], [["Sicilian Defence-6"], "operation"], ["operation"]]], "golden_sentence": [["Lee Sedol (Korean: \uc774\uc138\ub3cc; born 2 March 1983), or Lee Se-dol, is a former South Korean professional Go player of 9 dan rank."], ["The Sicilian Defence is a chess opening that begins with the following moves:"]]}, {"qid": "a12f6f8da2f13236aa80", "term": "Starbucks", "description": "American multinational coffee company", "question": "Would menu at Chinese Starbucks be familiar to an American?", "answer": false, "facts": ["American Starbucks sells a number of coffee beverages like Lattes and Cappucino.", "The Chinese Starbucks menu focuses on teas such as Blackcurrant Raspberry Juiced Tea and Iced Shaken Mango Herbal Juiced Tea.", "Mooncakes, Chinese bakery products traditionally eaten during the Mid-Autumn Festival, are popular items at Chinese Starbucks."], "decomposition": ["What is on the typical American Starbucks' menu?", "What is on the typical Chinese Starbucks' menu?", "Are most things in #2 not found in #1?"], "evidence": [[[["Starbucks-16"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Starbucks-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Starbucks-1"]], [["Starbucks-27"], "no_evidence"], ["operation"]]], "golden_sentence": [["The beverage was introduced under the Starbucks name in 1995 and as of 2012, Starbucks had annual Frappuccinos sales of over $2 billion."]]}, {"qid": "731c692e41bd38c681c0", "term": "Saturn", "description": "Sixth planet from the Sun in the Solar System", "question": "Are Saturn's famous rings solid?", "answer": false, "facts": ["The rings are made mostly of dust and particles.", "The ring matter is constantly in orbital motion around Saturn."], "decomposition": ["In what form do Saturn's famous rings exist in?", "Can #1 be considered solid?"], "evidence": [[[["Rings of Saturn-16"]], [["Solid-1"], "operation"]], [[["Saturn-30"]], [["Solid-1"]]], [[["Saturn-3"]], [["Debris-1", "Ice-1", "Rock (geology)-1"], "operation"]]], "golden_sentence": [[""], ["Unlike a liquid, a solid object does not flow to take on the shape of its container, nor does it expand to fill the entire available volume like a gas."]]}, {"qid": "7feb87f9145edaddfbc6", "term": "Noah", "description": "Biblical figure", "question": "Was Noah associated with a dove?", "answer": true, "facts": ["Noah sailed his Ark when the world was flooded.", "After the flood, he sent his dove to find land."], "decomposition": ["Which famous bible story was Noah associated with?", "Which creatures were the important characters in #1?", "Is any of #2 a dove?"], "evidence": [[[["Noah-1"]], [["Doves as symbols-5"]], ["operation"]], [[["Genesis flood narrative-1", "Noah-32"]], [["Columbidae-36", "Noah-2"]], ["operation"]], [[["Noah-2"]], [["Sign of the Dove-6"]], ["operation"]]], "golden_sentence": [["The Genesis flood narrative is among the best-known stories of the Bible."], ["According to the biblical story (Genesis 8:11), a dove was released by Noah after the flood in order to find land; it came back carrying a freshly plucked olive leaf (Hebrew: \u05e2\u05dc\u05d4 \u05d6\u05d9\u05ea alay zayit),[Gen 8:11] a sign of life after the Flood and of God's bringing Noah, his family and the animals to land."]]}, {"qid": "61319caff44855561110", "term": "Achilles", "description": "Greek mythological hero", "question": "Was Achilles a direct descendent of Gaia?", "answer": true, "facts": ["Achilles was the son of a Nereid. ", "The Nereids were the 50 daughters of Nereus.", "Nereus was the eldest son of the union between Gaia and Pontus."], "decomposition": ["Who were Achilles' parents?", "Who were the children of Gaia?", "Were any of #1 the children of #2?"], "evidence": [[[["Achilles-1"]], [["Gaia-1", "Nereus-1"]], [["Thetis-1"], "operation"]], [[["Achilles-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Achilles-1"]], [["Nereus-1"]], [["Thetis-1"], "operation"]]], "golden_sentence": [["He was the son of the Nereid Thetis and Peleus, king of Phthia."], ["She is the mother of Uranus (the sky), from whose sexual union she bore the Titans (themselves parents of many of the Olympian gods), the Cyclopes, and the Giants; of Pontus (the sea), from whose union she bore the primordial sea gods.", "Nereus and Doris became the parents of 50 daughters (the Nereids) and a son (Nerites), with whom Nereus lived in the Aegean Sea."], [""]]}, {"qid": "b9dc7253dec0bd7aaeb3", "term": "Starbucks", "description": "American multinational coffee company", "question": "Do any Islamic dominated countries have a Starbucks?", "answer": true, "facts": ["Starbucks is a coffee shop found in numerous countries including USA, China, and the United Arab Emirates.", "The United Arab Emirates has a Starbucks in Dubai.", "Islam is the largest and the official state religion of the United Arab Emirates.", "Pew Research estimates state that over 76% of the citizens of the United Arab Emirates are Islamic."], "decomposition": ["Which countries does Starbucks have branch(es) in?", "Is any of #1 an Islamic dominated country?"], "evidence": [[[["Starbucks-31"]], [["Islam by country-1"]]], [[["Starbucks-26"]], [["Starbucks-26"], "operation"]], [[["Middle East-9", "Starbucks-37"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], ["Islam is the dominant religion in Central Asia, Indonesia, Middle East, North Africa, the Sahel and some other parts of Asia."]]}, {"qid": "7b2605859543a1950ff8", "term": "Days of Our Lives", "description": "American daytime soap opera", "question": "Is a thousand dollars per Days of Our Lives episodes preferred to other soaps?", "answer": false, "facts": ["Days of Our Lives has aired around 13,900 episodes as of 2020.", "General Hospital aired their 14,000th episode on February 23, 2018."], "decomposition": ["How many episodes of 'Days of Our Lives' are there as of 2020?", "How many episodes of 'General Hospital' have been aired as of 2020?", "Is #1 greater than #2?"], "evidence": [[[["Days of Our Lives-3"], "no_evidence"], [["General Hospital-1", "General Hospital-3"], "no_evidence"], ["operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Days of Our Lives-3"], "no_evidence"], [["General Hospital-10"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["In November 2019, the show entered a planned four-month production hiatus; in January 2020, Days of Our Lives was renewed through September 2021."], ["", "The serial aired its 14,000th episode on February 23, 2018."]]}, {"qid": "b2865a31fbd35fd2e731", "term": "Pickled cucumber", "description": "Cucumber pickled in brine, vinegar, or other solution", "question": "Is pickled cucumber ever red?", "answer": true, "facts": ["Koolickles are a variety of pickled cucumber made with brine and red kool-aid.", "Korean cucumber kimchi is flavored with korean pepper powder.", "Korean pepper powder is red in color. "], "decomposition": ["What are the Koolickles made from?", "What is Korean cucumber kimchi flavored with?", "Are any of #1 or #2 red in color?"], "evidence": [[[["Pickled cucumber-19"]], [["Kimchi-13", "Kimchi-25", "Kimchi-28", "Kimchi-31"]], ["no_evidence", "operation"]], [[["Pickled cucumber-19"], "no_evidence"], [["Kimchi-1"]], ["operation"]], [[["Pickled cucumber-19"]], [["Kimchi-1"]], ["operation"]]], "golden_sentence": [["Kool-Aid pickles or \"koolickles\", enjoyed by children in parts of the Southern United States, are created by soaking dill pickles in a mixture of Kool-Aid and pickle brine."], ["Kimchi varieties are determined by the main vegetable ingredients and the mix of seasoning used to flavor the kimchi.", "Saeujeot (\uc0c8\uc6b0\uc813) or myeolchijeot is not added to the kimchi spice-seasoning mixture, but is simmered first to reduce odors, eliminate tannic flavor and fats, and then is mixed with a thickener made of rice or wheat starch (\ud480).", "It is made with pumpkin (bundi).", ""]]}, {"qid": "bcb1baf75fb89a73a702", "term": "Acetylene", "description": "chemical compound", "question": "Does welding with acetylene simulate the temperature of a star?", "answer": true, "facts": ["Acetylene is used for oxyacetylene welding ", "An acetylene/oxygen flame burns at about 3,773 K ", "The star Betelgeuse has a surface temperature of 3,500 K"], "decomposition": ["What temperature is reached when welding with acetylene?", "What temperature can stars reach?", "Are #1 and #2 similar in magnitude?"], "evidence": [[[["Acetylene-14"]], [["Star-91"]], ["operation"]], [[["Acetylene-8"], "no_evidence"], [["Star-91"]], ["operation"]], [[["Acetylene-14"]], [["Star-91"]], ["operation"]]], "golden_sentence": [["Combustion of acetylene with oxygen produces a flame of over 3,600\u00a0K (3,330\u00a0\u00b0C; 6,020\u00a0\u00b0F), releasing 11.8\u00a0kJ/g."], ["Massive main sequence stars can have surface temperatures of 50,000\u00a0K. Smaller stars such as the Sun have surface temperatures of a few thousand K. Red giants have relatively low surface temperatures of about 3,600\u00a0K; but they also have a high luminosity due to their large exterior surface area."]]}, {"qid": "af58844f88811c64d3cb", "term": "Sandal", "description": "Type of footwear with an open upper", "question": "Is it comfortable to wear sandals outside Esperanza Base?", "answer": false, "facts": ["Sandals are a type of footwear that leave parts of the foot exposed", "Esperanza Base is located in Antarctica", "Average temperatures in Antarctica range from -10.5C to 1.4C"], "decomposition": ["Where is Esperanza Base Located?", "What are the average temperatures in #1?", "What are the defining characteristics of sandals?", "Would #3 make a person's feet comfortable in #2 temperatures?"], "evidence": [[[["Esperanza Base-1"]], [["Esperanza Base-7"]], [["Sandal-1", "Sandal-2"]], ["operation"]], [[["Esperanza Base-1"]], [["Esperanza Base-7"]], [["Sandal-1"]], [["Esperanza Base-7", "Sandal-1"], "operation"]], [[["Esperanza Base-1"]], [["Esperanza Base-7"]], [["Sandal-1"]], ["operation"]]], "golden_sentence": [["Esperanza Base (Spanish: Base Esperanza, 'Hope Base') is a permanent, all-year-round Argentine research station in Hope Bay, Trinity Peninsula (in Graham Land on the Antarctic Peninsula)."], ["During summer (December\u2013February), the average high is between 3.7 and 4.3\u00a0\u00b0C (38.7 and 39.7\u00a0\u00b0F) while the average low is between \u22122.0 and \u22121.2\u00a0\u00b0C (28.4 and 29.8\u00a0\u00b0F)."], ["People may choose to wear sandals for several reasons, among them comfort in warm weather, economy (sandals tend to require less material than shoes and are usually easier to construct), and as a fashion choice.", ""]]}, {"qid": "45c9c8c9ba11c2f2e823", "term": "Art dealer", "description": "person that buys and sells works of art", "question": "Would an art dealer prize a print of a Van Goh? ", "answer": false, "facts": ["Van Goh painted many valuable pieces of artwork in his lifetime.", "Prints of Van Goh's artwork are readily available at a low price."], "decomposition": ["What kind of art do art dealers typically look for?", "What is the cost of a typically Van Goh print?", "Is something priced as #2 considered #1?"], "evidence": [[[["Art dealer-2"]], [["Dutch art-15"]], ["operation"]], [[["Art dealer-1"], "no_evidence"], [["Art forgery-18"], "no_evidence"], ["operation"]], [[["Art dealer-2"], "no_evidence"], [["Vincent van Gogh-4"], "no_evidence"], ["operation"]]], "golden_sentence": [["They often travel internationally, frequenting exhibitions, auctions, and artists' studios looking for good buys, little-known treasures, and exciting new works."], ["Those sold for over $100\u00a0million (today's equivalent) include Portrait of Dr. Gachet, Portrait of Joseph Roulin and Irises."]]}, {"qid": "cd0a9cdd554a7c7fa368", "term": "Lentil", "description": "Species of plant", "question": "Would Jean Harris's victim have avoided lentils?", "answer": true, "facts": ["Jean Harris spent 12 years in jail for killing her lover Herman Tarnower.", "Herman Tarnower was the creator of the Scarsdlae Diet.", "The Scarsdale Diet focuses on a simplified diet plan and forbids corn, beans, potatoes, lentils, and any beans except green beans."], "decomposition": ["Who was Jean Harris' victim?", "What is #1 famous for?", "What foods are forbidden in #2?", "Are lentils listed in #3?"], "evidence": [[[["Jean Harris-1"]], [["Scarsdale diet-1"]], [["Herman Tarnower-4"]], [["Lentil-30"], "no_evidence", "operation"]], [[["Jean Harris-1"]], [["Herman Tarnower-1"]], [["Scarsdale diet-2"]], [["Lentil-30"]]], [[["Jean Harris-6"]], [["Herman Tarnower-1"]], [["Scarsdale diet-2"], "no_evidence"], ["operation"]]], "golden_sentence": [["Jean Struven Harris (April 27, 1923 \u2013 December 23, 2012) was the headmistress of The Madeira School for girls in McLean, Virginia, who made national news in the early 1980s when she was tried and convicted of the murder of her ex-lover, Herman Tarnower, a well-known cardiologist and author of the best-selling book The Complete Scarsdale Medical Diet."], ["The Scarsdale diet is a high-protein low-carbohydrate fad diet designed for weight loss created in the 1970s by Herman Tarnower, named for the town in New York where he practiced cardiology, described in the book The Complete Scarsdale Medical Diet plus Dr. Tarnower's Lifetime Keep-Slim Program, which Tarnower wrote with self-help author Samm Sinclair Baker."], [""], [""]]}, {"qid": "c1d47d79939e2d30f3d0", "term": "The Doctor (Doctor Who)", "description": "fictional character from Doctor Who", "question": "Would Marvel's Gateway be envious of the Doctor (Doctor Who)'s TARDIS machine?", "answer": false, "facts": ["The Doctor (Doctor Who) used the TARDIS, a largely unreliable time traveling machine, to travel through time and space.", "Gateway is a Marvel comic character linked to the X-Men comics.", "Gateway has the power to create wormholes that allow him to travel through time and space."], "decomposition": ["What is the TARDIS's special power?", "What is Gateway's special power?", "Is #1 different than #2?"], "evidence": [[[["TARDIS-13", "TARDIS-14"]], [["Gateway (comics)-15"]], ["operation"]], [[["TARDIS-1"]], [["Gateway (comics)-15"]], ["operation"]], [[["TARDIS-1"]], [["Gateway (comics)-1"]], ["operation"]]], "golden_sentence": [["", ""], ["Gateway creates wormholes which allow travel through space, time and dimensions."]]}, {"qid": "da47b18ceee1f6c2dd3a", "term": "The CW", "description": "American broadcast television network", "question": "Did Supernatural break 2001 CW debuting shows seasons record?", "answer": true, "facts": ["Smallville debuted on the CW in 2001.", "Smallville had the record of most CW seasons for a show with 10.", "Supernatural concluded its run with its record breaking 15th season on the CW."], "decomposition": ["What was the debuting shows in a seasons record as of 2001 for CW?", "What was Supernatural's highest debuting shows in a season?", "Is #2 higher than #1?"], "evidence": [[[["The WB-16"], "no_evidence"], [["Supernatural (American TV series)-2"], "no_evidence"], ["no_evidence", "operation"]], [[["The CW-8"], "no_evidence"], [["Supernatural (American TV series)-102", "Supernatural (season 1)-20"], "no_evidence"], ["operation"]], [[["One Tree Hill (TV series)-4"]], [["Supernatural (American TV series)-2"]], ["operation"]]], "golden_sentence": [["With three hit shows in its roster, The WB continued to build its teen fanbase the following season with college drama Felicity (which made a star out of lead Keri Russell) and the wicca-themed Charmed (which was also produced by Aaron Spelling, and co-starred Alyssa Milano and 90210 alumnus Shannen Doherty), both of which set new records for the network when they respectively premiered to 7.1 and 7.7 million viewers; Charmed had the highest-rated premiere on the network until Smallville broke its record, debuting to 8.4 million viewers in October 2001."], [""]]}, {"qid": "613232946ef2d7830fb9", "term": "Internet slang", "description": "Slang languages used by different people on the Internet", "question": "Did Alfred Hitchcock include internet slang in his films?", "answer": false, "facts": ["Alfred Hitchcock died in 1908.", "The internet began developing slang in the 1990's."], "decomposition": ["What year did Alfred Hitchcock die?", "When did internet become available for people?", "Is #1 after #2?"], "evidence": [[[["Alfred Hitchcock-1"]], [["History of the World Wide Web-11"]], ["operation"]], [[["Alfred Hitchcock-1"]], [["World Wide Web-2"]], ["operation"]], [[["Alfred Hitchcock-70"]], [["Internet-10"]], ["operation"]]], "golden_sentence": [["Sir Alfred Joseph Hitchcock, KBE (13 August 1899\u00a0\u2013 29 April 1980) was an English film director and producer."], [""]]}, {"qid": "f2e122314b73bf83c898", "term": "Norman, Oklahoma", "description": "City in Oklahoma, United States", "question": "Is Norman Oklahoma named after a viking?", "answer": false, "facts": ["The Normans invaded England in 1066 and were originally vikings from Scandinavia.", "Norman Oklahoma was first surveyed by land surveyor Abner Norman in the 1800s.", "1066 is said to be the end of the Viking Era."], "decomposition": ["Who was the city of Norman, Oklahoma named after?", "Was #1 a viking?"], "evidence": [[[["Norman, Oklahoma-2"], "no_evidence"], ["no_evidence"]], [[["Norman, Oklahoma-5"]], ["operation"]], [[["Norman, Oklahoma-2"]], [["Vikings-1"]]]], "golden_sentence": [["The city was named in honor of Abner Norman, the area's initial land surveyor, and was formally incorporated on May 13, 1891."]]}, {"qid": "592714a5eb3bfbdae4c7", "term": "Pi", "description": "Ratio of the circumference of a circle to its diameter", "question": "Can every digit in Pi be memorized?", "answer": false, "facts": ["The digits of Pi are infinite. ", "The human mind cannot hold an infinite amount of information."], "decomposition": ["How many digits are in Pi?", "Can the human mind memorize #1 amount of information?"], "evidence": [[[["Pi-4"]], [["Memory-10", "Short-term memory-21"], "no_evidence"]], [[["Pi-16"]], ["operation"]], [[["Pi-3"]], [["Piphilology-65"], "operation"]]], "golden_sentence": [["Attempts to memorize the value of \u03c0 with increasing precision have led to records of over 70,000 digits."], ["For example, given a random seven-digit number, one may remember it for only a few seconds before forgetting, suggesting it was stored in short-term memory.", "However other prominent theories of short-term memory capacity argue against measuring capacity in terms of a fixed number of elements."]]}, {"qid": "d7f99ac77145a397d816", "term": "Greyhound", "description": "Dog breed used in dog racing", "question": "Do people associate greyhounds with the movie 'Homeward Bound'?", "answer": false, "facts": ["The movie homeward bound features a golden retriever. ", "The movie homeward bound features a pit bull type dog.", "There are no greyhounds in homeward bound."], "decomposition": ["What are the two types of dogs that are lost in Homeward Bound?", "Is a greyhound listed in #1?"], "evidence": [[[["Homeward Bound: The Incredible Journey-2"]], [["Homeward Bound: The Incredible Journey-2"], "operation"]], [[["Homeward Bound: The Incredible Journey-2"]], ["operation"]], [[["Homeward Bound: The Incredible Journey-2"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "264384b7291449ab0369", "term": "Alan Turing", "description": "British mathematician and computer scientist", "question": "Did Alan Turing suffer the same fate as Abraham Lincoln?", "answer": false, "facts": ["Alan Turing committed suicide via cyanide poisoning.", "Abraham Lincoln was killed by a gunshot wound to the head."], "decomposition": ["What did Alan Turing die of?", "What did Abraham Lincoln die of?", "Is #1 the same as #2?"], "evidence": [[[["Alan Turing-57"]], [["Abraham Lincoln-4"]], ["operation"]], [[["Alan Turing-57"]], [["Abraham Lincoln-113"]], ["operation"]], [[["Alan Turing-57"]], [["Mary Todd Lincoln-20"]], ["operation"]]], "golden_sentence": [["Cyanide poisoning was established as the cause of death."], [""]]}, {"qid": "f14adaa9ac0763fcde49", "term": "Game (hunting)", "description": "animal hunted for sport or for food", "question": "Does meat from cows fed only grass taste more like wild game?", "answer": true, "facts": ["The food an animal eats throughout its lifetime affect the way the meat from it will taste. ", "Grass-fed cows produce meat that tends to taste more mineral-dense.", "Wild game is known for a grassy, mineral taste."], "decomposition": ["What is wild game known to taste like?", "What does meat from grass-fed cows typically taste like?", "Is #1 similar to #2?"], "evidence": [[[["Game (hunting)-7"]], [["Cattle feeding-26"]], [["Cattle feeding-26", "Game (hunting)-7"]]], [[["Game (hunting)-7"]], [["Cattle feeding-5"], "no_evidence"], ["no_evidence", "operation"]], [[["Game (hunting)-7"]], ["no_evidence"], ["operation"]]], "golden_sentence": [["This difference in taste can be attributed to the wild diet of the animal, which usually results in a lower fat content compared to domestic farm raised animals."], [""], ["", ""]]}, {"qid": "d8b4a0df57a716b9ffbc", "term": "Great Pyramid of Giza", "description": "Largest pyramid in the Giza Necropolis, Egypt", "question": "Can 200 men end to end cover Great Pyramid of Giza's base?", "answer": true, "facts": ["The base of the Great Pyramid of Giza is 756 feet long.", "The average height of a man is 5 foot 9."], "decomposition": ["What is the height in inches of the average man?", "What is length in inches of the base of The Great Pyramid of Giza?", "What is 200 times #1?", "Is #3 more than #2?"], "evidence": [[[["Dinka people-3"], "no_evidence"], [["Great Pyramid of Giza-4"]], ["operation"], ["operation"]], [[["Dinka people-3"], "no_evidence"], [["Great Pyramid of Giza-4"], "no_evidence"], ["no_evidence", "operation"], ["no_evidence", "operation"]], [[["Human height-46"]], [["Great Pyramid of Giza-4"]], [["Foot (unit)-1"], "operation"], ["operation"]]], "golden_sentence": [["Roberts and Bainbridge reported the average height of 182.6\u00a0cm (5\u00a0ft 11.9\u00a0in) in a sample of 52 Dinka Agaar and 181.3\u00a0cm (5\u00a0ft 11.4\u00a0in) in 227 Dinka Ruweng measured in 1953\u20131954."], ["The lengths of the sides at the base are difficult to reconstruct, given the absence of the casing, but recent analyses put them in a range between 230.26 metres (755.4\u00a0ft) and 230.44 metres (756.0\u00a0ft)."]]}, {"qid": "d31fe27c56c0dfe2239b", "term": "Alan Turing", "description": "British mathematician and computer scientist", "question": "Would World War II have been the same without Alan Turing?", "answer": false, "facts": ["During WW2, the German Military used something called the Engima device to send messages secretly. ", "Alan Turing broke the Enigma code, allowing German messages to be understood and intercepted."], "decomposition": ["What code did Alan Turing discover during World War II?", "Without #1, would we have been able to beat the Germans?"], "evidence": [[[["Alan Turing-2"]], [["Alan Turing-3"], "no_evidence", "operation"]], [[["Alan Turing-2"]], [["Cryptanalysis of the Enigma-1"]]], [[["Alan Turing-2"]], [["Enigma machine-1"]]]], "golden_sentence": [["During the Second World War, Turing worked for the Government Code and Cypher School (GC&CS) at Bletchley Park, Britain's codebreaking centre that produced Ultra intelligence."], ["Due to the problems of counterfactual history, it is hard to estimate the precise effect Ultra intelligence had on the war, but at the upper end it has been estimated that this work shortened the war in Europe by more than two years and saved over 14\u00a0million lives."]]}, {"qid": "36b9fefa32d2761432b0", "term": "JPMorgan Chase", "description": "American multinational banking and financial services holding company", "question": "Could every citizen of Samoa send a letter to a unique JPMorgan Chase employee?", "answer": true, "facts": ["JPMorgan Chase had a total of 256,981 employees in the fourth quarter of 2019.", "The estimated population of Samoa as of July 1st, 2019 is 200,874."], "decomposition": ["How many employees does JPMorgan Chase have?", "What is the population of Samoa?", "Is #2 less than or equal to #1?"], "evidence": [[["no_evidence"], [["Samoa-64"]], ["operation"]], [[["JPMorgan Chase-83"]], [["Vatia, American Samoa-17"]], ["operation"]], [["no_evidence"], [["Samoa-64"]], ["no_evidence", "operation"]]], "golden_sentence": [["Samoa reported a population of 194,320 in its 2016 census."]]}, {"qid": "5a752c706c421f3cad65", "term": "East India Company", "description": "16th through 19th-century British trading company", "question": "Would East India Company prefer China's modern trade?", "answer": false, "facts": ["China accounts for 4.6 trillion or 12.4% of global trade.", "The East India Company took part in half of the world's trade from the 16th to 19th century."], "decomposition": ["What percent of the world's trade passed through the East India Company between the 16th and 19th centuries?", "What percent of the world's trade is accounted for by China presently?", "Is #2 greater than #1?"], "evidence": [[[["East India Company-2"]], ["no_evidence"], ["operation"]], [[["East India Company-41"], "no_evidence"], [["China-55"], "no_evidence"], ["no_evidence", "operation"]], [[["East India Company-42"], "no_evidence"], [["History of trade of the People's Republic of China-2"], "no_evidence"], ["operation"]]], "golden_sentence": [["Originally chartered as the \"Governor and Company of Merchants of London Trading into the East-Indies\", the company rose to account for half of the world's trade, particularly in basic commodities including cotton, silk, indigo dye, salt, spices, saltpetre, tea, and opium."]]}, {"qid": "23b29b971f90ef3d9907", "term": "Folk music", "description": "Music of the people", "question": "Is metal a type of folk music?", "answer": false, "facts": ["Folk music tends to be simple in structure, played on traditional acoustic instruments. Groups can be large but the song is designed to be playable by solo acts or small groups.", "Metal music is designed to be as loud and epic-sounding as possible, often with complex structures and almost always with electric instruments."], "decomposition": ["What are the basic characteristics of folk music?", "What are the characteristics of metal music?", "Does #2 exactly match #1?"], "evidence": [[[["Folk music-8"]], [["Heavy metal music-4"]], ["operation"]], [[["Folk music-15"]], [["Heavy metal music-19"]], ["operation"]], [[["Folk music-8"]], [["Heavy metal music-1"]], ["operation"]]], "golden_sentence": [[""], ["Heavy metal is traditionally characterized by loud distorted guitars, emphatic rhythms, dense bass-and-drum sound, and vigorous vocals."]]}, {"qid": "5a6ec091e72855f075a9", "term": "Lexus", "description": "luxury vehicle division of Toyota", "question": "Did George Washington drive a Lexus?", "answer": false, "facts": ["Lexus was established in 1989", "George Washington died in 1799"], "decomposition": ["In what year did George Washington die?", "What year was Lexus founded in?", "Is #1 after #2?"], "evidence": [[[["George Washington-1"]], [["Lexus-2"]], ["operation"]], [[["George Washington-1"]], [["Lexus-2"]], ["operation"]], [[["George Washington-121"]], [["Lexus-16"]], ["operation"]]], "golden_sentence": [["George Washington (February 22, 1732 \u2013 December 14, 1799) was an American political leader, military general, statesman, and founding father who served as the first president of the United States from 1789 to 1797."], ["Created at around the same time as Japanese rivals Honda and Nissan created their Acura and Infiniti luxury divisions respectively, Lexus originated from a corporate project to develop a new premium sedan, code-named F1, which began in 1983 and culminated in the launch of the Lexus LS in 1989."]]}, {"qid": "a8633540aa71bfbcb9a1", "term": "Funeral", "description": "ceremony for a person who has died", "question": "Are there special traffic laws associated with funerals?", "answer": true, "facts": ["Many funerals have a religious ceremony held in a chapel separate from the cemetery.", "The corpse and all the attendees have to move from the memorial site to the cemetery.", "Everyone at the funeral lines up their cars into a procession and follow special rules as they drive to keep the line assembled in transport."], "decomposition": ["What circumstances require special laws for the flow of traffic?", "Are funerals among #1?"], "evidence": [[[["Traffic code in the United States-3"], "no_evidence"], ["operation"]], [[["Traffic code in the United States-3"], "no_evidence"], ["operation"]], [["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""]]}, {"qid": "d6183cd4d643fc1b47d5", "term": "Chinese mountain cat", "description": "Small wild cat", "question": "Can Chinese mountain cat survive in the orbit? ", "answer": false, "facts": ["Chinese mountain cat needs to breathe oxygen for survival. ", "There is not enough oxygen for breathing in the orbit."], "decomposition": ["What does a Chinese mountain cat breathe?", "Is there enough #1 in space?"], "evidence": [[[["Chinese mountain cat-6"], "no_evidence"], [["Outer space-56"]]], [[["Breathing-18"]], [["Outer space-56"]]], [[["Chinese mountain cat-7"], "no_evidence"], [["Orbit-1", "Outer space-55", "Single-stage-to-orbit-36"], "operation"]]], "golden_sentence": [[""], ["This pressure is high enough to prevent ebullism, but evaporation of nitrogen dissolved in the blood could still cause decompression sickness and gas embolisms if not managed."]]}, {"qid": "84431b64d2d6cfeba815", "term": "Ham", "description": "Pork from a leg cut that has been preserved by wet or dry curing, with or without smoking", "question": "Can ham make a cut seal up quicker?", "answer": true, "facts": ["Salt helps cleanse the skin by displacing bacteria in a cut.", "1 Cup of ham contains 1.684 mg of sodium.", "Salt is a mineral made up mostly of sodium chloride."], "decomposition": ["Which substances can be used to cleanse skin around a cut?", "Which elements make up salt?", "Does ham contain any of #2 in considerable quantities and salt included in #1?"], "evidence": [[[["Antiseptic-1", "Wound-15"], "no_evidence"], [["Salt-1"]], [["Ham-6"], "no_evidence", "operation"]], [[["Saline (medicine)-1"]], [["Salt-1"]], [["Sodium chloride-22"], "operation"]], [[["Salt in the Wound-1"], "no_evidence"], [["Salt-1"]], [["Ham-11"], "operation"]]], "golden_sentence": [["Antiseptics (from Greek \u1f00\u03bd\u03c4\u03af anti, \"against\" and \u03c3\u03b7\u03c0\u03c4\u03b9\u03ba\u03cc\u03c2 s\u0113ptikos, \"putrefactive\") are antimicrobial substances that are applied to living tissue/skin to reduce the possibility of infection, sepsis, or putrefaction.", "For simple lacerations, cleaning can be accomplished using a number of different solutions, including tap water and sterile saline solution."], ["Salt is a mineral composed primarily of sodium chloride (NaCl), a chemical compound belonging to the larger class of salts; salt in its natural form as a crystalline mineral is known as rock salt or halite."], [""]]}, {"qid": "8383ec1b7881a009963a", "term": "Operation Barbarossa", "description": "1941 German invasion of the Soviet Union during the Second World War", "question": "Was 1941 Operation Barbarossa related to The Crusades?", "answer": true, "facts": ["Operation Barbarossa referred to the 1941 German invasion of the Soviet Union.", "Frederick Barbarosa was the Holy Roman Empire that drowned while marching his army to The Crusades.", "The Holy Roman Empire was a medieval empire that ruled over lands including what became modern Germany."], "decomposition": ["What historic figures were named Barbarosa?", "Of #1, which lived during the medieval period?", "Were any of #2 active during the Crusades?"], "evidence": [[[["Frederick I, Holy Roman Emperor-1"]], [["Frederick I, Holy Roman Emperor-1"]], [["Crusades-1", "Frederick I, Holy Roman Emperor-15"]]], [[["Frederick I, Holy Roman Emperor-1"], "no_evidence"], [["Frederick I, Holy Roman Emperor-1"]], [["Frederick I, Holy Roman Emperor-4"], "operation"]], [[["Frederick I, Holy Roman Emperor-1", "Hayreddin Barbarossa-1"]], [["Frederick I, Holy Roman Emperor-1", "Middle Ages-1"]], [["Frederick I, Holy Roman Emperor-4", "Third Crusade-1"]]]], "golden_sentence": [[""], [""], ["", ""]]}, {"qid": "b6606a73ea8d0d08866a", "term": "Prime number", "description": "Integer greater than 1 that has no positive integer divisors other than itself and 1", "question": "Would an actuary be confused about what prime numbers are?", "answer": false, "facts": ["Actuaries must go through college and rigorous studies in mathematics to obtain their jobs.", "Prime numbers are introduced in basic high school mathematics. "], "decomposition": ["Which subjects do actuaries study in college before getting their jobs?", "Which subject are prime numbers taught in?", "Is #2 included in #1?"], "evidence": [[[["Actuary-4"], "no_evidence"], [["Prime number-2"]], ["no_evidence", "operation"]], [[["Actuary-1"], "no_evidence"], [["Prime number-2"]], ["operation"]], [[["Actuary-1"]], [["Prime number-24"]], ["operation"]]], "golden_sentence": [["Actuaries use skills primarily in mathematics, particularly calculus-based probability and mathematical statistics, but also economics, computer science, finance, and business."], [""]]}, {"qid": "46f789f45ec8ff3dcb43", "term": "Yale University", "description": "Private research university in New Haven, Connecticut, United States", "question": "Can Ford F-350 tow entire Yale University student body?", "answer": false, "facts": ["The Yale University student body consists of 12,385 people according to a 2015 poll.", "The average US male weighs 195 pounds.", "The average US female weighs 168 pounds.", "The maximum towing capacity of the Ford F-350 is 15,000 pounds."], "decomposition": ["What is the maximum towing capacity of the Ford F-350?", "How much people attend Yale each year?", "What is the average weight of an adult?", "Is #2 times #3 less than #1?"], "evidence": [[[["Ford Super Duty-56"]], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Ford Super Duty-54"], "no_evidence"], [["Yale University-62"], "no_evidence"], [["Human body weight-12"], "no_evidence"], ["no_evidence", "operation"]], [[["Ford Super Duty-2"]], [["Yale University-62"]], [["Human body weight-15"], "no_evidence"], ["operation"]]], "golden_sentence": [["The F350 has a maximum 21,600 pounds (9,800\u00a0kg) of towing capacity and 7,110 pounds (3,230\u00a0kg) of payload."]]}, {"qid": "25d07ff1402822f1632c", "term": "Chuck Norris", "description": "American martial artist, actor, film producer and screenwriter", "question": "Could Chuck Norris ride a horse?", "answer": true, "facts": ["Chuck Norris is a person.", "Horses are bigger than people.", "People can ride horses. "], "decomposition": ["Who could ride a horse?", "Is Chuck Norris #1?"], "evidence": [[["no_evidence"], [["Chuck Norris-1"], "no_evidence", "operation"]], [["no_evidence"], ["no_evidence", "operation"]], [[["Equestrianism-6"]], ["operation"]]], "golden_sentence": [["Carlos Ray \"Chuck\" Norris (born March 10, 1940) is an American martial artist, actor, film producer and screenwriter."]]}, {"qid": "97e029b286cce036913e", "term": "Salmon", "description": "Family of fish related to trout", "question": "Do salmon mate in the Caspian Sea?", "answer": false, "facts": ["Salmon reproduce in freshwater", "The Caspian Sea is a saltwater lake"], "decomposition": ["What kind of water do salmon reproduce in?", "Is the Caspian Sea salt or freshwater?", "Is #1 the same as #2?"], "evidence": [[[["Salmon-2"]], [["Caspian Sea-54"]], ["operation"]], [[["Salmon-2"]], [["Caspian Sea-1"]], ["operation"]], [[["Salmon-2"]], [["Caspian Sea-2"]], ["operation"]]], "golden_sentence": [["Typically, salmon are anadromous: they hatch in fresh water, migrate to the ocean, then return to fresh water to reproduce."], [""]]}, {"qid": "35a543a1644e49d921cf", "term": "United States Army Rangers", "description": "Elite military formation of the United States Army", "question": "Would Michael J Fox qualify for the Army Rangers?", "answer": false, "facts": ["Michael J Fox has Parkinson's disease. ", "Parkinson's disease is a brain disorder that leads to shaking, stiffness, and difficulty with walking, balance, and coordination.", "To qualify for the Army Rangers,  you must complete a 12-mile march with a 35-pound rucksack and weapon in less than three hours."], "decomposition": ["What must you do to qualify for the Army Rangers?", "What condition does Michael J Fox have?", "What are some symptoms of #2?", "Could someone experiencing #3 complete #1?"], "evidence": [[[["Ranger School-17"]], [["Michael J. Fox-2"]], [["Parkinson's disease-1"]], ["operation"]], [[["75th Ranger Regiment-63"]], [["Michael J. Fox-2"]], [["Parkinson's disease-1"]], ["operation"]], [[["75th Ranger Regiment-76", "United States Army Rangers-48"], "no_evidence"], [["Michael J. Fox-2"]], [["Parkinson's disease-1"]], [["Parkinson's disease-20"], "operation"]]], "golden_sentence": [["The purpose of the course is learning to soldier as a combat leader while enduring the great mental and psychological stresses and physical fatigue of combat; the Ranger Instructors (RIs) \u2013 also known as Lane Graders \u2013 create and cultivate such a physical and mental environment."], ["Fox was diagnosed with Parkinson's disease in 1991 at age 29, and disclosed his condition to the public in 1998."], ["Other symptoms include sensory, sleep, and emotional problems."]]}, {"qid": "0f7777774b8ed6dbf670", "term": "Model (person)", "description": "person employed to display, advertise and promote products, or to serve as a visual aid", "question": "Would a model be appropriate to star in a LA Femme Nikita remake?", "answer": true, "facts": ["La Femme Nikita is a French movie about a beautiful female assassin.", "Models are known for their beauty and height.", "Peta Wilson and Maggie Q have played the lead role in La Femme Nikita spinoffs.", "Peta Wilson and Maggie Q have both done extensive modeling work."], "decomposition": ["What is La Femme Nikita?", "What is #1 about?", "Would a model be able to play #2?"], "evidence": [[[["La Femme Nikita (film)-2"]], [["La Femme Nikita (film)-2", "La Femme Nikita (film)-3"]], ["no_evidence"]], [[["La Femme Nikita (film)-1"]], [["La Femme Nikita (film)-2"]], [["Maggie Q-1", "Maggie Q-3", "Peta Wilson-1"], "operation"]], [[["La Femme Nikita (film)-1", "La Femme Nikita (film)-1"]], [["La Femme Nikita (TV series)-2", "La Femme Nikita (TV series)-3"]], ["operation"]]], "golden_sentence": [[""], ["", ""]]}, {"qid": "06e72b7d863eb35e8782", "term": "Rash", "description": "skin condition", "question": "Is CAS number 8009-03-8 harmful for a rash?", "answer": false, "facts": ["Some common substances that help rashes are creams, oils, and petroleum based products.", "CAS number 8009-03-8 is the identifier number for petroleum jelly."], "decomposition": ["What is CAS number 8009-03-8 the identifier number for?", "Is #1 harmful to put on a rash?"], "evidence": [[[["Petroleum jelly-1"]], [["Petroleum jelly-2"]]], [[["Petroleum jelly-1"]], [["Petroleum jelly-2"], "operation"]], [[["Petroleum jelly-1"]], [["Petroleum jelly-2"]]]], "golden_sentence": [["Petroleum jelly, petrolatum, white petrolatum, soft paraffin, or multi-hydrocarbon, CAS number 8009-03-8, is a semi-solid mixture of hydrocarbons (with carbon numbers mainly higher than 25), originally promoted as a topical ointment for its healing properties."], [""]]}, {"qid": "1ed1f25c9d7edbf9d769", "term": "Drew Carey", "description": "American actor, comedian, game show host, libertarian and photographer", "question": "Is Drew Carey important to the history of wrestling?", "answer": true, "facts": ["Drew Carey competed in the 2001 Royal Rumble.", "The Royal Rumble is one of the Big 4 yearly WWE pay per view events.", "Drew Carey was inducted into the WWE Hall of Fame in 2011."], "decomposition": ["What competition did Drew Carey compete in in 2001?", "Is #1 an important even for the WWE?"], "evidence": [[[["Drew Carey-2"]], [["Royal Rumble-2"], "operation"]], [[["Royal Rumble (2001)-2"]], [["Royal Rumble-2"], "operation"]], [[["Drew Carey-18"]], [["Royal Rumble-2"]]]], "golden_sentence": [["He briefly participated in professional wrestling, most notably entering the 2001 Royal Rumble, and was inducted into the WWE Hall of Fame in 2011."], ["After the initial event was broadcast as a television special on USA Network, the Royal Rumble has been shown on pay-per-view and is one of WWE's \"Big Four\", along with WrestleMania, SummerSlam, and Survivor Series."]]}, {"qid": "d858f5fd0eae33b0008e", "term": "Iggy Pop", "description": "American rock singer-songwriter, musician, and actor", "question": "Would Iggy Pop travel with Justin Bieber?", "answer": false, "facts": ["Iggy Pop is a famous punk rocker. ", "Justin Bieber is a famous pop singer.", "Punk is a music subculture that clashes against mainstream topics, ideals, and subjects.", "Pop music is a mainstream form of music constructed to appeal to the masses."], "decomposition": ["What genre of music does Iggy Pop play?", "What genre of music does Justin Beiber play?", "Would #1 and #2 go well together?"], "evidence": [[[["Iggy Pop-1"]], [["Justin Bieber-39"]], ["no_evidence"]], [[["Iggy Pop-1"]], [["Justin Bieber-39"]], [["Punk rock-1"], "operation"]], [[["Iggy Pop-1", "Iggy Pop-2"]], [["Justin Bieber-39"]], ["operation"]]], "golden_sentence": [["James Newell Osterberg Jr. (born April 21, 1947), better known as Iggy Pop, is an American singer, songwriter, musician, record producer, and actor."], ["Bieber's music is mainly pop and R&B."]]}, {"qid": "45030e86cc4e732184ef", "term": "Euro", "description": "European currency", "question": "Will a Euro sink in water?", "answer": true, "facts": ["The smallest Euro paper bill is Five Euro.", "One Euro is only available as a coin.", "Coins sink in water. ", "A metal coin is more dense than water"], "decomposition": ["What is the density of water?", "What material is an Euro coin made of?", "Is the density of #2 usually higher than #1?"], "evidence": [[[["Properties of water-14"]], [["Euro coins-50"]], [["Euro coins-50", "Properties of water-14"]]], [[["Properties of water-14"]], [["Euro coins-27"]], ["no_evidence", "operation"]], [[["Buoyancy-2", "Water-7"], "no_evidence"], [["Euro coins-50"]], [["Alloy-13"], "no_evidence", "operation"]]], "golden_sentence": [["The density of water is about 1 gram per cubic centimetre (62\u00a0lb/cu\u00a0ft): this relationship was originally used to define the gram."], ["The 1c, 2c, and 5c coins are copper-coated steel fourr\u00e9es."], ["", ""]]}, {"qid": "a5c5c2b3c0c060494c16", "term": "Larry King", "description": "American television and radio host", "question": "Did Larry King sign the Magna Carta?", "answer": false, "facts": ["The Magna Carta was a charter of rights signed by King John in 1215.", "Larry King was born in 1933."], "decomposition": ["When was Larry King born?", "When was the Magna Carta signed?", "Is #1 before #2?"], "evidence": [[[["Larry King-1"]], [["Magna Carta-1"]], ["operation"]], [[["Larry King-1"]], [["Magna Carta-1"]], ["operation"]], [[["Larry King-1"]], [["Magna Carta-5"]], ["operation"]]], "golden_sentence": [["Larry King (born Lawrence Harvey Zeiger, November 19, 1933) is an American television and radio host, whose work has been recognized with awards including two Peabodys, an Emmy award, and 10 Cable ACE Awards."], ["Magna Carta Libertatum (Medieval Latin for \"Great Charter of Freedoms\"), commonly called Magna Carta (also Magna Charta; \"Great Charter\"), is a charter of rights agreed to by King John of England at Runnymede, near Windsor, on 15 June 1215."]]}, {"qid": "4bf9b1cb67483fd05bc4", "term": "Dolce & Gabbana", "description": "Italian fashion house", "question": "Did Jackie Kennedy wear Dolce & Gabbana to her husband's inauguration?", "answer": false, "facts": ["Jackie Kennedy's husband was John F. Kennedy", "John F. Kennedy was inaugurated in 1961", "Dolce & Gabbana was founded in 1985"], "decomposition": ["Who was Jackie Kennedy married to?", "When was #1 inaugurated?", "When was Dolce & Gabbana founded?", "Was #3 before #2?"], "evidence": [[[["Jacqueline Kennedy Onassis-19"]], [["Presidency of John F. Kennedy-8"]], [["Dolce & Gabbana-4"]], [["Dolce & Gabbana-4", "Presidency of John F. Kennedy-8"], "operation"]], [[["Jacqueline Kennedy Onassis-18"]], [["Presidency of John F. Kennedy-8"]], [["Dolce & Gabbana-1"]], ["operation"]], [[["Jacqueline Kennedy Onassis-1"]], [["John F. Kennedy-47"]], [["Dolce & Gabbana-1"]], ["operation"]]], "golden_sentence": [[""], ["Kennedy was inaugurated as the nation's 35th president on January 20, 1961, on the East Portico of the United States Capitol."], [""], ["", ""]]}, {"qid": "b2bdf8cc9197642fc56c", "term": "Memory", "description": "information stored in the mind, or the mental processes involved in receiving, storing, and retrieving this information", "question": "Do quadragenarian's have little memory capacity?", "answer": false, "facts": ["Quadragenarians are people that are in their 40s.", "As people age, their memory can get worse.", "Half of people over age 50 have mild to severe memory loss.", "Ken Jennings was 46 years old when he won Jeopardy! The Greatest of All Time tournament."], "decomposition": ["How old do people generally get before their memory capacity starts getting limited?", "Quadragenarians are people within what age-range?", "Is #1 within or less than the range of #2?"], "evidence": [[[["Memory-54"]], [["2015 Chama Cha Mapinduzi presidential primaries-6"]], ["operation"]], [[["Memory-54"]], [["Aging and society-3"], "no_evidence"], ["no_evidence"]], [[["Old age-26"], "no_evidence"], ["no_evidence"], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "6db4635774ed4117febd", "term": "Islamophobia", "description": "Fear, hatred of, or prejudice against the Islamic religion or Muslims generally,", "question": "Was  Godfrey of Bouillon an Islamaphobe?", "answer": true, "facts": [" Godfrey of Bouillon lead troops during the Prince's Crusade.", "The Prince's Crusade was an attempt by Europeans to \"take back\" the city of Jerusalem from Islamic hands."], "decomposition": ["Islamaphobe indicates fear of what?", "What kind of interactions did Godfrey of Bouillon majorly have with Muslims?", "Is #2 an indication of #1?"], "evidence": [[[["Islamophobia-1"]], [["Godfrey of Bouillon-14"]], ["operation"]], [["no_evidence"], [["Godfrey of Bouillon-14"], "operation"], ["no_evidence"]], [[["Islamophobia-1"]], [["Godfrey of Bouillon-10"]], ["operation"]]], "golden_sentence": [["Islamophobia is the fear, hatred of, or prejudice against the Islamic religion or Muslims generally, especially when seen as a geopolitical force or the source of terrorism."], ["Godfrey continued to play a minor, but important,[clarification needed] role in the battles against the Muslims until the Crusaders finally reached Jerusalem in 1099."]]}, {"qid": "05a258ac4b11705ca105", "term": "Skull", "description": "bony structure that forms the skeleton of head in most vertebrates", "question": "Is the skull formed as one whole bone?", "answer": false, "facts": ["The skull forms inwards from the outside.", "There are fission lines where the multiple pieces of bone came together to form a skull. "], "decomposition": ["How many bones are found in the skull?", "Is #1 equal to one?"], "evidence": [[[["Skull-10"]], ["operation"]], [[["Skull-1"]], ["operation"]], [[["Skull-10"]], ["operation"]]], "golden_sentence": [["The human skull is generally considered to consist of twenty-two bones\u2014eight cranial bones and fourteen facial skeleton bones."]]}, {"qid": "891de45394329545fe11", "term": "Painting", "description": "Practice of applying paint, pigment, color or other medium to a surface", "question": "Is The Joy of Painting TV show still producing new episodes?", "answer": false, "facts": ["The Joy of Painting is hosted by painter Bob Ross.", "Bob Ross died in 1995.", "The episodes currently airing are reruns."], "decomposition": ["Who was the host of The Joy of Painting?", "Is #1 still alive?"], "evidence": [[[["The Joy of Painting-1"]], [["Bob Ross-26"], "operation"]], [[["The Joy of Painting-1"]], [["Bob Ross-1"], "operation"]], [[["The Joy of Painting-1"]], [["Bob Ross-26"]]]], "golden_sentence": [["The Joy of Painting was an American half-hour instructional television show created and hosted by painter Bob Ross which ran from January 11, 1983 to May 17, 1994."], ["Ross died at the age of 52 on July 4, 1995, due to complications from lymphoma, which he had battled for several years."]]}, {"qid": "2a343cc79ffdb1fe148f", "term": "James Watson", "description": "American molecular biologist, geneticist, and zoologist", "question": "Does James Watson believe that Africans are inferior to Europeans?", "answer": true, "facts": ["James Watson is a geneticist, who believes in his own work.", "James Watson is quoted as saying that genetic testing \"proves\" that Africans aren't as smart."], "decomposition": ["What profession is James Watson in? `", "As #1, what was James quoted with saying about African Americans?", "Did James Watson believe in his own work about #2?"], "evidence": [[[["James Watson-29"]], [["James Watson-48"]], ["operation"]], [[["James Watson-1"]], [["James Watson-48"]], [["James Watson-3"], "operation"]], [[["James Watson-1"]], [["James Watson-48"]], [["James Watson-50"], "operation"]]], "golden_sentence": [["In his roles as director, president, and chancellor, Watson led CSHL to articulate its present-day mission, \"dedication to exploring molecular biology and genetics in order to advance the understanding and ability to diagnose and treat cancers, neurological diseases, and other causes of human suffering.\""], [""]]}, {"qid": "1e995fe18f4e80fec2e6", "term": "Eiffel Tower", "description": "Tower located on the Champ de Mars in Paris, France", "question": "Was the Eiffel tower used as a symbol of the French Revolution?", "answer": false, "facts": ["The French Revolution took place 1789-1799.", "The Eiffel Tower was built a century later in 1888."], "decomposition": ["When was the French Revolution?", "When was the Eiffel Tower built?", "Is #2 before #1?"], "evidence": [[[["French Revolution-1"]], [["Eiffel Tower-2"]], ["operation"]], [[["French Revolution-1"]], [["Eiffel Tower-2"]], ["operation"]], [[["French Revolution-1"]], [["Eiffel Tower-2"]], ["operation"]]], "golden_sentence": [["The French Revolution (French: R\u00e9volution fran\u00e7aise [\u0281ev\u0254lysj\u0254\u0303 f\u0281\u0251\u0303s\u025b\u02d0z]) was a period of far-reaching social and political upheaval in France and its colonies beginning in 1789 and ending in 1799."], ["Constructed from 1887 to 1889 as the entrance to the 1889 World's Fair, it was initially criticised by some of France's leading artists and intellectuals for its design, but it has become a global cultural icon of France and one of the most recognisable structures in the world."]]}, {"qid": "954056f097a92f85d521", "term": "Al Pacino", "description": "American actor", "question": "Will Al Pacino and Margaret Qualley score same amount of Bacon Number points?", "answer": true, "facts": ["The Bacon Number refers to a game in which people find how close a person is to the actor Kevin Bacon based on similar costars.", "Margaret Qualley was in Novitiate with Julianne Nicholson who was in Black Mass with Kevin Bacon (Bacon Number of 2).", "Al Pacino was in The Devil's Advocate with Charlize Theron who was in Trapped with Kevin Bacon (Bacon Number of 2).", "The further away someone is from Kevin Bacon, the more points scored in Bacon Number.", "Major General William Rufus Shafter is believed to produce the highest Bacon Number score of 10."], "decomposition": ["What is Al Pacino's Bacon Number?", "What is Margaret Qualley's Bacon Number?", "Is #1 equal to #2?"], "evidence": [[[["Al Pacino-22", "Six Degrees of Kevin Bacon-10"], "no_evidence"], [["Margaret Qualley-16"], "no_evidence"], ["no_evidence", "operation"]], [[["Al Pacino-2"], "no_evidence"], [["Six Degrees of Kevin Bacon-10"], "no_evidence"], ["operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["", "The Bacon number of an actor is the number of degrees of separation he or she has from Bacon, as defined by the game."], [""]]}, {"qid": "bfed6907a64d11ee1eeb", "term": "Salsa (sauce)", "description": "Sauce", "question": "Would Carolina Reaper decrease sales if added to all US salsa?", "answer": true, "facts": ["On average, Americans prefer milder salsa than Europeans.", "The Carolina Reaper is the hottest pepper in the world. ", "The Carolina Reaper is rated as 2,200,000 Scoville Heat Units."], "decomposition": ["On average, what level of spice do Americans prefer for their salsa?", "Compared to other peppers, how hot is the Carolina Reaper?", "Would adding #2 to salsa create a salsa that is #1?"], "evidence": [[[["Salsa (sauce)-2"]], [["Carolina Reaper-2"]], ["operation"]], [["no_evidence"], [["Carolina Reaper-1"]], ["no_evidence", "operation"]], [[["Salsa (sauce)-2"], "no_evidence"], [["Carolina Reaper-2"]], [["Salsa (sauce)-6"], "no_evidence"]]], "golden_sentence": [[""], ["Bred in a Rock Hill, South Carolina greenhouse by \"Smokin\" Ed Currie, proprietor of the PuckerButt Pepper Company in Fort Mill, the Carolina Reaper was certified as the world's hottest chili pepper by the Guinness World Records on August 11, 2017."]]}, {"qid": "5a095e486fb30f1d59f7", "term": "HIV", "description": "Human retrovirus, cause of AIDS", "question": "Would fans of Jonathan Larson be unaware of HIV?", "answer": false, "facts": ["Jonathan Larson died of AIDS in 1996.", "Jonathan Larson produced music and plays about HIV, AIDS, and poverty."], "decomposition": ["What works did Jonathan Larson produce?", "Do all of #1 avoid the topic of HIV?"], "evidence": [[[["Jonathan Larson-1"]], [["Tick, Tick... Boom!-12"], "operation"]], [[["Jonathan Larson-1"]], [["Rent (musical)-1"]]], [[["Jonathan Larson-1"]], [["Rent (musical)-1"], "operation"]]], "golden_sentence": [["Typical examples of his use of these themes are found in his works Rent and Tick, Tick... Boom!"], [""]]}, {"qid": "ea0d0bb485e0e1418b6e", "term": "Paralympic Games", "description": "Major international sport event for people with disabilities", "question": "Can Josh Blue participate in Paralympics Games? ", "answer": true, "facts": ["Josh Blue has cerebral palsy. ", "People with cerebral palsy can compete in the Paralympic Games."], "decomposition": ["What chronic illness does Josh Blue have?", "What conditions make one eligible to compete in the Paralympic Games?", "Is #1 included in #2?"], "evidence": [[[["Josh Blue-1"]], [["Paralympic Games-4"]], [["Ataxia-1", "Cerebral palsy-1"], "operation"]], [[["Josh Blue-4"], "no_evidence"], [["Paralympic Games-40"], "no_evidence"], ["no_evidence"]], [[["Josh Blue-1"]], [["Paralympic Games-1", "Paralympic Games-40"], "no_evidence"], ["operation"]]], "golden_sentence": [["Blue has cerebral palsy, and much of his self-deprecating humor is centered on this."], ["The categories are impaired muscle power, impaired passive range of movement, limb deficiency, leg length difference, short stature, hypertonia, ataxia, athetosis, vision impairment and intellectual impairment."], ["", ""]]}, {"qid": "41bc86d7421a4a421be8", "term": "Macaque", "description": "genus of Old World monkeys", "question": "Can you hide a pet macaque under your desk?", "answer": true, "facts": ["Macaques grow up to 28 inches in length", "A typical desk is 29 to 30 inches from the ground"], "decomposition": ["How tall is a macaque?", "How tall is a typical desk?", "Is #2 more than #1?"], "evidence": [[[["Japanese macaque-3"]], [["Standing desk-4"]], ["operation"]], [[["Macaque-4"]], [["Desk-2"], "no_evidence"], ["operation"]], [[["Macaque-3"], "operation"], [["Desk-12"]], ["no_evidence"]]], "golden_sentence": [["Male average height is 57.01\u00a0cm (22.44\u00a0in) and female average height is 52.28\u00a0cm (20.58\u00a0in)."], ["While height of most seated desks is standardized, standing desks are made in many different heights ranging from 70 to 128 centimetres (28 to 50\u00a0in)."]]}, {"qid": "7912115eb4ff2add7da5", "term": "Horseradish", "description": "species of plant", "question": "Does horseradish have a fetlock?", "answer": false, "facts": ["Horseradish is a type of plant that is used as a condiment.", "Fetlock is the common name used for a joint found in horses."], "decomposition": ["What kingdom is horseradish in?", "Where is a fetlock found?", "What kingdom is #2 in?", "is #1 the same as #3?"], "evidence": [[[["Horseradish-1"], "no_evidence"], [["Fetlock-1"]], [["Horse-1"], "no_evidence"], ["operation"]], [[["Horseradish-4"]], [["Fetlock-1"]], [["Horse-48"], "no_evidence"], ["operation"]], [[["Horseradish-1"], "no_evidence"], [["Fetlock-1"]], [["Horse-1"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], ["Fetlock is the common name in horses, large animals, and sometimes dogs for the metacarpophalangeal and metatarsophalangeal joints (MCPJ and MTPJ)."], [""]]}, {"qid": "0feed95cf03b53198455", "term": "Diamond", "description": "Allotrope of carbon often used as a gemstone and an abrasive", "question": "Can a diamond float on water?", "answer": false, "facts": ["Diamonds are formed by extreme heat and pressure being applied to carbon under the earth's crust.", "The density of a diamond is 3.51 g/cm\u00b3.", "The density of water is 997 kg/m\u00b3.", "A diamond is more dense than water.", "A diamond will sink in water."], "decomposition": ["What is the density of a diamond?", "What is the density of water?", "Is #1 less than #2?"], "evidence": [[["no_evidence"], [["Maximum density-3"]], ["operation"]], [[["Diamond-5"]], [["Water-7"]], ["no_evidence"]], [[["Diamond-5"]], [["Water-7"]], ["operation"]]], "golden_sentence": [["An especially notable irregular maximum density is that of water, which reaches a density peak at 3.98\u00a0\u00b0C (39.16\u00a0\u00b0F)."]]}, {"qid": "0cd26b505dfe6719e167", "term": "Canon Inc.", "description": "Japanese multinational corporation specialised in the manufacture of imaging and optical products", "question": "Is Canon Inc. a Kabushiki gaisha?", "answer": true, "facts": ["Canon, Inc. is a large corporation listed on the Tokyo Stock Exchange.", "Companies are listed on a stock exchange so brokers can buy and sell stock in those companies.", "Kabushiki gaisha is often translated as \"stock company\", \"joint-stock company\" or \"stock corporation\". "], "decomposition": ["What is Kabushiki gaisha usually translated as?", "Is Canon Inc. listed in the appropriate organization to be considered #1?"], "evidence": [[[["Kabushiki gaisha-1"]], [["Canon Inc.-1", "Canon Inc.-2"]]], [[["Kabushiki gaisha-1"]], [["Canon Inc.-1"]]], [[["Kabushiki gaisha-1"]], [["Canon Inc.-1"]]]], "golden_sentence": [["A kabushiki gaisha (Japanese: \u682a\u5f0f\u4f1a\u793e, pronounced\u00a0[kab\u026f\u0255i\u0325ki \u0261a\ua71ci\u0255a]; literally \"share company\") or kabushiki kaisha, commonly abbreviated KK, is a type of company (\u4f1a\u793e, kaisha) defined under the Companies Act of Japan."], ["", ""]]}, {"qid": "de1eb35c5b5d9890c1c7", "term": "Very Large Telescope", "description": "telescope in the Atacama Desert, Chile", "question": "Is it possible to get killed walking to the Very Large Telescope?", "answer": true, "facts": ["The Very Large Telescope is in the Atacama Desert", "The Atacama Desert is the driest hot desert in the world."], "decomposition": ["Where is the Very Large Telescope?", "How hot is it in #1?", "Is it possible to die from being somewhere that is #2?"], "evidence": [[[["Very Large Telescope-1"]], [["Atacama Desert-7"]], [["Desert-4"]]], [[["Very Large Telescope-1"]], [["Arabian Desert-8"]], [["Heat stroke-1"], "operation"]], [[["Very Large Telescope-1"]], [["Atacama Desert-23"], "no_evidence"], [["Evan Tanner-27", "Evan Tanner-29"], "operation"]]], "golden_sentence": [["The Very Large Telescope (VLT) is a telescope facility operated by the European Southern Observatory on Cerro Paranal in the Atacama Desert of northern Chile."], [""], [""]]}, {"qid": "b79d8c05fe63b718f9cf", "term": "June", "description": "sixth month in the Julian and Gregorian calendars", "question": "Is it possible that June got its name from mythology?", "answer": true, "facts": ["June may have been named after Juno.", "Juno was a Roman goddess and wife of the Roman king of the gods Jupiter."], "decomposition": ["Who was June possibly named after?", "Is #1 a figure in mythology?"], "evidence": [[[["June-4"]], [["Juno (mythology)-1"]]], [[["June-4"]], ["operation"]], [[["June-4"]], [["Juno (mythology)-1"]]]], "golden_sentence": [["The first is that the month is named after the Roman goddess Juno, the goddess of marriage and the wife of the supreme deity Jupiter; the second is that the name comes from the Latin word iuniores, meaning \"younger ones\", as opposed to maiores (\"elders\") for which the preceding month May (Maius) may be named."], ["As the patron goddess of Rome and the Roman Empire, Juno was called Regina (\"Queen\") and was a member of the Capitoline Triad (Juno Capitolina), centered on the Capitoline Hill in Rome; it consisted of her, Jupiter, and Minerva, goddess of wisdom."]]}, {"qid": "f2faee9cb9597b7f6bf6", "term": "Reza Shah", "description": "Shah of Iran, Founder of the Imperial state of iran", "question": "Could Reza Shah be related to Queen Elizabeth I?", "answer": false, "facts": ["Queen Elizabeth I was from English parents.", "Reza Shah was Mazanderani.", "Mazanderani people are indigenous people of Iran.", "Iran is nearly 4,000 miles from England."], "decomposition": ["Where are Queen Elizabeth I's parents from?", "Where is Reza Shah's family from?", "Is #1 near #2?"], "evidence": [[[["Anne Boleyn-6", "Elizabeth I of England-6", "Henry VIII of England-5"]], [["Reza Shah-4"]], [["England-1", "Iran-1"]]], [[["Elizabeth I (disambiguation)-1"]], [["Reza Shah-4"]], [["Elizabeth I (disambiguation)-1"]]], [[["Elizabeth I of England-6"]], [["Reza Shah-4"]], ["operation"]]], "golden_sentence": [["However, the siblings were born in Norfolk at the Boleyn home at Blickling.", "Elizabeth was born at Greenwich Palace and was named after her grandmothers, Elizabeth of York and Elizabeth Howard.", "Born 28 June 1491 at the Palace of Placentia in Greenwich, Kent, Henry Tudor was the third child and second son of Henry VII and Elizabeth of York."], ["His mother was a Muslim immigrant from Georgia (then part of the Russian Empire), whose family had emigrated to mainland Persia (Iran) after Qajar Empire was forced to cede all of its territories in the Caucasus following the Russo-Persian Wars several decades prior to Reza Shah's birth."], ["", ""]]}, {"qid": "231e5f5b1350d6f38613", "term": "J. D. Salinger", "description": "American writer", "question": "Was Anthony Quinn more prolific at making children than J.D. Salinger?", "answer": true, "facts": ["Author J.D. Salinger had two children.", "Actor Anthony Quinn had twelve children."], "decomposition": ["How many children did J. D. Salinger have?", "How many children did Anthony Quinn have?", "Is #2 greater than #1?"], "evidence": [[[["J. D. Salinger-29"]], [["Anthony Quinn-30", "Anthony Quinn-31", "Anthony Quinn-32", "Anthony Quinn-33"]], ["operation"]], [[["J. D. Salinger-29"]], [["Anthony Quinn-31", "Anthony Quinn-33"]], ["operation"]], [[["J. D. Salinger-29"], "no_evidence"], [["Anthony Quinn-30"]], ["operation"]]], "golden_sentence": [["They had two children, Margaret (also known as Peggy \u2013 born December 10, 1955) and Matthew (born February 13, 1960)."], ["The couple had five children: Christopher (1938\u20131941), Christina (born December 1, 1941), Catalina (born November 21, 1942), Duncan (born August 4, 1945), and Valentina (born December 26, 1952).", "They had three children: Francesco Quinn (March 22, 1963 \u2013 August 5, 2011), Danny Quinn (born April 16, 1964), and Lorenzo Quinn (born May 7, 1966).", "citation needed] In the 1970s, during his marriage to Addolori, Quinn also had two children with an event producer in Los Angeles named Friedel Dunbar: Sean Quinn (born February 7, 1973) and Alexander Anthony Quinn (born December 30, 1976).", "By the 1990s, Quinn then had two children with his secretary, Katherine Benvin; daughter Antonia Patricia Rose Quinn (born July 23, 1993) and son Ryan Nicholas Quinn (born July 5, 1996)."]]}, {"qid": "865cceb63399e528bd70", "term": "Menthol", "description": "chemical compound", "question": "Does menthol make cigarettes less addictive?", "answer": false, "facts": ["The addition of menthol to cigarettes does not reduce the amount of nicotine in them.", "Menthol itself is an addictive chemical. ", "Nicotine is the primary addictive component of cigarettes."], "decomposition": ["What is the primary addictive components in cigarettes?", "Does addition of menthol cause a reduction in #1?"], "evidence": [[[["Cigarette-3"]], [["Menthol cigarette-32"]]], [[["Nicotine-11"]], [["Menthol cigarette-29"]]], [[["Cigarette-1"]], [["Menthol cigarette-29"]]]], "golden_sentence": [["Cigarette smoke contains over 7,000 chemical compounds, including arsenic, formaldehyde, hydrogen cyanide, lead, nicotine, carbon monoxide, acrolein, and other poisonous substances."], [""]]}, {"qid": "baead18331077f292533", "term": "French Revolution", "description": "Revolution in France, 1789 to 1798", "question": "Did France win the French Revolution?", "answer": false, "facts": ["The French Revolution was a period of  social and political upheaval in France and its colonies.", "War is an intense military conflict between two states.", "The French Revolution involved only France as citizens overthrew the monarchy."], "decomposition": ["Which parties were involved in the French Revolution?", "Did #1 involve France and another country or state?"], "evidence": [[[["French Revolution-1"]], ["operation"]], [[["French Revolution-1"]], [["The Old Regime and the Revolution-3"], "operation"]], [[["French Revolution-1"]], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "084a7fc181a1d772d5cc", "term": "Earth Day", "description": "Annual event on 22 April", "question": "Do people celebrate Earth Day with a ceremonial tire fire?", "answer": false, "facts": ["Earth Day is a global holiday to show support for environmental protection.", "Tire fire is a large quantity of tires burning at once.", "Smoke from burning tires contain heavy metals and other harmful pollutants.", "Smoke is harmful to the environment. "], "decomposition": ["What is the major focus/purpose of the Earth Day holiday?", "What are the environmental implications of tire fire?", "Is #2 consistent with #1?"], "evidence": [[[["Earth Day-30"]], [["Tire fire-3"]], [["Tire fire-3"]]], [[["Earth Day-1"]], [["Scientific consensus on climate change-43"]], ["operation"]], [[["Earth Day-1"]], [["Tire fire-1"]], ["operation"]]], "golden_sentence": [[""], [""], [""]]}, {"qid": "6b9592cb8842aa6ca191", "term": "Olive", "description": "Species of plant", "question": "Would you find olives at a heladeria?", "answer": false, "facts": ["Olives are fruits of the olive tree used in savory dishes and preparations like olive oil and tapenade", "A heladeria is an ice cream parlour"], "decomposition": ["What kinds of foods are served at a heladeria?", "Are olives a type of #1?"], "evidence": [[[["Lares Ice Cream Parlor-4"]], [["Olive-6"]]], [[["Helader\u00eda Coromoto-1"]], [["Olive-2"], "operation"]], [[["Helader\u00eda Coromoto-1"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "05dc1c0d16a5080080b1", "term": "Pope Alexander VI", "description": "Pope of the Catholic Church 1492\u20131503", "question": "Was Pope Alexander VI's origin country least represented in papal history?", "answer": false, "facts": ["Pope Alexander VI, born Rodrigo Borgia, was born in a town in eastern Spain.", "There have been two Popes whose origins are from Spain, including Pope Alexander VI and Pope Callixtus III.", "Pope John Paul II was born in Poland.", "Pope John Paul II is the only pope of Polish origin."], "decomposition": ["What is Pope Alexander VI's home country?", "How many popes have come from #1?", "Is it the case that no countries have produced a non-zero number of popes that is less than #2?"], "evidence": [[[["Pope Alexander VI-2"]], [["Pope Alexander VI-2", "Pope Callixtus III-2"], "no_evidence"], [["Pope-51"], "operation"]], [[["Pope Alexander VI-2"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Pope Alexander VI-1"]], [["House of Borgia-8", "Pope Callixtus I-1"], "no_evidence"], [["Pope Francis-1"], "no_evidence", "operation"]]], "golden_sentence": [["Born into the prominent Borgia family in X\u00e0tiva in the Crown of Aragon (Now Spain), Rodrigo studied law at the University of Bologna."], ["He proceeded to serve in the Curia under the next four popes, acquiring significant influence and wealth in the process.", ""], [""]]}, {"qid": "91c7fe042f7648304252", "term": "Newspeak", "description": "fictional language in the novel Nineteen Eighty-Four", "question": "Is Newspeak considered very straightforward?", "answer": false, "facts": ["The vocabulary in Newspeak is extremely restricted.", "In Newspeak, the Ministry of Truth manufactures lies for the public to consume.", "In Newspeak, the Ministry of Plenty artificially creates scarcity.", "To be straightforward is to be uncomplicated and easy to understand."], "decomposition": ["In Newspeak, how do words relate to the ideas they represent?", "Is #1 explicit and easy to understand?"], "evidence": [[[["Newspeak-2"]], [["Newspeak-11"]]], [[["Ministries of Nineteen Eighty-Four-2"]], [["Ministries of Nineteen Eighty-Four-3"]]], [[["Newspeak-11"], "no_evidence"], ["operation"]]], "golden_sentence": [["Linguistically, the political contractions of Newspeak\u2014Ingsoc (English Socialism), Minitrue (Ministry of Truth), etc.\u2014derive from the syllabic abbreviations of Russian, which identify the government and social institutions of the Soviet Union, such as politburo (Political Bureau of the Central Committee of the Communist Party of the Soviet Union), Comintern (Communist International), kolkhoz (collective farm), and Komsomol (Young Communists' League)."], ["ante \u2014 The prefix that replaces before artsem \u2014 Artificial insemination bb \u2014 Big Brother bellyfeel \u2014 The blind, enthusiastic acceptance of an idea blackwhite \u2014 When used on an opponent, it means to believe that black is white, despite the facts; on a Party member, it means the ability to believe that black is white, to know that black is white, and to forget that one ever believed the contrary crimestop \u2014 To rid oneself of unorthodox thoughts that interfere with believing the tenets of Ingsoc's ideology crimethink \u2014 The criminal act of holding politically unorthodox thoughts that contradict the tenets of Ingsoc, frequently referred to by the standard English \u201cthoughtcrime\u201d dayorder \u2014 Order of the day doubleplusgood \u2014 The word that replaced Oldspeak words meaning \"superlatively good\", such as excellent, fabulous, and fantastic doubleplusungood \u2014 The word that replaced Oldspeak words meaning \"superlatively bad\", such as terrible and horrible doublethink \u2014 The act of simultaneously believing two, mutually contradictory ideas duckspeak \u2014 Automatic, vocal support of political orthodoxies facecrime \u2014 A facial expression which communicates that they have committed thoughtcrime Ficdep \u2014 The Ministry of Truth's Fiction Department free \u2014 The absence and the lack of something \u2013ful \u2014 The suffix for forming an adjective good \u2014 A synonym for \"orthodox\" and orthodoxy goodthink \u2014 Political orthodoxy as defined by the Party goodsex \u2014 Sexual intercourse only for procreation, with zero physical pleasure on the woman's part, and strictly in a marriage context ingsoc \u2014 English Socialism joycamp \u2014 Labour camp malquoted \u2014 Inaccurate representations of the words of Big Brother and of the Party Miniluv \u2014 The Ministry of Love, where the secret police interrogate and torture the enemies of Oceania Minipax \u2014 The Ministry of Peace, who wage defensive war for Oceania Minitrue \u2014 The Ministry of Truth, who manufacture consent by way of propaganda and distorted historical records, while supplying the proles (proletariat) with culture and entertainment Miniplenty \u2014 The Ministry of Plenty, who keep the population in continual economic hardship (starvation and rationing) Oldspeak \u2013 Standard English oldthink \u2014 Ideas from the time before the Party's revolution ownlife \u2014 A person's anti-social tendency to enjoy solitude and individualism plusgood \u2014 The word that replaced Oldspeak words meaning \"very good\", such as great plusungood \u2014 The word that replaced Oldspeak words meaning \"very bad\" Pornosec \u2014 The pornography production section (Pornography sector) of the Ministry of Truth's Fiction Department prolefeed \u2014 Popular culture for entertaining Oceania's working class Recdep \u2014 The Ministry of Truth's Records Department, where Winston Smith edits historical records so they conform to the Party's agenda rectify \u2014 The Ministry of Truth's euphemism for manipulating a historical record ref \u2014 To refer (to someone or something) sec \u2014 Sector sexcrime \u2014 A sexual immorality, such as fornication, adultery, oral sex, and homosexuality speakwrite \u2014 A machine that transcribes speech into text Teledep \u2014 The Ministry of Truth's Telecommunications Department telescreen \u2014 A two-way television set with which the Party spy upon Oceania's population thinkpol \u2014 The Thought Police unperson \u2014 An executed person whose existence is erased from public and private memory upsub \u2014 An upwards submission to higher authority \u2013wise \u2014 The only suffix for forming an adverb The words of the A vocabulary describe the functional concepts of daily life (e.g."]]}, {"qid": "afdf6ddf15dec507ef80", "term": "Myocardial infarction", "description": "Interruption of blood supply to a part of the heart", "question": "Is myocardial infarction a brain problem?", "answer": false, "facts": ["Myocardial infarction is a problem in the heart.", "The equivalent in the brain would be similar to a stroke."], "decomposition": ["Which organ in the body does myocardial infarction affect?", "Is #1 the same as the brain?"], "evidence": [[[["Myocardial infarction-1"]], ["operation"]], [[["Myocardial infarction-1"]], [["Brain-1"], "operation"]], [[["Myocardial infarction-1"]], ["operation"]]], "golden_sentence": [["A myocardial infarction (MI), also known as a heart attack, occurs when blood flow decreases or stops to a part of the heart, causing damage to the heart muscle."]]}, {"qid": "026d179b57d6492798ca", "term": "Northwest Airlines", "description": "1926\u20132010 major airline, merged into Delta Air Lines", "question": "Did Northwest Airlines' longevity surpass Betty White?", "answer": false, "facts": ["Northwest Airlines lasted 84 years from 1926-2010.", "Betty White is 98 years old as of 2020."], "decomposition": ["How many years was Northwest Airlines in business?", "How old is Betty White?", "Is #1 greater than #2?"], "evidence": [[[["Northwest Airlines-1"]], [["Betty White-1"]], ["operation"]], [[["Northwest Airlines-1"]], [["Betty White-1"]], ["operation"]], [[["Northwest Airlines-1"]], [["Betty White-1"]], ["operation"]]], "golden_sentence": [[""], ["Betty Marion White Ludden (n\u00e9e White; born January 17, 1922) is an American actress and comedian, with the longest television career of any entertainer, spanning more than 80 years."]]}, {"qid": "3c1f3e0e62bb2d9a3a7c", "term": "Hurricane Maria", "description": "Category 5 Atlantic hurricane in 2017", "question": "Could you windsurf in Puerto Rico during Hurricane Maria?", "answer": false, "facts": ["Hurricane Maria was a deadly category 5 hurricane with wind speed up to 175mph.", "It is extremely dangerous and impossible to windsurf with wind speed higher than 100mph. "], "decomposition": ["What wind speed did Hurricane Maria reach?", "Above what wind speed is windsurf extremely dangerous?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Hurricane Maria-2"]], [["Windsurfing-60"]], ["operation"]], [[["Hurricane Maria-2"]], [["Windsurfing-60"], "no_evidence"], ["operation"]], [[["Hurricane Maria-2"]], [["Windsurfing-86"], "no_evidence"], ["operation"]]], "golden_sentence": [["After weakening slightly due to crossing Dominica, Maria achieved its peak intensity over the eastern Caribbean with maximum sustained winds of 175\u00a0mph (280\u00a0km/h) and a pressure of 908\u00a0mbar (hPa; 26.81\u00a0inHg)."], [""]]}, {"qid": "2f298bec5cd04f4010b7", "term": "Tourism", "description": "travel for recreational or leisure purposes", "question": "Do tourists prefer Tuvalu to Niue?", "answer": false, "facts": ["Tuvalu receives an average of 2,000 annual tourists.", "Niue receives an average of 10,000 annual tourists."], "decomposition": ["What is the average number of tourists that visit Tuvalu annually?", "What is the average number of tourists that visit Niue annually?", "Is #1 more than #2?"], "evidence": [[[["Tuvalu-6"], "no_evidence"], [["Niue-17"], "no_evidence"], ["no_evidence"]], [[["Tuvalu-105"]], [["Niue-54"], "no_evidence"], ["operation"]], [[["Tuvalu-105"]], [["Niue-54"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "6f028f57ea6f44a2cc04", "term": "Pacific War", "description": "Theater of World War II fought in the Pacific and Asia", "question": "Were muskets used in the Pacific War?", "answer": false, "facts": ["The Pacific War took place between 1941 and 1945.", "The musket became obsolete in modern warfare starting near 1870."], "decomposition": ["When was the Pacific War?", "When did muskets become obsolete?", "Is #1 before #2?"], "evidence": [[[["Pacific War-2"]], [["Musket-29"]], ["operation"]], [[["Pacific War-2"]], [["Musket-1"]], ["operation"]], [[["Pacific War-1"]], [["Musket-1"]], ["operation"]]], "golden_sentence": [["However, it is more widely accepted that the Pacific War itself began on 7/8 December 1941, when the Japanese invaded Thailand and attacked the British colonies of Malaya, Singapore, and Hong Kong as well as the United States military and naval bases in Hawaii, Wake Island, Guam, and the Philippines."], ["From this period (c. 1870) on, the musket was obsolete in modern warfare."]]}, {"qid": "374e803e4a1ea878412a", "term": "Autopilot", "description": "system to maintain vehicle trajectory in lieu of direct operator command", "question": "Does autopilot rely on fossil fuels?", "answer": true, "facts": ["Autopilot is used in vehicles.", "Vehicles are powered by engines.", "Engines rely on fossil fuels to operate."], "decomposition": ["What uses autopilot?", "What powers #1?", "Does #2 rely on fossil fuels to run?"], "evidence": [[[["Autopilot-1"]], [["Aircraft-1"]], [["Jet engine-3"], "operation"]], [[["Autopilot-12"]], [["Jet fuel-1"]], ["operation"]], [[["Autopilot-1"]], [["Aviation fuel-4", "Jet fuel-5"]], ["operation"]]], "golden_sentence": [["An autopilot is a system used to control the trajectory of an aircraft, marine craft or spacecraft without constant manual control by a human operator being required."], [""], [""]]}, {"qid": "c8e84725c89fec4095ab", "term": "Louvre", "description": "Art museum and Historic site in Paris, France", "question": "Can nitric acid break the Louvre?", "answer": true, "facts": ["Parts of the Louvre are built of limestone.", "Nitric acid dissolves limestone."], "decomposition": ["What materials were used to build the Louvre?", "Can any of #1 be destroyed by nitric acid?"], "evidence": [[[["Louvre Pyramid-2"]], [["Nitric acid-18"], "no_evidence", "operation"]], [[["Louvre-21"], "no_evidence"], ["no_evidence"]], [[["Louvre-1"], "no_evidence"], [["Nitric acid-25"], "no_evidence", "operation"]]], "golden_sentence": [["The structure, which was constructed entirely with glass segments and metal poles,"], [""]]}, {"qid": "db9ada8e53b188b72565", "term": "Durian", "description": "genus of plants", "question": "Would a Durian be dangerous if it fell on your head?", "answer": true, "facts": ["Durian weight 2-7 lbs.", "Durian have a hard, spiky external shell.", "There are several reports of injury and death related to falling Durian fruit."], "decomposition": ["How much does a durian usually weigh?", "What is the rind of a durian covered with?", "Is an object with a weight of #1 covered with #2 dangerous?"], "evidence": [[[["Durian-2"]], [["Durian-2"]], ["operation"]], [[["Durian-2"]], [["Durian-2"]], [["Durian-2"]]], [[["Durian-2"]], [["Durian-14"]], ["operation"]]], "golden_sentence": [["The fruit can grow as large as 30 centimetres (12\u00a0in) long and 15 centimetres (6\u00a0in) in diameter, and it typically weighs one to three\u00a0kilograms (2 to 7\u00a0lb)."], ["Named in some regions as the \"king of fruits\", the durian is distinctive for its large size, strong odour, and thorn-covered rind."]]}, {"qid": "9d6f9e18b9fcbc85dcae", "term": "Coen brothers", "description": "American filmmakers", "question": "Did the Coen brothers ever collaborate with the Brothers Grimm?", "answer": false, "facts": ["The Coen brothers were born in 1954 and 1957.", "The Brothers Grimm died in 1859 and 1863."], "decomposition": ["In what century were the Coen brothers born?", "In what century did the Brothers Grimm die?", "Is #1 before #2?"], "evidence": [[[["Coen brothers-1"]], [["Brothers Grimm-1"]], ["operation"]], [[["Coen brothers-1"]], [["Brothers Grimm-1"]], ["operation"]], [[["20th century-2", "Coen brothers-1"]], [["19th century-1", "Brothers Grimm-1"]], ["operation"]]], "golden_sentence": [[""], ["The Brothers Grimm (die Br\u00fcder Grimm or die Gebr\u00fcder Grimm, German: [di\u02d0 \u0261\u0259\u02c8b\u0281y\u02d0d\u0250 \u0261\u0281\u026am] (listen)), Jacob Ludwig Karl Grimm (1785\u20131863) and Wilhelm Carl Grimm (1786\u20131859), were German academics, philologists, cultural researchers, lexicographers and authors who together collected and published folklore during the 19th century."]]}, {"qid": "120375162915b0a2116f", "term": "Asian black bear", "description": "species of mammal", "question": "Can an Asian black bear use chopsticks?", "answer": false, "facts": ["Asian black bear are a species of bear found in asia. ", "Asian black bear don't have opposable thumbs", "Chopsticks are eating utensils use requires opposable thumbs."], "decomposition": ["In order to use chopsticks, what body part does one need?", "Do Asian black bears have #1?"], "evidence": [[[["Chopsticks-18"]], [["Bear-1"]]], [[["Finger-2"], "no_evidence"], [["Asian black bear-6"], "no_evidence"]], [[["Chopsticks-18"]], [["Asian black bear-41"], "operation"]]], "golden_sentence": [["To use chopsticks, the lower chopstick is stationary, and rests at the base of the thumb, and between the ring finger and middle finger."], [""]]}, {"qid": "b47285929cb67547d08c", "term": "Yin and yang", "description": "philosophical concept", "question": "Are rainbows devoid of color made by mixing yin and yang colors?", "answer": true, "facts": ["Yin and Yang are a philosophical Chinese concept represented by the color black and white.", "Black and white when mixed together create the color gray.", "The rainbow contains the colors: red, orange, yellow, green, blue, indigo and violet."], "decomposition": ["What colors do the 'yin and yang' concept represent?", "What are the colors of the rainbow?", "What color would be obtained by mixing #1?", "Is #3 included in #2?"], "evidence": [[[["Yin and yang-1"]], [["ROYGBIV-1"]], [["Shades of gray-7"]], ["operation"]], [[["Yin and yang-40"]], [["ROYGBIV-1"]], [["Grey-1"]], ["operation"]], [[["Yin and yang-40"]], [["ROYGBIV-1"]], [["Grey-21"]], ["operation"]]], "golden_sentence": [["Yin is the receptive and Yang the active principle, seen in all forms of change and difference such as the annual cycle (winter and summer), the landscape (north-facing shade and south-facing brightness), sexual coupling (female and male), the formation of both women and men as characters and sociopolitical history (disorder and order)."], ["ROYGBIV or Roy G. Biv is an acronym for the sequence of hues commonly described as making up a rainbow: red, orange, yellow, green, blue, indigo and violet."], ["The various tones of achromatic gray are along the axis of the color sphere from white at the top of the axis to black at the bottom of the axis."]]}, {"qid": "ff1fa32d969a5e9df8fb", "term": "Jerry Seinfeld", "description": "American comedian and actor", "question": "Does Jerry Seinfeld hang out at the Budweiser Party Deck?", "answer": false, "facts": ["The Budweiser Party Deck is a social gathering spot in Yankee Stadium", "Yankee Stadium is home to the New York Yankees baseball team", "Jerry Seinfeld is a fan of the New York Mets"], "decomposition": ["Where is The Budweiser Party Deck located?", "Which sports team is #1 home to?", "Is Jerry Seinfeld a fan of #2?"], "evidence": [[[["Appalachian Power Park-14"]], [["Appalachian Power Park-1"]], [["Jerry Seinfeld-28"], "operation"]], [[["Yankee Stadium-22"]], [["Yankee Stadium-1"]], [["Jerry Seinfeld-28"]]], [[["Appalachian Power Park-14"]], [["Appalachian Power Park-1"]], [["Jerry Seinfeld-28"], "operation"]]], "golden_sentence": [["In 2007, a party deck was built near the right field foul pole that can accommodate 250 people."], ["It also has been used by the baseball programs of West Virginia University, Marshall University, and the University of Charleston."], ["A fan of the New York Mets, Seinfeld periodically calls Steve Somers' show on WFAN-AM, a sports talk radio station, as \"Jerry from Queens\"."]]}, {"qid": "80f33e0606a1b73a07ee", "term": "Gospel", "description": "description of the life of Jesus, canonical or apocryphal", "question": "Do most fans follow Katy Perry for gospel music?", "answer": false, "facts": ["Katy Perry's gospel album sold about 200 copies.", "Katy Perry's most recent pop albums sold over 800,000 copies."], "decomposition": ["What type of music is Katy Perry known for?", "Is Gospel music the same as #1?"], "evidence": [[[["Katy Perry-2"]], ["operation"]], [[["Katy Perry-2"]], [["Gospel music-1"], "operation"]], [[["Katy Perry-1", "Katy Perry-2"]], ["operation"]]], "golden_sentence": [["Perry rose to fame in 2008 with her second album, One of the Boys, a pop rock record containing the singles \"I Kissed a Girl\" and \"Hot n Cold\"."]]}, {"qid": "f66f2787ffc98b45b76e", "term": "Eighth Amendment to the United States Constitution", "description": "prohibits cruel and unusual punishment and excessive bail", "question": "Does the Eighth Amendment to the United States Constitution protect freedom of speech?", "answer": false, "facts": ["The Eighth Amendment (Amendment VIII) of the United States Constitution prohibits the federal government from imposing excessive bail, excessive fines, or cruel and unusual punishments.", "The First Amendment (Amendment I) to the United States Constitution protects freedom of speech."], "decomposition": ["What changes were made by the Eighth Amendment to the United States Constitution?", "Is the protection of freedom of speech among #1?"], "evidence": [[[["Eighth Amendment to the United States Constitution-1"]], ["operation"]], [[["Eighth Amendment to the United States Constitution-1"]], ["operation"]], [[["Eighth Amendment to the United States Constitution-5"]], ["operation"]]], "golden_sentence": [["The Eighth Amendment (Amendment VIII) of the United States Constitution prohibits the federal government from imposing excessive bail, excessive fines, or cruel and unusual punishments."]]}, {"qid": "be1b2bb26243a9e5d068", "term": "Apollo", "description": "God in Greek mythology", "question": "Could all of the famous Apollo's hypothetically defeat all of the famous D'Artagnan's?", "answer": true, "facts": ["The famous D'artagnan was a musketeer based on a count that served Louis XIV", "There are at least three famous Apollo's: Apollo Creed, Apollo (Greek mythology), and Apollo Crews.", "Apollo, the Greek god of the sun and healing, is immortal."], "decomposition": ["Who were the famous D'artagnan?", "Who were the famous Apollos?", "What special power did one of the #2's have?", "Can #1 be defeated by someone who is #3?"], "evidence": [[[["Charles de Batz de Castelmore d'Artagnan-1"]], [["Apollo-1"]], [["Apollo-155"]], [["Apollo-155", "Charles de Batz de Castelmore d'Artagnan-3"]]], [[["Charles de Batz de Castelmore d'Artagnan-1"], "no_evidence"], [["Apollo (band)-1", "Apollo program-2", "Apollo-1"], "no_evidence"], [["Apollo-183", "Coronis (lover of Apollo)-3"], "no_evidence"], ["no_evidence", "operation"]], [[["Charles de Batz de Castelmore d'Artagnan-1"]], [["Apollo Creed-1", "Apollo-1"]], [["Apollo-208"], "no_evidence"], ["operation"]]], "golden_sentence": [["A fictionalized account of his life by Gatien de Courtilz de Sandras formed the basis for the d'Artagnan Romances of Alexandre Dumas, p\u00e8re, most famously including The Three Musketeers (1844)."], ["He is the son of Zeus and Leto, and the twin brother of Artemis, goddess of the hunt."], [""], ["", ""]]}, {"qid": "ee4cefbfb263f2d65744", "term": "Family of Barack Obama", "description": "List of members of the family of Barack Obama", "question": "Can Family of Barack Obama ride comfortably in 2020 Jaguar F Type?", "answer": false, "facts": ["Barack Obama has a wife and two children.", "The 2020 Jaguar F Type is a car that seats two people."], "decomposition": ["How many people are in Barack Obama's immediate family?", "How many people can sit in a 2020 Jaguar F Type?", "Is #2 greater than #1?"], "evidence": [[[["Family of Barack Obama-2"], "no_evidence"], [["Jaguar F-Type-13"], "no_evidence"], ["no_evidence"]], [[["Family of Barack Obama-5"]], [["Car-42"]], ["operation"]], [[["Barack Obama-15"]], [["Jaguar F-Type-1"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "a4a8d388e1fb0b1452a9", "term": "Student", "description": "learner, or someone who attends an educational institution", "question": "Do Elementary School students typically need graphing calculators?", "answer": false, "facts": ["Elementary schools typically teach K-5th grade.", "5th Graders are reaching a point in their education where they are just beginning to understand decimals and fractions.", "Graphing calculators are used for higher level math work including complex equations and functions.", "Students are usually introduced to functions in late middle or high school math."], "decomposition": ["What grades are part of elementary schools?", "Out of all the grades in #1, what do students in the highest grade learn in math?", "What topics in math require students to use graphing calculators?", "Is #2 the same s #3?"], "evidence": [[[["Primary school-1"]], [["Arithmetic-54"]], [["Graphing calculator-10"]], ["operation"]], [[["Primary school-17"]], [["Primary education-2"], "no_evidence"], [["Graphing calculator-10"]], ["operation"]], [[["Primary school-17"]], [["Mathematics education-15"], "no_evidence"], [["Graphing calculator-10"], "no_evidence"], ["operation"]]], "golden_sentence": [["A primary school, junior school (in UK), elementary school or grade school (in US & Canada) is a school for children from about four to eleven years old, in which they receive primary or elementary education."], [""], ["[citation needed] Singapore \u2013 graphing calculators are used in junior colleges; it is required in the Mathematics paper of the GCE 'A' Levels, and most schools use the TI-84 Plus or TI-84 Plus Silver Edition."]]}, {"qid": "7bc8ec80c009c03b5094", "term": "Composer", "description": "person who creates music, either by musical notation or oral tradition", "question": "Would King Leonidas have succeeded with an army the size of Mozart's compositions?", "answer": false, "facts": ["King Leonidas led 300 Spartans and 700 Thespians against the Persian army at the Battle of Thermopylae.", "The Persian army had around 150,000 soldiers at the Battle of Thermopylae.", "Mozart composed 600 works."], "decomposition": ["How many compositions did Mozart write?", "How many soldiers did King Leonidas lead?", "Is #1 larger than #2?"], "evidence": [[[["Wolfgang Amadeus Mozart-3"]], [["Leonidas I-10"]], ["operation"]], [[["Wolfgang Amadeus Mozart-3"]], [["Leonidas I-9"]], ["operation"]], [[["Wolfgang Amadeus Mozart-3"]], [["Leonidas I-1"]], ["operation"]]], "golden_sentence": [["He composed more than 600 works, many of which are acknowledged as pinnacles of symphonic, concertante, chamber, operatic, and choral music."], ["They faced a Persian army who had invaded from the north of Greece under Xerxes I. Herodotus stated that this army consisted of over two million men; modern scholars consider this to be an exaggeration and give estimates ranging from 70,000 to 300,000."]]}, {"qid": "c02235b606594c278e39", "term": "Prophet", "description": "person claiming to speak for divine beings", "question": "Did the leader of Heaven's Gate consider himself a prophet?", "answer": true, "facts": ["The leader of Heaven's Gate was Marshall Applewhite.", "Marshall Applewhite said he was called to be a messenger of the divine."], "decomposition": ["Who was the leader of Heaven's Gate?", "What did #1 say he was called upon to do?", "What is the definition of a prophet?", "Is #2 the same as #3?"], "evidence": [[[["Marshall Applewhite-1"]], [["Marshall Applewhite-10", "Marshall Applewhite-2"]], [["Prophet-1"]], ["operation"]], [[["Heaven's Gate (religious group)-1"]], [["Marshall Applewhite-13"]], [["Prophet-1"]], ["operation"]], [[["Heaven's Gate (religious group)-1"]], [["Marshall Applewhite-20"]], [["Prophet-1"]], ["operation"]]], "golden_sentence": [["Marshall Herff Applewhite Jr. (May\u00a017, 1931\u00a0\u2013 March 26, 1997), also known as Do, among other names, was an American cult leader who founded what became known as the Heaven's Gate religious group and organized their mass suicide in 1997, claiming the lives of 39 people."], ["", ""], ["In religion, a prophet is an individual who is regarded as being in contact with a divine being and is said to speak on that entity's behalf, serving as an intermediary with humanity by delivering messages or teachings from the supernatural source to other people."]]}, {"qid": "42d4d6c0ba7848748876", "term": "The Powerpuff Girls", "description": "American animated television series", "question": "Are the names of The Powerpuff Girls alliterative? ", "answer": true, "facts": ["To be alliterative, words must have the same initial consonant sound.", "The names of The Powerpuff Girls are Blossom, Buttercup, and Bubbles."], "decomposition": ["What are the names of The Powerpuff Girls?", "What features are necessary for a group of words to be considered alliterative?", "Are #2 present in #1?"], "evidence": [[[["The Powerpuff Girls-7"]], [["Alliteration-1"]], ["operation"]], [[["The Powerpuff Girls-16"]], [["Alliteration-1"]], ["operation"]], [[["The Powerpuff Girls-1"]], [["Alliteration-1"]], ["operation"]]], "golden_sentence": [["As depicted in the opening sequence of each episode, the Powerpuff Girls Blossom, Bubbles, and Buttercup were created by Professor Utonium in an attempt to create the \"perfect little girl\" using a mixture of \"sugar, spice, and everything nice\"."], ["In literature, alliteration is the conspicuous repetition of identical initial consonant sounds in successive or closely associated syllables within a group of words, even those spelled differently."]]}, {"qid": "6086a7a06a13a19682af", "term": "Soy milk", "description": "Beverage made from soybeans", "question": "Would Cardi B. benefit from soy milk?", "answer": true, "facts": ["Cardi B became lactose intolerant in her early twenties.", "People who are lactose intolerant cannot have dairy.", "Soy milk is an alternative to dairy milk."], "decomposition": ["What food intolerance does Cardi B. suffer from?", "What must people with #1 avoid?", "Is soy milk free from #2?"], "evidence": [[["no_evidence"], [["Lactose intolerance-1"]], [["Soy milk-1"], "operation"]], [[["Cardi B-1"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence"]]], "golden_sentence": [[""], [""]]}, {"qid": "b566ef303d486ab089f0", "term": "Pope John Paul I", "description": "263rd Pope of the Catholic Church", "question": "Phileas Fogg's around the world would be difficult to achieve during Pope John Paul I's reign?", "answer": true, "facts": ["Phileas Fogg is a character in Jules Verne's Around the World in Eighty Days.", "Phileas Fogg attempts to circumnavigate the globe in 80 days.", "Pope John Paul I reigned for only 33 days."], "decomposition": ["How long did it take Phileas Fogg to go around the world?", "How long did Pope John Paul I reign?", "Is #1 longer than #2?"], "evidence": [[[["Phileas Fogg-1"]], [["Pope John Paul I-1"]], ["operation"]], [[["Around the World in Eighty Days-1"]], [["Pope John Paul I-1"]], ["operation"]], [[["Phileas Fogg-2"]], [["Pope John Paul I-1"]], ["operation"]]], "golden_sentence": [["Phileas Fogg (/\u02c8f\u026ali\u0259s \u02c8f\u0252\u0261/) is the protagonist in the 1872 Jules Verne novel Around the World in Eighty Days."], ["Pope John Paul I (Latin: Ioannes Paulus I; Italian: Giovanni Paolo I; born Albino Luciani [al\u02c8bi\u02d0no lu\u02c8t\u0283a\u02d0ni]; 17 October 1912\u00a0\u2013 28 September 1978) was head of the Catholic Church and sovereign of the Vatican City from 26 August 1978 to his death 33 days later."]]}, {"qid": "b3c7dfa86f76997951fb", "term": "Music", "description": "form of art using sound and silence", "question": "Are deaf people left out of enjoying music?", "answer": false, "facts": ["Deafness exists on a spectrum of total hearing loss to partial hearing loss.", "Individuals with total hearing loss can still enjoy the bass and beat of music through vibration.", "Deaf people with cochlear implants can hear music, albeit in a different way than hearing people."], "decomposition": ["In what different ways can music be perceived?", "Does partial or total hearing loss make one unable to detect any of #1?"], "evidence": [[[["Hearing loss-38"], "no_evidence"], [["Vibration-4"], "no_evidence", "operation"]], [[["Music-1"], "no_evidence"], [["Dance-1"], "no_evidence", "operation"]], [[["Sound-6"]], [["Deaf hearing-2"], "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "b6b4a96f9a2356a4aa10", "term": "Potato", "description": "plant species producing the tuber used as a staple food", "question": "Can someone with celiac disease have potato vodka?", "answer": true, "facts": ["Celiac disease makes it unsafe for someone to eat gluten.", "Potato vodka is a gluten free product."], "decomposition": ["For people with celiac disease, what must they avoid?", "Does Potato Vodka contain #1?"], "evidence": [[[["Gluten-15"]], [["Grey Goose (vodka)-7"]]], [[["Coeliac disease-2"]], [["Vodka-24"], "no_evidence", "operation"]], [[["Coeliac disease-2"]], [["Potato-1"], "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "0e6662bb5cfa104b0d0d", "term": "Cucumber", "description": "species of plant", "question": "Are cucumbers often found in desert?", "answer": false, "facts": ["Cucumbers are a kind of vegetable.", "With the exception of carrot cake, deserts are not typically made with vegetables.", "Cucumbers are not the vegetable used in carrot cake."], "decomposition": ["What food group is a cucumber?", "Does #1 grow in the desert?"], "evidence": [[[["Cucumber-1"]], ["operation"]], [[["Cucumber-1"]], [["Desert-1"]]], [[["Cucumber-25"]], [["Cucumber-17"], "no_evidence"]]], "golden_sentence": [["It is a creeping vine that bears cucumiform fruits that are used as vegetables."]]}, {"qid": "514f847c470c90797927", "term": "Lactic acid", "description": "group of stereoisomers", "question": "Is it bad to have lactic acid in your body?", "answer": false, "facts": ["The body naturally produces and uses lactic acid to convert glucose into energy", "Lactic acid bacteria are particularly good for digestive health."], "decomposition": ["What are the functions of lactic acid in the human body?", "Is a majority of #1 harmful to the body?"], "evidence": [[[["Lactic acid-14"]], ["operation"]], [[["Lactic acid-4"]], [["Exercise-1"], "operation"]], [[["Lactic acid-4"], "no_evidence"], [["Lactic acid-14"], "no_evidence"]]], "golden_sentence": [[""]]}, {"qid": "1329c142f928bfb2ee1e", "term": "Dermatitis", "description": "skin disease", "question": "Can someone with dermatitis be a hand model?", "answer": false, "facts": ["Dermatitis causes rashes, redness, blisters, cracking of the skin, and scarring. ", "Hand models tend to have flawless skin and hands."], "decomposition": ["What does dermatitis cauas a person's skin to look like?", "What must a hand model's skin look like?", "Is #1 the same as #2?"], "evidence": [[[["Dermatitis-1"]], [["Hand model-1"]], ["operation"]], [[["Dermatitis-5"]], [["Hand model-1"]], [["Dermatitis-5", "Hand model-1"]]], [[["Dermatitis-1"]], [["Hand model-1"]], ["operation"]]], "golden_sentence": [["These diseases are characterized by itchiness, red skin and a rash."], ["Hand models tend to have flawless skin and hands."]]}, {"qid": "b41ea420361b934bb3ba", "term": "Tonsure", "description": "hairstyle related to religious devotion", "question": "Does ancient Olympics crown fail to hide tonsure?", "answer": true, "facts": ["Tonsure refers to the religious hairstyle in which hair from the top of the head is shaved off.", "Winners of the ancient Olympics were given a laurel crown to wear.", "A laurel crown involves leaves arranged in a circle that cover only the edges of the head."], "decomposition": ["Which part of the head is affected by tonsure?", "What kind of crown was given to winners in ancient Olympics?", "Does #2 cover #1 when worn?"], "evidence": [[[["Tonsure-1"]], [["Ancient Olympic Games-2"]], [["Olive wreath-1"], "operation"]], [[["Tonsure-1"]], [["Olive wreath-1"]], ["operation"]], [[["Tonsure-9"]], [["Olive wreath-1"]], ["operation"]]], "golden_sentence": [["Tonsure can also refer to the secular practice of shaving all or part of the scalp to show support or sympathy, or to designate mourning."], ["The prizes for the victors were olive leaf wreaths or crowns."], [""]]}, {"qid": "0b6bfa5f9282256a0c74", "term": "Dessert", "description": "A course that concludes a meal; usually sweet", "question": "Can dessert be made with vegetables?", "answer": true, "facts": ["A popular desert is carrot cake.", "Carrot cake is made with carrots.", "Carrots are a kind of vegetable."], "decomposition": ["What are some popularly known desserts?", "Do any of #1 contain vegetables?"], "evidence": [[[["Cake-14"]], [["Carrot cake-1"]]], [[["Dessert-1"]], ["no_evidence", "operation"]], [[["Carrot cake-1"]], [["Carrot-1"]]]], "golden_sentence": [["Such cakes are often very traditional in form and include such pastries as babka and stollen."], [""]]}, {"qid": "ee2514f7541db61acaa0", "term": "Edgar Allan Poe", "description": "19th-century American author, poet, editor and literary critic", "question": "Was proofreading Edgar Allan Poe works lucrative?", "answer": false, "facts": ["Proofreaders get paid a set rate based on the number of words in a document.", "Edgar Allan Poe wrote many short stories including the Oval Portrait which is two pages in length.", "Edgar Allan Poe's only complete novel: The Narrative of Arthur Gordon Pym of Nantucket was a mere 166 pages.", "A book like Jeyamohan's Venmurasu is 11,159 pages."], "decomposition": ["What is the typical length of each of Edgar Allan Poe's works?", "Is #1 relatively long?"], "evidence": [[[["Edgar Allan Poe-1"]], [["Artam\u00e8ne-1", "Short story-7"], "operation"]], [[["Edgar Allan Poe-1"]], ["operation"]], [[["Edgar Allan Poe-1"]], ["operation"]]], "golden_sentence": [[""], ["At 1,954,300 words, it is considered one of the longest novels ever published.", "In terms of length, word count is typically anywhere from 1,000 to 4,000 for short stories, however some have 20,000 words and are still classed as short stories."]]}, {"qid": "9e3dca043f01dd701ba9", "term": "Sudoku", "description": "Logic-based number-placement puzzle", "question": "Can Roman numerals fill the normal number of Sudoku box options?", "answer": false, "facts": ["Sudoku boxes can be filled with one of 9 numbers.", "There are only seven Roman numerals: I, V, X, L, C, D and M"], "decomposition": ["How many symbols are used in the Roman numeral system?", "How many numbers are employed in Sudoku?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Roman numerals-1"]], [["Sudoku-1"]], ["operation"]], [[["Roman numerals-1"]], [["Sudoku-1"]], ["operation"]], [[["Roman numerals-1"]], [["Sudoku-1"]], ["operation"]]], "golden_sentence": [["Modern usage employs seven symbols, each with a fixed integer value:"], ["The objective is to fill a 9\u00d79 grid with digits so that each column, each row, and each of the nine 3\u00d73 subgrids that compose the grid (also called \"boxes\", \"blocks\", or \"regions\") contain all of the digits from 1 to 9."]]}, {"qid": "3fa422b5c02fa3e8fee8", "term": "Guitarist", "description": "person who plays the guitar", "question": "Does being good at guitar hero make you a good guitarist?", "answer": false, "facts": ["Guitar Hero is a game that features a guitar-shaped controller with buttons that the player must hit in time with a song.", "Guitars as instruments do not have any buttons, but have strings that must be strummed in a particular way to create sound."], "decomposition": ["How is a guitar played?", "How is Guitar Hero played?", "Do the steps in #1 match those of #2?"], "evidence": [[[["Guitar-1"]], [["Guitar Hero-44"]], ["operation"]], [[["Guitar-1"]], [["Guitar controller-1"]], ["operation"]], [[["Guitar-1"]], [["Guitar Hero-1"]], ["operation"]]], "golden_sentence": [["It is typically played with both hands by strumming or plucking the strings with either a guitar pick or the fingers/fingernails of one hand, while simultaneously fretting (pressing the strings against the frets) with the fingers of the other hand."], ["There is a window of time for hitting each note, similar to other rhythm games such as Dance Dance Revolution, but unlike these games, scoring in Guitar Hero is not affected by accuracy; as long as the note is hit within that window, the player receives the same number of points."]]}, {"qid": "a4b4e2c4856a3e155617", "term": "San Antonio", "description": "City in Texas, United States", "question": "Did any citizen of San Antonio vote for Boris Johnson?", "answer": false, "facts": ["San Antonio is a city in Texas in the United States of America", "Boris Johnson is the Prime Minister of the UK", "Only UK and commonwealth citizens may vote in UK elections"], "decomposition": ["Is San Antonio a city in the UK?", "Is Boris Johnson the Prime Minister of the UK?", "Are American citizens allowed to vote in the UK elections?", "Are #1 and #3 the same answer as #2?"], "evidence": [[[["San Antonio-18"]], [["Boris Johnson-100"]], ["no_evidence"], ["operation"]], [[["San Antonio-41"]], [["Boris Johnson-100"]], [["Elections in the United Kingdom-7"]], ["operation"]], [[["San Antonio-1"]], [["Boris Johnson-1"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "a5c3c7c7859cfa0db010", "term": "Justin Bieber", "description": "Canadian singer-songwriter and actor", "question": "Will Justin Bieber take over Mike Pence's position in 2020?", "answer": false, "facts": ["Mike Pence is Vice President of the United States.", "The Vice President must be a US citizen.", "The Vice President must be at least 35 years of age.", "Justin Bieber is a Canadian citizen.", "Justin Bieber is 26 years old in 2020."], "decomposition": ["What is Mike Pence's present position?", "What is the age/nationality requirement to be a #1?", "What is Justin Bieber's age/nationality by 2020?", "Does #3 match #2?"], "evidence": [[[["Mike Pence-4"]], [["Age of candidacy-8"]], [["Justin Bieber-1"]], ["operation"]], [[["Mike Pence-4"]], ["no_evidence"], [["Justin Bieber-1"]], [["Justin Bieber-1"], "operation"]], [[["Natural-born-citizen clause-1", "Vice President of the United States-24"]], [["Vice President of the United States-25"]], [["Justin Bieber-1"]], ["operation"]]], "golden_sentence": [["Pence was inaugurated as vice president of the United States on January 20, 2017."], ["Since the U.S. Constitution requires that the President and Vice President be at least 35 years old, Jenness was not able to receive ballot access in several states in which she otherwise qualified."], [""]]}, {"qid": "531f32f116075a8743cb", "term": "Richard Wagner", "description": "German composer", "question": "Was Mozart accused of stealing from Richard Wagner?", "answer": false, "facts": ["Mozart died in 1791.", "Richard Wagner was born in 1813."], "decomposition": ["When did Mozart die?", "When was Richard Wagner born?", "Is #2 an earlier date than #1?"], "evidence": [[[["Wolfgang Amadeus Mozart-1"]], [["Richard Wagner-1"]], ["operation"]], [[["Wolfgang Amadeus Mozart-50"]], [["Richard Wagner-1"]], ["operation"]], [[["Wolfgang Amadeus Mozart-50"]], [["Richard Wagner-1"]], ["operation"]]], "golden_sentence": [["Wolfgang Amadeus Mozart (27 January 1756\u00a0\u2013 5 December 1791), baptised as Johannes Chrysostomus Wolfgangus Theophilus Mozart, was a prolific and influential composer of the Classical period."], ["Wilhelm Richard Wagner (/\u02c8v\u0251\u02d0\u0261n\u0259r/ VAHG-n\u0259r, German: [\u02c8\u0281\u026a\u00e7a\u0281t \u02c8va\u02d0\u0261n\u0250] (listen); 22 May 1813\u00a0\u2013 13 February 1883) was a German composer, theatre director, polemicist, and conductor who is chiefly known for his operas (or, as some of his mature works were later known, \"music dramas\")."]]}, {"qid": "ad08a05c0e0599529e50", "term": "Hamburger", "description": "Sandwich consisting of buns, a patty, and some other fillings", "question": "Do seven McDonald's hamburgers exceed USDA recommended fat allowance?", "answer": false, "facts": ["The fat content of one McDonald's hamburgers is 10 grams.", "The USDA recommends between 44 and 77 grams of fat a day."], "decomposition": ["How much fat is in a McDonald's hamburger?", "What is #1 multiplied by 7?", "How many grams of fat per day does the USDA recommend for the average person?", "Is #2 greater than the maximum value of #3?"], "evidence": [[["no_evidence"], ["no_evidence", "operation"], ["no_evidence"], ["no_evidence", "operation"]], [[["McDonald's France-3"], "no_evidence"], ["no_evidence", "operation"], [["Dietary Reference Intake-3"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], ["operation"], ["no_evidence"], ["operation"]]], "golden_sentence": []}, {"qid": "0faf1f281768a6a8cb86", "term": "Lieutenant", "description": "junior commissioned officer in many nations' armed forces", "question": "Can children become lieutenants?", "answer": true, "facts": ["Lieutenant the second junior-most or in some cases the junior-most commissioned officer in the armed forces, fire services, police, and other organizations of many nations.", "Many gangs use military rankings to describe their internal heirarchy.", "Many young children are inducted into gangs with heirarchies.", "Children can grow up to be whatever they want to be. "], "decomposition": ["What position in a hierarchical system does lieutenant imply?", "Would a gang use the system and assign children to #1 positions?"], "evidence": [[[["Lieutenant-11"]], [["Ghetto Boys-7"]]], [[["Lieutenant-1"]], [["Gang-24", "Gang-47"], "no_evidence", "operation"]], [[["Lieutenant-1"]], ["no_evidence", "operation"]]], "golden_sentence": [["In most English-speaking and Arabic-speaking countries, as well as a number of European and South American nations, full lieutenants (and equivalents) usually wear two stars (pips) and second lieutenants (and equivalents) one."], [""]]}, {"qid": "aaf11e9e47f9a9e38eb2", "term": "Cousin", "description": "any descendant of an ancestor's sibling", "question": "Could SNL be why Jenny McCarthy does not get along with her cousin?", "answer": true, "facts": ["Jenny McCarthy is cousin's with Melissa McCarthy.", "Melissa McCarthy and Jenny McCarthy are not close and Melissa did not even attend Jenny's wedding.", "Jenny McCarthy was spoofed in n episode of SNL (Saturday Night Live) for a rant she did on The View.", "Melissa McCarthy has been a frequent guest on SNL (Saturday Night Live) from 2011-2017.", "Melissa McCarthy was nominated five times for a Primetime Emmy Award for Outstanding Guest Actress in a Comedy Series for her appearances on SNL."], "decomposition": ["Who is Jenny McCarthy's cousin?", "What show has #1 been a frequent guest of?", "Was Jenny McCarthy made fun of on #2?"], "evidence": [[[["Jenny McCarthy-3"]], [["Melissa McCarthy-2"]], ["no_evidence"]], [[["Melissa McCarthy-1"]], [["Melissa McCarthy-2"]], ["no_evidence", "operation"]], [[["Melissa McCarthy-5"]], [["Melissa McCarthy-8"]], ["no_evidence", "operation"]]], "golden_sentence": [["She is the second of four daughters \u2013 her sisters are named Lynette, Joanne, and Amy; actress Melissa McCarthy is her cousin."], ["McCarthy's appearances as a host on Saturday Night Live led to a win for the Primetime Emmy Award for Outstanding Guest Actress in a Comedy Series in 2017."]]}, {"qid": "56c08e1e2fccd4731657", "term": "Winter", "description": "one of the Earth's four temperate seasons, occurring between autumn and spring", "question": "Would Phineas and Ferb enjoy winter?", "answer": false, "facts": ["Phineas and Ferb is a tv show that takes place in the summer time.", "Phineas and Ferb are students and they get summer break in the summer.", "They enjoy summer break because of the freetime they have.", "If it were winter, they would not have summer break."], "decomposition": ["What is Phineas and Ferb?", "What season does #1 take place during?", "Why do Phineas and Ferb enjoy #2?", "If it were winter, would Phineas and Ferb still have #3?"], "evidence": [[[["Phineas and Ferb-1"]], [["Phineas and Ferb-1"]], [["Phineas and Ferb-3"], "no_evidence"], ["no_evidence"]], [[["Phineas and Ferb-1"]], [["Summer vacation-108"]], [["Phineas and Ferb-1"]], ["operation"]], [[["Phineas and Ferb-1"]], [["Phineas and Ferb-1"]], [["Phineas and Ferb-16", "Phineas and Ferb-3"]], ["operation"]]], "golden_sentence": [["The program follows Phineas Flynn and his stepbrother Ferb Fletcher on summer vacation."], [""], ["The show follows the adventures of stepbrothers Phineas Flynn (Vincent Martella) and Ferb Fletcher (Thomas Sangster), who live in the fictional city of Danville, in a (never specified) tri-state area, as they seek ways to occupy their time during their summer vacation."]]}, {"qid": "97d283d2fccee54d7d65", "term": "H", "description": "letter in the Latin alphabet", "question": "Is H's most common two letter pair partner a freebie in Wheel of Fortune bonus round?", "answer": true, "facts": ["H forms the most common two letter pair in the English language along with the letter T.", "The Wheel of Fortune bonus round gives the player six free letters: R, S, T, L, N, E."], "decomposition": ["What letter forms the most common two letter pair in English along with the letter H?", "What free letters does the Wheel of Fortune bonus round give players?", "Is #1 included in #2?"], "evidence": [[[["Th (digraph)-1"]], [["Wheel of Fortune (American game show)-13"]], ["operation"]], [[["Letter frequency-11"]], [["Wheel of Fortune (Australian game show)-33"]], ["operation"]], [[["Most common words in English-5"], "no_evidence"], [["Wheel of Fortune (Australian game show)-33"]], ["operation"]]], "golden_sentence": [["Th is a digraph in the Latin script."], ["The puzzle is revealed, as is every instance of the letters R, S, T, L, N, and E. The contestant provides three more consonants (four if he/she is holding the Wild Card) and one more vowel."]]}, {"qid": "4d36c142bd0e69f9f5b4", "term": "Curiosity (rover)", "description": "American robotic rover exploring the crater Gale on Mars", "question": "Can Curiosity take samples of rocks from Lacus Temporis?", "answer": false, "facts": ["Curiosity is a rover exploring Mars", "Lacus Temporis is located on the moon"], "decomposition": ["Which planet is Curiosity on?", "Where is Lacus Temporis located?", "Is #1 the same as #2?"], "evidence": [[[["Curiosity (rover)-1"]], [["Lacus Temporis-1"]], ["operation"]], [[["Curiosity (rover)-1"]], [["Lacus Temporis-1"]], ["operation"]], [[["Curiosity (rover)-1"]], [["Lacus Temporis-1"]], ["operation"]]], "golden_sentence": [["Curiosity is a car-sized rover designed to explore the crater Gale on Mars as part of NASA's Mars Science Laboratory mission (MSL)."], ["Lacus Temporis /\u02c8le\u026ak\u0259s \u02c8t\u025bmp\u0259r\u026as/ (Latin temporis, Lake of Time) is a small lunar mare that is located in the northeastern quadrant of the Moon's near side."]]}, {"qid": "c52d96bcaf4a7ddbfcae", "term": "Diamond", "description": "Allotrope of carbon often used as a gemstone and an abrasive", "question": "Is the title of Shirley Bassey's 1971 diamond song a true statement?", "answer": false, "facts": ["Shirley Bassey recorded the song Diamonds are Forever in 1971,", "Over time, diamonds degrade and turn into graphite.", "Graphite is the same chemical composition found in pencils."], "decomposition": ["What is the title to Shirley Bassey's 1971 diamond song?", "Do diamonds last for the time span in #1?"], "evidence": [[[["Diamonds Are Forever (soundtrack)-2"]], [["Material properties of diamond-8"], "no_evidence", "operation"]], [[["Shirley Bassey-1"]], [["Material properties of diamond-31"], "operation"]], [[["Shirley Bassey-1"], "no_evidence"], [["Diamond-48"], "no_evidence"]]], "golden_sentence": [["\"Diamonds Are Forever\", the title song with lyrics by Don Black, was the second Bond theme to be performed by Shirley Bassey, after \"Goldfinger\"."], [""]]}, {"qid": "961a2c6e7bbfa146e649", "term": "Potato", "description": "plant species producing the tuber used as a staple food", "question": "Can Tame Impala's studio band play a proper game of Hot Potato?", "answer": false, "facts": ["Hot Potato is a game in which two or more people toss a potato until the music stops.", "Tame Impala is a band with one member, multi-instrumentalist Kevin Parker."], "decomposition": ["How many studio members are in Tame Impala's band?", "What is the minimum number of people that can play hot potato?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Tame Impala-1"]], [["Hot potato-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Tame Impala-1"]], [["Hot potato-1"]], ["operation"]], [[["Tame Impala-1"]], [["Hot potato-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["As a touring act, Tame Impala consists of Parker (guitar, vocals), Dominic Simper (guitar, synthesiser), Jay Watson (synthesiser, vocals, guitar), Cam Avery (bass guitar, vocals), and Julien Barbagallo (drums, vocals)."], [""]]}, {"qid": "10ccc95fa9383fb55f4b", "term": "Cell (biology)", "description": "The basic structural and functional unit of all organisms; the smallest unit of life.", "question": "Can a cell fit inside of a shoebox?", "answer": true, "facts": ["The average shoebox is around 14 inches by 10 inches by 5 inches", "The average eukaryotic cell is between 1 and 100 micrometers in diameter"], "decomposition": ["How big is a cell?", "How big is a shoebox?", "Is #1 smaller than #2?"], "evidence": [[[["Cell (biology)-2"]], ["no_evidence"], ["operation"]], [[["Cell (biology)-2"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Cell (biology)-1", "Electrochemical cell-1", "Fuel cell-1", "Monastic cell-1", "Prison cell-1", "Solar cell-1", "Storm cell-1"], "no_evidence"], [["Shoe-1"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Most plant and animal cells are only visible under a microscope, with dimensions between 1 and 100\u00a0micrometres."]]}, {"qid": "e32a4dc059483418b318", "term": "U.S. Route 1", "description": "highway in the United States", "question": "Is US route 1 dominated by historically red states?", "answer": false, "facts": ["US route 1 is a highway in the US that spans 15 states.", "There are 5 historically red states along US Route 1.", "There are 10 historically blue states along US route 1."], "decomposition": ["What states does US Rte. 1 pass through?", "How many states in #1 are historically \"red states\"?", "How many states in #1 are historically \"blue states\"?", "Is #2 greater than #3?"], "evidence": [[[["U.S. Route 1-1"]], [["Red states and blue states-1"], "no_evidence"], [["Red states and blue states-17"], "no_evidence"], ["no_evidence", "operation"]], [[["U.S. Route 1-1"]], [["Red states and blue states-1"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["U.S. Route 1-1"]], [["Red states and blue states-29"], "no_evidence"], [["Red states and blue states-29"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""], [""]]}, {"qid": "ae098d3989ae2a143f5a", "term": "Lactobacillus", "description": "genus of bacteria", "question": "Is overfeeding Lactobacillus unwise for people without dental insurance?", "answer": true, "facts": ["Lactobacillus species convert sugars they digest to lactic acid ", "The lactic acid of some Lactobacillus species is associated with tooth decay", "Dental procedures can be expensive without insurance"], "decomposition": ["What are the products of Lactobacillus?", "What conditions are caused by #1?", "What medical procedures would be required to fix #2?", "Would #3 be more affordable with dental insurance?"], "evidence": [[[["Lactobacillus-1"]], [["Lactic acid-5"]], [["Tooth decay-77"]], [["Dental insurance-1"]]], [[["Lactobacillus-1", "Lactobacillus-2"]], [["Lactic acid bacteria-14"]], [["Tooth decay-77"]], [["Dental insurance-1"]]], [[["Lactobacillus-10"]], [["Lactobacillus-10"]], [["Tooth decay-76"]], [["Dental insurance-1"]]]], "golden_sentence": [["Lactobacillus is the most common probiotic found in food such as yogurt, and it is diverse in its application to maintain human well-being, as it can help treat diarrhea, vaginal infections, and skin disorders such as eczema."], [""], ["Once the caries is removed, the missing tooth structure requires a dental restoration of some sort to return the tooth to function and aesthetic condition."], [""]]}, {"qid": "168544deb30fa10d9746", "term": "Pikachu", "description": "Pok\u00e9mon species and the mascot of the Pok\u00e9mon franchise", "question": "Does Pikachu like Charles Darwin?", "answer": true, "facts": ["Charles Darwin developed the theory of evolution", "Pikachu is a Pokemon", "Pokemon rely on evolution in order to transform into stronger forms"], "decomposition": ["How does a Pokemon improve upon itself? ", "What theory is Charles Darwin best known for?", "Is #1 and #2 related?"], "evidence": [[[["Pok\u00e9mon-6"]], [["Charles Darwin-1"]], ["operation"]], [[["Pok\u00e9mon-6"]], [["Charles Darwin-2"]], ["operation"]], [[["Gameplay of Pok\u00e9mon-57"]], [["Charles Darwin-1"]], ["operation"]]], "golden_sentence": [["If a Pok\u00e9mon fully defeats an opponent in battle so that the opponent is knocked out (\"faints\"), the winning Pok\u00e9mon gains experience points and may level up."], ["In a joint publication with Alfred Russel Wallace, he introduced his scientific theory that this branching pattern of evolution resulted from a process that he called natural selection, in which the struggle for existence has a similar effect to the artificial selection involved in selective breeding."]]}, {"qid": "aa38a101f508c7cb0b86", "term": "Nickel", "description": "Chemical element with atomic number 28", "question": "Is nickel a better payout than mercury if given a dollar per atomic number?", "answer": false, "facts": ["Nickel is a metallic substance with the chemical atomic number of 28.", "Mercury is a silvery liquid substance with a chemical number of 80."], "decomposition": ["What is nickel's atomic number?", "What is Mercury's atomic number?", "Is #1 greater than #2?"], "evidence": [[[["Nickel-1"]], [["Mercury (element)-1"]], ["operation"]], [[["Nickel-1"]], [["Mercury (element)-1"]], ["operation"]], [[["Nickel-1"]], [["Mercury (element)-1"]], ["operation"]]], "golden_sentence": [["Nickel is a chemical element with the symbol Ni and atomic number 28."], ["Mercury is a chemical element with the symbol Hg and atomic number 80."]]}, {"qid": "52b79f548fc433856c10", "term": "Duck", "description": "common name for many species in the bird family Anatidae", "question": "Would a duck ever need a Caesarean section?", "answer": false, "facts": ["A Caesarean section is a medical procedure in which surgery is performed to remove the baby from inside the mother.", "Ducks do not give live birth, they lay eggs."], "decomposition": ["Cesarean sections are only performed on animals that produce offspring via what method?", "What method do ducks use to produce offspring?", "Is #2 the same as #1?"], "evidence": [[[["Caesarean section-1"]], [["Duck-15"]], ["operation"]], [[["Caesarean section-1"]], [["Mallard-2"], "no_evidence"], ["operation"]], [[["Caesarean section-1"]], [["Duck-15"]], [["Egg-10"], "operation"]]], "golden_sentence": [["A caesarean section is often necessary when a vaginal delivery would put the baby or mother at risk."], ["Ducks also tend to make a nest before breeding, and, after hatching, lead their ducklings to water."]]}, {"qid": "c5f947dfdb99a3b9ece9", "term": "Mitsubishi", "description": "group of autonomous, Japanese multinational companies", "question": "Can Aerosmith fit in a 2020 Mitsubishi Outlander?", "answer": true, "facts": ["Aerosmith is an American rock band that has five active members.", "The 2020 Mitsubishi Outlander has flexible seating that allows for seven seat capacity."], "decomposition": ["How many people are members of the band Aerosmith?", "What is the searing capacity of the 2020 Mitsubishi Outlander?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Aerosmith-1"]], [["Mitsubishi Outlander-10"]], [["Aerosmith-1", "Mitsubishi Outlander-10"], "operation"]], [[["Aerosmith-1"]], [["Mitsubishi Outlander-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Aerosmith-1"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["180\u00a0kg (400\u00a0lb) In its home market of Japan it was the best-selling SUV from October 2006 to March 2007, while in the U.S. market it achieved 1,694 and 2,108 sales in November and December 2006, the first two full months it was available; Mitsubishi ultimately hopes for at least 4,000 sales per month in the United States, and after the cancellation of the Mitsubishi Grandis in model year 2011, the Outlander is now Mitsubishi's only MPV capable of carrying seven passengers with all wheel drive."], ["", "180\u00a0kg (400\u00a0lb) In its home market of Japan it was the best-selling SUV from October 2006 to March 2007, while in the U.S. market it achieved 1,694 and 2,108 sales in November and December 2006, the first two full months it was available; Mitsubishi ultimately hopes for at least 4,000 sales per month in the United States, and after the cancellation of the Mitsubishi Grandis in model year 2011, the Outlander is now Mitsubishi's only MPV capable of carrying seven passengers with all wheel drive."]]}, {"qid": "d75ee4a688afd4f7ea1d", "term": "30th Street Station", "description": "United States historic place", "question": "Could all of the people who pass through 30th Street Station every day fit in Dorton Arena?", "answer": false, "facts": ["J. S. Dorton Arena is a 7,610-seat multi-purpose arena located in Raleigh, North Carolina.", "On an average day in fiscal 2013, about 12,000 people boarded or left trains at 30th Street."], "decomposition": ["How many people can sit in J. S. Dorton Arena?", "How many people passed through the 30th Street Station daily in 2013?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Dorton Arena-1"]], [["30th Street Station-5"]], ["operation"]], [[["Dorton Arena-1"]], [["30th Street Station-5"]], ["operation"]], [[["Dorton Arena-1"]], [["30th Street Station-5"]], [["30th Street Station-5", "Dorton Arena-1"], "operation"]]], "golden_sentence": [["J. S. Dorton Arena is a 7,610-seat multi-purpose arena located in Raleigh, North Carolina, on the grounds of the North Carolina State Fair."], ["On an average day in fiscal 2013, about 12,000 people boarded or left trains in Philadelphia, nearly twice as many as in the rest of the Pennsylvania stations combined."]]}, {"qid": "fbecca9bd26d29b479fc", "term": "Amazons", "description": "warrior women from Greek mythology", "question": "Did any of the amazons on Xena: Warrior Princess star on later shows?", "answer": true, "facts": ["Xena\" Warrior Princess was a fantasy TV series based on Greek mythology.", "Amazons on Xena: Warrior Princess were played by numerous actresses including: Danielle Cormack and Melinda Clarke.", "Melinda Clarke starred in numerous TV shows after Xena: Warrior Princess including The O.C. and Nikita."], "decomposition": ["Who played the roles of Amazons on Xena: Warrior Princess?", "Did Melinda Clarke do any other shows after Xena\" Warrior Princess?", "Is #2 listed in #1?"], "evidence": [[[["Melinda Clarke-3"]], [["Melinda Clarke-4"]], ["operation"]], [[["Xena: Warrior Princess-14"], "no_evidence"], [["Melinda Clarke-3"]], ["operation"]], [[["Melinda Clarke-3"]], [["Melinda Clarke-1"]], ["operation"]]], "golden_sentence": [["She also guest starred on Xena: Warrior Princess as the Amazon chieftain Velasca, Firefly as the brothel madam Nandi, Charmed as the Siren, and has had six appearances on CSI where she played the dominatrix, Lady Heather."], [""]]}, {"qid": "e1f5fc3f3f242d431c14", "term": "General Motors", "description": "American automotive manufacturing company", "question": "Can you purchase General Motors products at a movie theater?", "answer": false, "facts": ["General Motors sells automobiles, automobile parts, and financial services", "Movie theaters sell movie tickets, snacks, and beverages"], "decomposition": ["What kinds of products does General Motors sell?", "What kinds of products are sold in movie theaters?", "Are #1 the same as #2?"], "evidence": [[[["General Motors-1"]], [["Movie theater-23"]], ["operation"]], [[["General Motors-6"]], [["Movie theater-51"]], ["operation"]], [[["General Motors-1"]], [["Movie theater-3"]], ["operation"]]], "golden_sentence": [["General Motors Company, commonly referred to as General Motors (GM), is an American multinational corporation headquartered in Detroit that designs, manufactures, markets, and distributes vehicles and vehicle parts, and sells financial services, with global headquarters in Detroit's Renaissance Center."], ["Movie theaters also often have a concession stand for buying snacks and drinks within the theater's lobby."]]}, {"qid": "9b86acb1f5716f066ee4", "term": "Chief executive officer", "description": "Highest-ranking corporate officer or administrator", "question": "Would a CEO typically clean the toilets in a company's building?", "answer": false, "facts": ["The CEO is the highest-ranking corporate position in an organization.", "Cleaning toilets is a job typically done by janitors or facility workers in a company."], "decomposition": ["What are the general duties of the CEO of an organization?", "Is cleaning the toilets of the company's building one of #1?"], "evidence": [[[["Chief executive officer-3"]], ["operation"]], [[["Chief executive officer-3"]], ["operation"]], [[["Founder CEO-7"]], ["operation"]]], "golden_sentence": [["The responsibilities of an organization's CEO are set by the organization's board of directors or other authority, depending on the organization's legal structure."]]}, {"qid": "cb337da694e4d8a6f0af", "term": "Lapidary", "description": "gemstone cutter", "question": "Was Dioskourides a lapidary?", "answer": true, "facts": ["A lapidary is a person or machine who cuts gemstones; classically, it refers to a person who engraves gemstones.", "Dioskourides put his signature on a Roman amethyst ringstone with a portrait of Demosthenes circa late 1st century BC.", "Artists sign their work."], "decomposition": ["What is a lapidary?", "What do #1's do once they finish their work?", "Did Dioskourides do #1 and #2?"], "evidence": [[[["Lapidary-1"]], ["no_evidence"], [["Pedanius Dioscorides-1"], "operation"]], [[["Lapidary-1"]], [["Lapidary-12"], "no_evidence"], [["Pedanius Dioscorides-1"], "no_evidence", "operation"]], [[["Lapidary-1"]], [["Lapidary-11"]], [["Pedanius Dioscorides-1"], "operation"]]], "golden_sentence": [["A lapidary (lapidarist, Latin: lapidarius) is an artist or artisan who forms stone, minerals, or gemstones into decorative items such as cabochons, engraved gems (including cameos), and faceted designs."], [""]]}, {"qid": "ce317d53bfe0a74023e0", "term": "Quartz", "description": "mineral composed of silicon and oxygen atoms in a continuous framework of SiO\u2084 silicon\u2013oxygen tetrahedra, with each oxygen being shared between two tetrahedra, giving an overall chemical formula of SiO\u2082", "question": "Could Quartz be useful to humans if plants died off and there was no oxygen?", "answer": true, "facts": ["Plants produce oxygen which is needed by humans to survive.", "Quartz is a hard mineral substance made of several elements.", "Quartz is composed of silicon and oxygen.", "Quartz can be melted at high temperatures."], "decomposition": ["What are the constituents elements of quartz?", "Is oxygen included in #1?"], "evidence": [[[["Quartz-1"]], ["operation"]], [[["Quartz-1"]], ["operation"]], [[["Quartz-1"]], [["Oxygen-1"]]]], "golden_sentence": [["Quartz is a hard, crystalline mineral composed of silicon and oxygen atoms."]]}, {"qid": "6607e58a6578cfd5a88d", "term": "United States Secretary of State", "description": "U.S. cabinet member and head of the U.S. State Department", "question": "Can United States Secretary of State do crimes in U.K. without being arrested?", "answer": true, "facts": ["Diplomatic Immunity allows for diplomats in other countries to not be tried for their transgressions.", "Countries that signed the Vienna Convention on Diplomatic Relations allow for Diplomatic Immunity.", "All UN member states besides Palau, The Solomon Islands, and South Sudan have signed the Vienna Convention on Diplomatic Relations treaty.", "The U.K. is one of the original UN member nations."], "decomposition": ["Under which agreement is modern diplomatic immunity applicable?", "Which countries have signed #1?", "Is the U.K. included in #2?"], "evidence": [[[["Diplomatic immunity-14"]], [["Vienna Convention on Consular Relations-7"], "no_evidence"], ["no_evidence", "operation"]], [[["Diplomatic immunity-11", "Diplomatic immunity-14", "Diplomatic immunity-17"], "no_evidence"], [["Vienna Convention on Diplomatic Relations-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Diplomatic immunity-14"]], [["Vienna Convention on Diplomatic Relations-8"], "no_evidence"], [["Diplomatic immunity-10", "Diplomatic immunity-53"], "operation"]]], "golden_sentence": [["Currently, diplomatic relations, including diplomatic immunity, are governed internationally by the Vienna Convention on Diplomatic Relations, which has been ratified by almost every country in the world."], ["The UN member states that have neither signed nor ratified the convention are: Afghanistan, Burundi, Chad, Comoros, Guinea-Bissau, Ethiopia, Palau, San Marino, Solomon Islands, South Sudan and Uganda."]]}, {"qid": "0f6ba7d91c755d1df274", "term": "Wembley Arena", "description": "An indoor arena in Wembley, London", "question": "Can you see the moon in Wembley Arena?", "answer": false, "facts": ["Wembley Arena is an indoor arena.", "The moon is located in the sky.", " You cannot see the sky if you are indoors."], "decomposition": ["What is Wembley Arena?", "Where is the moon located?", "Can you see #2 from #1?"], "evidence": [[[["Wembley Arena-1"]], [["Moon-1"]], ["operation"]], [[["Wembley Arena-1"]], [["Moon-3"]], ["operation"]], [[["Wembley Arena-59"]], [["Moon-1"]], [["Moon-1"], "operation"]]], "golden_sentence": [["Wembley Arena /\u02c8w\u025bmbli/ (originally the Empire Pool and, since 1 July 2014, currently known as The SSE Arena, Wembley for sponsorship reasons) is an indoor arena adjacent to Wembley Stadium in Wembley, London."], [""]]}, {"qid": "1bc9473120a8deb0d358", "term": "5", "description": "Natural number", "question": "Does Homer Simpson need two hands worth of fingers to count to 5?", "answer": true, "facts": ["Homer Simpson is a character of the long running comedy animated series \"The Simpsons\".", "All characters in \"The Simpsons\" have 4 fingers on each hand."], "decomposition": ["How many fingers does Homer Simpson have on each hand?", "Is #1 less than 5?"], "evidence": [[[["Trilogy of Error-1"], "no_evidence"], ["operation"]], [["no_evidence"], ["no_evidence", "operation"]], [[["Homer Simpson-1"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""]]}, {"qid": "2ee0dbb23e37a9122020", "term": "Swan Lake", "description": "Ballet by Pyotr Ilyich Tchaikovsky", "question": "Does open heart surgery finish before entirety of American Ballet Theatre's Swan Lake?", "answer": false, "facts": ["The American Ballet theatre's Swan Lake has a run time of 145 minutes.", "The National Heart, Lung, and Blood Institute states that a coronary artery bypass takes 3 to 6 hours"], "decomposition": ["How long is a performance of Swan Lake?", "How long does it take to perform a coronary artery bypass?", "Is #1 longer than #2?"], "evidence": [[[["Swan Lake-2"], "no_evidence"], [["Coronary artery bypass surgery-2"], "no_evidence"], ["operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], ["no_evidence"], ["operation"]]], "golden_sentence": [["The scenario, initially in two acts, was fashioned from Russian and German folk tales and tells the story of Odette, a princess turned into a swan by an evil sorcerer's curse."], [""]]}, {"qid": "1c55b56c1480db9f323b", "term": "Rand Paul", "description": "American politician, ophthalmologist, and United States Senator from Kentucky", "question": "Is Rand Paul guilty of catch-phrase used to attack John Kerry in 2004?", "answer": true, "facts": ["John Kerry was attacked by his opponents in the 2004 Presidential Campaign by being called a Flip-Flopper.", "A Flip-Flopper is someone that makes a complete change in policy from one thing to another.", "In May 2010 Rand Paul called for an electronic fence to keep out immigrants and deplored amnesty in any form.", "In 2013 Rand Paul said he was in favor of undocumented immigrants being granted  legal status."], "decomposition": ["What catch-phrase was used against John Kerry in 2004?", "What policy changes has Rand Paul expressed?", "Would #2 be considered #1?"], "evidence": [[[["Flip-flop (politics)-5"]], [["Political positions of Rand Paul-63"]], ["operation"]], [["no_evidence"], [["Rand Paul-85"], "no_evidence"], ["no_evidence", "operation"]], [[["Flip-flop (politics)-5"]], [["Rand Paul-67"], "no_evidence"], ["operation"]]], "golden_sentence": [["Famously, on March 16, 2004, during an appearance at Marshall University Kerry tried to explain his vote for an $87 billion supplemental appropriation for military operations in Iraq and Afghanistan by telling the crowd, \"I actually did vote for the $87 billion, before I voted against it.\""], ["On Medicare, Paul has suggested higher deductibles as well as changes to premiums or eligibility rules as ways to address what he sees as the program's looming financial problems, saying \"You want to have more participation by the person who's receiving the entitlement... by that I mean that they need to be more involved with some sort of economic transaction every time they use their entitlement, and that means they have to bear more of the burden.\""]]}, {"qid": "b9230619e316b4d930d4", "term": "Robert De Niro", "description": "American actor, director and producer", "question": "Does Robert De Niro use a microscope at work?", "answer": false, "facts": ["A microscope is a tool used by scientists.", "Robert De Niro works on movies and television shows, and he is not a scientist."], "decomposition": ["What general profession uses a microscope?", "What is Robert De Niro's profession?", "Is #1 the same as #2?"], "evidence": [[[["Microscope-1"], "no_evidence"], [["Robert De Niro-1"]], ["no_evidence", "operation"]], [[["Microscope-1"]], [["Robert De Niro-1"]], ["operation"]], [[["Microscope-1", "Scientist-1"]], [["Robert De Niro-4"]], ["operation"]]], "golden_sentence": [["Microscopy is the science of investigating small objects and structures using such an instrument."], ["Robert Anthony De Niro Jr. (/d\u0259 \u02c8n\u026a\u0259ro\u028a/, Italian:\u00a0[de \u02c8ni\u02d0ro]; born August 17, 1943) is an American actor, producer, and director."]]}, {"qid": "cd7a730c7012d80ae1e7", "term": "Uranium", "description": "Chemical element with atomic number 92", "question": "Would Gordon Ramsey use uranium as a seasoning?", "answer": false, "facts": ["Gordon Ramsey is a chef known for producing high quality food ", "Uranium is a toxic and weakly radioactive metal"], "decomposition": ["What was Gordon Ramsay's major occupation?", "Is Uranium commonly used as seasoning by a #1?"], "evidence": [[[["Gordon Ramsay-1"]], [["Uranium-13"], "operation"]], [[["Gordon Ramsay-1"]], [["Depleted uranium-50", "Seasoning-2", "Uranium-3"]]], [[["Gordon Ramsay-1"]], [["Uranium-3"], "operation"]]], "golden_sentence": [["Gordon James Ramsay OBE (born 8 November 1966) is a British chef, restaurateur, writer and television personality."], [""]]}, {"qid": "3c01d6221b46bcbed91b", "term": "Richard Dawkins", "description": "English ethologist, evolutionary biologist and author", "question": "Would Jacques Du\u00e8ze have been friends with Richard Dawkins?", "answer": false, "facts": ["Jacques Du\u00e8ze was later Pope John XXII.", "The Pope is the head of the Catholic Church, a Christian organization.", "Christianity is a religion.", "Richard Dawkins is a prominent critic of religion."], "decomposition": ["What is the occupation of Jacques Du\u00e8ze?", "In what field or industry is #1?", "Is #2 a field or industry which Richard Dawkins supports?"], "evidence": [[[["Pope John XXII-1"]], [["Pope-1"]], [["Richard Dawkins-4"], "operation"]], [[["Pope John XXII-2"]], [["Pope John XXII-1"]], [["Richard Dawkins-1", "Richard Dawkins-3"], "operation"]], [[["Pope John XXII-1"]], [["Catholic Church-1", "Christianity-1"]], [["Atheism-1", "Richard Dawkins-3"]]]], "golden_sentence": [["Pope John XXII (Latin: Ioannes XXII; 1244 \u2013 4 December 1334), born Jacques Du\u00e8ze (or d'Euse), was head of the Catholic Church from 7 August 1316 to his death."], [""], [""]]}, {"qid": "179c2d5107fff7cca04c", "term": "Robert Downey Jr.", "description": "American actor", "question": "Was Robert Downey Jr. a good role model as a young man?", "answer": false, "facts": ["As a young man, Robert Downey Jr. struggled with drug and alcohol addiction.", "Robert Downey Jr. shot a gun out of a car window while doing drugs as a young adult."], "decomposition": ["Did Robert Downey Jr. completely avoid dangerous, irresponsible behavior as a young adult?"], "evidence": [[[["Robert Downey Jr.-12", "Robert Downey Jr.-13"], "operation"]], [[["Robert Downey Jr.-12", "Robert Downey Jr.-5"]]], [[["Robert Downey Jr.-12", "Robert Downey Jr.-5"]]]], "golden_sentence": [["", ""]]}, {"qid": "78642b0a3647c27093c5", "term": "Melania Trump", "description": "First Lady of the United States", "question": "Did Melania Trump have same profession as Olga Kurylenko?", "answer": true, "facts": ["Melania Trump is the first lady of the United States and was previously a model.", "Olga Kurylenko is a professional actress that also works as a model."], "decomposition": ["What professions has Melania Trump had?", "What professions has Olga Kurylenko had?", "Is at least one profession listed in #1 also found in #2?"], "evidence": [[[["Melania Trump-1"]], [["Olga Storozhenko-1"]], ["operation"]], [[["Melania Trump-1"]], [["Olga Kurylenko-2"]], ["operation"]], [[["Melania Trump-1"]], [["Olga Kurylenko-2"]], ["operation"]]], "golden_sentence": [["Melania Trump (/m\u0259\u02c8l\u0251\u02d0ni\u0259/; born Melanija Knavs; [m\u025b\u02c8la\u02d0nija \u02c8kna\u02d0u\u032fs], Germanized to Melania Knauss; April 26, 1970) is a Slovenian-American former model and businesswoman and the current first lady of the United States, as the wife of the 45th president of the United States Donald Trump."], ["Olga Storozhenko (born 14 April 1992) is a Ukrainian actress, model, teacher and beauty pageant titleholder who won Miss Ukraine Universe 2013."]]}, {"qid": "b9dd4fb2b013a8cb1281", "term": "BBC World Service", "description": "The BBC's international Chor radio station", "question": "Is the BBC World Service hosted in Europe?", "answer": true, "facts": ["The BBC World Service is part of the BBC network.", "The BBC operates in England.", "England is part of Europe."], "decomposition": ["Where is the BBC World Service located?", "Is #1 located in Europe?"], "evidence": [[[["BBC World Service-15"]], [["London-1", "Outline of the United Kingdom-1"]]], [[["BBC World Service-15"]], ["operation"]], [[["BBC World Service-2"]], [["United Kingdom-25"]]]], "golden_sentence": [["The Service broadcasts from Broadcasting House in London, which is also headquarters of the Corporation."], ["", "The following outline is provided as an overview of and topical guide to the United Kingdom of Great Britain and Northern Ireland; a sovereign state in Europe, commonly known as the United Kingdom (UK), or Britain."]]}, {"qid": "23dc728980bfb4e943e6", "term": "Atheism", "description": "Absence of belief in the existence of deities", "question": "Was Mother Theresa a follower of atheism?", "answer": false, "facts": ["Mother Theresa was a Catholic nun.", "Atheism is the absence of belief in a diety.", "Catholics believe in the Holy Trinity, which is a representation of God."], "decomposition": ["What was Mother Teresa's religion?", "Can an adherent of #1 be regarded a follower of atheism?"], "evidence": [[[["Mother Teresa-1"]], [["Atheism-1", "Catholic Church-1", "Christianity-1"]]], [[["Missionaries of Charity-1"]], [["Atheism-1"], "operation"]], [[["Mother Teresa-19"]], [["Atheism-7"]]]], "golden_sentence": [["Mother Mary Teresa Bojaxhiu (born Anjez\u00eb Gonxhe Bojaxhiu, Albanian:\u00a0[a\u02c8\u0272\u025bz\u0259 \u02c8\u0261\u0254nd\u0292\u025b b\u0254ja\u02c8d\u0292iu]; 26 August 1910\u00a0\u2013 5 September 1997), honoured in the Catholic Church as Saint Teresa of Calcutta, was an Albanian-Indian Roman Catholic nun and missionary."], ["Less broadly, atheism is a rejection of the belief that any deities exist.", "Its central administration is the Holy See.", "It is the world's largest religion with about 2.4 billion followers."]]}, {"qid": "916e0d9a2ee207bfd33e", "term": "EastEnders", "description": "British soap opera", "question": "Is it possible to binge entire EastEnders series without water?", "answer": false, "facts": ["British TV series EastEnders has over 6,000 episodes as of 2020.", "It would take approximately 125 days to binge watch the entire EastEnders TV series.", "A human can last only 4 days without water."], "decomposition": ["How many days can a human last without water?", "How many episodes are there in the EastEnders series?", "How many days would it take to binge watch #2 average-length episodes?", "Is #3 less than or equal to #1?"], "evidence": [[[["Survival skills-13"]], ["no_evidence"], [["EastEnders-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Dehydration-2"], "no_evidence"], [["EastEnders-1"], "no_evidence"], ["no_evidence", "operation"], ["no_evidence", "operation"]], [[["Dehydration-2"], "no_evidence"], [["EastEnders-86"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["A human being can survive an average of three to five days without the intake of water."], ["On 18 March 2020, BBC made the decision to suspend production of the programme until further notice, in addition to reducing the broadcast frequency to two 30-minute episodes per week."]]}, {"qid": "d917725da423544eca7b", "term": "Telescope", "description": "Optical instrument that makes distant objects appear magnified", "question": "Would stargazers prefer binoculars over a telescope?", "answer": false, "facts": ["Depending on a stargazer's goal, the scope of view necessary can change. ", "Companies produce both telescopes and binoculars for stargazing. "], "decomposition": ["How does the scope of a stargazer's observation vary?", "Does #1 stay the same?"], "evidence": [[[["Telescope-1"]], [["Binoculars-1"]]], [[["Observational astronomy-1"], "no_evidence"], [["Binoculars-1"], "operation"]], [[["Amateur astronomy-1"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "2dcc3e63ca77285a1468", "term": "Psychic", "description": "person who claims to use extrasensory perception to identify information hidden from the normal senses", "question": "Would Carrie Poppy be likely to trust a psychic?", "answer": false, "facts": ["Carrie Poppy is an American podcaster who is on a show called 'Oh No! Ross and Carrie.\"", "\"Oh No Ross and Carrie\" is a show that critically examines religious claims and those of the supernatural.", "Carrie Poppy considers herself a skeptic and an investigative reporter."], "decomposition": ["What is Carrie Poppy's profession?", "What show is Carrie Poppy the #1 of?", "What is the main of #2?", "On #3, what stance does Carrie Poppy take as a reporter?", "Would someone who is #4 likely trust psychics?"], "evidence": [[[["Oh No, Ross and Carrie!-14"]], [["Oh No, Ross and Carrie!-1"]], [["Oh No, Ross and Carrie!-4"]], [["Oh No, Ross and Carrie!-14"]], ["operation"]], [[["Oh No, Ross and Carrie!-2"]], [["Oh No, Ross and Carrie!-1"]], [["Oh No, Ross and Carrie!-1"], "no_evidence"], [["Oh No, Ross and Carrie!-8"]], ["no_evidence"]], [[["Oh No, Ross and Carrie!-14"]], [["Oh No, Ross and Carrie!-1"]], ["no_evidence"], [["Oh No, Ross and Carrie!-4"], "no_evidence"], ["operation"]]], "golden_sentence": [["Carrie Poppy is a writer and actress living in Los Angeles."], [""], [""], [""]]}, {"qid": "b18d8edbe7f20fa42d38", "term": "Fran\u00e7ois Mitterrand", "description": "21st President of the French Republic", "question": "Did Francois Mitterrand ever meet Barak Obama while they both held the position of President?", "answer": false, "facts": ["Mitterand was President of France from 1981 through 1995.", "Obama was President of the United States from 2009 to 2017"], "decomposition": ["When was Mitterand's final year as President of France?", "When was Obama's first year as President of the United States?", "Is #2 before #1?"], "evidence": [[[["Fran\u00e7ois Mitterrand-2"]], [["Presidency of Barack Obama-1"]], ["operation"]], [[["Fran\u00e7ois Mitterrand-1"]], [["Barack Obama-1"]], ["operation"]], [[["Fran\u00e7ois Mitterrand-1"]], [["Barack Obama-1"]], ["operation"]]], "golden_sentence": [["He was re-elected in 1988 and remained in office until 1995."], ["The presidency of Barack Obama began at noon EST on January 20, 2009, when Barack Obama was inaugurated as the 44th President of the United States, and ended on January 20, 2017."]]}, {"qid": "dbdeca26d9348fd0b17c", "term": "Jackson Pollock", "description": "American painter", "question": "Was Jackson Pollock trained by Leonardo da Vinci?", "answer": false, "facts": ["Leonardo lived during the Italian Renaissance in the 17th century.", "Jackson Pollock lived during the 20th century."], "decomposition": ["When did Leonardo da Vinci die?", "When was Jackson Pollock born?", "Is #2 before #1?"], "evidence": [[[["Leonardo da Vinci-1"]], [["Jackson Pollock-1"]], ["operation"]], [[["Leonardo da Vinci-1"]], [["Jackson Pollock-1"]], ["operation"]], [[["Leonardo da Vinci-27"], "no_evidence"], [["Jackson Pollock-4"], "operation"], ["no_evidence"]]], "golden_sentence": [["Leonardo di ser Piero da Vinci (Italian:\u00a0[leo\u02c8nardo di \u02ccs\u025br \u02c8pj\u025b\u02d0ro da (v)\u02c8vint\u0283i] (listen); 14/15 April 1452\u00a0\u2013\u00a02 May 1519), known as Leonardo da Vinci (English: /\u02ccli\u02d0\u0259\u02c8n\u0251\u02d0rdo\u028a d\u0259 \u02c8v\u026ant\u0283i, \u02ccli\u02d0o\u028a\u02c8-, \u02ccle\u026ao\u028a\u02c8-/ LEE-\u0259-NAR-doh d\u0259 VIN-chee, LEE-oh-, LAY-oh-), was an Italian polymath of the Renaissance whose areas of interest included invention, drawing, painting, sculpture, architecture, science, music, mathematics, engineering, literature, anatomy, geology, astronomy, botany, paleontology, and cartography."], ["Paul Jackson Pollock (/\u02c8p\u0252l\u0259k/; January 28, 1912 \u2013 August 11, 1956) was an American painter and a major figure in the abstract expressionist movement."]]}, {"qid": "a60fc40ee8a2b0af154f", "term": "Mona Lisa", "description": "Painting by Leonardo da Vinci", "question": "Is the Mona Lisa in the same museum as the Venus de Milo?", "answer": true, "facts": ["The Mona Lisa is in the Louvre.", "The Venus de Milo is in the Louvre."], "decomposition": ["What museum stores the Mona Lisa?", "What museum stores the Venus de Milo?", "Is #1 the same as #2?"], "evidence": [[[["Mona Lisa-54"]], [["Venus de Milo-2"]], ["operation"]], [[["Mona Lisa-18"]], [["Venus de Milo-17"]], ["operation"]], [[["Mona Lisa-2"]], [["Venus de Milo-2"]], ["operation"]]], "golden_sentence": [[""], ["It is currently on permanent display at the Louvre Museum in Paris."]]}, {"qid": "12b7b19a3858a68cca91", "term": "Confederate States Army", "description": "Southern army in American Civil War", "question": "Did Confederate States Army influence West Point fashion?", "answer": true, "facts": ["The Confederate States Army was clad in cadet gray uniforms.", "West Point uniforms are cadet gray and white.", "Confederate States Army uniforms contained Generally, the uniform jacket of the Confederate soldier was single breasted, made of gray or brown fabric, with a six to nine button front and hat.", " West Point uniforms contain a standing collar, white trousers, and black shakos (known as a \"tarbucket hat\" in U.S. Army nomenclature)."], "decomposition": ["What were the main features of the Confederate States Army uniforms?", "What are the most notable features of West Point uniforms?", "Is there a significant overlap between #1 and #2?"], "evidence": [[[["Uniforms of the Confederate States Armed Forces-17"]], [["United States Military Academy-62"], "no_evidence"], ["operation"]], [[["Confederate States Army-49"], "no_evidence"], [["United States Military Academy-74"], "no_evidence"], ["no_evidence", "operation"]], [[["Uniforms of the Confederate States Armed Forces-12"]], ["no_evidence"], ["operation"]]], "golden_sentence": [["The colors were red for artillery, yellow for cavalry, light blue for infantry, and black for medical."], [""]]}, {"qid": "e388b5ecaf846e34ab24", "term": "Biologist", "description": "Scientist studying living organisms", "question": "Is it possible for biologist Rachel Carson to have flown to the moon?", "answer": false, "facts": ["Rachel Carlson died in 1964", "The first humans landed on the moon in 1969"], "decomposition": ["When did Rachel Carson die?", "When did humans first land on the moon?", "Is #2 before #1?"], "evidence": [[[["Rachel Carson-44"]], [["Moon landing-12"]], ["operation"]], [[["Rachel Carson-1"]], [["Apollo 11-1"]], ["operation"]], [[["Rachel Carson-44"]], [["Apollo 11-1"]], ["operation"]]], "golden_sentence": [["She died of a heart attack on April 14, 1964, in her home in Silver Spring, Maryland."], ["This was accomplished with two US pilot-astronauts flying a Lunar Module on each of six NASA missions across a 41-month period starting 20 July 1969, with Neil Armstrong and Buzz Aldrin on Apollo 11, and ending on 14 December 1972 with Gene Cernan and Jack Schmitt on Apollo 17."]]}, {"qid": "e8d78651d59bdf89cab0", "term": "1980 United States presidential election", "description": "49th quadrennial presidential election in the United States", "question": "Can 1980 United States presidential election result be considered a photo finish?", "answer": false, "facts": ["A photo finish refers to a race or finish in which contestants are so close that a photograph of them as they cross the finish line has to be examined.", "Ronald Reagan had 489 electoral votes while Jimmy Carter had 49 in the 1980 United States presidential election.", "Ronald Reagan won 44 states while Jimmy Carter won 6 in the 1980 United States presidential election."], "decomposition": ["Who ran for the Democrats in the 1980 US presidential election?", "Who ran for the Republicans in the 1980 US presidential election?", "How many electoral votes did #1 receive?", "How many electoral votes did #2 receive?", "Is #3 close to #4?"], "evidence": [[[["1980 United States presidential election-1"]], [["1980 United States presidential election-1"]], [["1980 United States presidential election-4"]], [["1980 United States presidential election-4"]], [["History of the United States (1980\u20131991)-22"]]], [[["1980 United States presidential election-1"]], [["1980 United States presidential election-1"]], [["History of the United States (1980\u20131991)-22"]], [["History of the United States (1980\u20131991)-22"]], [["History of the United States (1980\u20131991)-22"], "operation"]], [[["1980 United States presidential election-1"]], [["1980 United States presidential election-1"]], [["1980 United States presidential election-50"]], [["1980 United States presidential election-50"]], ["operation"]]], "golden_sentence": [["Republican nominee Ronald Reagan defeated incumbent Democrat Jimmy Carter."], ["Republican nominee Ronald Reagan defeated incumbent Democrat Jimmy Carter."], ["Reagan won the election by a landslide, taking a large majority of the electoral vote and 50.7% of the popular vote."], ["Reagan won the election by a landslide, taking a large majority of the electoral vote and 50.7% of the popular vote."], [""]]}, {"qid": "f805d0218d81c8f31b0d", "term": "Lenovo", "description": "Chinese multinational technology company", "question": "Could a monolingual American read Lenovo's native name?", "answer": false, "facts": ["Lenovo's native name is \u8054\u60f3\u96c6\u56e2\u6709\u9650\u516c\u53f8.", "Someone who is monolingual only speaks one language.", "The typical monolingual American would only be able to read English.", "Someone who can only read English is therefore unable to read Chinese."], "decomposition": ["What is Lenovo's native name?", "What language is #1 in?", "What language does a monolingual American speak?", "Is #3 the same as #2?"], "evidence": [[[["Lenovo-51"]], [["Lenovo-1"]], [["American English-2"]], ["operation"]], [[["Lenovo-1"], "no_evidence"], [["Chinese language-1"], "no_evidence"], [["American English-2"]], ["operation"]], [[["Lenovo-51"]], [["Lenovo-51"]], [["Official language-39"]], ["operation"]]], "golden_sentence": [["\"Lenovo\" is a portmanteau of \"Le-\" (from Legend) and \"novo\", Latin ablative for \"new\"."], ["Lenovo Group Limited, often shortened to Lenovo (/l\u025b\u02c8no\u028avo\u028a/ leh-NOH-voh), is a Chinese multinational technology company with headquarters in Beijing."], ["English is explicitly given official status by 32 of the 50 state governments."]]}, {"qid": "2daa8208570bf20077fa", "term": "Reproduction", "description": "Biological process by which new organisms are generated from one or more parent organisms", "question": "Would an environmentalist advocate for preventing domestic canine reproduction?", "answer": true, "facts": ["Domestic dogs are a large contributor to species depopulation and displacement.", "Domestic dogs have a diet that largely contributes to a harmful environmental impact. "], "decomposition": ["What do environmentalists try to protect?", "Do domestic dogs harm #1?"], "evidence": [[[["Environmentalism-1"]], [["Dog-61"], "no_evidence", "operation"]], [[["Environmentalist-1"]], [["Dog-42"], "no_evidence", "operation"]], [[["Environmentalist-1"]], [["Dog-30", "Overpopulation in domestic pets-4"], "no_evidence"]]], "golden_sentence": [["Environmentalism or environmental rights is a broad philosophy, ideology, and social movement regarding concerns for environmental protection and improvement of the health of the environment, particularly as the measure for this health seeks to incorporate the impact of changes to the environment on humans, animals, plants and non-living matter."], [""]]}, {"qid": "97b4ba60bc141f74b548", "term": "Othello", "description": "play by Shakespeare", "question": "Are there options for students who struggle to understand the writing style of Othello?", "answer": true, "facts": ["\"No Fear Shakespeare\" is a line of books that translate the language of original Shakespeare plays into modern English.", "Scripts can be understood more easily when read alongside a production of the play itself.", "\"No Fear Shakespeare\" features Othello in their book lineup."], "decomposition": ["What is the name of a line of books that translate the language of Shakespeare plays into modern English?", "Does #1 feature Othello in their book lineup?"], "evidence": [[[["Hamlet-2"]], ["no_evidence", "operation"]], [[["SparkNotes-5"]], ["no_evidence", "operation"]], [[["SparkNotes-5"]], ["no_evidence"]]], "golden_sentence": [[""]]}, {"qid": "9bc082a4c2760558413e", "term": "French Defence", "description": "Chess opening", "question": "Would most grand masters know what the French Defense is?", "answer": true, "facts": ["Grand master is the highest title a chess player can get.", "The French Defense is a well known chess opening that is in many books."], "decomposition": ["What is the French Defence?", "What is a Grandmaster?", "Would #2 likely know what #1 is?"], "evidence": [[[["French Defence-1"]], [["Grandmaster (chess)-1"]], ["operation"]], [[["French Defence-46"]], [["Grandmaster (chess)-1"]], ["operation"]], [[["French Defence-2"]], [["Grandmaster (chess)-1"]], ["operation"]]], "golden_sentence": [["The French Defence is a chess opening characterised by the moves:"], ["Grandmaster (GM) is a title awarded to chess players by the world chess organization FIDE."]]}, {"qid": "294af7c084886cb8ce51", "term": "MF Doom", "description": "US-based English rapper and producer", "question": "Is MF Doom a Fantastic Four villain?", "answer": false, "facts": ["MF Doom is a British rapper raised on Long Island.", "Doctor Victor Von Doom is a fictional supervillain that made his debut in the Fantastic Four.", "The MF in MF Doom stands for Metal Face."], "decomposition": ["Which villains are featured in Fantastic Four?", "Is MF doom one of #1?"], "evidence": [[[["MF Doom-1"]], ["operation"]], [[["Fantastic Four-54"]], ["operation"]], [[["Fantastic Four-54"]], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "3dbe6c18cb6f323dd387", "term": "Snow White", "description": "fairy tale", "question": "Is Snow White an example of good consent?", "answer": false, "facts": ["Snow White is unknowingly poisoned by a witch.", "Snow White is unconscious when a man kisses her without her knowledge.", "Consent involves knowingly allowing something to happen to oneself. "], "decomposition": ["What conditions can prevent someone from giving consent?", "Was Snow White free of any of #1?"], "evidence": [[[["Sexual consent-25"]], [["Snow White and the Seven Dwarfs (1937 film)-9"], "operation"]], [[["Consent-18"]], [["Snow White and the Seven Dwarfs (1937 film)-9"], "operation"]], [[["Consent-3"]], ["no_evidence"]]], "golden_sentence": [["To address these concerns, there was a shift from 'no means no' to 'yes means yes' (affirmative consent), to ensure that people were not having sexual actions taken on them due to not speaking up or not resisting."], ["The dwarfs return to their cottage and find Snow White seemingly dead, being kept in a deathlike slumber by the poison."]]}, {"qid": "d64abfe5cb2fa38c1f59", "term": "The Matrix", "description": "1999 science fiction action film directed by the Wachowskis", "question": "Will Gremlins sequels tie number of Matrix sequels?", "answer": true, "facts": ["The Matrix films had two sequels.", "Gremlins has one sequel, Gremlins 2: The New Batch.", "The script for a Gremlins 3 is being written by Carl Ellsworth."], "decomposition": ["How many sequels did The Matrix have?", "How many sequels did Gremlins have?", "How many Gremlins movies are currently being worked on?", "What is the sum of #2 and #3?", "Is #4 equal to #1?"], "evidence": [[[["The Matrix (franchise)-1"]], [["Gremlins-48"]], [["Gremlins-51"]], [["The Matrix (franchise)-1"]], [["The Matrix (franchise)-1"]]], [[["The Matrix (franchise)-1"]], [["Gremlins-2"]], [["Gremlins: Secrets of the Mogwai-2"], "operation"], ["operation"], ["operation"]], [[["The Matrix (franchise)-1"]], [["Gremlins-2"]], [["Gremlins 2: The New Batch-46"]], ["operation"], ["operation"]]], "golden_sentence": [["The series primarily consists of a trilogy of science fiction action films beginning with The Matrix (1999) and continuing with two sequels, The Matrix Reloaded and The Matrix Revolutions (both in 2003), all written and directed by the Wachowskis and produced by Joel Silver."], ["The film not only spawned the sequel, Gremlins 2: The New Batch, and an advertisement for British Telecom, but is believed to have been the inspiration for several unrelated films about small monsters."], ["In a December 2016 interview with Bleeding Cool, Galligan again spoke about a third film saying that \"Warner Bros. definitely wants it, Chris Columbus wants to do it because he\u2019d like to undo the Gremlins 2 thing as he wasn\u2019t thrilled with it, and Spielberg wants to.\""], [""], [""]]}, {"qid": "d17fe2d9ab76cba4b44e", "term": "Leipzig", "description": "Place in Saxony, Germany", "question": "Is the tree species that the name Leipzig refers to an evergeen tree?", "answer": false, "facts": ["Leipzig is derived from the Slavic word Lipsk", "Lipsk means \"settlement where the linden trees stand\"", "Linden trees are deciduous trees"], "decomposition": ["Which species of tree is mentioned in the meaning of the name (of a city) Leipzig?", "Classifying by seasonal traits, what kind of tree is #1?", "Is being evergreen a characteristic of #2?"], "evidence": [[[["Leipzig-6"]], [["Tilia-1", "Tilia-2"]], ["operation"]], [[["Leipzig-6"]], [["Tilia-2"]], [["Evergreen-6"]]], [[["Leipzig-6"]], [["Tilia-1"]], ["operation"]]], "golden_sentence": [["The name Leipzig is derived from the Slavic word Lipsk, which means \"settlement where the linden trees (British English: lime trees; U.S. English: basswood trees) stand\"."], ["", ""]]}, {"qid": "35b172ac98428e5060b2", "term": "Bandy", "description": "ballgame on ice played using skates and sticks", "question": "Can Kate Gosselin's household fill out a Bandy team?", "answer": false, "facts": ["The game of Bandy includes 11 players on each team.", "Reality star Kate Gosselin has eight children."], "decomposition": ["How many players make up a Bandy team?", "How many people are in Kate Gosselin's household?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Bandy-2"]], [["Kate Gosselin-12", "Kate Gosselin-4"]], ["operation"]], [[["Bandy-2"]], [["Kate Plus 8-1"]], ["operation"]], [[["Bandy-2"]], [["Kate Gosselin-1"]], ["operation"]]], "golden_sentence": [["Bandy has also been influenced by the rules of association football: both games are normally played in halves of 45 minutes, there are 11 players on each team, and the fields in both games are about the same size."], ["While Hannah and Collin now live with Jon, the other four sextuplets still live with Kate.", "[citation needed] The early birth required the six infants to be placed on ventilators."]]}, {"qid": "9ba674acb3275fee2936", "term": "People for the Ethical Treatment of Animals", "description": "American animal rights organization", "question": "Is Michael Vick on People for the Ethical Treatment of Animals's hypothetical blacklist?", "answer": true, "facts": ["People for the Ethical Treatment of Animals (PETA) is an animal rights organization that fights for the welfare of animals.", "A blacklist is a list of people that are unacceptable and should be avoided or excluded.", "Michael Vick is an ex-football player that spent 21 months in prison for his vicious dog fighting enterprise."], "decomposition": ["What do the People for the Ethical Treatment of Animals advocate for?", "What crime has Michael Vick done time for?", "Does #2 strongly violate #1?"], "evidence": [[[["People for the Ethical Treatment of Animals-1"]], [["Michael Vick-2"]], [["Dog fighting in the United States-1"]]], [[["People for the Ethical Treatment of Animals-11"]], [["Michael Vick-2"]], ["operation"]], [[["People for the Ethical Treatment of Animals-1"]], [["Michael Vick-2"]], [["Dog fighting-2"], "operation"]]], "golden_sentence": [[""], ["Vick's NFL career came to a halt in 2007 after he pleaded guilty for his involvement in a dog fighting ring and spent 21 months in federal prison."], [""]]}, {"qid": "3fc502ad9239015f5b81", "term": "Middle Ages", "description": "Period of European history from the 5th to the 15th century", "question": "Was dynamite used during Middle Ages warfare?", "answer": false, "facts": ["The Middle Ages ended with the Fall of Constantinople in 1453.", "Dynamite was invented by Swedish chemist Alfred Nobel in the 1870s."], "decomposition": ["When was dynamite invented?", "When did the Middle Ages warfare take place?", "Is #1 within or before #2?"], "evidence": [[[["Dynamite-3"]], [["Middle Ages-7"]], ["operation"]], [[["Dynamite-1"]], [["Middle Ages-1"]], ["operation"]], [[["Dynamite-1"]], [["Medieval warfare-25"]], ["operation"]]], "golden_sentence": [["Dynamite was invented by Swedish chemist Alfred Nobel in the 1860s and was the first safely manageable explosive stronger than black powder, which had been invented in China in the 9th century."], [""]]}, {"qid": "eb5d48eaa716533af295", "term": "Downton Abbey", "description": "British historical drama television series", "question": "Would Downton Abbey finale viewership defeat every Kazakhstan citizen in tug of war?", "answer": false, "facts": ["Downton Abbey's finale had a total of 9.6 million viewers.", "Kazakhstan has 18.7 million citizens as of 2020."], "decomposition": ["How many people watched Downton Abbey finale?", "How many people are Kazakh citizens?", "Is #1 greater than #2?"], "evidence": [[["no_evidence"], [["Kazakhstan-2"]], ["no_evidence", "operation"]], [["no_evidence"], [["Kazakhstan-167"]], ["operation"]], [["no_evidence"], [["Demographics of Kazakhstan-2"]], ["operation"]]], "golden_sentence": [["Kazakhstan has an estimated 18.3 million people as of 2018[update]."]]}, {"qid": "1d9471ed8d278df9f5d3", "term": "Roman numerals", "description": "Numbers in the Roman numeral system", "question": "Is MIX a word and a roman numeral?", "answer": true, "facts": ["\"Mix\" means to combine in english.", "M equals one thousand in roman numerals", "I equals one in roman numerals ", "I before X in roman numerals equals nine.", "MIX equals one thousand nine in roman numerals. "], "decomposition": ["What does Mix mean in english language?", "Is Mix a number in Roman numerals?", "Based on #1 and #2, is mix both a word and a roman numeral?"], "evidence": [[[["Audio mixing (recorded music)-1"]], [["Roman numerals-5"]], [["Audio mixing (recorded music)-1", "Roman numerals-5"]]], [[["Mix (magazine)-1", "Mixing (process engineering)-38"], "no_evidence"], [["1009-1", "Roman numerals-1"]], ["operation"]], [[["DJ mix-1"]], [["Roman numerals-5"]], ["operation"]]], "golden_sentence": [[""], [""], ["", "MCM, signifying \"a thousand, and a hundred less than another thousand\", means 1900, so 1912 is written MCMXII."]]}, {"qid": "ddb2f25c4e602184d2ee", "term": "Little Women", "description": "1860s novel by Louisa May Alcott", "question": "Could Little Women have been a book read by veterans of the civil war?", "answer": true, "facts": ["Little Women was published in 1868.", "The civil war ended in 1865."], "decomposition": ["When was the book Little Women written?", "When did the civil war take place?", "Could adults as of #2 still be alive by #1?"], "evidence": [[[["Little Women-1"]], [["American Civil War-1"]], ["operation"]], [[["Little Women-1"]], [["American Civil War-1"]], ["operation"]], [[["Little Women-1"]], [["American Civil War-1"]], ["operation"]]], "golden_sentence": [["Little Women is a novel by American author Louisa May Alcott (1832\u20131888) which was originally published in two volumes in 1868 and 1869."], ["The American Civil War (also known by other names) was a civil war in the United States from 1861 to 1865, fought between the northern United States (loyal to the Union) and the southern United States (that had seceded from the Union and formed the Confederacy)."]]}, {"qid": "1d90ba4f62c78abc50e6", "term": "Stoning", "description": "execution method", "question": "Will a celibate cleric likely suffer a stoning in Somalia?", "answer": false, "facts": ["A cleric is the term for a Muslim priest.", "Celibate people remain chaste and do not engage in relations with others.", "Stoning is a penalty in Somalia used to punish adulterers.", "Many Islamic militants have been in control of various parts of Somalia."], "decomposition": ["Which crime is punishable by stoning in Somalia?", "What relationship must a person guilty of #1 be in in order to be deemed guilty?", "Would a celibate cleric be involved in #2?"], "evidence": [[[["Sharia-16", "Sharia-4", "Somalia-158"]], [["Adultery-1"]], [["Celibacy-1"], "operation"]], [[["Stoning-65"]], [["Adultery-1"]], [["Celibacy-1"], "operation"]], [[["Stoning-83"]], [["Stoning-83"]], [["Stoning-83"]]]], "golden_sentence": [["", "The Islamic revival of the late 20th century brought along calls by Islamist movements for full implementation of Sharia, including hudud corporal punishments, such as stoning.", ""], ["A single act of sexual intercourse is generally sufficient to constitute adultery, and a more long-term sexual relationship is sometimes referred to as an affair."], [""]]}, {"qid": "50e8f8141002c869f5af", "term": "Gray whale", "description": "species of mammal", "question": "Would a Gray Whale fit easily in an above ground pool?", "answer": false, "facts": ["Gray whales are, on average, 39ft long.", "The average above ground pool is 10-33ft in diameter. "], "decomposition": ["What is the average size of an above ground pool?", "What is the size of average gray whale?", "Is #2 smaller than #1?"], "evidence": [[[["Swimming pool-18"]], [["Gray whale-7"]], ["operation"]], [[["Swimming pool-1", "Swimming pool-23"], "no_evidence"], [["Gray whale-1"]], ["operation"]], [[["Swimming pool-23"], "no_evidence"], [["Gray whale-1"]], ["operation"]]], "golden_sentence": [["There are also many pools 33\u2153\u00a0m long, so that 3 lengths = 100\u00a0m. This pool dimension is commonly used to accommodate water polo."], ["Gray whales measure from 4.9\u00a0m (16\u00a0ft) in length for newborns to 13\u201315\u00a0m (43\u201349\u00a0ft) for adults (females tend to be slightly larger than adult males)."]]}, {"qid": "065d694d78632dbc7d1d", "term": "Lactobacillus", "description": "genus of bacteria", "question": "Do you need lactobacillus to make pickles?", "answer": false, "facts": ["Lactobacillus is used in the production of fermented-style pickles. ", "\"Quick\" pickles are made with a mixture of brine and vinegar."], "decomposition": ["What are some common methods of making pickles?", "Is lactobacillus required for all of #1?"], "evidence": [[[["Pickled cucumber-12", "Pickled cucumber-3", "Pickled cucumber-7"], "no_evidence"], [["Lactobacillus-1"], "operation"]], [[["Pickling-35", "Pickling-36", "Pickling-38"], "no_evidence"], ["operation"]], [[["Pickling-1"]], [["Lactobacillus-1"]]]], "golden_sentence": [["Elsewhere, these pickles may sometimes be termed \"old\" and \"new\" dills.", "", "Bread-and-butter pickles are a marinated pickle produced with sliced cucumbers in a solution of vinegar, sugar, and spices which may be processed either by canning or simply chilled as refrigerator pickles."], [""]]}, {"qid": "0d57e09b28f3151170fa", "term": "Holy Saturday", "description": "Saturday before Easter Sunday", "question": "Did Holy Saturday 2019 have special significance to pot smokers?", "answer": true, "facts": ["Holy Saturday 2019 took place on April 20th.", "April 20th, known as 4/20 day, National Pot Smokers Day, Weed Day or National Weed Day, is a holiday for pot smokers."], "decomposition": ["What date was Holy Saturday in 2019?", "What date is an unofficial holiday for pop smokers?", "Is #1 the same as #2?"], "evidence": [[[["Holy Saturday-3"], "no_evidence"], [["420 (cannabis culture)-1"]], ["operation"]], [["no_evidence"], [["420 (cannabis culture)-1"]], ["no_evidence", "operation"]], [[["2019 Australian federal election-38"], "no_evidence"], [["420 (cannabis culture)-15"]], ["operation"]]], "golden_sentence": [[""], ["420, 4:20, or 4/20 (pronounced four-twenty) is cannabis culture slang for marijuana consumption, especially smoking around the time 4:20 pm, and also refers to cannabis-oriented celebrations that take place annually on April 20 (which is 4/20 in U.S. form)."]]}, {"qid": "7e7edfe5ca0fd94a0df8", "term": "Korea under Japanese rule", "description": "Japanese occupation of Korea from 1910\u20131945", "question": "Did people in Korea under Japanese Rule watch a lot of Iron Chef?", "answer": false, "facts": ["The first televisions were sold in 1946.", "Iron Chef started airing in 1993."], "decomposition": ["During what years was Korea under the rule of the Japanese?", "In what year did Iron Chef first appear on television?", "Did #1 occur after #2?"], "evidence": [[[["Korea under Japanese rule-1"]], [["Iron Chef-1"]], ["operation"]], [[["Korea under Japanese rule-1"]], [["Iron Chef-1"]], ["operation"]], [[["World War II by country-161"]], [["Iron Chef-1"]], ["operation"]]], "golden_sentence": [["Japanese Korea (Japanese: \u5927\u65e5\u672c\u5e1d\u56fd (\u671d\u9bae), Dai-Nippon Teikoku [Ch\u014dsen]) was the period when Korea was under Japanese rule, between 1910 and 1945."], ["The series, which premiered on October 10, 1993, is a stylized cook-off featuring guest chefs challenging one of the show's resident \"Iron Chefs\" in a timed cooking battle built around a specific theme ingredient."]]}, {"qid": "c8078d297ee386db0638", "term": "Soup", "description": "primarily liquid food", "question": "Is shoe soup innocuous?", "answer": true, "facts": ["Soup is a primarily liquid food containing various meats and beans.", "Director Werner Herzog lost a bet and cooked his shoe into a soup and ate it in 1980.", "Werner Herzog turned 77 in 2019 and had a role in the hit TV series the Mandalorian."], "decomposition": ["What film director ate shoe soup in the year 1980?", "Is #1 still alive?"], "evidence": [[[["Werner Herzog Eats His Shoe-1"]], [["Werner Herzog-1"], "operation"]], [[["Werner Herzog-12"]], [["Werner Herzog-30"], "operation"]], [[["Werner Herzog Eats His Shoe-1"]], [["Werner Herzog-1"]]]], "golden_sentence": [["Werner Herzog Eats His Shoe is a short documentary film directed by Les Blank in 1980 which depicts director Werner Herzog living up to his promise that he would eat his shoe if Errol Morris ever completed the film Gates of Heaven."], [""]]}, {"qid": "f7e86c75f7cf11c56e7c", "term": "Swallow", "description": "family of birds", "question": "In a hypothetical race between a Swallow and an American Woodcock, would the Swallow win?", "answer": true, "facts": ["Swallow can fly about 30-40mph. ", "The American woodcock can fly approximately 5mph. "], "decomposition": ["How quickly can a swallow fly?", "How quickly can an American woodcock fly?", "Is #1 greater than #2?"], "evidence": [[[["Barn swallow-18"]], [["American woodcock-14"]], [["Barn swallow-18"]]], [["no_evidence"], [["American woodcock-14"]], ["no_evidence", "operation"]], [[["Swallow-16"], "no_evidence"], [["American woodcock-14"]], ["no_evidence", "operation"]]], "golden_sentence": [["It is not a particularly fast flier, with a speed estimated at about 11\u00a0m/s, up to 20\u00a0m/s and a wing beat rate of approximately 5, up to 7\u20139 times each second."], ["Flight speeds of migrating birds have been clocked at 16 to 28 miles per hour (26 to 45\u00a0kilometers per hour)."], [""]]}, {"qid": "5578fa931e75753da8c6", "term": "Sea turtle", "description": "superfamily of reptiles", "question": "Can a sea turtle play tennis using a tennis racket?", "answer": false, "facts": ["to play tennis, a human-like hand is needed to properly hold the tennis racket", "sea turtles have flippers and not human-like hands"], "decomposition": ["What body part does one need to hold a tennis racket?", "Do turtles have #1?"], "evidence": [[[["Hand-3", "Hand-6"]], [["Turtle-26", "Turtle-27"], "operation"]], [[["Racket (sports equipment)-17"]], [["Turtle-27"], "operation"]], [[["Racket (sports equipment)-5"]], [["Turtle-26", "Turtle-27"], "operation"]]], "golden_sentence": [["The human hand normally has five digits: four fingers plus one thumb; these are often referred to collectively as five fingers, however, whereby the thumb is included as one of the fingers.", "The scientific use of the term hand in this sense to distinguish the terminations of the front paws from the hind ones is an example of anthropomorphism."], ["", "These species swim in the same way as sea turtles do (see below)."]]}, {"qid": "3161a8719c19cac08f64", "term": "Marxism", "description": "Economic and sociopolitical worldview based on the works of Karl Marx", "question": "Are right wing Amreicans opposed to marxism?", "answer": true, "facts": ["Right Wing Americans view socialism as an enemy to civil liberties and the economy.", "Socialism is a tenant of Marxism, giving workers the means of the production."], "decomposition": ["What stance do most right-wing Americans take towards socialism?", "Is #1 against that which Marxists proposes on the subject?"], "evidence": [[[["Right-wing politics-7"]], [["Marxism-21"], "operation"]], [[["Conservatism-1", "Conservatism-7"]], [["Means of production-5"], "operation"]], [[["Right-wing politics-7"]], [["Timeline of Karl Marx-2"]]]], "golden_sentence": [[""], [""]]}, {"qid": "aedab05ccfc5216a8f23", "term": "Confederate States Army", "description": "Southern army in American Civil War", "question": "Are there Americans still enlisted in the Confederate States Army?", "answer": false, "facts": ["The Confederate States Army disbanded in 1865.", "The last living confederate soldier died in 1951."], "decomposition": ["What is the present status of the Confederate States Army?", "Considering #1, can there still be anyone enlisted?"], "evidence": [[[["Confederate States of America-8"]], ["operation"]], [[["Confederate States Army-5"]], ["operation"]], [[["Confederate States Army-1", "Confederate States Army-5"]], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "5e432086a8f734e4f8a9", "term": "Stone Cold Steve Austin", "description": "American professional wrestler", "question": "Can Stone Cold Steve Austin apply his finisher to a mule deer?", "answer": true, "facts": ["Steve Austin's finisher, The Stone Cold Stunner, involves lifting the opponent and then pulling their head down.", "The largest male mule deer's are around 330 pounds.", "Steve Austin has applied his finisher to the wrestler The Big Show.", "The Big Show weighs 383 pounds."], "decomposition": ["What activity is involved in Stone Cold Steve Austin's finisher?", "How much does a mule deer weigh?", "What was the weight of Stone Cold Steve Austin's largest opponent that he used #1 on?", "Is #2 less than or equal to #3?"], "evidence": [[[["Stone Cold Steve Austin-70", "Stunner (professional wrestling)-13"]], [["Mule deer-6"]], [["The Undertaker-72"], "no_evidence"], ["operation"]], [[["Stone Cold Steve Austin-70"]], [["Mule deer-6"]], ["no_evidence"], ["operation"]], [[["Stone Cold Steve Austin-15", "Stunner (professional wrestling)-1"]], [["Mule deer-6"]], [["Stone Cold Steve Austin-21"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["During his time in WCW, Austin used the Stun Gun (a move innovated by Eddie Gilbert called Hot Shot) and Hollywod & Vine (a standing modified figure-four Leglock) as his finishers.", "The true Stone Cold stunner as popularized and stated by Steve Austin, always involves a boot to the gut before the 3/4 facelock is applied."], ["This race is markedly smaller than other mule deer, with an average weight of 54.5\u00a0kg (120\u00a0lb) and 36\u00a0kg (79\u00a0lb) in males and females, respectively."], [""]]}, {"qid": "997f76e72e525e554643", "term": "DC Comics", "description": "U.S. comic book publisher", "question": "Did President William Howard Taft read DC Comics?", "answer": false, "facts": ["DC Comics were founded in 1934.", "President William Howard Taft died on March 8, 1930."], "decomposition": ["When was DC Comics founded?", "When did President William Howard Taft die?", "Is #1 before #2?"], "evidence": [[[["DC Comics-4"]], [["William Howard Taft-1"]], ["operation"]], [[["DC Comics-4"]], [["William Howard Taft-1"]], ["operation"]], [[["DC Comics-4"]], [["William Howard Taft-1"]], ["operation"]]], "golden_sentence": [["The character Doctor Occult, created by Jerry Siegel and Joe Shuster in December 1935 within the issue No."], ["William Howard Taft (September 15, 1857\u00a0\u2013 March 8, 1930) was the 27th president of the United States (1909\u20131913) and the tenth chief justice of the United States (1921\u20131930), the only person to have held both offices."]]}, {"qid": "45bc751c573f4ce74fbb", "term": "Reformation", "description": "Schism within the Christian Church in the 16th century", "question": "Did Barack Obama participate in the Reformation?", "answer": false, "facts": ["The Reformation took place in the 16th century. ", "Barack Obama was born in 1961."], "decomposition": ["When did the Reformation take place?", "When was Barack Obama born?", "Is #2 before #1?"], "evidence": [[[["Christianity in the 16th century-39"]], [["Barack Obama-1"]], ["operation"]], [[["Reformation-19"]], [["Barack Obama-6"]], ["operation"]], [[["Reformation-1"]], [["Barack Obama-1"]], ["operation"]]], "golden_sentence": [["The political separation of the Church of England from Rome, beginning in 1529 and completed in 1536, brought England alongside this broad Reformed movement."], ["Barack Hussein Obama II (/b\u0259\u02c8r\u0251\u02d0k hu\u02d0\u02c8se\u026an o\u028a\u02c8b\u0251\u02d0m\u0259/ (listen); born August 4, 1961) is an American politician and attorney who served as the 44th president of the United States from 2009 to 2017."]]}, {"qid": "977c0ffb9c27b442a0bb", "term": "Seinfeld", "description": "American sitcom", "question": "Could you watch a new Seinfeld episode every day for a year?", "answer": false, "facts": ["There are 365 days in a year.", "There are a total of 180 Seinfeld episodes."], "decomposition": ["How many days are there in a year?", "How many Seinfeld episodes are there?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Seinfeld-31"]], [["Year-3"]], ["operation"]], [[["Seinfeld-1"]], [["Year-4"]], ["operation"]], [[["Year-3"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "739f0db41e8551513109", "term": "Honey badger", "description": "species of mammal", "question": "Are honey badgers and hyenas anatomically dissimilar? ", "answer": false, "facts": ["Honey Badgers and Hyenas both have anal sacs.", "The anal sacs of Honey Badgers and Hyenas are both able to turn inside out."], "decomposition": ["What are the features of the anal sacs of the Honey Badgers?", "What are the features of the Hyenas' anal sacs?", "Is #1 anatomically dissimilar from #2?"], "evidence": [[["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Honey badger-13"]], [["Hyena-18"]], ["operation"]], [[["Honey badger-13"]], [["Honey badger-13"]], ["operation"]]], "golden_sentence": []}, {"qid": "ccd449afcb844930095a", "term": "Martin Luther", "description": "Saxon priest, monk and theologian, seminal figure in Protestant Reformation", "question": "If Martin Luther did one theses a day would he run out in half a year?", "answer": true, "facts": ["Martin Luther published a list of 95 theses as his critique of the church.", "There are approximately 182 days in 6 months."], "decomposition": ["How many theses did Martin Luther publish in a list?", "How many days are in a year?", "What is #2 divided by 2?", "Is #1 less than #3?"], "evidence": [[[["Martin Luther-1"]], [["Year-4"]], ["operation"], ["operation"]], [[["Ninety-five Theses-1"]], [["Calendar year-2"]], ["operation"], ["operation"]], [[["Martin Luther-1"]], [["Year-3"]], ["operation"], ["operation"]]], "golden_sentence": [["Luther proposed an academic discussion of the practice and efficacy of indulgences in his Ninety-five Theses of 1517."], ["In astronomy, the Julian year is a unit of time; it is defined as 365.25 days of exactly 86,400 seconds (SI base unit), totalling exactly 31,557,600 seconds in the Julian astronomical year."]]}, {"qid": "b7434fb9544c0e22fcc9", "term": "Antarctic Peninsula", "description": "peninsula", "question": "Would a 75 degree Fahrenheit day be unusual on the Antarctic Peninsula? ", "answer": true, "facts": ["The warmest days on record in the Antarctic Peninsula are in the high 50's. ", "On average, the Antarctic Peninsula is between -4 and 36 degrees Fahrenheit. "], "decomposition": ["What is the average temperature in the Antarctic Peninsula?", "Does 75 degrees Fahrenheit fall outside the range of #1?"], "evidence": [[[["Antarctic Peninsula-21"]], ["operation"]], [[["Antarctic Peninsula-21"]], ["operation"]], [[["Antarctic Peninsula-21"]], ["operation"]]], "golden_sentence": [["Its temperatures are warmest in January, averaging 1 to 2\u00a0\u00b0C (34 to 36\u00a0\u00b0F), and coldest in June, averages from \u221215 to \u221220\u00a0\u00b0C (5 to \u22124\u00a0\u00b0F)."]]}, {"qid": "ecf8311da2a3ef364d9f", "term": "Napoleonic Wars", "description": "Series of early 19th century European wars", "question": "Did earth complete at least one orbit around the sun during the Napoleonic Wars?", "answer": true, "facts": ["Earth orbits around the Sun in 365 days.", "Napoleonic Wars lasted 12 years, 5 months and 4 weeks."], "decomposition": ["How long is the orbit of the earth around the sun?", "How long were the Napoleonic Wars?", "Is #2 greater than #1?"], "evidence": [[[["Year-1"]], [["Napoleonic Wars-1"]], ["operation"]], [[["Earth's orbit-1"]], [["Napoleonic Wars-1"]], ["operation"]], [[["Earth's orbit-1"]], [["Napoleonic Wars-1"]], ["operation"]]], "golden_sentence": [["A year is the orbital period of Earth moving in its orbit around the Sun."], ["The Napoleonic Wars (1803\u20131815) were a series of major conflicts pitting the French Empire and its allies, led by Napoleon I, against a fluctuating array of European powers formed into various coalitions, financed and usually led by the United Kingdom."]]}, {"qid": "9063dbb03f315cfe5b26", "term": "Dodo", "description": "Extinct large flightless pigeon from Mauritius", "question": "Would a Dodo hypothetically tower over Ma Petite?", "answer": true, "facts": ["A Dodo was an extinct bird that was over 3 feet tall.", "Ma Petite was a character on American Horror Story played by Jyoti Amge.", "Jyoti Amge is around 2 feet tall."], "decomposition": ["How tall were dodos?", "Who played the role of Ma Petite?", "How tall is #2?", "Is #1 greater than #3?"], "evidence": [[[["Dodo-2"]], [["Jyoti Amge-1"]], [["Jyoti Amge-2"]], ["operation"]], [[["Dodo-2"]], [["Jyoti Amge-3"]], [["Jyoti Amge-2"]], ["operation"]], [[["Dodo-2"]], [["Jyoti Amge-3"]], [["Jyoti Amge-2"]], ["operation"]]], "golden_sentence": [["Subfossil remains show the dodo was about 1 metre (3\u00a0ft 3\u00a0in) tall and may have weighed 10.6\u201317.5\u00a0kg (23\u201339\u00a0lb) in the wild."], [""], ["Following Amge's 18th birthday on 16 December 2011, she was officially declared the world's smallest woman by Guinness World Records with a height of 62.8\u00a0centimetres (2 ft 0.6 in)."]]}, {"qid": "e0970560c2a3a59a874d", "term": "World of Warcraft", "description": "video game by Blizzard Entertainment", "question": "Can you find Depala's race in World of Warcraft?", "answer": true, "facts": ["World of Warcraft has several races including humans, night elves, and dwarves.", "Depala is a character in the Magic the Gathering card game that is a dwarf."], "decomposition": ["What are the different races found in World of Warcraft?", "What race is Depala?", "Is #2 listed in #1?"], "evidence": [[[["World of Warcraft-8"]], [["Magic: The Gathering-2"], "no_evidence"], ["operation"]], [[["Gameplay of World of Warcraft-5"], "no_evidence"], [["Depala Vas-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Gameplay of World of Warcraft-5"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["The player selects the new character's race, such as orcs or trolls for the Horde, or humans or dwarves for the Alliance."], [""]]}, {"qid": "b4867d274d55f885fb01", "term": "Klingon", "description": "Fictional species in Star Trek", "question": "Did Klingons appear in the movie The Last Jedi?", "answer": false, "facts": ["Klingons are a race in the fictional universe of Star Trek.", "The Last Jedi is a movie set in the fictional universe of Star Wars."], "decomposition": ["Which fictional universe do the Klingons exist in?", "Which fictional universe is The Last Jedi movie set in?", "Is #1 the same as #2?"], "evidence": [[[["Klingon-2"]], [["Star Wars: The Last Jedi-1"]], ["operation"]], [[["Klingon-1"]], [["Star Wars: The Last Jedi-1"]], ["operation"]], [[["Klingon-1"]], [["Star Wars: The Last Jedi-1"]], ["operation"]]], "golden_sentence": [[""], ["Star Wars: The Last Jedi (also known as Star Wars: Episode VIII \u2013 The Last Jedi) is a 2017 American epic space opera film written and directed by Rian Johnson."]]}, {"qid": "68a1003e77597fbbd848", "term": "Sandal", "description": "Type of footwear with an open upper", "question": "If one of your feet is in a leg cast, should the other be in a sandal?", "answer": false, "facts": ["If you are using crutches, it is advised to have non-slip shoes with a closed toe.", "Most sandals do not have non-slip traction and, by definition, don't have closed toes. ", "If you are in a leg cast you are likely to be using crutches."], "decomposition": ["If you are in a leg cast, what are you likely using to help yourself walk?", "What kind of shoes do doctors recommend for you to wear if you are using #1?", "Do sandals fit the requirements of #2?"], "evidence": [[[["Crutch-1"], "no_evidence"], [["Orthotics-1"], "no_evidence"], [["Sandal-1"], "operation"]], [[["Crutch-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Crutch-1"]], ["no_evidence"], ["no_evidence"]]], "golden_sentence": [["A crutch is a mobility aid that transfers weight from the legs to the upper body."], [""], [""]]}, {"qid": "2c8492afccd96813b673", "term": "Apollo 15", "description": "Fourth crewed mission to land on the Moon", "question": "Would a triples tandem bike support Apollo 15 crew?", "answer": true, "facts": ["A tandem bike has seats for more than one rider.", "A triples tandem bike has three seats and can support three riders.", "The crew of Apollo 15 consisted of three people."], "decomposition": ["How many people were on the Apollo 15 mission?", "How many people can ride a triple tandem bike?", "Is #2 at least #1?"], "evidence": [[[["Alfred Worden-1", "Apollo 15-2"]], [["Tandem bicycle-11"]], ["operation"]], [[["Apollo 15-8"]], [["Tandem bicycle-2"], "no_evidence"], ["operation"]], [[["Apollo 15-6"]], [["Tandem bicycle-11"]], ["operation"]]], "golden_sentence": [["One of only 24 people to have flown to the Moon, he orbited it 74 times in the command module (CM) Endeavour.", ""], ["Tandems can have more than 2 riders \u2014 tandem refers to the arrangement of the riders one behind the other rather than the number of riders."]]}, {"qid": "c7513d3733dfc62f0895", "term": "Spider wasp", "description": "family of insects", "question": "Would a spider wasp be more effective than a bullet ant to stop a criminal?", "answer": false, "facts": ["Tasers are used by police to jolt criminals and temporarily paralyze them.", "Spider wasps sting their prey and cause intense pain.", "The Schmidt sting pain index rates the sting of spider wasps as a 4.", "The sting of a bullet ant earns the highest rank on the Schmidt sting pain index with a rating of 4+."], "decomposition": ["What is the Schmidt sting pain index of the spider wasp's sting?", "What is the Schmidt sting pain index of the bullet ant's sting?", "Is #1 greater than #2?"], "evidence": [[[["Spider wasp-18"]], [["Spider wasp-18"]], ["operation"]], [[["Spider wasp-18"]], [["Ant-31"]], ["operation"]], [[["Spider wasp-18"]], [["Schmidt sting pain index-14"]], ["operation"]]], "golden_sentence": [["In 1984, Justin O. Schmidt developed a hymenopteran sting pain scale, now known as the Schmidt sting pain index."], ["In 1984, Justin O. Schmidt developed a hymenopteran sting pain scale, now known as the Schmidt sting pain index."]]}, {"qid": "7999e57f18d825fb3601", "term": "Library of Alexandria", "description": "one of the largest libraries in the ancient world, located in Alexandria, Egypt", "question": "Would Library of Alexandria need less shelf space than Library of Congress?", "answer": true, "facts": ["The Library of Alexandria was an ancient library that was destroyed.", "The Library of Alexandria is estimated to have had around 100,000 books.", "The Library of Congress has over 170 million items."], "decomposition": ["What is the number of books (and other materials) housed in the Library of Congress?", "What was the number of books (and other materials) housed in the Library of Alexandria?", "Is #2 less than #1?"], "evidence": [[[["Library of Congress-33"]], [["Library of Alexandria-45"]], [["Library of Alexandria-45", "Library of Congress-33"]]], [[["Library of Congress-33"]], [["Library of Alexandria-1"]], ["operation"]], [[["Library of Congress-33"]], [["Library of Alexandria-45"]], ["operation"]]], "golden_sentence": [["The collections of the Library of Congress include more than 32 million catalogued books and other print materials in 470 languages; more than 61 million manuscripts; the largest rare book collection in North America, including the rough draft of the Declaration of Independence, a Gutenberg Bible (originating from the Saint Blaise Abbey, Black Forest) (one of only three perfect vellum copies known to exist); over 1 million U.S. government publications; 1 million issues of world newspapers spanning the past three centuries; 33,000 bound newspaper volumes; 500,000 microfilm reels; over 6,000 titles in all, totaling more than 120,000 issues comic book titles; films; 5.3 million maps; 6 million works of sheet music; 3 million sound recordings; more than 14.7 million prints and photographic images including fine and popular art pieces and architectural drawings; the Betts Stradivarius; and the Cassavetti Stradivarius."], ["At its height, the library was said to possess nearly half a million scrolls, and, although historians debate the precise number, the highest estimates claim 400,000 scrolls while the most conservative estimates are as low as 40,000, which is still an enormous collection that required vast storage space."], ["", "The collections of the Library of Congress include more than 32 million catalogued books and other print materials in 470 languages; more than 61 million manuscripts; the largest rare book collection in North America, including the rough draft of the Declaration of Independence, a Gutenberg Bible (originating from the Saint Blaise Abbey, Black Forest) (one of only three perfect vellum copies known to exist); over 1 million U.S. government publications; 1 million issues of world newspapers spanning the past three centuries; 33,000 bound newspaper volumes; 500,000 microfilm reels; over 6,000 titles in all, totaling more than 120,000 issues comic book titles; films; 5.3 million maps; 6 million works of sheet music; 3 million sound recordings; more than 14.7 million prints and photographic images including fine and popular art pieces and architectural drawings; the Betts Stradivarius; and the Cassavetti Stradivarius."]]}, {"qid": "104c59cba2fb5d303ebf", "term": "Cucumber", "description": "species of plant", "question": "Are all cucumbers the same texture?", "answer": false, "facts": ["Kirby cucumbers are known for being covered in bumps.", "English cucumbers are usually covered in ridges."], "decomposition": ["What texture do kirby cucumbers have?", "What texture do English cucumbers have?", "Is #1 the same as #2?"], "evidence": [[[["Cucumber-11"]], [["Cucumber-14", "European cucumber-1", "European cucumber-2"]], ["operation"]], [["no_evidence"], [["European cucumber-3"], "no_evidence"], ["operation"]], [[["Cucumber-9"], "no_evidence"], [["Cucumber-14"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["Like the English cucumber, Lebanese cucumbers are nearly seedless.", "The European cucumber (also known as English cucumber) is a variety of \"seedless\" cucumber that is longer and slimmer than other varieties of cucumber.", ""]]}, {"qid": "9867b03417eed7fa2f31", "term": "Drain fly", "description": "family of insects", "question": "Do calico cat patterns cover every drain fly color variety?", "answer": false, "facts": ["Drain flies come in two color varieties, black and grey.", "Calico cats have a traditional color pattern that includes white, orange, and black."], "decomposition": ["What colors do drain flies come in?", "What colors are seen on calico cats?", "Is every element of #1 also in #2?"], "evidence": [[["no_evidence"], [["Calico cat-1"]], ["operation"]], [[["Drain fly-1"], "no_evidence"], [["Calico cat-1"]], ["no_evidence", "operation"]], [[["Drain fly-1"], "no_evidence"], [["Calico cat-1"]], ["operation"]]], "golden_sentence": [["A calico cat is a domestic cat with a coat that is typically 25% to 75% white with large orange and black patches (or sometimes cream and grey patches)."]]}, {"qid": "0a50f64b1ce0befb6f35", "term": "Auburn, New York", "description": "City in New York, United States", "question": "Can you fit every resident of Auburn, New York, in Tropicana Field?", "answer": true, "facts": ["The capacity of Tropicana Field is 36,973", "The population of Auburn, NY is 27,687"], "decomposition": ["What is the capacity of Tropicana Field?", "What is the population of Auburn, NY?", "Is #1 greater than #2?"], "evidence": [[[["Tropicana Field-31"]], [["Auburn, New York-1"]], [["Tropicana Field-31"], "operation"]], [[["Tropicana Field-31"]], [["Auburn, New York-1"]], ["operation"]], [[["Tropicana Field-31"]], [["Auburn, New York-1"]], ["operation"]]], "golden_sentence": [["This increased the capacity of the stadium to nearly 41,000, depending on standing-room-only tickets sold."], ["As of the 2010 census, the city had a population of 27,687."], [""]]}, {"qid": "34acc5fbde1264c26d1d", "term": "Beaver", "description": "Genus of mammals", "question": "Would a Beaver's teeth rival that of a Smilodon?", "answer": false, "facts": ["A beaver has teeth measuring 25 mm on average.", "The Smilodon was a prehistoric saber-toothed tiger.", "The Smilodon's teeth were around 11 inches long."], "decomposition": ["How long is a beaver's tooth?", "How long was a Smilodon's tooth?", "Is #1 larger than #2?"], "evidence": [[["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Beaver-1", "Beaver-7"], "no_evidence"], [["Smilodon-10"]], ["no_evidence", "operation"]], [[["Beaver-6"], "no_evidence"], [["Smilodon-10"]], ["operation"]]], "golden_sentence": []}, {"qid": "80f2271b68114c0e24ff", "term": "Albanian Declaration of Independence", "description": "declaration of independence", "question": "Can an Arvanite Greek understand some of the Albanian Declaration of Independence?", "answer": true, "facts": ["The Albanian Declaration of Independence is written in Albanian, Gheg, Tosk, and Ottoman Turkish.", "The Arvanite Greek's are a major Tosk speaking group of southern Albania."], "decomposition": ["What languages is the Albanian Declaration of Independence written in?", "What language do Arvanite Greeks speak?", "Is #2 in #1?"], "evidence": [[["no_evidence"], [["Arvanites-1"]], ["operation"]], [[["Albanian Declaration of Independence-3"]], [["Arvanites-1"]], ["operation"]], [[["Albanian Declaration of Independence-1", "Albanian Declaration of Independence-7", "Albanian language-1"], "no_evidence"], [["Arvanites-1"]], ["operation"]]], "golden_sentence": [["Arvanites (/\u02c8\u0251\u02d0rv\u0259na\u026ats/; Greek: \u0391\u03c1\u03b2\u03b1\u03bd\u03af\u03c4\u03b5\u03c2, Arvanites; Arvanitika: Arb\u00ebresh\u00eb/\u0391\u03c1b\u03b5\u0330\u03c1\u03b5\u03c3\u0308\u03b5\u0330 or Arb\u00ebror\u00eb) are a bilingual population group in Greece who traditionally speak Arvanitika, an Albanian language variety, along with Greek."]]}, {"qid": "95e7e392f5201fb90482", "term": "Panth\u00e9on", "description": "mausoleum in Paris", "question": "Does Pantheon in Paris have a unique name?", "answer": false, "facts": ["The Pantheon in Paris is a historical monument.", "The Pantheon was a former Roman temple in antiquity.", "The Pantheon is a mythical or imaginary creature used in heraldry, particularly in Britain often depicted as white deer with the tail of a fox with purple stars along their back."], "decomposition": ["What is referred to as the Pantheon in Paris?", "What other concepts are named Pantheon?", "Is #1 differently-named from #2?"], "evidence": [[[["Panth\u00e9on-1"]], [["Pantheon, Rome-1"]], ["operation"]], [[["Panth\u00e9on-1"]], [["Pantheon (mythical creature)-1", "Pantheon (religion)-1", "Pantheon (software)-1"]], ["operation"]], [[["Panth\u00e9on-1"]], [["Pantheon (religion)-1", "Pantheon, Rome-1"]], ["operation"]]], "golden_sentence": [["The Panth\u00e9on (French:\u00a0[p\u0251\u0303.te.\u0254\u0303], from the Classical Greek word \u03c0\u03ac\u03bd\u03b8\u03b5\u03b9\u03bf\u03bd, p\u00e1ntheion, '(temple) to all the gods') is a monument in the 5th arrondissement of Paris, France."], [""]]}, {"qid": "a10b91f8c638686e1d45", "term": "1960", "description": "Year", "question": "Were there footprints on the moon in 1960?", "answer": false, "facts": ["The first man to walk on the moon was aboard Apollo 11.", "Apollo 11 took off in 1969."], "decomposition": ["When did humans first land on the moon?", "Is #1 before or in 1960?"], "evidence": [[[["Moon landing-2"]], ["operation"]], [[["Moon landing-2"]], ["operation"]], [[["Apollo 11-1"]], ["operation"]]], "golden_sentence": [["The United States' Apollo 11 was the first crewed mission to land on the Moon, on 20 July 1969."]]}, {"qid": "9a300f4814fa1f58373f", "term": "Darth Vader", "description": "fictional character in the Star Wars franchise", "question": "Was Darth Vader monogamous?", "answer": true, "facts": ["Monogamy refers to when a person is married or committed to only one person.", "Darth Vader was only married to Padme Amidala and had two children with her."], "decomposition": ["If someone is monogamous, how many people are they committed to?", "How many people was Darth Vader committed to?", "Does #2 equal #1?"], "evidence": [[[["Monogamy-1"]], [["Padm\u00e9 Amidala-1"]], ["operation"]], [[["Monogamy-1"]], [["Padm\u00e9 Amidala-1"]], ["operation"]], [[["Monogamy-1"]], [["Darth Vader-2"]], [["Darth Vader-2"]]]], "golden_sentence": [["Monogamy (/m\u0259\u02c8n\u0252\u0261\u0259mi/ m\u0259-NOG-\u0259-mee) is a form of dyadic relationship in which an individual has only one partner during their lifetime\u2014alternately, only one partner at any one time (serial monogamy)\u2014as compared to non-monogamy (e.g., polygamy or polyamory)."], [""]]}, {"qid": "e584397a3163734e7468", "term": "Paella", "description": "Valencian rice dish", "question": "Would a vegan eat a traditional Paella dish?", "answer": false, "facts": ["Vegans do not consume animals or products derived from animals.", "The traditional Paella recipe includes rabbit and chicken as two of the ingredients."], "decomposition": ["What kind of products are forbidden in a vegan diet?", "What are the main ingredients of Paella?", "Are all of #2 totally excluded from #1?"], "evidence": [[[["Veganism-1"]], [["Paella-5"]], ["operation"]], [[["Veganism-1"]], [["Paella-5"]], ["operation"]], [[["Vegan Outreach-3"]], [["Paella-19"]], ["operation"]]], "golden_sentence": [["Veganism is the practice of abstaining from the use of animal products, particularly in diet, and an associated philosophy that rejects the commodity status of animals."], ["Paella valenciana is the traditional paella of the Valencia region, believed to be the original recipe, and consists of round grain rice, bajoqueta and tavella (varieties of green beans), rabbit, chicken, sometimes duck, garrof\u00f3 (a variety of lima or butter bean), and optionally snails."]]}, {"qid": "8826aa448f640dd7de13", "term": "B", "description": "letter in the Latin alphabet", "question": "Is B's place in alphabet same as Prince Harry's birth order?", "answer": true, "facts": ["B is the second letter of the alphabet.", "Prince Harry was the second son of Charles, Prince of Wales and Diana, Princess of Wales."], "decomposition": ["What position in the alphabet does \"B\" hold?", "What is the nominal number associated with #1?", "Does Prince Harry have exactly #2 minus 1 older siblings?"], "evidence": [[[["B-1"]], [["Nominal number-1", "Nominal number-2"], "operation"], [["Prince Harry, Duke of Sussex-1", "Prince Harry, Duke of Sussex-2"], "operation"]], [[["B-1"]], [["Ordinal numeral-9"]], [["Prince Harry, Duke of Sussex-4"]]], [[["B-1"]], ["operation"], [["Prince Harry, Duke of Sussex-4"], "operation"]]], "golden_sentence": [["B or b is the second letter of the Latin-script alphabet."], ["", ""], ["", ""]]}, {"qid": "f6bb7eb3c748879b4879", "term": "Mark Twain", "description": "American author and humorist", "question": "Was Mark Twain a struggling inventor?", "answer": false, "facts": ["Twain patented three inventions.", "Twain created and sold over 25,000 self-pasting scrapbook creations.", "Twain invented an improvement in adjustable and detachable straps for garments to replace suspenders."], "decomposition": ["What did Mark Twain invent?", "How much of #1 did Mark Twain sell?", "Were the sales of #2 not enough to turn a profit?"], "evidence": [[[["Mark Twain-23"]], [["Mark Twain-23"]], [["Mark Twain-23"]]], [[["Mark Twain-23"]], [["Mark Twain-23"], "no_evidence"], ["no_evidence", "operation"]], [[["Mark Twain-23"]], [["Mark Twain-23"]], ["operation"]]], "golden_sentence": [["Twain patented three inventions, including an \"Improvement in Adjustable and Detachable Straps for Garments\" (to replace suspenders) and a history trivia game."], ["Over 25,000 were sold."], [""]]}, {"qid": "e894ba2ad678ee5ada1e", "term": "Al Capone", "description": "American gangster and businessman", "question": "Did Al Capone carry a smartphone?", "answer": false, "facts": ["Al Capone died in 1947.", "Smartphones were invented in 1992."], "decomposition": ["In what year did Al Capone die?", "What year was the first smartphone invented?", "Is #1 after #2?"], "evidence": [[[["Al Capone-1", "Al Capone-34"]], [["Smartphone-16"]], ["operation"]], [[["Al Capone-1"]], [["Smartphone-5"]], ["operation"]], [[["Al Capone-34"]], [["Smartphone-6"]], ["operation"]]], "golden_sentence": [["Alphonse Gabriel Capone (/\u02c8k\u0259\u02c8po\u028an/, Italian:\u00a0[ka\u02c8po\u02d0ne]; January 17, 1899\u00a0\u2013 January 25, 1947), sometimes known by the nickname \"Scarface\", was an American gangster and businessman who attained notoriety during the Prohibition era as the co-founder and boss of the Chicago Outfit.", "On January 21, 1947, Capone had a stroke."], [""]]}, {"qid": "1548d5004290070d68fc", "term": "New Brunswick", "description": "province in Canada", "question": "Can Burundi's communicate with citizens of New Brunswick?", "answer": true, "facts": ["French and English are the official languages of New Brunswick.", "French is one of the official languages of Burundi."], "decomposition": ["What are the official languages of New Brunswick, Canada?", "What are the official languages of Burundi?", "Are some elements of #2 also in #1?"], "evidence": [[[["New Brunswick-36"]], [["Burundi-87"]], ["operation"]], [[["Official language-14"]], [["Burundi-6"]], ["operation"]], [[["Languages of Canada-58"], "operation"], [["Languages of Burundi-1"], "operation"], ["no_evidence"]]], "golden_sentence": [["For education, English-language and French-language systems serve the two linguistic communities at all levels."], ["The official languages of Burundi are Kirundi, French and, since 2014, English."]]}, {"qid": "4a753beda87d4cf84f87", "term": "Michael Bloomberg", "description": "American billionaire businessman and politician, former mayor of New York City", "question": "Can Michael Bloomberg fund the debt of Micronesia for a decade?", "answer": true, "facts": ["Michael Bloomberg is worth an estimated 60 billion dollars as of 2020.", "Micronesia has annual expenses of nearly 200 million dollars."], "decomposition": ["What is Micheal Bloomberg's worth?", "What is the annual expense for Micronesia?", "Is #1 greater than #2?"], "evidence": [[[["Michael Bloomberg-2"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Michael Bloomberg-2"]], [["Micronesia-31"], "no_evidence"], ["no_evidence", "operation"]], [[["Michael Bloomberg-2"]], [["Economy of the Federated States of Micronesia-7"]], ["operation"]]], "golden_sentence": [["Since signing The Giving Pledge, Bloomberg has given away $8.2 billion."]]}, {"qid": "aa154f3a4fcc5383b4cd", "term": "Reiki", "description": "Pseudoscientific healing technique", "question": "Would a physician be unlikely to recommend Reiki?", "answer": true, "facts": ["Physicians typically only recommend treatments that have a significant amount of clinical evidence behind them.", "Reiki has no clinical evidence of effectiveness, and there is no scientific basis for its utility."], "decomposition": ["What is the basis for physician's recommendations?", "Does Reiki have #1?"], "evidence": [[[["Evidence-based medicine-1"]], [["Reiki-1", "Reiki-2"], "operation"]], [[["Physician-1"]], [["Reiki-2"], "no_evidence", "operation"]], [[["Physician-1"]], ["operation"]]], "golden_sentence": [["Evidence-based medicine (EBM) is \"the conscientious, explicit and judicious use of current best evidence in making decisions about the care of individual patients.\""], ["", ""]]}, {"qid": "1282f77eea9f27e64cd7", "term": "Bengal fox", "description": "species of mammal", "question": "Could Ryan Crouser throw a bengal fox with ease?", "answer": true, "facts": ["Ryan Crouser is a professional shot putter who won the gold medal at the 2016 Olympics.", "The men's shot weighs 16.01 pounds.", "The typical weight of a Bengal fox is between 5 to 9 pounds."], "decomposition": ["What sport is Ryan Crouser a professional in?", "How much does the equipment for #1 weigh?", "How much does a Bengal fox weigh?", "Is #2 greater than #3?"], "evidence": [[[["Ryan Crouser-1"]], [["Shot put-11"]], [["Bengal fox-2"]], ["operation"]], [[["Ryan Crouser-1"]], [["Shot put-11"]], [["Bengal fox-2"]], ["operation"]], [[["Ryan Crouser-1"]], [["Shot put-11"], "no_evidence"], [["Bengal fox-2"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Ryan Crouser (born December 18, 1992) is an American shot putter and discus thrower."], [""], ["Typical weight is 5 to 9 pounds (2. to 4.1\u00a0kg)."]]}, {"qid": "98fedf8bc3ab9888e3b1", "term": "Paralympic Games", "description": "Major international sport event for people with disabilities", "question": "Would Jimmy Vee be eligible to compete in the Paralympic Games?", "answer": true, "facts": ["Jimmy Vee is a dwarf.", "Dwarfism is defined as someone who is medically short-statured.", "Short stature due to a bone deficiency is one of the categories for paralympic athletes."], "decomposition": ["What disability does Jimmy Vee suffer from?", "What is the medical definition of #1?", "Is #2 one of the categories for the paralympic athletes?"], "evidence": [[[["Jimmy Vee-5"]], [["Dwarfism-1"]], [["Paralympic Games-1"]]], [[["Jimmy Vee-5"]], [["Dwarfism-2"]], [["Paralympic Games-42"], "operation"]], [[["Jimmy Vee-5"]], [["Dwarfism-1"]], [["Paralympic Games-4"]]]], "golden_sentence": [[""], [""], [""]]}, {"qid": "be80c9797073aa63298a", "term": "Society", "description": "Social group involved in persistent social interaction", "question": "Can a jet plane be made without society?", "answer": false, "facts": ["A jet plane requires many materials to build.", "A jet plane requires much prior knowledge to build.", "The specialized knowledge and materials is not obtainable without other people."], "decomposition": ["What materials do jet planes require to be built?", "Is #1 obtainable without people?"], "evidence": [[[["Jet aircraft-8"]], [["Jet engine-5"], "operation"]], [[["Aircraft-43"]], [["Aircraft-41"], "no_evidence"]], [[["Components of jet engines-3", "Jet engine-2"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "71eb1b20f5cc3bfdddd9", "term": "Tsar", "description": "title given to a male monarch in Russia, Bulgaria and Serbia", "question": "Would a duke hypothetically be subservient to a Tsar?", "answer": true, "facts": ["The Tsar was the highest ruler in several eastern countries.", "A duke was a title given to important european nobles.", "Dukes ranked below princes, kings, and queens.", "Tsars were the equivalents of English Kings."], "decomposition": ["What is the equivalent of a Tsar in English hierarchy/royalty?", "Do dukes rank below #1?"], "evidence": [[[["Tsar-1"]], [["Duke-1"]]], [[["Tsar-1"]], [["Duke-1"], "operation"]], [[["Tsar-6"]], [["Duke-1"], "operation"]]], "golden_sentence": [["The term is derived from the Latin word caesar, which was intended to mean \"emperor\" in the European medieval sense of the term\u2014a ruler with the same rank as a Roman emperor, holding it by the approval of another emperor or a supreme ecclesiastical official (the Pope or the Ecumenical Patriarch)\u2014but was usually considered by western Europeans to be equivalent to king, or to be somewhat in between a royal and imperial rank."], ["A duke (male) can either be a monarch ranked below the emperor, king, and grand duke ruling over a duchy or a member of royalty or nobility, historically of highest rank, below princes of nobility and grand dukes."]]}, {"qid": "15dab629293b9032c4b2", "term": "Louvre", "description": "Art museum and Historic site in Paris, France", "question": "Is the Louvre in billionaire George Soros's price range?", "answer": false, "facts": ["The Louvre including all of its paintings has a value of around 45 billion.", "George Soros has a net worth around 8 billion as of 2020."], "decomposition": ["What is the estimated value of the Louvre?", "What is George Soros' estimated 2020 net worth?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Louvre-1"], "no_evidence"], [["George Soros-1"]], ["no_evidence", "operation"]], [[["Louvre-16"], "no_evidence"], [["George Soros-1"]], ["operation"]], [["no_evidence"], [["George Soros-113"], "no_evidence"], ["no_evidence"]]], "golden_sentence": [[""], [""]]}, {"qid": "70b472732101eceef7ee", "term": "Chinese calendar", "description": "Lunisolar calendar from China", "question": "Are quadrupeds represented on Chinese calendar?", "answer": true, "facts": ["Quadrupeds are animals that walk on four legs.", "The Chinese calendar has a number of symbols including monkeys, goats, and tigers.", "Tigers have four paws and balance themselves by walking on their toes."], "decomposition": ["What are the symbols of the Chinese calendar?", "What is the defining characteristic of quadrapeds?", "Do any of the animals in #1 have #2?"], "evidence": [[[["Chinese calendar-1", "Earthly Branches-6"]], [["Quadrupedalism-1"]], [["Ox-10"], "operation"]], [[["Chinese zodiac-20"]], [["Quadrupedalism-1"]], ["operation"]], [[["Chinese zodiac-5"]], [["Quadrupedalism-1"]], ["operation"]]], "golden_sentence": [["The traditional Chinese calendar (officially known as the Agricultural Calendar [\u8fb2\u66c6; \u519c\u5386; N\u00f3ngl\u00ec; 'farming calendar'], Former Calendar [\u820a\u66c6; \u65e7\u5386; Ji\u00f9l\u00ec], Traditional Calendar [\u8001\u66c6; \u8001\u5386; L\u01ceol\u00ec] or Lunar Calendar [\u9670\u66c6; \u9634\u5386; Y\u012bnl\u00ec; 'yin calendar']), is a lunisolar calendar which reckons years, months and days according to astronomical phenomena.", ""], ["An animal or machine that usually moves in a quadrupedal manner is known as a quadruped, meaning \"four feet\" (from the Latin quattuor for \"four\" and pes for \"foot\")."], ["Unlike horses, oxen are not easily able to balance on three legs while a farrier shoes the fourth."]]}, {"qid": "e1e05b4482e4fcb98158", "term": "Scottish people", "description": "ethnic inhabitants of Scotland", "question": "Are Scottish people descended from Mary, Queen of Scots part French?", "answer": true, "facts": ["Mary, Queen of Scots was Queen of Scotland in the 1500s.", "Mary, Queen of Scots was the daughter of Mary of Guise.", "Mary of Guise was born to a French nobleman, and her mother was French as well."], "decomposition": ["Who was the mother of Mary, Queen of Scots?", "Who were the parents of #1?", "Were #2 French?"], "evidence": [[[["Mary of Guise-1"]], [["Antoinette de Bourbon-1", "Claude, Duke of Guise-1"]], ["operation"]], [[["Mary, Queen of Scots-5"]], [["Mary of Guise-2"]], [["Claude, Duke of Guise-1"], "operation"]], [[["Mary, Queen of Scots-5"]], [["Mary of Guise-2"]], [["Antoinette de Bourbon-1", "Claude, Duke of Guise-1"]]]], "golden_sentence": [[""], ["", ""]]}, {"qid": "7a7265747a0cb7e50a90", "term": "Eurovision Song Contest", "description": "Annual song competition held among the member countries of the European Broadcasting Union", "question": "Can actress Dafne Keen win the Eurovision Song Contest finals in 2020?", "answer": false, "facts": ["Contestants must be at least 16 years of age to compete in the finals of Eurovision Song Contest.", "Dafne Keen is 15 years old in 2020."], "decomposition": ["What is the minimum age for constests on  \"Eurovision Song Contest\"?", "How old is Dafne Keen?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Eurovision Song Contest-68"], "no_evidence"], [["Dafne Keen-1"], "no_evidence"], ["operation"]], [[["Rules of the Eurovision Song Contest-11"]], [["Dafne Keen-1"]], ["operation"]], [[["Eurovision Song Contest-68"]], [["Dafne Keen-1"]], ["operation"]]], "golden_sentence": [["In 2004, this was amended to the current rule, which requires all performers to be at least 16 years of age by the time of the contest."], ["Dafne Mar\u00eda Keen Fern\u00e1ndez (born 4 January 2005) is a British-Spanish actress."]]}, {"qid": "ce79e6cf08fb1e841143", "term": "Amy Winehouse", "description": "English singer and songwriter", "question": "Was Amy Winehouse familiar with Brexit?", "answer": false, "facts": ["Amy Winehouse died in 2011.", "Brexit began in 2017."], "decomposition": ["When did Amy Winehouse die?", "When did Brexit begin?", "Did #2 occur before #1?"], "evidence": [[[["Amy Winehouse-1"]], [["Brexit-1"]], ["operation"]], [[["Amy Winehouse-1"]], [["Brexit-1"]], ["operation"]], [[["Amy Winehouse-1"], "operation"], [["Brexit-3"], "operation"], ["no_evidence"]]], "golden_sentence": [["Amy Jade Winehouse (14 September 1983 \u2013 23 July 2011) was an English singer and songwriter."], ["Following a UK-wide referendum in June 2016, in which 52% voted to leave and 48% voted to remain in the EU, the British government formally announced the country's withdrawal in March 2017, beginning the Brexit process."]]}, {"qid": "0fee97dc15bf5a8740f1", "term": "Eleventh grade", "description": "educational year", "question": "Would a student in eleventh grade be unable to run for president of the United States?", "answer": true, "facts": ["Students in the eleventh grade are typically 16\u201317 years of age.", "To serve as president, one must be at least 35 years old."], "decomposition": ["What is the minimum age one must be to run for president?", "Would a typical eleventh grader be way younger than #1?"], "evidence": [[[["President of the United States-5"]], [["Eleventh grade-1"], "operation"]], [[["Age of candidacy-60"]], [["Education in Alberta-13"], "operation"]], [[["President of the United States-5"]], ["operation"]]], "golden_sentence": [["Article II, Section 1, Clause 5 sets three qualifications for holding the presidency: natural-born U.S. citizenship; at least thirty-five years of age; and residency in the United States for at least fourteen years."], ["Students are typically 16\u201317 years of age, depending on the country and the students' birthdays."]]}, {"qid": "fcb0080fc4f75fbbc044", "term": "Cauliflower", "description": "cauliflower plants (for the vegetable see Q23900272)", "question": "Would a cauliflower farmer prosper at a latitude of 75\u00b0 N?", "answer": false, "facts": ["Cauliflower grows best between temperatures of 70 to 85 degrees", "The latitude of 75\u00b0 N is in the Arctic"], "decomposition": ["What country is at the latitude of 75\u00b0 N?", "What is the average temperature of #1?", "What is the best temperature to grow cauliflower?", "Is there any overlap between #2 and #3?"], "evidence": [[[["75th parallel north-1"]], [["Antarctica-42"]], [["Cauliflower-6"]], ["operation"]], [[["75th parallel north-1"], "no_evidence"], [["New Siberian Islands-16"]], [["Cauliflower-6"]], ["operation"]], [[["75th parallel north-1", "75th parallel north-2"]], [["Arctic-5"]], [["Cauliflower-6"]], ["operation"]]], "golden_sentence": [[""], [""], ["Because weather is a limiting factor for producing cauliflower, the plant grows best in cool daytime temperatures 21\u201329\u00a0\u00b0C (70\u201385\u00a0\u00b0F), with plentiful sun, and moist soil conditions high in organic matter and sandy soils."]]}, {"qid": "a7e9fabdbbe56af558d1", "term": "Jack Dempsey", "description": "American boxer", "question": "Did Jack Dempsey fight the current WBC heavyweight champion?", "answer": false, "facts": ["Jack Dempsey died in 1983", "The current WBC heavyweight champion is Tyson Fury", "Tyson Fury was born in 1988"], "decomposition": ["When did Jack Dempsey die?", "When was the current WBC heavyweight champion born?", "Is #2 before #1?"], "evidence": [[[["Jack Dempsey-1"]], [["Tyson Fury-1"]], ["operation"]], [[["Jack Dempsey-1"]], [["Tyson Fury-1"]], ["operation"]], [[["Jack Dempsey-53"]], [["Deontay Wilder-1"]], ["operation"]]], "golden_sentence": [["William Harrison \"Jack\" Dempsey (June 24, 1895 \u2013 May 31, 1983), nicknamed Kid Blackie, and The Manassa Mauler, was an American professional boxer who competed from 1914 to 1927, and reigned as the world heavyweight champion from 1919 to 1926."], ["He is a two-time heavyweight world champion, having held the WBC, The Ring magazine and lineal titles since defeating Deontay Wilder in February 2020; previously he held the unified WBA (Super), IBF, WBO, IBO, The Ring, and lineal titles after defeating Wladimir Klitschko in 2015."]]}, {"qid": "71b03e1ab691255c3280", "term": "Tank", "description": "Tracked heavy armored fighting vehicle", "question": "Could a cat ride Panzer VIII Maus tank missile from Barcelona to Madrid?", "answer": false, "facts": ["The Panzer VIII Maus tank missile had a range of around 2.2 miles.", "The distance from Barcelona to Madrid is 385 miles.", "Motion sickness is a common problem in cats."], "decomposition": ["What is the maximum range of a Panzer VIII Maus tank missile?", "What is the distance between Barcelona and Madrid?", "Is #1 greater than #2?"], "evidence": [[[["Panzer VIII Maus-2"]], [["AVE-17"]], ["operation"]], [[["Panzer VIII Maus-1"], "no_evidence"], [["Barcelona-1", "Madrid-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Panzer VIII Maus-2"]], [["Madrid\u2013Barcelona high-speed rail line-1"]], ["operation"]]], "golden_sentence": [[""], ["This line is currently one of the world's fastest long-distance trains in commercial operation, with non-stop trains covering the 621\u00a0km (386\u00a0mi) between the two cities in just 2 hours 30 minutes, and those calling at all stations in 3 hours 10 minutes."]]}, {"qid": "1f0ca88f3a9bbcc1d7a0", "term": "Elizabeth I of England", "description": "Queen regnant of England and Ireland from 17 November 1558 until 24 March 1603", "question": "Did Elizabeth I of England own any viscose fiber?", "answer": false, "facts": ["Viscose is a rayon fiber invented in 1892.", "Elizabeth I of England died in the 1600s."], "decomposition": ["When did Elizabeth I die?", "When was viscose fiber invented?", "Is #2 before #1?"], "evidence": [[[["Elizabeth I (disambiguation)-1"]], [["Viscose-10"]], ["operation"]], [[["Elizabeth I of England-68"]], [["Viscose-10"]], ["operation"]], [[["Elizabeth I of England-1"]], [["Rayon-16"]], ["operation"]]], "golden_sentence": [["Elizabeth I (1533\u20131603) was Queen of England and Ireland from 1558."], ["French scientist and industrialist Hilaire de Chardonnet (1838\u20131924), inventor of the first artificial textile fiber, artificial silk, created viscose."]]}, {"qid": "3085c8839290ba5fd4e3", "term": "Sternum", "description": "flat bone in the middle front part of the rib cage", "question": "Is sternum connected to any of the bones mentioned in James Weldon Johnson's Dem Bones?", "answer": true, "facts": ["Dem Bones is a spiritual song composed by James Weldon Johnson.", "The lyrics to Dem Bones mention the hip bone, back bone, and knee bone among others.", "The back bone is the spine, which is connected to the first 7 rib sets.", "The sternum, or breastbone, is a flat bone at the front center of the chest.", "The sternum and ribs are connected and make up the ribcage."], "decomposition": ["Which bones were mentioned in the spiritual song Dem Bones by James Weldon Johnson?", "Is the sternum connected to any of #1?"], "evidence": [[[["Dem Bones-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Dem Bones-1"], "no_evidence"], [["Sternum-5"], "operation"]], [[["Dem Bones-3"], "no_evidence"], [["Sternum-2"], "operation"]]], "golden_sentence": [["\"Dem Bones\" (also called \"Dry Bones\" and \"Dem Dry Bones\") is a spiritual song."]]}, {"qid": "fbbe7abdc4683d0b8a2e", "term": "Futurama", "description": "American animated sitcom for the Fox Broadcasting Company and Comedy Central", "question": "Has the creator of Futurama lived in multiple centuries?", "answer": true, "facts": ["The creator of Futurama is Matt Groening.", "Matt Groening was born in 1954.", "The 20th (twentieth) century was a century that began on January 1, 1901 and ended on December 31, 2000.", "The 21st (twenty-first) century began on January 1, 2001, and will end on December 31, 2100."], "decomposition": ["Who is the creator of Futurama?", "How old is #1?", "What is 2020 minus #2?", "When did the most recent new century begin?", "Is #4 between #3 and 2020?"], "evidence": [[[["Futurama-1"]], [["Matt Groening-1"]], ["operation"], [["2000-1"]], ["operation"]], [[["Futurama-1"]], [["Matt Groening-1"]], ["operation"], [["21st century-1"]], ["operation"]], [[["Futurama-1"]], [["Matt Groening-1"]], ["operation"], [["21st century-1"]], ["operation"]]], "golden_sentence": [["Futurama is an American adult animated science-fiction sitcom created by Matt Groening that aired on Fox from March 28, 1999 to August 10, 2003 and on Comedy Central from March 23, 2008 to September 4, 2013."], [""], ["2000 (MM) was a century leap year starting on Saturday of the Gregorian calendar, the 2000th year of the Common Era (CE) and Anno Domini (AD) designations, the 1000th and last year of the 2nd\u00a0millennium, the 100th and last year of the 20th\u00a0century, and the 1st year of the 2000s decade."]]}, {"qid": "150791e1f6b0e1f49f27", "term": "French Riviera", "description": "Riviera", "question": "Did Pink Floyd have a song about the French Riviera?", "answer": true, "facts": ["Pink Floyd included the song San Tropez on the album Meddle", "San Tropez, also known as Saint Tropez, is a town located on the French Riviera"], "decomposition": ["What songs by Pink Floyd are named after towns in France?", "What are the names of the major towns in the French Riviera?", "Are any towns in #1 also in #2?"], "evidence": [[[["San Tropez (song)-1"]], [["French Riviera-1"]], ["operation"]], [[["San Tropez (song)-1"]], [["Saint-Tropez-1"]], ["operation"]], [[["San Tropez (song)-1"]], [["French Riviera-4"]], [["Saint-Tropez-1"], "operation"]]], "golden_sentence": [[""], ["There is no official boundary, but it is usually considered to extend from Cassis, Toulon or Saint-Tropez on the west to Menton at the France\u2013Italy border in the east, where the Italian Riviera joins."]]}, {"qid": "8b9b9d3332f39350e3b3", "term": "Microsoft Excel", "description": "Spreadsheet editor, part of Microsoft Office", "question": "Is electricity necessary to balance an account in Microsoft Excel?", "answer": true, "facts": ["Microsoft Excel is a computer program", "Computers require a power source"], "decomposition": ["Which devices can run Microsoft Excel software?", "Do all of #1 require electricity to work?"], "evidence": [[[["Microsoft Excel-1"]], [["Smartphone-2"], "operation"]], [[["Microsoft Excel-25"]], [["Electricity-4"]]], [[["Microsoft Excel-1", "Spreadsheet-1"]], [["Computer-43", "Digital electronics-6"], "no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "5d794dee5ecc203356cf", "term": "Bucharest", "description": "Capital of Romania", "question": "Was historical Dracula from a town in Bucharest?", "answer": false, "facts": ["Vlad III Prince of Wallachia, also called Vlad the Impaler, is believed to be the historical inspiration for Dracula.", "Vlad III was born in Sighi\u0219oara, Romania, which is located in the historic region of Transylvania.", "Bucharest is located 276 km away from Transylvania."], "decomposition": ["What was Dracula's real name?", "Where was #1 born?", "What is the distance from #2 to Bucharest?"], "evidence": [[[["Count Dracula-1"]], [["Vlad the Impaler-8"], "no_evidence"], ["operation"]], [[["Vlad the Impaler-1"]], [["Vlad the Impaler-8"]], ["no_evidence"]], [[["Vlad the Impaler-1"]], [["Vlad the Impaler-8"]], [["Bucharest-1"], "no_evidence", "operation"]]], "golden_sentence": [["Count Dracula (/\u02c8dr\u00e6kj\u028al\u0259, -j\u0259l\u0259/) is the title character of Bram Stoker's 1897 gothic horror novel Dracula."], ["Historian Radu Florescu writes that Vlad was born in the Transylvanian Saxon town of Sighi\u0219oara (then in the Kingdom of Hungary), where his father lived in a three-storey stone house from 1431 to 1435."]]}, {"qid": "7f6efabddada40c38e26", "term": "Richard Wagner", "description": "German composer", "question": "Did Richard Wagner support the Nazis?", "answer": false, "facts": ["Richard Wagner died in 1883.", "The Nazi Party was established in 1919."], "decomposition": ["When did Richard Wagner die?", "When was the Nazi Party formed?", "Is #2 before #1?"], "evidence": [[[["Richard Wagner-1"]], [["Nazi Party-1"]], ["operation"]], [[["Richard Wagner-1"]], [["Nazi Party-1"]], ["operation"]], [[["Richard Wagner-1"]], [["Nazi Party-1"]], ["operation"]]], "golden_sentence": [["Wilhelm Richard Wagner (/\u02c8v\u0251\u02d0\u0261n\u0259r/ VAHG-n\u0259r, German: [\u02c8\u0281\u026a\u00e7a\u0281t \u02c8va\u02d0\u0261n\u0250] (listen); 22 May 1813\u00a0\u2013 13 February 1883) was a German composer, theatre director, polemicist, and conductor who is chiefly known for his operas (or, as some of his mature works were later known, \"music dramas\")."], ["The National Socialist German Workers' Party (German: Nationalsozialistische Deutsche Arbeiterpartei\u00a0(help\u00b7info), abbreviated NSDAP), commonly referred to in English as the Nazi Party (English: /\u02c8n\u0251\u02d0tsi, \u02c8n\u00e6tsi/), was a far-right political party in Germany that was active between 1920 and 1945, that created and supported the ideology of National Socialism."]]}, {"qid": "daba4e76704207e8dae1", "term": "Goofy", "description": "Disney cartoon character", "question": "If Goofy were a pet, would he need heartworm prevention?", "answer": true, "facts": ["Goofy is an anthropomorphic dog character. ", "Dogs require regular heartworm prevention. "], "decomposition": ["What kind of animal is Goofy?", "Does a #1 require regular heartworm prevention?"], "evidence": [[[["Goofy-1"]], [["Dog-18"]]], [[["Goofy-1"]], [["Dog health-50"]]], [[["Goofy-1"]], ["operation"]]], "golden_sentence": [["Goofy is a funny animal cartoon character created in 1932 at Walt Disney Productions."], [""]]}, {"qid": "9018bc094dc509b83dd6", "term": "Alan Greenspan", "description": "13th Chairman of the Federal Reserve in the United States", "question": "Has Alan Greenspan lived through at least a baker's dozen of president's?", "answer": true, "facts": ["A baker's dozen refers to the number 13.", "Alan Greenspan was born in 1926 and turned 94 years old in March of 2020.", "There have been 16 different president's from 1926-2020."], "decomposition": ["When was Alan Greenspan born?", "What number is represented by a baker's dozen?", "How many US presidents have served since #1?", "Is #3 greater than or equal to #2?"], "evidence": [[[["Alan Greenspan-1"]], [["Dozen-7"]], [["Calvin Coolidge-1", "Donald Trump-1"]], ["operation"]], [[["Alan Greenspan-1"]], [["Dozen-7"]], [["Calvin Coolidge-1", "Donald Trump-1"]], ["operation"]], [[["Alan Greenspan-1"]], [["Dozen-7"]], ["no_evidence"], ["operation"]]], "golden_sentence": [["Alan Greenspan KBE (/\u02c8\u00e6l\u0259n \u02c8\u0261ri\u02d0nsp\u00e6n/; born March 6, 1926) is an American economist who served as Chair of the Federal Reserve of the United States from 1987 to 2006."], ["page\u00a0needed] A baker's dozen, devil's dozen, long dozen, or long measure is 13, one more than a standard dozen."], ["Calvin Coolidge (born John Calvin Coolidge Jr.; /\u02c8ku\u02d0l\u026ad\u0292/; July 4, 1872 \u2013 January 5, 1933) was an American politician and lawyer who served as the 30th president of the United States from 1923 to 1929.", "Donald John Trump (born June 14, 1946) is the 45th and current president of the United States."]]}, {"qid": "f58156aac5e0267d2b5f", "term": "Bill Gates", "description": "American business magnate and philanthropist", "question": "Did Bill Gates achieve Latin honors?", "answer": false, "facts": ["Bill Gates left Harvard after two years.", "Latin honors are Latin phrases used in some colleges and universities to indicate the level of distinction with which an academic degree has been earned.", "You cannot earn an academic degree if you drop out of the program."], "decomposition": ["What are Latin honors used to describe in colleges and universitiies?", "What program must one complete to be awarded #1?", "Did Bill gates finish #2?"], "evidence": [[[["Latin honors-3"]], [["Latin honors-3"]], [["Bill Gates-10", "Latin honors-3"]]], [[["Latin honors-2"]], [["Bill Gates-10"]], ["operation"]], [[["Latin honors-1"]], [["Academic degree-1"]], [["Bill Gates-10"]]]], "golden_sentence": [[""], ["Latin honors are often conferred upon law school students graduating as a Juris Doctor or J.D., in which case they are generally based upon class rank or grade point average."], ["", ""]]}, {"qid": "05084937071eafcd229f", "term": "Tsar", "description": "title given to a male monarch in Russia, Bulgaria and Serbia", "question": "Was the son of Tsar Nicholas a daredevil?", "answer": false, "facts": ["Tsar Nicholas had only one son, Alexei. ", "Alexei had hemophilia and had to be carefully guarded.", "Hemophilia is the inability for blood to form clots, making any small cut dangerous."], "decomposition": ["Who was the son of Tsar Nicholas?", "What did #1 suffer from medically?", "With #2, is one able to be injured?"], "evidence": [[[["Alexei Nikolaevich, Tsarevich of Russia-1"]], [["Alexei Nikolaevich, Tsarevich of Russia-1"]], [["Haemophilia-1"]]], [[["Alexei Nikolaevich, Tsarevich of Russia-1"]], [["Alexei Nikolaevich, Tsarevich of Russia-8"]], [["Alexei Nikolaevich, Tsarevich of Russia-8"]]], [[["Nicholas II of Russia-63"]], [["Alexei Nikolaevich, Tsarevich of Russia-1"]], [["Contaminated blood scandal in the United Kingdom-5"], "operation"]]], "golden_sentence": [["He was the youngest child and only son of Emperor Nicholas II and Empress Alexandra Feodorovna."], ["He was born with haemophilia, which was treated by the faith healer Grigori Rasputin."], [""]]}, {"qid": "9ff4f39381a486c409a9", "term": "Judo", "description": "modern martial art, combat and Olympic sport", "question": "Does the judo rank system reach the triple digits?", "answer": false, "facts": ["A triple digit number would be equal to at least 100.", "The judo dan-rank system was capped at 10th dan after the death of judo's founder, Kan\u014d Jigor\u014d."], "decomposition": ["Was is the lowest three digit number?", "What is the highest rank a person can reach in Judo?", "Is #2 a higher number than #1?"], "evidence": [[[["100-1", "Numerical digit-1"]], [["Judo-55"]], ["operation"]], [[["Numerical digit-1"], "no_evidence"], [["Rank in Judo-6"]], ["operation"]], [[["100-1"]], [["Judo-55"]], ["operation"]]], "golden_sentence": [["100 or one hundred (Roman numeral: \u216d) is the natural number following 99 and preceding 101.", "The name \"digit\" comes from the fact that the ten digits (Latin digiti meaning fingers) of the hands correspond to the ten symbols of the common base\u00a010 numeral system, i.e."], ["On July 28, 2011, the promotion board of USA Judo awarded Keiko Fukuda the rank of 10th dan, who was the first woman to be promoted to judo's highest level, albeit not a Kodokan-recognized rank."]]}, {"qid": "e013bd5c89b6d704efb4", "term": "Greeks", "description": "people of southeastern Europe", "question": "Did Cleopatra have ethnicity closer to Egyptians than Greeks?", "answer": false, "facts": ["Cleopatra was the last ruler of the Kingdom of Egypt.", "Cleopatra was the descendant of Ptolemy I Soter, a Greek general from Macedonia.", "Cleopatra's father, Ptolemy XII Auletes, was most likely descended from an Alexandrian Greek mother.", "Cleopatra is believed to be mostly Greek with some Persian and Syrian ancestry as well."], "decomposition": ["Who was Cleopatra the daughter of?", "Was #1 descended from Egyptians?"], "evidence": [[[["Cleopatra-8"]], [["Ptolemy XII Auletes-3"]]], [[["Cleopatra-8"]], [["Ptolemy-6"]]], [[["Cleopatra-1"]], ["operation"]]], "golden_sentence": [["Cleopatra VII was born in early 69 BC to the ruling Ptolemaic pharaoh Ptolemy XII and an unknown mother, presumably Ptolemy XII's wife Cleopatra VI Tryphaena (also known as Cleopatra V Tryphaena), the mother of Cleopatra's older sister, Berenice IV Epiphaneia."], ["By his wife Cleopatra\u00a0V, Ptolemy XII had at least one child, Berenice\u00a0IV, and probably Cleopatra\u00a0VII; he had his three youngest children, Arsinoe\u00a0IV, Ptolemy\u00a0XIII, and Ptolemy\u00a0XIV, with an unknown mother."]]}, {"qid": "bee584e7046cb39d1276", "term": "Soup", "description": "primarily liquid food", "question": "Can soup be eaten with the hands?", "answer": false, "facts": ["Soup is mostly liquid.", "Hands cannot hold liquid."], "decomposition": ["What state of matter is soup?", "Can your hands hold #1?"], "evidence": [[[["Soup-1"]], ["operation"]], [[["Soup-1"]], ["operation"]], [[["Soup-1"]], ["operation"]]], "golden_sentence": [["Soup is a primarily liquid food, generally served warm or hot (but may be cool or cold), that is made by combining ingredients of meat or vegetables with stock, or water."]]}, {"qid": "3fa3a44c4a6f286294f6", "term": "Cooking", "description": "Preparing food for consumption with the use of heat", "question": "If your electric stove has a glass top, should you use cast iron skillets?", "answer": false, "facts": ["Cast iron skillets can scratch or crack flat top stoves.", "Glass top stoves are considered 'flat tops'."], "decomposition": ["What would cast iron skillets do to flat top serves?", "What kind of stove are glass top stoves?", "Would someone want their #2 to be #1?"], "evidence": [[[["Cooktop-8"], "no_evidence"], [["Cooktop-5"]], ["operation"]], [[["Cast-iron cookware-1"], "no_evidence"], [["Kitchen stove-1"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], [["Electric stove-13"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "1f8ad9c13905d64c957d", "term": "Woodrow Wilson", "description": "28th president of the United States", "question": "Did Woodrow Wilson consider Blacks to be equal members of society?", "answer": false, "facts": ["Woodrow Wilson supported the Ku Klux Klan.", "The Ku Klux Klan consider Blacks to be inferior. "], "decomposition": ["What group did Woodrow Wilson support?", "Did #1 consider Blacks to be equal members of society?"], "evidence": [[[["Woodrow Wilson-79"]], [["Ku Klux Klan-1", "Ku Klux Klan-2"]]], [[["Woodrow Wilson-79"]], [["Ku Klux Klan-104"]]], [[["Woodrow Wilson-79"], "no_evidence"], [["Woodrow Wilson-76"]]]], "golden_sentence": [["In terms of Reconstruction, Wilson held the common southern view that the South was demoralized by northern carpetbaggers and that overreach on the part of the Radical Republicans justified extreme measures to reassert democratic, white majority control of Southern state governments."], ["All three movements have called for the \"purification\" of American society and all are considered \"right-wing extremist\" organizations.", ""]]}, {"qid": "0ecf22842971dc77c906", "term": "Friday", "description": "day of the week", "question": "Would an astrologer focus on the densest terrestrial planet for a Friday horoscope?", "answer": true, "facts": ["Friday is associated with Venus in astrology", "Venus is the densest of the terrestrial planets "], "decomposition": ["What astrological body is associated with Friday?", "Which is the densest terrestrial planet?", "Is #2 the same as #1?"], "evidence": [[[["Planetary hours-4"]], [["Venus-19"]], [["Planetary hours-4", "Venus-19"]]], [[["Friday-3"]], [["Outline of Venus-2"]], ["operation"]], [[["Friday-12"]], [["Venus-2"]], ["operation"]]], "golden_sentence": [["a day with its first hour ruled by the Sun (\"Sunday\") is followed by a day with its first hour ruled by the Moon (\"Monday\"), followed by Mars (\"Tuesday\"), Mercury (\"Wednesday\"), Jupiter (\"Thursday\"), Venus (\"Friday\") and Saturn (\"Saturday\"), again followed by Sunday, yielding the familiar naming of the days of the week."], ["Venus has an extremely dense atmosphere composed of 96.5% carbon dioxide, 3.5% nitrogen, and traces of other gases including sulfur dioxide."], ["", ""]]}, {"qid": "4d30399a270e7fb30cda", "term": "Elizabeth II", "description": "Queen of the United Kingdom and the other Commonwealth realms", "question": "Was Elizabeth II the Queen during the Persian Gulf War?", "answer": true, "facts": ["Elizabeth II became Queen in 1952.", "The Persian Gulf War occurred 1990-1991."], "decomposition": ["When did Elizabeth II become the Queen?", "When was the Persian Gulf War?", "Was Elizabeth II alive in #2?", "Is #2 after #1?", "Are the answers to #3 and #4 both yes?"], "evidence": [[[["Elizabeth II-3"]], [["Persian Gulf-3"]], [["Elizabeth II-1"]], ["operation"], ["operation"]], [[["Head of the Commonwealth-8"]], [["Gulf War-1"]], [["Elizabeth II-41"]], ["operation"], ["operation"]], [[["Elizabeth II-3"]], [["Gulf War-1"]], [["Elizabeth II-42"]], ["operation"], ["operation"]]], "golden_sentence": [["Significant events have included her coronation in 1953 and the celebrations of her Silver, Golden, and Diamond Jubilees in 1977, 2002, and 2012, respectively."], ["It is the namesake of the 1991 Gulf War, the largely air- and land-based conflict that followed Iraq's invasion of Kuwait."], [""]]}, {"qid": "a3286dd27fd7c8023090", "term": "Ocelot", "description": "Small wild cat", "question": "Could an ocelot outrun a kindergartner? ", "answer": true, "facts": ["An ocelot can run up to 61 kilometers per hour.", "Kindergartners are human children usually aged five to six years old.", "Human children under 10 are not likely to exceed 20 kilometers per hour."], "decomposition": ["How quickly can an ocelot run?", "How old is a kindergartner?", "How quickly can someone run at the age of #2?", "Is #1 greater than #3?"], "evidence": [[[["Ocelot-25", "Tiger-54"], "no_evidence"], [["Kindergarten-1"]], [["Running-43"], "no_evidence"], ["operation"]], [["no_evidence"], [["Kindergarten-89"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Ocelot-25"], "no_evidence"], [["Kindergarten-1"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["They walk slowly at a speed of about 0.3\u00a0km/h (0.2\u00a0mph) searching for prey.", "Despite their large size, tigers can reach speeds of about 49\u201365\u00a0km/h (30\u201340\u00a0mph) but only in short bursts; consequently, tigers must be close to their prey before they break cover."], ["Today, the term is used in many countries to describe a variety of educational institutions and learning spaces for children ranging from 2 to 6 or 7 years of age, based on a variety of teaching methods."], [""]]}, {"qid": "10b83286334bfee4ad69", "term": "Music", "description": "form of art using sound and silence", "question": "Can music be used as a weapon?", "answer": true, "facts": ["Music is an art form whose medium is sound.", "Music can help elevate or subdue emotions.", "People connect to music through the sound.", "The military uses loud music to cause psychological disorientation and confusion ", "The military calls the use of loud disorienting music part of psychological operations. "], "decomposition": ["In what ways does the military used music in operations?", "Is any of #1 as a weapon?"], "evidence": [[[["Music in psychological operations-1"]], [["Music in psychological operations-2"], "operation"]], [[["Manuel Noriega-44"], "no_evidence"], ["operation"]], [[["Music in psychological operations-2"]], [["Weapon-1"], "operation"]]], "golden_sentence": [["Music can be used as a tool of psychological warfare."], [""]]}, {"qid": "c35239b24b6171611c0e", "term": "United Nations Framework Convention on Climate Change", "description": "international treaty", "question": "Can the United Nations Framework Convention on Climate Change be held at the Javits Center?", "answer": false, "facts": ["The United Nations Framework Convention on Climate Change is an international treaty, not a physical meeting", "The Javits Center is a convention center in New York City"], "decomposition": ["What kind of event is the United Nations Framework Convention on Climate Change?", "Does #1 have to meet at a convention centre?"], "evidence": [[[["United Nations Framework Convention on Climate Change-6"]], [["Javits Center-2"], "no_evidence", "operation"]], [[["United Nations Framework Convention on Climate Change-39"], "no_evidence"], ["no_evidence"]], [[["United Nations Framework Convention on Climate Change-6"]], [["United Nations Framework Convention on Climate Change-39"], "no_evidence"]]], "golden_sentence": [[""], [""]]}, {"qid": "fead77ebb25f9891d87a", "term": "San Antonio", "description": "City in Texas, United States", "question": "Was San Antonio the site of a major battle in the 19th century?", "answer": true, "facts": ["The Alamo is located in San Antonio.", "The Alamo was the site of a major battle during the Texan Revolution against Mexico in 1836."], "decomposition": ["Where did the most notable battle during the Texas Revolution take place?", "Is #1 located in San Antonio in present day US?", "Did the Texas revolution happen during the 19th century?", "Are #2 and #3 positive?"], "evidence": [[[["Battle of the Alamo-1"]], [["Battle of the Alamo-1"]], [["19th century-1"]], ["operation"]], [[["Battle of the Alamo-1"]], [["Battle of the Alamo-1"]], [["19th century-1", "Battle of the Alamo-1"]], ["operation"]], [[["Battle of the Alamo-1"]], ["operation"], ["operation"], ["operation"]]], "golden_sentence": [["The Battle of the Alamo (February 23 \u2013 March 6, 1836) was a pivotal event in the Texas Revolution."], ["Following a 13-day siege, Mexican troops under President General Antonio L\u00f3pez de Santa Anna reclaimed the Alamo Mission near San Antonio de B\u00e9xar (modern-day San Antonio, Texas, United States), killing the Texian and immigrant occupiers."], [""]]}, {"qid": "350c67137d68c5ea5fc8", "term": "Higher education", "description": "Academic tertiary education, such as from colleges and universities", "question": "Did Emma Stone pursue a higher education?", "answer": false, "facts": ["Higher education, also called post-secondary education, third-level or tertiary education, is an optional final stage of formal learning that occurs after completion of secondary education.", "Stone attended Xavier College Preparatory\u200d\u2014\u200can all-girl Catholic high school\u200d\u2014\u200cas a freshman, but dropped out after one semester to become an actress.", "Xavier College Preparatory is a Catholic, all-female private high school.", "High school is a secondary education school."], "decomposition": ["What is the highest institution Emma Stone attended?", "Is #1 considered a \"higher education\"?"], "evidence": [[[["Emma Stone-7"]], [["Higher education-1"]]], [[["Emma Stone-7"]], ["operation"]], [[["Emma Stone-7"]], ["operation"]]], "golden_sentence": [["Stone attended Xavier College Preparatory\u200d\u2014\u200can all-girl Catholic high school\u200d\u2014\u200cas a freshman, but dropped out after one semester to become an actress."], [""]]}, {"qid": "270f49ed8dd249c44c40", "term": "Billionaire", "description": "person who has a net worth of at least one billion (1,000,000,000) units of a given currency", "question": "Is Cambodia too expensive for 2020 richest billionaire to own?", "answer": false, "facts": ["The richest billionaire in 2020 is Jeff Bezos.", "Jeff Bezos has an estimated worth of 145 billion dollars.", "GDP is a measure of how much the economy of a country is worth.", "Cambodia has an estimated GDP of 28 billion in 2020."], "decomposition": ["Who is currently the richest person alive?", "What is the net worth of #1?", "What is the GDP of Cambodia?", "Is #2 less than #3?"], "evidence": [[[["Jeff Bezos-1"]], [["Jeff Bezos-1"]], [["Cambodia-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Jeff Bezos-1"]], [["Jeff Bezos-28"]], [["Thailand and the International Monetary Fund-2"], "no_evidence"], ["operation"]], [[["Jeff Bezos-1"]], [["Jeff Bezos-1"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Jeffrey Preston Bezos (/\u02c8be\u026azo\u028as/; n\u00e9 Jorgensen; born January 12, 1964) is an American industrialist, media proprietor, and investor."], ["The first centi-billionaire on the Forbes wealth index, Bezos has been the world's richest person since 2017 and was named the \"richest man in modern history\" after his net worth increased to $150\u00a0billion in July 2018."], [""]]}, {"qid": "bc9978abe100db0f8be8", "term": "Phobos (moon)", "description": "natural satellite of Mars", "question": "Would you have to wear a coat when on Phobos?", "answer": true, "facts": ["A coat is a garment used to keep a person warm.", "The surface temperatures on Phobos range from about \u22124 \u00b0C (25 \u00b0F) on the sunlit side to \u2212112 \u00b0C (\u2212170 \u00b0F) on the shadowed side."], "decomposition": ["What is the average temperature on Phobos?", "At what temperature would people need to start wearing coats to stay warm?", "Is #1 below #2?"], "evidence": [[[["Phobos (moon)-4"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Phobos (moon)-4"]], [["Coat-1"], "no_evidence"], ["operation"]], [[["Phobos (moon)-4"]], [["Coat-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["Surface temperatures range from about \u22124\u00a0\u00b0C (25\u00a0\u00b0F) on the sunlit side to \u2212112\u00a0\u00b0C (\u2212170\u00a0\u00b0F) on the shadowed side."]]}, {"qid": "308f192a0602d73150c6", "term": "Mary, mother of Jesus", "description": "religious figure and mother of Jesus of Nazareth", "question": "Would Mary, mother of Jesus have hypothetically prayed to Artemis if she was Greek?", "answer": true, "facts": ["Mary, Mother of Jesus is known for being a virgin and giving birth to Christ.", "Artemis was the Greek goddess of the hunt, childbirth, and virgins."], "decomposition": ["What is Greek Artemis god of?", "What were the things Mary, mother of Jesus was well known for?", "Is any of #2 included in #1?"], "evidence": [[[["Artemis-1"]], [["Mary, mother of Jesus-14"], "no_evidence"], ["operation"]], [[["Artemis-1"]], [["Mary, mother of Jesus-2"]], ["operation"]], [[["Artemis-16"]], [["Mary, mother of Jesus-2"]], ["operation"]]], "golden_sentence": [["Artemis (/\u02c8\u0251\u02d0rt\u026am\u026as/; Greek: \u1f0c\u03c1\u03c4\u03b5\u03bc\u03b9\u03c2 Artemis, Attic Greek:\u00a0[\u00e1r.te.mis]) is the Greek goddess of the hunt, the wilderness, wild animals, the Moon, and chastity."], [""]]}, {"qid": "b78e51c1185214b41a25", "term": "Moss", "description": "Division of plants", "question": "Is a beard is moss that grows on a human?", "answer": false, "facts": ["Moss is a type of non-vascular and flowerless plant.", "Moss typically grows in dark green clumps in damp and shady locations.", "A beard is facial hair that grows on the chin and cheeks of a human.", "Facial hair is not a plant. "], "decomposition": ["What is moss?", "What is a beard?", "Is #1 the same thing as #2?"], "evidence": [[[["Moss-1"]], [["Beard-1"]], ["operation"]], [[["Moss-1"]], [["Beard-1"]], ["operation"]], [[["Moss-1"]], [["Beard-1"]], ["operation"]]], "golden_sentence": [["Mosses, or the taxonomic division Bryophyta, are small, non-vascular flowerless plants that typically form dense green clumps or mats, often in damp or shady locations."], ["A beard is the hair that grows on the chin, upper lip, cheeks and neck of humans and some non-human animals."]]}, {"qid": "04889063808f8fa1f274", "term": "Pacific War", "description": "Theater of World War II fought in the Pacific and Asia", "question": "Did Archduke Franz Ferdinand of Austria participate in the Pacific War?", "answer": false, "facts": ["Archduke Franz Ferdinand of Austria was assassinated in 1914.", "The Pacific War took place between 1941 and 1945."], "decomposition": ["During what years did the Pacific War occur?", "When did Archduke Franz Ferdinand of Austria die?", "Did #2 occur after #1?"], "evidence": [[[["Pacific War-14"]], [["Assassination of Archduke Franz Ferdinand-46"]], ["operation"]], [[["Pacific War-2", "Pacific War-3"]], [["Archduke Franz Ferdinand of Austria-1"]], ["operation"]], [[["Pacific War-14"]], [["Archduke Franz Ferdinand of Austria-1"]], ["operation"]]], "golden_sentence": [["citation needed] Between 1942 and 1945, there were four main areas of conflict in the Pacific War: China, the Central Pacific, South-East Asia and the South West Pacific."], ["The trial was held from 12 October to 23 October with the verdict and sentences announced on 28 October 1914."]]}, {"qid": "87ac6bba774522028d68", "term": "The Doctor (Doctor Who)", "description": "fictional character from Doctor Who", "question": "Does The Doctor keep his ship in his childhood home?", "answer": false, "facts": ["The Doctor grew up on a planet called Gallifrey.", "The planet Gallifrey was destroyed in a time war.", "The Doctor's ship doesn't require docking."], "decomposition": ["Where is The Doctor's childhood home?", "Can The Doctor still visit #1?"], "evidence": [[[["The Doctor (Doctor Who)-7"]], [["Gallifrey-30"], "operation"]], [[["Doctor Who-53"], "no_evidence"], ["operation"]], [[["The Doctor (Doctor Who)-7"]], [["Gallifrey-3"]]]], "golden_sentence": [["The Doctor's subsequent childhood on Gallifrey has been little described."], [""]]}, {"qid": "e9ccdfd332d2c761123d", "term": "Marco Rubio", "description": "United States Senator from Florida", "question": "Does Marco Rubio have a close relationship with Allah?", "answer": false, "facts": ["Marco Rubio adheres to the religious sect of Christianity known as Catholicism.", "Catholics and other Christians worship God.", "Allah is worshiped by believers of Islam."], "decomposition": ["What is Marco Rubio's religion?", "Which deity does #1 worship?", "Is #2 Allah?"], "evidence": [[[["Marco Rubio-86"]], [["Catholic Church-2"]], [["God in Islam-13"], "operation"]], [[["Marco Rubio-7"]], [["God in Catholicism-38"]], ["operation"]], [[["Marco Rubio-86"]], [["Christianity-1"]], ["operation"]]], "golden_sentence": [["Rubio attends Catholic Mass at Church of the Little Flower in Coral Gables, Florida."], ["The Catholic Church teaches that it is the One, Holy, Catholic and Apostolic church founded by Jesus Christ in his Great Commission, that its bishops are the successors of Christ's apostles, and that the pope is the successor to Saint Peter upon whom primacy was conferred by Jesus Christ."], ["\u2014\u2009Quran\u00a051:47\u00a0(Translated by\u00a0Pickthall) Verily We created man from a product of wet earth; Then placed him as a drop (of seed) in a safe lodging; Then fashioned We the drop a clot, then fashioned We the clot a little lump, then fashioned We the little lump bones, then clothed the bones with flesh, and then produced it as another creation."]]}, {"qid": "c70d37e597818ba4d0b5", "term": "Ethiopian cuisine", "description": "Culinary traditions of Ethiopia", "question": "Is shrimp prevalent in Ethiopian cuisine?", "answer": false, "facts": ["Ethiopian cuisine specializes in vegetables and spicy meat dishes.", "Ethiopia is a landlocked country without access to seas or oceans."], "decomposition": ["What kind of aquatic environments are shrimp caught in?", "Does the geography of Ethiopia include any of #1?"], "evidence": [[[["Shrimp-2"]], [["Ethiopia-91"]]], [[["Shrimp and prawn as food-1"]], [["Ethiopia-1"], "operation"]], [[["Shrimp-8"]], [["Ethiopia-90"], "no_evidence"]]], "golden_sentence": [[""], ["Ethiopia is an ecologically diverse country, ranging from the deserts along the eastern border to the tropical forests in the south to extensive Afromontane in the northern and southwestern parts."]]}, {"qid": "04e6a9154cdeed09ec4b", "term": "Pancreas", "description": "A glandular organ that plays a role in the digestive and endocrine systems of vertebrates.", "question": "Can pancreas removal cause bankruptcy?", "answer": true, "facts": ["Pancreas removal is a medical procedure.", "Medical procedures are expensive in come countries. ", "Expensive procedures can cause debt.", "Debt can cause bankruptcy. "], "decomposition": ["What medical procedures are involved when a pancreas be removed?", "In what places are #1 sometimes directly paid for by the patient?", "Among any of #2, what consequences exist for medical debt?", "Is bankruptcy included in #3?"], "evidence": [[[["General surgery-1"]], [["Health care systems by country-55"]], [["Medical debt-4"]], ["operation"]], [[["Pancreatectomy-4"]], [["Medical debt-4"]], [["Medical debt-4"]], ["operation"]], [[["Pancreas-37", "Pancreas-44"], "no_evidence"], [["Health care in the United States-4"], "no_evidence"], [["Health care in the United States-14"], "no_evidence"], ["operation"]]], "golden_sentence": [["They also deal with diseases involving the skin, breast, soft tissue, trauma, Peripheral artery disease and hernias and perform endoscopic procedures such as gastroscopy and colonoscopy."], [""], ["One of the surveys that has been conducted by the Kaiser Family Foundation showed that the amount of debt that incurred by individuals for the health care costs is likely to cause personal bankruptcy."]]}, {"qid": "c8854292bea4710f9c62", "term": "Suburb", "description": "Human settlement that is part of or near to a larger city", "question": "Do suburbs encourage the use of cars?", "answer": true, "facts": ["Suburbs are generally built outside of walking distance from city centers.", "City centers contain jobs and stores.", "Suburb dwellers need to access jobs and stores to survive."], "decomposition": ["How far are suburbs usually situated from city centres?", "Is #1 usually greater than reasonable walking distance?"], "evidence": [[[["Suburb-1"], "no_evidence"], ["operation"]], [[["Suburb-1"]], [["Commuting-13"]]], [[["Suburb-1"], "no_evidence"], [["Suburb-2"], "no_evidence", "operation"]]], "golden_sentence": [[""]]}, {"qid": "a207f42cfa94178102b2", "term": "Gandalf", "description": "Fictional character created by J. R. R. Tolkien", "question": "Is Gandalf hypothetically a formidable foe for Charmed's Barbas?", "answer": false, "facts": ["Gandalf was a wizard in the Hobbit and Lord of the Rings series.", "Gandalf used his staff to cast powerful spells.", "Barbas was a demon on Charmed that had resistance to magic."], "decomposition": ["In what way did Gandalf attack his enemies?", "Is Barbas from Charmed susceptible to #1?"], "evidence": [[[["Gandalf-23"]], ["no_evidence"]], [[["Gandalf-2"], "no_evidence"], [["Barbas (Charmed)-1"], "no_evidence", "operation"]], [[["Gandalf-2"], "no_evidence"], [["Barbas (Charmed)-1", "Marbas-1"], "no_evidence", "operation"]]], "golden_sentence": [[""]]}, {"qid": "ff4a920bce45fabc8583", "term": "Moose", "description": "A genus of mammals belonging to the deer, muntjac, roe deer, reindeer, and moose family of ruminants", "question": "Would a moose hypothetically be too much for a minotaur to devour whole?", "answer": true, "facts": ["A minotaur was a mythological beast with the head of a bull and body of a human.", "Human stomachs process about three to four pounds of food a day.", "A moose can weigh up to 1500 pounds.", "Bulls can consume around 33 pounds of food a day."], "decomposition": ["What is the body structure of a minotaur?", "What kind of stomach do they have due to #1?", "What is the average weight of a moose?", "How much food weight can #2 process per day?", "Is #3 far greater than #4?"], "evidence": [[[["Minotaur-7"]], [["Minotaur-7"], "no_evidence"], [["Alaska moose-3"]], ["no_evidence"], ["operation"]], [[["Minotaur-1"]], [["Human digestive system-36"]], [["Moose-40"]], [["Moose-41"]], ["operation"]], [[["Minotaur-1"]], [["Stomach-1", "Stomach-3"]], [["Moose-40"]], [["Food energy-14"], "no_evidence"], ["operation"]]], "golden_sentence": [["The Minotaur is commonly represented in Classical art with the body of a man and the head and tail of a bull."], [""], ["Female Alaska moose stand on average 1.8\u00a0m (5.9\u00a0ft) at the shoulder and can weigh close to 478\u00a0kg (1,054\u00a0lb)."]]}, {"qid": "83a4904128eb4dfe50ab", "term": "Ahura Mazda", "description": "highest deity of Zoroastrianism", "question": "Can you worship Ahura Mazda at a mosque?", "answer": false, "facts": ["Ahura Mazda is a deity in Zoroastrianism", "A mosque is a place of worship for Muslims"], "decomposition": ["Which religious group worships in a mosque?", "Does #1 believe in or worship Ahura Mazda?"], "evidence": [[[["Mosque-1"]], [["Ahura Mazda-1"], "operation"]], [[["Mosque-30"]], [["Ahura Mazda-1", "Zoroastrianism-27"]]], [[["Islam-28"]], [["Ahura Mazda-1"]]]], "golden_sentence": [["is a place of worship for Muslims."], [""]]}, {"qid": "749ed307b6524fae3ce1", "term": "Spinal cord", "description": "long, thin, tubular structure made up of nervous tissue", "question": "Would a hedgehog avoid animals without a spinal cord?", "answer": false, "facts": ["A hedgehog has a wide ranging diet including birds, toads, slugs, and snails.", "Slugs are animals known as invertebrates because they have no backbones."], "decomposition": ["What animals do hedgehog mainly eats?", "Out of #1, do all animals have a spinal cord?"], "evidence": [[[["Hedgehog-11"]], [["Frog-54", "Vertebrate-1"], "operation"]], [[["Hedgehog-11"]], ["no_evidence", "operation"]], [[["Hedgehog-11"]], ["operation"]]], "golden_sentence": [["They feed on insects, snails, frogs and toads, snakes, bird eggs, carrion, mushrooms, grass roots, berries, melons and watermelons."], ["", ""]]}, {"qid": "a008b443531264ec6f3b", "term": "Fraktur", "description": "Typeface", "question": "Does Fraktur have a sordid history?", "answer": true, "facts": ["Fraktur is a type of font that originated in Germany.", "Fraktur was used on official Nazi documents.", "Fraktur was used on the cover of Hitler's Mein Kampf."], "decomposition": ["What is Fraktur?", "Which group in Germany used #1 for their official documents?", "Did #2 have a sordid past?"], "evidence": [[[["Fraktur-1"]], [["Fraktur-10"]], ["operation"]], [[["Fraktur-1"]], [["Fraktur-10"]], [["The Holocaust-1"]]], [[["Fraktur-1"]], [["Fraktur-10"]], [["Jewish ghettos in German-occupied Poland-3"], "operation"]]], "golden_sentence": [["Fraktur (German: [f\u0281ak\u02c8tu\u02d0\u0250\u032f] (listen)) is a calligraphic hand of the Latin alphabet and any of several blackletter typefaces derived from this hand."], ["The Nazis heavily used these fonts themselves, though the shift remained controversial and the press was at times scolded for its frequent use of \"Roman characters\" under \"Jewish influence\" and German \u00e9migr\u00e9s were urged to use only \"German script\"."]]}, {"qid": "d7aec49b8e6477168f5f", "term": "Andes", "description": "Mountain range in South America", "question": "Was the Peak of the Andes hidden from the view of the Colossus of Rhodes?", "answer": true, "facts": ["The highest point of the Andes is almost 23,000 feet high.", "The Colossus of Rhodes, a massive ancient statue, was around 108 feet tall.", "The Andes are located in South America.", "The Colossus of Rhodes was found in ancient Greece."], "decomposition": ["How high is the peak of the Andes?", "How high is the Colossus of Rhodes?", "Where are the Andes located?", "Where is the Colossus of Rhodes located?", "Is #1 higher than #2 and are #3 and #4 different countries?"], "evidence": [[[["Andes-3"]], [["Colossus of Rhodes-1"]], [["Andes-1"]], [["Colossus of Rhodes-1"]], ["operation"]], [[["Andes-3"]], [["Colossus of Rhodes-1"]], [["Andes-1"]], [["Colossus of Rhodes-1"]], ["operation"]], [[["Andes-3"]], [["Colossus of Rhodes-1"]], [["Andes-1"]], [["Colossus of Rhodes-1"]], ["operation"]]], "golden_sentence": [[""], ["According to most contemporary descriptions, the Colossus stood approximately 70 cubits, or 33 metres (108 feet) high\u2014the approximate height of the modern Statue of Liberty from feet to crown\u2014making it the tallest statue of the ancient world."], ["The Andes extend from north to south through seven South American countries: Venezuela, Colombia, Ecuador, Peru, Bolivia, Chile, and Argentina."], ["The Colossus of Rhodes (Ancient Greek: \u1f41 \u039a\u03bf\u03bb\u03bf\u03c3\u03c3\u1f78\u03c2 \u1fec\u03cc\u03b4\u03b9\u03bf\u03c2, romanized:\u00a0ho Koloss\u00f2s Rh\u00f3dios Greek: \u039a\u03bf\u03bb\u03bf\u03c3\u03c3\u03cc\u03c2 \u03c4\u03b7\u03c2 \u03a1\u03cc\u03b4\u03bf\u03c5, romanized:\u00a0Koloss\u00f3s tes Rh\u00f3dou) was a statue of the Greek sun-god Helios, erected in the city of Rhodes, on the Greek island of the same name, by Chares of Lindos in 280 BC."]]}, {"qid": "4ab38e29a19b0360b855", "term": "LG Electronics", "description": "South Korean multinational electronics company", "question": "Is LG Electronics located in a city with an official bird that has a purplish/blue tail?", "answer": true, "facts": ["LG Electronics is headquarted in Seoul.", "The official bird of Seoul is the Korean magpie.", "The Korean magpie has a purplish/blue colored tail."], "decomposition": ["Which city is LG Electronics headquarters located in?", "Which bird is officially associated with #1", "Is the tail color of #2 purplish-blue?"], "evidence": [[[["LG Electronics-1"]], [["Oriental magpie-1"], "no_evidence"], [["Oriental magpie-5"], "operation"]], [[["LG Electronics-1"]], [["Oriental magpie-1"]], [["Oriental magpie-5"]]], [[["LG Electronics-1"]], [["Oriental magpie-1"], "no_evidence"], [["Oriental magpie-5"]]]], "golden_sentence": [["LG Electronics Inc. (Korean:\u00a0LG \uc804\uc790; RR:\u00a0LG Jeonja) is a South Korean multinational electronics company headquartered in Yeouido-dong, Seoul, South Korea."], ["The Oriental magpie (Pica serica) is a species of magpie found from south-eastern Russia and Myanmar to eastern China, Taiwan and northern Indochina."], ["The back, tail, and particularly the remiges show strong purplish-blue iridescence with few if any green hues."]]}, {"qid": "835e978560b7d8e2a257", "term": "Naruto", "description": "Japanese manga and anime series", "question": "Could you watch Naruto and Puzzle Place on the same channel?", "answer": false, "facts": ["Puzzle Place aired on PBS between 1995 and 1998.", "Naruto aired on Cartoon Network in 2005."], "decomposition": ["What channel did Puzzle Place air on?", "What channel did Naruto air on?", "Is #1 the same as #2?"], "evidence": [[[["The Puzzle Place-1"]], [["Naruto-2"]], ["operation"]], [[["The Puzzle Place-1"]], [["Naruto-2"]], ["operation"]], [[["The Puzzle Place-1"]], [["Naruto-24"]], ["operation"]]], "golden_sentence": [["It became one of PBS Kids' most popular series on the line-up since Barney & Friends and Sesame Street."], ["The English adaptation is still airing weekly on Adult Swim to this day."]]}, {"qid": "3ee42038e9ff468cff65", "term": "Christopher Nolan", "description": "British\u2013American film director, screenwriter, and producer", "question": "Could Christopher Nolan borrow pants from Danny Devito?", "answer": false, "facts": ["Christopher Nolan is 6 feet tall.", "Danny Devito is 4'10\" tall.", "Pant sizes relate to height."], "decomposition": ["How tall is Christopher Nolan?", "What was Danny Devito's height?", "Does #1 match #2?"], "evidence": [[["no_evidence"], [["Danny DeVito-4"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], [["Danny DeVito-4"], "no_evidence"], ["operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""]]}, {"qid": "358219f8ae4cb1b2090a", "term": "Joker (character)", "description": "Fictional character in the DC Universe", "question": "Was the Joker an enemy of the Avengers?", "answer": false, "facts": ["The Joker is a DC Comics villain.", "The Avengers are a group of heroes from Marvel Comics.", "Being from different publishers, they do not meet."], "decomposition": ["Which world does the Joker exist in?", "The Avengers are from which universe?", "Is #1 the same as #2?"], "evidence": [[[["Joker (2019 film)-46"]], [["The Avengers (2012 film)-37"]], [["The Avengers (2012 film)-45"], "operation"]], [[["Joker (character)-1"]], [["Avengers (comics)-1"]], ["operation"]], [[["Joker (character)-1"]], [["Avengers (comics)-1"]], ["operation"]]], "golden_sentence": [["Actor Josh Brolin, who portrays Thanos in the Marvel Cinematic Universe, found the film powerful: \"To appreciate Joker I believe you have to have either gone through something traumatic in your lifetime (and I believe most of us have) or understand somewhere in your psyche what true compassion is.\""], [""], [""]]}, {"qid": "86a5328a8d5ae82d3e32", "term": "Pea", "description": "species of plant", "question": "Will twenty pea pods contents cover entire chess board?", "answer": true, "facts": ["Pea pods on average have 5 to 6 peas inside.", "A standard chess board has 64 squares."], "decomposition": ["On average, how many peas do twenty pea pods contain?", "How many squares does a standard chess board have?", "Is #1 greater than #2?"], "evidence": [[[["Pea-1"], "no_evidence"], [["Chessboard-3"]], ["no_evidence", "operation"]], [[["Pea-4"], "no_evidence"], [["Chessboard-3"]], ["operation"]], [[["Pea-1"], "no_evidence"], [["Chess-1"]], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["In western chess the board has a square shape, with its side being divided into eight parts, resulting in a total of sixty-four squares."]]}, {"qid": "641cba5520e4134a59a0", "term": "Phobos (moon)", "description": "natural satellite of Mars", "question": "Is Phobos (moon) name origin similar to Roman god Pavor?", "answer": true, "facts": ["Phobos (moon) derives its name from ancient Greek mythology.", "Phobos was the god of fear.", "In Roman mythology, Pavor or Terror is known as the personification of fear."], "decomposition": ["What was Phobos (moon) named after?", "What is #1 referred to in Roman mythology?", "Is #2 the same as Pavor or Terror?"], "evidence": [[[["Phobos (moon)-2"]], [["Phobos (mythology)-2"]], ["operation"]], [[["Phobos (moon)-6"]], [["Phobos (mythology)-2"]], ["operation"]], [[["Phobos (mythology)-1"]], [["Phobos (mythology)-2"]], ["operation"]]], "golden_sentence": [["Phobos is named after the Greek god Phobos, a son of Ares (Mars) and Aphrodite (Venus) and the personification of fear (cf."], ["In Roman mythology, he has also been referred to as Pavor or Terror."]]}, {"qid": "b2f4ac9609b4f0d62112", "term": "Binary number", "description": "system that represents numeric values using two symbols; 0 and 1", "question": "Can binary numbers and standard alphabet satisfy criteria for a strong password?", "answer": false, "facts": ["The criteria for a strong password according to cybersecurity company Avast is: at least 15 characters. uppercase letters. lowercase letters. numbers. and symbols.", "The standard alphabet contains twenty six letters but no special characters.", "Binary numbers only contain 0 and 1."], "decomposition": ["Which characters make up binary numbers?", "Which characters make up the standard English alphabet", "Does #1 or #2 include special characters or symbols?"], "evidence": [[[["English alphabet-1"], "no_evidence"], [["Binary number-1"]], ["operation"]], [[["Binary number-1"]], [["Alphabet-1"]], [["Password strength-13"], "operation"]], [[["Binary number-1"]], [["English alphabet-1"]], ["no_evidence"]]], "golden_sentence": [["The word alphabet is a compound of first two letters of the Greek alphabet, alpha and beta."], ["In mathematics and digital electronics, a binary number is a number expressed in the base-2 numeral system or binary numeral system, which uses only two symbols: typically \"0\" (zero) and \"1\" (one)."]]}, {"qid": "b0f8388ac44ab0f41315", "term": "Ice", "description": "water frozen into the solid state", "question": "Would a diet of ice eventually kill a person?", "answer": true, "facts": ["Humans can survive without water for four days.", "Ice can be melted into water, which consists of hydrogen and oxygen, using a simple cigarette lighter.", "Humans can survive without food for 30 to 40 days on average.", "Humans need carbohydrates, proteins, and fats that are contained in foods.", "Water does not contain fat, carbohydrates or protein."], "decomposition": ["Ice is the solid state of what?", "What nutrients are needed to sustain human life?", "Are most of #2 absent from #1?"], "evidence": [[[["Ice-7"]], [["Table of food nutrients-1"]], [["Table of food nutrients-1"], "operation"]], [[["Ice-1"]], [["Nutrient-1"]], [["Water-1"], "operation"]], [[["Ice-1"]], [["Food-1"]], ["operation"]]], "golden_sentence": [[""], ["Included for each food is its weight in grams, its calories, and (also in grams,) the amount of protein, carbohydrates, dietary fiber, fat, and saturated fat."], [""]]}, {"qid": "e36fd3baeb35963cca95", "term": "Quran", "description": "The central religious text of Islam", "question": "Would an adherent of Zoroastrianism consult the Quran for religious guidance?", "answer": false, "facts": ["The Quran is the central religious text of Islam", "Zoroastrianism is an ancient religion predating Islam by several centuries"], "decomposition": ["Which religious group mainly uses the Quran for their consultation?", "Is Zoroastrianism closely related to #1?"], "evidence": [[[["Quran-1"]], [["Zoroastrianism-1"], "operation"]], [[["Quran-20"]], [["Zoroastrianism-48"], "operation"]], [[["Quran-1"]], [["Zoroastrianism-1"]]]], "golden_sentence": [[""], ["It is a multi-tendency faith centered on a dualistic cosmology of good and evil and an eschatology predicting the ultimate conquest of evil with theological elements of henotheism, monotheism/monism, and polytheism."]]}, {"qid": "96860a7352f4e465fe54", "term": "Joker (character)", "description": "Fictional character in the DC Universe", "question": "Is the Joker in a healthy romantic relationship?", "answer": false, "facts": ["Healthy relationships are characterized by mutual trust and respect.", "The Joker is dating Harley Quinn.", "The Joker frequently abuses and talks down to Harley."], "decomposition": ["Who is the Joker in a relationship with?", "Does the Joker respect #1?", "Is respect necessary in a healthy romantic relationship?", "Are #2 and #3 the same?"], "evidence": [[[["Joker (character)-27"]], [["Joker (character)-53"]], [["Interpersonal relationship-21"]], ["operation"]], [[["Joker (character)-3"]], [["Joker (character)-3"]], ["no_evidence"], ["operation"]], [[["Harley Quinn-1"]], [["Harley Quinn-2"]], ["no_evidence", "operation"], ["operation"]]], "golden_sentence": [["This story also introduced the Joker's girlfriend, Harley Quinn."], ["Joker mirrors his identity through Harley in her appearance, and even though he may ignore or act indifferent towards her, he continues to try to subject her to his control."], ["Less time between a breakup and a subsequent relationship predicts higher self-esteem, attachment security, emotional stability, respect for your new partner, and greater well-being."]]}, {"qid": "81f0d1ab3abc0aefe7fe", "term": "Lighthouse of Alexandria", "description": "Ancient lighthouse in Egypt", "question": "Were Greeks essential to crafting Egyptian Lighthouse of Alexandria?", "answer": true, "facts": ["The Lighthouse of Alexandria was an impressive monument in Egypt.", "The Lighthouse of Alexandria was built by pharaoh Ptolemy II.", "Ptolemy II was the son of Ptolemy I Soter.", "Ptolemy I Soter was a Greek bodyguard of Alexander the Great and became pharaoh of Egypt."], "decomposition": ["Who built the Lighthouse of Alexandria?", "Who was #1's father?", "Was #2 Greek?"], "evidence": [[[["Lighthouse of Alexandria-7", "Sostratus of Cnidus-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Lighthouse of Alexandria-6"]], [["Ptolemy I Soter-2"]], [["Philip II of Macedon-1"], "operation"]], [[["Lighthouse of Alexandria-6"]], [["Ptolemy I Soter-1"]], ["operation"]]], "golden_sentence": [["In the second century AD Lucian wrote that Sostratus hid his name under plaster bearing the name of Ptolemy so that when the plaster fell off, Sostratus's name would be visible in the stone.", "Sostratus of Cnidus (/\u02c8s\u0252str\u0259t\u0259s/; Greek: \u03a3\u03ce\u03c3\u03c4\u03c1\u03b1\u03c4\u03bf\u03c2 \u1f41 \u039a\u03bd\u03af\u03b4\u03bf\u03c2; born 3rd century BC), was a Greek architect and engineer."]]}, {"qid": "a17593fb44bf96b43336", "term": "J. D. Salinger", "description": "American writer", "question": "Did J. D. Salinger ever ask his father for a quincea\u00f1era?", "answer": false, "facts": ["A quincea\u00f1era is celebration of a girl's 15th birthday.", "J. D. Salinger was male.", "A quincea\u00f1era is a Hispanic tradition.", "J. D. Salinger was Jewish."], "decomposition": ["What gender is a quincea\u00f1era usually held for?", "What gender is J. D. Salinger?", "Is #1 the same as #2?"], "evidence": [[[["Quincea\u00f1era-1"]], [["J. D. Salinger-1", "J. D. Salinger-12"]], ["operation"]], [[["Quincea\u00f1era-1"]], [["J. D. Salinger-18"]], ["operation"]], [[["Quincea\u00f1era-1"]], [["J. D. Salinger-2"]], ["operation"]]], "golden_sentence": [["The girl celebrating her 15th birthday is a quincea\u00f1era (Spanish pronunciation:\u00a0[kinsea\u02c8\u0272e\u027ea]; feminine form of \"15-year-old\")."], ["", ""]]}, {"qid": "e41870e7448c71ec281b", "term": "Game engine", "description": "Software-development environment designed for building video games", "question": "Does a game engine have a fuel injector?", "answer": false, "facts": ["A game engine is the software used to develop video games", " A fuel injector is part of an automotive engine"], "decomposition": ["Which kind of engine uses a fuel injector?", "Is 'game engine' a kind of #1?"], "evidence": [[[["Fuel injection-1"]], [["Game engine-1"], "operation"]], [[["Fuel injection-62"]], ["operation"]], [[["Fuel injection-1"]], [["Game engine-1"]]]], "golden_sentence": [["Fuel injection is the introduction of fuel in an internal combustion engine, most commonly automotive engines, by the means of an injector."], ["The core functionality typically provided by a game engine includes a rendering engine (\"renderer\") for 2D or 3D graphics, a physics engine or collision detection (and collision response), sound, scripting, animation, artificial intelligence, networking, streaming, memory management, threading, localization support, scene graph, and may include video support for cinematics."]]}, {"qid": "416dddf66277a01b450d", "term": "Vitamin C", "description": "nutrient found in citrus fruits and other foods", "question": "Would drinking a glass of lemonade provide Vitamin C?", "answer": true, "facts": ["Lemonade is made from lemons, sugar, and water.", "Lemons are a citrus fruit.", "Citrus fruit is high in Vitamin C."], "decomposition": ["What are the ingredients of lemonade?", "Are any of #1 high in Vitamin C?"], "evidence": [[[["Lemonade-1"]], ["no_evidence"]], [[["Lemonade-2"]], [["Lemon-6"], "operation"]], [[["Lemonade-2"]], [["Lemon-12"]]]], "golden_sentence": [[""]]}, {"qid": "0a1d20faea74172054e0", "term": "Chevrolet Cruze", "description": "compact car marketed by GM from 2008-2019", "question": "Is Chinese successor to Chevrolet Cruze name a town far from Milan?", "answer": false, "facts": ["The Chinese successor to Chevrolet Cruze is the Chevrolet Monza.", "Monza is a city near the north of Milan."], "decomposition": ["What is the Chinese successor to Chevrolet Cruze?", "What is the city that #1 is named after?", "Is #2 located far from Milan?"], "evidence": [[[["Chevrolet Monza (China)-1"]], [["Monza-1"]], ["operation"]], [[["Chevrolet Cruze-71"]], [["Monza-1"]], [["Monza-1"], "operation"]], [[["Chevrolet Cruze-71"]], [["Monza-1"]], ["operation"]]], "golden_sentence": [[""], ["Monza is best known for its Grand Prix motor racing circuit, the Autodromo Nazionale Monza, which hosts the Formula One Italian Grand Prix with a massive Italian support tifosi for the Ferrari team."]]}, {"qid": "bc9fa837256afe3fdef7", "term": "Tom Cruise", "description": "American actor and producer", "question": "Could Tom Cruise explain mental auditing?", "answer": true, "facts": ["Mental auditing is a practice within the church of Scientology.", "Tom Cruise is a long standing member of the church of Scientology and is high in the ranks."], "decomposition": ["What church practices mental auditing?", "Is Tom Cruise a member of #1?"], "evidence": [[[["Auditing (Scientology)-1", "Auditing (Scientology)-2"]], ["no_evidence", "operation"]], [[["Scientology beliefs and practices-1"]], [["Tom Cruise-36"]]], [[["Auditing (Scientology)-1"]], [["Tom Cruise-4"]]]], "golden_sentence": [["In the Church of Scientology, auditing is a process whereby the auditor takes an individual, known as a PC or \"preclear,\" through times in their life and gets rid of any past or current negative situations that may have a hold on to them due to falsehoods that may reside with those situations, and have heretofore been uninspected.", ""]]}, {"qid": "5efc4d16a89afb38c315", "term": "Mount Sharp", "description": "mountain on Mars", "question": "Are human footprints absent from Mount Sharp?", "answer": true, "facts": ["Mount Sharp is located on Mars.", "Human beings have not traveled to Mars.", "Human footprints could only be present if human feet touched down on Mount Sharp."], "decomposition": ["Where is Mount Sharp?", "What would produce a human footprint?", "Have #2 never traveled to #1?"], "evidence": [[[["Mount Sharp-1"]], [["Footprint-1"]], ["operation"]], [[["Mount Sharp-1"]], [["Footprint-1"]], [["Human mission to Mars-73"], "operation"]], [[["Mount Sharp-1"]], [["Footprint-1"]], [["Human mission to Mars-2"]]]], "golden_sentence": [["Mount Sharp, officially Aeolis Mons (IPA:\u00a0[\u02c8i\u02d0\u0259l\u0268s \u02c8m\u0252nz]), is a mountain on Mars."], ["Hoofprints and pawprints are those left by animals with hooves or paws rather than feet, while \"shoeprints\" is the specific term for prints made by shoes."]]}, {"qid": "1b20bdcfe7928bc5aa1b", "term": "Apollo 15", "description": "Fourth crewed mission to land on the Moon", "question": "Did the crew of Apollo 15 take pictures of Mount Sharp?", "answer": false, "facts": ["Mount Sharp is a mountain on Mars.", "The crew of Apollo 15 landed on the Moon, not Mars.", "No humans have ever landed on Mars."], "decomposition": ["Where is Mount Sharp located", "Did the crew of Apollo 15 travel to #1 or a place very close to #1?"], "evidence": [[[["Mount Sharp-1"]], [["Apollo 15-1"]]], [[["Mount Sharp-1"]], [["Apollo 15-1"]]], [[["Mount Sharp-1"]], [["Apollo 15-2"]]], [[["Mount Sharp-1"]], [["Apollo 15-1"]]]], "golden_sentence": [["Mount Sharp, officially Aeolis Mons (IPA:\u00a0[\u02c8i\u02d0\u0259l\u0268s \u02c8m\u0252nz]), is a mountain on Mars."], [""]]}, {"qid": "006350ff7b263132b68d", "term": "Geometry", "description": "Branch of mathematics that studies the shape, size and position of objects", "question": "Do carpenters understand geometry?", "answer": true, "facts": ["Carpenters work in building and maintaining structures such as homes, buildings, and gazebos.", "In order to build a home, one must be able to follow the geometry in the blueprints. "], "decomposition": ["What kind of buildings/structures do carpenters help in constructing?", "Do #1 require knowledge of geometry to carry out?"], "evidence": [[[["Carpentry-1"]], [["Geometry-38"], "operation"]], [[["Carpentry-1"]], [["Geometry-1"], "operation"]], [[["Carpentry-1"]], [["Geometry-1"]]]], "golden_sentence": [["Carpentry is a skilled trade and a craft in which the primary work performed is the cutting, shaping and installation of building materials during the construction of buildings, ships, timber bridges, concrete formwork, etc."], [""]]}, {"qid": "281c49a96a4b6d42b140", "term": "Iyer", "description": "caste of Hindu Brahmin communities of Tamil origin", "question": "Do people of the Iyer caste eat meat?", "answer": false, "facts": ["Iyer is a caste of Hindu Brahmin.", "Brahmin is the priest caste of Hinduism.", "Devout Hindus do not eat meat. ", "Priests of a religion are devout followers of that religion."], "decomposition": ["What caste is Iyer part of?", "What larger caste is #1 part of?", "Do people who follow #2 eat meat?"], "evidence": [[[["Iyer-11"]], [["Brahmin (disambiguation)-1"]], [["Brahmin-15"], "operation"]], [[["Iyer-35"]], [["Brahmin-1", "History of Brahmin diet-7", "Tamil Brahmin-1"]], [["History of Brahmin diet-7"]]], [[["Iyer-1"]], [["Brahmin-1"]], [["Brahmin-15"], "operation"]]], "golden_sentence": [["According to the Buddhist scripture Mahavamsa, the presence of Brahmins have been recorded in Sri Lanka as early as 500BC when the first migrations from the Indian mainland supposedly took place."], ["Brahmin is a varna (class) in Hinduism specialising as scholars, priests, teachers (acharya) and protectors of sacred learning across generations."], ["The chapter 1.20 of Apastamba, states Olivelle, forbids the trade of the following under any circumstances: human beings, meat, skins, weapons, barren cows, sesame seeds, pepper, and merits."]]}, {"qid": "9aa3c896ccccfc975a89", "term": "Pottery", "description": "Craft of making objects from clay", "question": "Is a pottery kiln inappropriate for use with glass blowing?", "answer": false, "facts": ["Pottery kilns heat from the sides.", "Glass kilns heat from the top.", "Glass can be fused in a ceramic pottery kiln without trouble."], "decomposition": ["From what sides does a pottery kiln Heat?", "From what sides does a glass kiln heat?", "Is #1 listed in #2?"], "evidence": [[[["Kiln-16"], "no_evidence"], [["Kiln-10"], "no_evidence"], ["operation"]], [[["Kiln-10"], "no_evidence"], [["Glass fusing-7"], "no_evidence"], ["operation"]], [[["Kiln-1"], "no_evidence"], [["Kiln-5"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "b79a16e2dfc77efa04f2", "term": "Swan", "description": "large water bird", "question": "Would WWF be angrier if you killed koala instead of black swan?", "answer": true, "facts": ["The WWF is an international organization that works for the preservation of animals.", "Black swans are designated as least concern species meaning they are not close to being endangered.", "Koalas are designated as  vulnerable to extinction\u2014just a step above endangered."], "decomposition": ["What is the black swan's listing on the IUCN red list?", "How is the Koala listed on the IUCN red list?", "What does WWF represent?", "Considering #3, is #2 in more dire straits than #1?"], "evidence": [[[["Black swan-24"]], [["Koala-3"]], [["World Wide Fund for Nature-1"]], ["operation"]], [[["Black swan-24"]], [["Koala-44"]], [["World Wide Fund for Nature-10"]], ["operation"]], [[["Black swan-13", "IUCN Red List-1"]], [["Koala-50", "Vulnerable species-1"], "no_evidence"], [["World Wide Fund for Nature-1"]], ["operation"]]], "golden_sentence": [["It is evaluated as Least Concern on the IUCN Red List of Threatened Species."], [""], ["The World Wide Fund for Nature (WWF) is an international non-governmental organization founded in 1961, working in the field of wilderness preservation, and the reduction of human impact on the environment."]]}, {"qid": "d771f7154f9809c40ebd", "term": "Twin", "description": "One of two offspring produced in the same pregnancy. Use with P31 on items for one twin", "question": "Are twins always born during the same year?", "answer": false, "facts": ["Some twins are born right before the New Year, and right after the New Year.", "There are some twins, implanted through IVF, who are born decades apart."], "decomposition": ["What external fertilization processes can result in twins?", "What process can split embryo twins from #1?", "What process is used to preserve embryos from #1?", "Is it impossible to apply #2 and #3 to embryo twins created from #1?"], "evidence": [[[["In vitro fertilisation-32"]], [["Twin-16"]], [["Embryo cryopreservation-1"]], [["Embryo cryopreservation-2"], "operation"]], [[["Twin-39"]], [["In vitro fertilisation-32"]], [["In vitro fertilisation-71"]], ["operation"]], [[["In vitro fertilisation-32"], "no_evidence"], ["no_evidence"], [["Oocyte cryopreservation-1"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""], ["Cryopreservation of embryos is the process of preserving an embryo at sub-zero temperatures, generally at an embryogenesis stage corresponding to pre-implantation, that is, from fertilisation to the blastocyst stage."], [""]]}, {"qid": "465b4063f2eacf63a3bd", "term": "Pain", "description": "type of unpleasant feeling", "question": "Does acupuncture cause pain in many people?", "answer": false, "facts": ["Acupuncture doesn't usually feel painful for most people.", "The needles used in acupuncture are very thin and do not go very deep into the skin."], "decomposition": ["What kind of instruments are used during acupuncture?", "Are #1 likely to be painful for most people?"], "evidence": [[[["Acupuncture-10"]], [["Acupuncture-11"]]], [[["Acupuncture-1"]], [["Acupuncture-1"]]], [[["Acupuncture-1"]], ["operation"]]], "golden_sentence": [[""], ["Since most pain is felt in the superficial layers of the skin, a quick insertion of the needle is recommended."]]}, {"qid": "921dced505f891668961", "term": "Clouded leopard", "description": "species of mammal found from the Himalayan foothills through mainland Southeast Asia into China", "question": "Would a clouded leopard encounter an awake pangolin?", "answer": true, "facts": ["Pangolins and clouded leopards have an overlap of ranges", "Pangolins are nocturnal", "Clouded leopards are nocturnal"], "decomposition": ["What is the range of the clouded leopard?", "What time of day is the clouded leopard active?", "What is the range of the pangolin?", "What time of day is the pangolin active?", "Do #1 and #3 overlap while #2 and #4 overlap?"], "evidence": [[[["Clouded leopard-1"]], [["Clouded leopard-24"]], [["Pangolin-1"]], [["Pangolin-2"]], [["Clouded leopard-1", "Clouded leopard-24", "Pangolin-1", "Pangolin-2"]]], [[["Clouded leopard-1"]], [["Clouded leopard-24"]], [["Pangolin-1"], "no_evidence"], [["Pangolin-2"]], ["no_evidence", "operation"]], [[["Clouded leopard-1"]], [["Clouded leopard-24"]], [["Pangolin-1"]], [["Pangolin-2"]], ["operation"]]], "golden_sentence": [["The clouded leopard (Neofelis nebulosa) is a medium-size wild cat occurring from the Himalayan foothills through mainland Southeast Asia into southern China."], [""], ["These species range in size from 30 to 100\u00a0cm (12 to 39\u00a0in)."], [""], ["", "", "", "They tend to be solitary animals, meeting only to mate and produce a litter of one to three offspring, which they raise for about two years."]]}, {"qid": "d6c3b640d265b973bfa9", "term": "Alligator", "description": "Genus of large reptiles", "question": "Would a crocodile survive longer in Great Salt Lake than alligator?", "answer": true, "facts": ["The Great Salt Lake is a Utah lake composed of salt water.", "Crocodiles natural habitat is salt water and they can breathe underwater for hours.", "Alligators have a natural habitat of fresh water."], "decomposition": ["What kind of water habitat is the Great Salt Lake?", "Which water habitats are crocodiles adapted to survive in?", "Which water habitats are alligators adapted to survive in?", "Is #1 included in #2 and excluded from #3?"], "evidence": [[[["Great Salt Lake-1"]], [["Crocodile-2", "Crocodile-3"]], [["Alligator-8"]], ["operation"]], [[["Great Salt Lake-22"]], [["Crocodile-3"]], [["Alligator-8"]], ["operation"]], [[["Great Salt Lake-1"]], [["Crocodile-2"]], [["Crocodile-2"]], ["operation"]]], "golden_sentence": [[""], ["", "All crocodiles are semiaquatic and tend to congregate in freshwater habitats such as rivers, lakes, wetlands and sometimes in brackish water and saltwater."], ["American alligators live in freshwater environments, such as ponds, marshes, wetlands, rivers, lakes, and swamps, as well as in brackish water."]]}, {"qid": "9b96677775f89d81e6b4", "term": "The Who", "description": "English rock band", "question": "Did The Who have to cancel tours due to World War II?", "answer": false, "facts": ["The Who was formed in 1964", "World War II ended in 1945"], "decomposition": ["When was The Who formed?", "In what year did World War II end?", "Is #1 before #2?"], "evidence": [[[["The Who-1"]], [["World War II-1"]], ["operation"]], [[["The Who-1"]], [["The Second World War (disambiguation)-1"]], ["operation"]], [[["The Who-1"]], [["World War II-1"]], ["operation"]]], "golden_sentence": [["The Who are an English rock band formed in London in 1964."], ["World War\u00a0II (often abbreviated as WWII or WW2), also known as the Second World War, was a global war that lasted from 1939 to 1945."]]}, {"qid": "dd56b507e358aeb63801", "term": "Numerology", "description": "any study of the purported divine, mystical or other special relationship between a number and some coinciding observed (or perceived) events", "question": "Has numerology helped shape hotel layouts?", "answer": true, "facts": ["Numerology is the study of numbers and how they relate to events.", "Numbers such as 3 and 7 hold biblical significance.", "Numbers such as 6 and 13 are said to be unlucky.", "The thirteenth floor is a designation of a level of a multi-level building that is often omitted in countries where the number 13 is considered unlucky.", "Many hotels do not have thirteenth floors because of the enduring superstition."], "decomposition": ["What numbers are often considered unlucky?", "What number is usually omitted in numbering hotel floors?", "Is #2 part of #1?"], "evidence": [[[["13 (number)-14"], "no_evidence"], [["13 (number)-14"]], ["operation"]], [[["13 (number)-14"]], [["13 (number)-14", "Thirteenth floor-2"]], ["operation"]], [[["13 (number)-16", "Number of the Beast-1"], "no_evidence"], [["Thirteenth floor-7"]], ["operation"]]], "golden_sentence": [["It is also considered unlucky to have thirteen guests at a table."], ["The superstitious sufferers of triskaidekaphobia try to avoid bad luck by keeping away from anything numbered or labelled thirteen."]]}, {"qid": "62b3416e3f39c3e3512e", "term": "Operation Barbarossa", "description": "1941 German invasion of the Soviet Union during the Second World War", "question": "Did Operation Barbarossa or Barbarossa's last expedition succeed?", "answer": false, "facts": ["Operation Barbarossa was the Nazi advance on Russia during World War II.", "Operation Barbarossa was a failure that resulted in Nazi Germany being pushed back by a Soviet counter offensive.", "Operation Barbarossa was named after Holy Roman Emperor Frederick Barbarossa.", "On his final expedition, Frederick Barbarossa drowned while leading an army to help the Crusaders during the Third Crusade.", "The Crusaders failed to recapture Jerusalem during the Third Crusade without the support of Barbarossa and his troops."], "decomposition": ["What was the objective of Operation Barbarossa?", "What was the goal of the final expedition of Frederick Barbarossa?", "Did #1 and #2 succeed?"], "evidence": [[[["Operation Barbarossa-1"]], [["Frederick I, Holy Roman Emperor-44"]], [["Frederick I, Holy Roman Emperor-46", "Operation Barbarossa-4"]]], [[["Operation Barbarossa-1"]], [["Frederick I, Holy Roman Emperor-37"]], [["Frederick I, Holy Roman Emperor-46", "Operation Barbarossa-4"], "operation"]], [[["Operation Barbarossa-1"]], [["Frederick I, Holy Roman Emperor-36"]], [["Frederick I, Holy Roman Emperor-44", "Operation Barbarossa-4"]]]], "golden_sentence": [["The operation put into action Nazi Germany's ideological goal of conquering the western Soviet Union so as to repopulate it with Germans."], [""], ["", ""]]}, {"qid": "a6fd5d4961709b348ee3", "term": "Anchovy", "description": "Family of fishes", "question": "Are anchovies associated with Italian food?", "answer": true, "facts": ["Pizza is an Italian food.", "Anchovies are occasionally used as a pizza topping."], "decomposition": ["What food item are anchovies sometimes eaten with?", "Is #1 an Italian food?"], "evidence": [[[["Anchovies as food-5"]], [["Anchovies as food-5"], "operation"]], [[["Anchovies as food-3"]], [["Pizza-1"], "operation"]], [[["Anchovies as food-3"]], [["Pizza-1"]]]], "golden_sentence": [["In Sweden and Finland, the name anchovies is related strongly to a traditional seasoning, hence the product \"anchovies\" is normally made of sprats and also herring can be sold as \"anchovy-spiced\", leading to confusion when translating recipes."], ["Fresh anchovies, known in Italy as alici\u00a0[it], have a much milder flavor."]]}, {"qid": "481015583c20b23d12f8", "term": "Onion", "description": "vegetable", "question": "Can chemicals in onion help create a thermonuclear bomb?", "answer": true, "facts": ["A thermonuclear bomb, also called a hydrogen bomb, uses hydrogen under high temperatures to create an explosive reaction.", "While chopping onions, cells inside the onion are broken and the gas that comes out forms sulfenic acid.", "Sulfenic acid is composed of several elements including hydrogen."], "decomposition": ["What elements are used in a thermonuclear bomb?", "When onions are chopped what gas is released?", "What elements are found in #2?", "Is #1 a subset of #3?"], "evidence": [[[["Thermonuclear weapon-2"]], [["Onion-30"], "no_evidence"], [["Syn-Propanethial-S-oxide-1"], "no_evidence"], ["operation"]], [[["Thermonuclear weapon-1"], "no_evidence"], [["Onion-30"]], ["operation"], ["no_evidence", "operation"]], [[["Thermonuclear weapon-2"]], [["Onion-30"]], [["Syn-Propanethial-S-oxide-1"]], ["operation"]]], "golden_sentence": [["Modern fusion weapons consist essentially of two main components: a nuclear fission primary stage (fueled by U-235 or Pu-239) and a separate nuclear fusion secondary stage containing thermonuclear fuel: the heavy hydrogen isotopes deuterium and tritium, or in modern weapons lithium deuteride."], ["This gas is produced by a chain of reactions which serve as a defence mechanism: chopping an onion causes damage to cells which releases enzymes called alliinases."], [""]]}, {"qid": "1d6df57a663181157b2b", "term": "Snowboarding", "description": "winter sport", "question": "Would it be difficult to snowboard on Venus?", "answer": true, "facts": ["Snowboarding involves descending a snow-covered slope while standing on a snowboard.", "Snow is formed by the freezing of water.", "Water has a freezing point of 32\u00b0F.", "Venus has a mean surface temperature of 737 K (464 \u00b0C; 867 \u00b0F)."], "decomposition": ["What kind of surface is suitable for snowboarding?", "What temperature range facilitates the formation of #1?", "What is the average surface temperature on Venus?", "Is #3 within #2?"], "evidence": [[[["Snowboarding-1"]], [["Snow-16"]], [["Venus-2"]], ["operation"]], [[["Snowboarding-1"]], [["Freezing-5"], "no_evidence"], [["Venus-2"]], ["operation"]], [[["Snowboarding-1"]], [["Snow-3"], "no_evidence"], [["Venus-2"]], ["operation"]]], "golden_sentence": [["Snowboarding is a recreational activity and Winter Olympic and Paralympic sport that involves descending a snow-covered slope while standing on a snowboard attached to a rider's feet."], ["These droplets are able to remain liquid at temperatures lower than \u221218\u00a0\u00b0C (0\u00a0\u00b0F), because to freeze, a few molecules in the droplet need to get together by chance to form an arrangement similar to that in an ice lattice."], ["Venus is by far the hottest planet in the Solar System, with a mean surface temperature of 735\u00a0K (462\u00a0\u00b0C; 863\u00a0\u00b0F), even though Mercury is closer to the Sun."]]}, {"qid": "2bdefad4506868999ca0", "term": "Pope Alexander VI", "description": "Pope of the Catholic Church 1492\u20131503", "question": "Were any of despised Pope Alexander VI's descendants canonized?", "answer": true, "facts": ["Pope Alexander the VI was a controversial pope born as Rodrigo Borgia.", "Rodrigo Borgia had several children including the despised Juan Borgia who was murdered in 1497.", "Juan Borgia's grandson, Francis Borgia, was a Jesuit priest and the third Superior General of the Society of Jesus.", "Canonization is the process by which the Catholic Church names someone a saint.", "Francis Borgia was named a Catholic saint in June 1670."], "decomposition": ["What dynastic house was Pope Alexander VI a member of?", "Were any members of #1 canonized?", "Was #2 a direct descendent of Alexander VI?"], "evidence": [[[["Pope Alexander VI-2"]], [["Francis Borgia, 4th Duke of Gand\u00eda-1"]], ["operation"]], [[["Pope Alexander VI-2"]], [["Francis Borgia, 4th Duke of Gand\u00eda-1"]], ["operation"]], [[["House of Borgia-2"]], [["Francis Borgia, 4th Duke of Gand\u00eda-1"]], [["House of Borgia-21"], "operation"]]], "golden_sentence": [["Born into the prominent Borgia family in X\u00e0tiva in the Crown of Aragon (Now Spain), Rodrigo studied law at the University of Bologna."], [""]]}, {"qid": "290311beb8659d2699c5", "term": "Saturn", "description": "Sixth planet from the Sun in the Solar System", "question": "Is Saturn named after king of gods in Greek mythology?", "answer": false, "facts": ["Saturn, the sixth planet from the sun is named after the Roman god Saturn.", "The Roman god Saturn is derived from its Greek equivalent, Kronos.", "The king of the gods in Greek mythology was Zeus.", "Kronos was Zeus's father, and was the leader of the Titans."], "decomposition": ["Who were the king of the gods in Greek mythology?", "Which god was the planet Saturn named after?", "Is #2 the same as any of #1?"], "evidence": [[[["Zeus-1"]], [["Saturn-35"]], ["operation"]], [[["Zeus-1"]], [["Saturn-1"]], ["operation"]], [[["Cronus-1", "Uranus (mythology)-1", "Zeus-1"]], [["Saturn (mythology)-1", "Saturn-1"]], ["operation"]]], "golden_sentence": [["Zeus is the sky and thunder god in ancient Greek religion, who rules as king of the gods of Mount Olympus."], ["The Romans considered the god Saturnus the equivalent of the Greek god Cronus; in modern Greek, the planet retains the name Cronus\u2014\u039a\u03c1\u03cc\u03bd\u03bf\u03c2: Kronos.)"]]}, {"qid": "4b2ae6b6b6b88707ed88", "term": "Abortion", "description": "Intentionally ending pregnancy", "question": "Is there any absolute way to prevent abortion?", "answer": false, "facts": ["In areas where professional medical abortions are illegal, women get unsafe illegal abortions from unlicensed practitioners. ", "Women have successfully aborted their own children through physical or chemical means for centuries."], "decomposition": ["In places where medical abortions are illegal, are women absolutely unable to get abortions?"], "evidence": [[[["Unsafe abortion-2"]]], [[["Abortion-41"]]], [[["Unsafe abortion-1", "Unsafe abortion-2"], "operation"]]], "golden_sentence": [["Most unsafe abortions occur where abortion is illegal, or in developing countries where affordable and well-trained medical practitioners are not readily available, or where modern birth control is unavailable."]]}, {"qid": "f16f1bef934f19b568b2", "term": "Charles Manson", "description": "American criminal, cult leader", "question": "Did any killer Manson band members were named for exceed Charles Manson's kills?", "answer": true, "facts": ["Many of the members of the band Marilyn Manson combined the names of a model or actress with a serial killer.", "Marilyn Manson band memberTwiggy Ramirez took his name from model Twiggy and serial killer Richard Ramirez.", "Richard Ramirez was charged with 13 counts of murder.", "Charles Manson was charged with 9 counts of murder."], "decomposition": ["What serial killers were members of the Manson band named after?", "How many counts of murder was Charles Manson charged with?", "Were the murder charges of any of #1 greater than #2?"], "evidence": [[[["Marilyn Manson (band)-4"]], [["Charles Manson-1"]], [["John Wayne Gacy-4"]]], [[["Charles Manson-1"], "no_evidence"], [["Charles Manson-1"]], ["operation"]], [[["Marilyn Manson (band)-1", "Marilyn Manson (band)-4"]], [["Charles Manson-1"]], [["Ted Bundy-1"], "operation"]]], "golden_sentence": [[""], ["In 1971, he was convicted of first-degree murder and conspiracy to commit murder for the deaths of seven people, including the film actress Sharon Tate."], [""]]}, {"qid": "eee67f6d07003fc5798d", "term": "James Brown", "description": "American singer, songwriter, producer and bandleader from South Carolina", "question": "Could James Brown's ex-wives hold a doubles game of tennis?", "answer": true, "facts": ["James Brown had four ex-wives", "Doubles tennis requires two players per team, with two teams playing against each other"], "decomposition": ["How many people are required for tennis doubles?", "How many ex-wives did James Brown have?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Tennis games-23"]], [["James Brown-62"]], ["operation"]], [[["Types of tennis match-3"]], [["James Brown-62"]], ["operation"]], [[["Tennis-75"]], [["James Brown-62"]], ["operation"]]], "golden_sentence": [["With that set rule, the 2 players play on a complete singles court until one player remains to be designated the winner."], ["Less than a year after Rodriguez died in 1996, Brown hired Tomi Rae Hynie to be a background singer for his band and she later became his fourth wife."]]}, {"qid": "593a06835ec01c23093f", "term": "Mount Sharp", "description": "mountain on Mars", "question": "Do bald eagles nest on Mount Sharp?", "answer": false, "facts": ["Bald eagles are birds found on earth", "Mount Sharp is a mountain on Mars", "To date, no life forms have been detected on Mars"], "decomposition": ["Where is Mount Sharp located?", "Has any form of life ever been discovered on #1?"], "evidence": [[[["Mount Sharp-1"]], [["Mars-4"]]], [[["Mount Sharp-1"]], [["Planetary habitability-64"]]], [[["Mount Sharp-1"]], [["Life on Mars-1"]]]], "golden_sentence": [["Mount Sharp, officially Aeolis Mons (IPA:\u00a0[\u02c8i\u02d0\u0259l\u0268s \u02c8m\u0252nz]), is a mountain on Mars."], [""]]}, {"qid": "3d2029fc860cb4721d3f", "term": "Reality", "description": "Sum or aggregate of all that is real or existent", "question": "Could Plato have agreed with the beliefs of Jainism?", "answer": true, "facts": ["One principle of reality in Jainism is karma, or asrava.", "Jainism began around 500 B.C.", "Plato was born around 428 B.C., so he was alive while Jainism existed.", "Plato believed in karma and reincarnation."], "decomposition": ["What are the major beliefs in Jainism?", "What were Plato's major beliefs?", "When did Jainism begin?", "When was Plato born?", "Is there an overlap between #1 and #2, and is #4 more recent than #3?"], "evidence": [[[["Jainism-1"]], [["Plato-43"]], [["Jainism-1"]], [["Plato-1"]], ["operation"]], [[["Jainism-4"], "no_evidence"], [["Plato-3"], "no_evidence"], [["Jainism-61"]], [["Plato-1"]], ["operation"]], [[["Jainism-2"]], [["Plato-3", "Plato-39", "Plato-43"], "no_evidence"], [["Jainism-1"]], [["Plato-1"]], ["no_evidence", "operation"]]], "golden_sentence": [["Jains believe that Jainism is an eternal dharma with the tirthankaras guiding every cycle of Jain cosmology."], ["Plato advocates a belief in the immortality of the soul, and several dialogues end with long speeches imagining the afterlife."], [""], ["Plato (/\u02c8ple\u026ato\u028a/ PLAY-toe; Greek: \u03a0\u03bb\u03ac\u03c4\u03c9\u03bd Pl\u00e1t\u014dn, pronounced\u00a0[pl\u00e1.t\u0254\u02d0n] in Classical Attic; 428/427 or 424/423 \u2013 348/347 BC) was an Athenian philosopher during the Classical period in Ancient Greece, founder of the Platonist school of thought, and the Academy, the first institution of higher learning in the Western world."]]}, {"qid": "ee18be33a6e089e3a2ae", "term": "Wolverine", "description": "Species of the family Mustelidae", "question": "Would a Wolverine and a Lynx be hard to tell apart?", "answer": false, "facts": ["Wolverines have rounded ears and a bear-like appearance.", "Lynxes have a feline body with pointed ears."], "decomposition": ["What are the physical characteristics of wolverines?", "What are the physical characteristics of lynxes?", "Is there any significant overlap between #1 and #2?"], "evidence": [[[["Wolverine-6"]], [["Lynx-3"]], ["operation"]], [[["Wolverine-6"]], [["Lynx-4"]], [["Lynx-4", "Wolverine-6"], "operation"]], [[["Wolverine-6"]], [["Lynx-3", "Lynx-4"]], ["operation"]]], "golden_sentence": [["Anatomically, the wolverine is a stocky and muscular animal."], ["Lynx have a short tail, characteristic tufts of black hair on the tips of their ears, large, padded paws for walking on snow and long whiskers on the face."]]}, {"qid": "45d813856a79583f63df", "term": "Water skiing", "description": "surface water sport", "question": "Can you go water skiing on Venus?", "answer": false, "facts": ["Water skiing requires sufficient area on a smooth stretch of water, one or two skis, a tow boat with tow rope, two or three people, and a personal flotation device.", "Venus has a mean surface temperature of 863 \u00b0F.", "There may have been substantial quantities of liquid water on the surface of Venus at one point, but after a period of 600 million to several billion years, a runaway greenhouse effect was caused by the evaporation of that original water."], "decomposition": ["What is the basic requirement for water skiing?", "Is #1 present on Venus in sufficient quantities?"], "evidence": [[[["Water skiing-1"]], [["Venus-20"], "operation"]], [[["Water skiing-1"]], [["Venus-2"], "operation"]], [[["Water skiing-1"]], [["Venus-2"]]]], "golden_sentence": [["The sport requires sufficient area on a smooth stretch of water, one or two skis, a tow boat with tow rope, three people (depending on state boating laws), and a personal flotation device."], [""]]}, {"qid": "a6191318469ed26eff27", "term": "Eagle", "description": "large carnivore bird", "question": "Can shooting bald eagle get a person more prison time than Michael Vick?", "answer": true, "facts": ["Michael Vick spent 21 months in prison for an illegal dog fighting ring.", "Shooting a bald eagle carries a penalty of up to two years in prison for a second conviction."], "decomposition": ["How long of a penalty is it for shooting a bald eagle?", "How many months did Michael Vick serve in prison?", "Is #1 longer than #2?"], "evidence": [[[["Bald and Golden Eagle Protection Act-18"]], [["Michael Vick-2"]], ["operation"]], [[["Bald and Golden Eagle Protection Act-4"]], [["Michael Vick-2"]], ["no_evidence", "operation"]], [[["Bald and Golden Eagle Protection Act-18"]], [["Michael Vick-2"]], ["operation"]]], "golden_sentence": [["The criminal penalty stipulation was increased from a maximum fine of $500 and six months imprisonment to a maximum fine of $5,000 and one year's imprisonment."], ["Vick's NFL career came to a halt in 2007 after he pleaded guilty for his involvement in a dog fighting ring and spent 21 months in federal prison."]]}, {"qid": "6fac55de9a0ea9993fa0", "term": "Popular science", "description": "Interpretation of science intended for a general audience", "question": "Is \"A Tale of Two Cities\" a popular science novel?", "answer": false, "facts": ["\"A Tale of Two Cities\" is a historical fiction novel.", "Popular science books focus on scientific facts presented to a mainstream audience.", "Fiction is not fact."], "decomposition": ["What genre is the novel 'A Tale of Two Cities' classified as?", "Is #1 based on scientific facts?"], "evidence": [[[["A Tale of Two Cities-1"]], ["operation"]], [[["A Tale of Two Cities-1"]], ["operation"]], [[["A Tale of Two Cities-1"]], [["Historical fiction-3"]]]], "golden_sentence": [["A Tale of Two Cities is an 1859 historical novel by Charles Dickens, set in London and Paris before and during the French Revolution."]]}, {"qid": "edc3c1ee0c0decb12cdb", "term": "Western honey bee", "description": "Species of insect", "question": "Would Topa Inca Yupanqui have encountered the western honey bee?", "answer": false, "facts": ["Topa Inca Yupanqui was an Inca ruler in the 15th century", "Western honey bees were first introduced to the Americas in the 16th century"], "decomposition": ["When century was Topa Inca Yupanqui alive?", "When were western honey bees first introduced to The Americas", "Was #1 before #2?"], "evidence": [[[["Topa Inca Yupanqui-1"]], [["Western honey bee-5"]], ["operation"]], [[["Topa Inca Yupanqui-1"]], [["Western honey bee-5"]], ["operation"]], [[["Topa Inca Yupanqui-1"]], [["Western honey bee-5"]], ["operation"]]], "golden_sentence": [[""], ["Humans are responsible for its considerable additional range, introducing European subspecies into North America (early 1600s), South America, Australia, New Zealand, and eastern Asia."]]}, {"qid": "eb34d15122ae38b51757", "term": "Middle Ages", "description": "Period of European history from the 5th to the 15th century", "question": "Did eggs need to be kept cold in the middle ages?", "answer": false, "facts": ["When eggs are freshly laid, they are covered in a film called a 'bloom.' ", "Eggs with their bloom intact are able to stay at room temperature for one month.", "Pasteurization destroys the bloom on eggs. ", "Pasteurization was introduced in the 1990's."], "decomposition": ["What naturally protects eggs from spoiling?", "What process removes #1 from eggs?", "Did #2 exist during the Middle Ages?"], "evidence": [[[["Egg as food-35"]], [["Egg as food-34"]], [["Middle Ages-1", "Refrigeration-6"], "operation"]], [[["Egg as food-34"], "no_evidence"], [["Egg as food-35"]], [["Refrigeration-9"], "operation"]], [[["Egg as food-35"], "no_evidence"], [["Egg as food-34"]], ["operation"]]], "golden_sentence": [[""], [""], ["", ""]]}, {"qid": "6493958775c18bcbcaa7", "term": "Ginger", "description": "Species of plant", "question": "Could the Port of Baltimore handle the entire world's cargo production of ginger each year?", "answer": true, "facts": ["In 2018, the world production of ginger was 2.8 million tons.", "The Port of Baltimore handles about 2.8 million tons of cargo per fiscal quarter. ", "A fiscal quarter is shorter than a year."], "decomposition": ["How much cargo does the Port of Baltimore handle each fiscal quarter?", "How much ginger cargo is produced each year?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Port of Baltimore-19"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Port of Baltimore-18"]], [["Ginger-8"]], ["operation"]], [[["Port of Baltimore-19"]], [["Ginger-8"]], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "00450e9708370dc2d0b3", "term": "Cream", "description": "Dairy product", "question": "Does store bought milk have cream at the top?", "answer": false, "facts": ["When milk is non-homogenized, the cream will separate and rise to the top.", "Most store bought milk is homogenized. "], "decomposition": ["What processes does store-bought milk go through?", "What are the characteristics of milk that is treated with #1?", "Is \"cream on the top\" a characteristic listed in #2?"], "evidence": [[[["Pasteurization-1"]], [["Pasteurization-11"], "no_evidence"], ["operation"]], [[["Milk-55", "Milk-57", "Milk-59"]], [["Cream-1", "Milk-61"]], [["Cream-1"]]], [[["Milk-61"]], [["Milk-63"]], ["operation"]]], "golden_sentence": [[""], ["low-temperature, slow heating at 60\u00a0\u00b0C (140\u00a0\u00b0F) for 20 minutes \u2013 for the pasteurization of milk while at the United States Marine Hospital Service, notably in his publication of The Milk Question (1912)."]]}, {"qid": "3b0e273e2cb63fba8a4d", "term": "Watermelon", "description": "A large fruit with a smooth hard rind, of the gourd family", "question": "Are more watermelons grown in Brazil than Antarctica?", "answer": true, "facts": ["Watermelons are plants grown in climates from tropical to temperate, needing temperatures higher than about 25 \u00b0C (77 \u00b0F) to thrive.", "The climate of Antarctica is the coldest on Earth.", "The climate of Brazil comprises a wide range of weather conditions across a large area and varied topography, but most of the country is tropical."], "decomposition": ["Which climate is suitable for the cultivation of watermelon?", "What are the prevalent climatic conditions in Brazil?", "What are the prevalent climatic conditions in Antarctica?", "Is #2 more similar to #1 than #3?"], "evidence": [[[["Watermelon-2"]], [["Brazil-47"]], [["Antarctica-42"]], ["operation"]], [[["Watermelon-15"]], [["Climate of Brazil-5"]], [["Antarctica-42"]], ["operation"]], [[["Watermelon-2"]], [["Brazil-47"]], [["Antarctica-42"]], ["operation"]]], "golden_sentence": [["Watermelon is grown in favorable climates from tropical to temperate regions worldwide for its large edible fruit, which is a berry with a hard rind and no internal divisions, and is botanically called a pepo."], ["The climate of Brazil comprises a wide range of weather conditions across a large area and varied topography, but most of the country is tropical."], [""]]}, {"qid": "ac5ec2c2ee240c9601e9", "term": "The Tonight Show Starring Jimmy Fallon", "description": "American late-night talk show", "question": "On August 20, 2020,  does The Tonight Show Starring Jimmy Fallon air after moonset EST?", "answer": true, "facts": ["On August 20th, The Tonight Show Starring Jimmy Fallon airs at 11:35PM", "On August 20th, the moon on the east coast of the USA will set around 9PM"], "decomposition": ["The Tonight Show Starring Jimmy Fallon airs at 11:35 p.m. ET/PT.", "On August 20th, the moon on the east coast of the USA  set 9PM", "Does #1 occur after #2?"], "evidence": [[[["The Tonight Show Starring Jimmy Fallon-2"]], [["Moonlight-1", "Sunset-1"], "no_evidence"], ["operation"]], [[["The Tonight Show Starring Jimmy Fallon-2"]], [["Lunar phase-16"]], [["Lunar phase-16", "The Tonight Show Starring Jimmy Fallon-2"], "no_evidence"]], [[["The Tonight Show-30"], "no_evidence"], [["Moonrise-1"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["The program airs weeknights at 11:35 p.m. ET/PT."], ["Moonlight consists of mostly sunlight (with little earthlight) reflected from the parts of the Moon's surface where the Sun's light strikes.", ""]]}, {"qid": "52516eaf11ee836ec945", "term": "Paparazzi", "description": "profession", "question": "Were paparazzi directly responsible for the death of Amy Winehouse?", "answer": false, "facts": ["Amy Winehouse died at home and was found in her home by her bodyguard.", "Amy Winehouse's cause of death was alcohol poisoning. "], "decomposition": ["What was the cause of Amy Winehouse's death?", "Did paparazzi have a direct involvement in #1?"], "evidence": [[[["Amy Winehouse-92", "Amy Winehouse-94"]], [["Amy Winehouse-90", "Amy Winehouse-94"]]], [[["Amy Winehouse-92"]], [["Amy Winehouse-92"]]], [[["Amy Winehouse-92"]], ["operation"]]], "golden_sentence": [["", "In a late June 2013 interview, Alex Winehouse revealed his belief that his sister's eating disorder, and the consequent physical weakness, was the primary cause of her death:"], ["", ""]]}, {"qid": "221319be235a364097d2", "term": "Alexander Graham Bell", "description": "scientist and inventor known for his work on the telephone", "question": "Did the phone Alexander Graham Bell use have call waiting?", "answer": false, "facts": ["Call waiting was invented in the 1970's to allow phone users to suspend one call to accept another.", "Alexander Graham Bell's phone was used in 1876."], "decomposition": ["When was call waiting service introduced?", "When was Alexander Graham Bell's phone used?", "Is #1 before #2?"], "evidence": [[[["Call waiting-9"]], [["Alexander Graham Bell-29"]], ["operation"]], [[["Call waiting-9"]], [["Alexander Graham Bell-2"]], [["Alexander Graham Bell-2", "Call waiting-9"], "operation"]], [[["Call waiting-9"]], [["Alexander Graham Bell-2"]], ["operation"]]], "golden_sentence": [["Call waiting was introduced to North America in the early 1970s when the first generation of electronic switch machines built by Western Electric, Electronic Signaling System 1 started to replace older mechanical equipment in the old Bell System local telephone companies."], ["In 1875, Bell developed an acoustic telegraph and drew up a patent application for it."]]}, {"qid": "afb68010abacc2bae1de", "term": "Zoology", "description": "Study of the animal kingdom", "question": "Is zoology unconcerned with strigoi?", "answer": true, "facts": ["Zoology is the study of the behavior and classification of animals.", "Strigoi are spirits that can transform into animals in Romanian mythology.", "Zoology is based on science and fossils."], "decomposition": ["What does the study of zoology entail?", "What kind of creatures are the strigoi?", "Is #2 unrelated to #1"], "evidence": [[[["Zoology-3"]], [["Strigoi-5"]], [["Strigoi-5", "Zoology-3"], "operation"]], [[["Zoology-1"]], [["Strigoi-1"]], ["operation"]], [[["Zoology-1"]], [["Strigoi-1"]], ["operation"]]], "golden_sentence": [[""], [""], ["", ""]]}, {"qid": "b87c5318ee8519478444", "term": "Kidney", "description": "internal organ in most animals, including vertebrates and some invertebrates", "question": "Is it safe to eat kidney?", "answer": true, "facts": ["Traditional British cuisine includes \"Kidney Pie\", or \"Steak and Kidney Pie\".", "Kidney Pie contains kidney."], "decomposition": ["What are the various kinds of meat safe for human consumption?", "Is kidney included in #1?"], "evidence": [[[["Offal-95"]], ["operation"]], [[["Meat-1"]], ["no_evidence"]], [[["Meat-17"], "operation"], ["no_evidence"]]], "golden_sentence": [[""]]}, {"qid": "486fc545411cba3ecfd4", "term": "Gorillaz", "description": "British virtual band", "question": "Has Gorillaz creator been in more bands than Bernard Sumner?", "answer": true, "facts": ["Gorillaz was created by Damon Albarn.", "Damon Albarn has been in five bands: Gorillaz, Blur, The Good, the Bad & the Queen, Elastica, and DRC Music.", "Bernard Sumner has been in three bands: New Order, Joy Division, and Electronic, Bad Lieutenant."], "decomposition": ["Who was the primary creator of Gorillaz?", "How many bands has #1 been a member of?", "How many bands has Bernard Sumner been a member of?", "Is #2 greater than #3?"], "evidence": [[[["Damon Albarn-1"]], [["Damon Albarn-1", "Damon Albarn-3"]], [["Bernard Sumner-1", "Bernard Sumner-2", "Bernard Sumner-6"]], ["operation"]], [[["Gorillaz-1"]], [["Blur (band)-1", "Gorillaz-1", "Rocket Juice & the Moon-1", "The Good, the Bad & the Queen-1"]], [["Bad Lieutenant (band)-1", "Bernard Sumner-4", "Electronic (band)-1", "Joy Division-1"]], ["operation"]], [[["Damon Albarn-1"]], [["Damon Albarn-1", "Damon Albarn-3"]], [["Bernard Sumner-3", "Bernard Sumner-4", "Bernard Sumner-5", "Bernard Sumner-6"]], ["operation"]]], "golden_sentence": [["Damon Albarn OBE (/\u02c8de\u026am\u0259n \u02c8\u00e6lb\u0251\u02d0rn/; born 23 March 1968) is an English musician, singer, songwriter, multi-instrumentalist, and record producer, best known as the lead singer, multi-instrumentalist, and lyricist of the rock band Blur and as the co-founder, lead vocalist, instrumentalist, and songwriter of the virtual band Gorillaz."], ["", ""], ["", "", ""]]}, {"qid": "2e993f5760998b5a3f9a", "term": "The Hague", "description": "City and municipality in South Holland, Netherlands", "question": "Does Abdulqawi Yusuf go to the Hague on a typical work day?", "answer": true, "facts": ["Abdulqawi Yusuf is the current president of the International Court of Justice", "The International Court of Justice is headquartered in The Hague"], "decomposition": ["What organization does Abdulqawi Yusuf's work for?", "Where is #1 headquartered?"], "evidence": [[[["Abdulqawi Yusuf-1"]], [["International Court of Justice-3"], "operation"]], [[["Abdulqawi Yusuf-1"]], [["International Court of Justice-3"]]], [[["Abdulqawi Yusuf-1"]], [["United Nations-1"]]]], "golden_sentence": [["Abdulqawi Ahmed Yusuf (Somali: Abdulqaawi Ahmed Yuusuf) is a Somali lawyer and judge who is the current President of the International Court of Justice."], ["The court is seated in the Peace Palace in The Hague, Netherlands, making it the only principal UN organ not located in New York City."]]}, {"qid": "189064bd44308ad2fde0", "term": "Snow White", "description": "fairy tale", "question": "Are Disney's seven dwarves the original ones?", "answer": false, "facts": ["In the original fairy tale, the dwarves were unnamed, but first named in a 1912 stage version: Blick, Flick, Glick, Snick, Plick, Whick, and Quee.", "In Disney's version, the dwarves are named Happy, Sleepy, Sneezy, Grumpy, Dopey, Bashful, and Doc."], "decomposition": ["What were the original names of the seven dwarfs?", "What are the names of the seven dwarfs in Disney films?", "Is #1 identical to #2?"], "evidence": [[[["Seven Dwarfs-6"]], [["Snow White and the Seven Dwarfs (1937 film)-7"]], ["operation"]], [["no_evidence"], [["Snow White and the Seven Dwarfs (1937 film)-7"]], ["no_evidence", "operation"]], [[["Seven Dwarfs-6"]], [["Snow White and the Seven Dwarfs (1937 film)-7"]], [["Seven Dwarfs-6"], "operation"]]], "golden_sentence": [["1912 play Blick, Flick, Glick, Plick, Quee, Snick, Whick 1937 animated Disney film, The 7D (2014\u20132016), Descendants (2015\u2013) Dopey, Doc, Bashful, Happy, Grumpy, Sleepy, Sneezy Schneewittchen (1961) Huckepack, Naseweis, Packe, Pick, Puck, Purzelbaum, Rumpelbold Mr. Magoo's Little Snow White (1965) Axlerod, Bartholomew, Cornelius, Dexter, Eustace, Ferdinand, George Faerie Tale Theatre (1984) Bertram, Bubba, Barnaby, Bernard, Boniface, Bruno, Baldwin 1987 film Biddy, Diddy, Fiddy, Giddy, Iddy, Kiddy, Liddy Grimm's Fairy Tale Classics (1988) Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday Happily Ever After (1993) Blossom, Critterina, Marina, Moonbeam, Muddy, Sunburn, Thunderella The Legend of Snow White (1994) Boss, Gourmet, Woody, Goldie, Chamomile, Vet, Jolly Snow White (1995) Sunbeam, Cricket, Fawn, Hedgehog, Toadstool, Robin, Tadpole Happily Ever After: Fairy Tales for Every Child Bright Silver, Sharp Flint, Fools Gold, Smelly Sulfur, Heavy Metal, Rough Copper, Hard Jade 2001 film Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday Sydney White (2007) Terrence, Gurkin, Spanky, Embele, Jeremy, Lenny, George Schneewittchen (2009) Gorm, Knirps, Niffel, Quarx, Querx, Schrat, Wichtel My Fair Godmother (2009) Reginald, Percival, Cedric, Edgar, Cuthbert, Ethelred, Edwin Once Upon a Time (2011\u20132018) Doc, Grumpy/Dreamy (Leroy), Happy, Sleepy (Walter), Bashful, Sneezy (Tom Clark), Dopey, Stealthy Mirror Mirror (2012) Butcher, Will Grimm, Half Pint, Napoleon, Grub, Chuck/Chuckles, Wolf Snow White and the Huntsman (2012) Beith, Coll, Duir, Gort, Muir, Nion, Quert, Gus."], ["In reality, the cottage belongs to seven adult dwarfs\u2014named Doc, Grumpy, Happy, Sleepy, Bashful, Sneezy, and Dopey\u2014who work in a nearby mine."]]}, {"qid": "97b004f4bb227797b927", "term": "Guam", "description": "Island territory of the United States of America", "question": "Does Guam have a state capital?", "answer": false, "facts": ["Guam is not a state.", "Only states can have a state capital."], "decomposition": ["Is Guam a country or state?", "Does #1 have state capitals?"], "evidence": [[[["Guam-1"]], [["Hag\u00e5t\u00f1a, Guam-1"]]], [[["Guam-1"], "no_evidence"], [["Guam-1"]]], [[["Guam-1"]], [["Hag\u00e5t\u00f1a, Guam-1"], "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "371e1050826a0d53648a", "term": "Heracles", "description": "divine hero in Greek mythology, son of Zeus and Alcmene", "question": "Did Heracles famous labors exceed a baker's dozen?", "answer": false, "facts": ["Heracles had twelve labors he was tasked with such as cleaning the Augean stables and killing the nine-headed Lernaean Hydra.", "A baker's dozen is a term used in cooking that refers to 13 of anything."], "decomposition": ["How many labors was Heracles made to carry out?", "How many is a baker's dozen?", "Is #1 greater than #2?"], "evidence": [[[["Labours of Hercules-1"]], [["Dozen-7"]], ["operation"]], [[["Labours of Hercules-1"]], [["Dozen-7"]], ["operation"]], [[["Heracles-21"]], [["Dozen-7"]], ["operation"]]], "golden_sentence": [["The Twelve Labours of Heracles or Hercules (Greek: \u03bf\u1f31 \u1f29\u03c1\u03b1\u03ba\u03bb\u03ad\u03bf\u03c5\u03c2 \u1f06\u03b8\u03bb\u03bf\u03b9, hoi H\u0113rakl\u00e9ous \u00e2thloi) are a series of episodes concerning a penance carried out by Heracles, the greatest of the Greek heroes, whose name was later romanised as Hercules."], ["The broadest use of baker's dozen today is simply a group of thirteen objects (often baked goods)."]]}, {"qid": "5494bd034fe75f2687c2", "term": "Great Wall of China", "description": "wall along the historical northern borders of China", "question": "Could the Great Wall of China connect the Dodgers to the White Sox?", "answer": true, "facts": ["The Dodgers are a baseball team located in Los Angeles", "The White Sox are a baseball team located in Chicago", "The distance between Los Angeles and Chicago is 2,015 miles", "The length of the main section of the Great Wall of China is 2,145 miles"], "decomposition": ["Where are the Dodgers located?", "Where are the White Sox located?", "What is the distance between #1 and #2?", "How long is The Great Wall of China?", "Is #4 larger than #3?"], "evidence": [[[["Los Angeles Dodgers-1"]], [["Chicago White Sox-1"]], [["Southwest Chief-1"]], [["Great Wall of China-3"]], ["operation"]], [[["Los Angeles Dodgers-1"]], [["Chicago White Sox-1"]], ["no_evidence"], [["Great Wall of China-3"]], ["no_evidence", "operation"]], [[["Los Angeles Dodgers-1"]], [["Chicago White Sox-1"]], [["Southwest Chief-1"]], [["Ming Great Wall-1"]], ["operation"]]], "golden_sentence": [["The Los Angeles Dodgers are an American professional baseball team based in Los Angeles, California."], ["The Chicago White Sox are an American professional baseball team based in Chicago, Illinois."], ["The Southwest Chief (formerly the Southwest Limited and Super Chief) is a passenger train operated by Amtrak on a 2,265-mile (3,645\u00a0km) route through the Midwestern and Southwestern United States."], ["A comprehensive archaeological survey, using advanced technologies, has concluded that the walls built by the Ming dynasty measure 8,850\u00a0km (5,500\u00a0mi)."]]}, {"qid": "441394a7c0c2e93fbdfc", "term": "Halloween", "description": "Holiday celebrated October 31", "question": "If a baby was born on Halloween would they be a Scorpio?", "answer": true, "facts": ["Halloween is a holiday where people dress up and happens on October 31 each year.", "The zodiac sign of Scorpio encompasses the dates from October 23 to November 22."], "decomposition": ["On what date does Halloween occur each year?", "What dates are included in the Zodiac sign of Scorpio?", "Does #1 fall in the date span listed in #2?"], "evidence": [[[["Halloween-1"]], [["Scorpio (astrology)-1"]], ["operation"]], [[["Halloween-1"]], [["Scorpio (astrology)-1"]], ["operation"]], [[["Halloween-12"], "no_evidence"], [["Zodiac-31"], "no_evidence"], ["operation"]]], "golden_sentence": [["Halloween or Hallowe'en (a contraction of Hallows' Even or Hallows' Evening), also known as Allhalloween, All Hallows' Eve, or All Saints' Eve, is a celebration observed in many countries on 31 October, the eve of the Western Christian feast of All Hallows' Day."], ["Under the sidereal zodiac (most commonly used in Hindu astrology), the Sun is in Scorpio from approximately November 16 to December 15."]]}, {"qid": "73df1ad4806617c6e6c6", "term": "Bottlenose dolphin", "description": "genus of dolphin", "question": "Can bottlenose dolphins hypothetically outbreed human women?", "answer": false, "facts": ["Bottlenose dolphins have a gestation period of 12 months.", "Human women have a gestation period around 9 months."], "decomposition": ["What is the gestation period of bottlenose dolphins?", "What is the gestation period of humans?", "Is #1 lower than #2?"], "evidence": [[[["Bottlenose dolphin-42"]], [["Gestation-5"]], ["operation"]], [[["Bottlenose dolphin-42"]], [["Human-55"]], ["operation"]], [[["Bottlenose dolphin-42"]], [["Pregnancy-1"]], ["operation"]]], "golden_sentence": [["The gestation period averages 12 months."], ["Birth normally occurs at a gestational age of about 40 weeks, though it is common for births to occur from 37 to 42 weeks."]]}, {"qid": "d7e40b563c9c59997f48", "term": "Johnny Carson", "description": "American talk show host and comedian", "question": "Did Johnny Carson win enough Emmy's to fill a carton if Emmy's were eggs?", "answer": false, "facts": ["There are 12 eggs in a carton.", "Johnny Carson won 6 Emmys.", "6 is less than 12."], "decomposition": ["How many eggs can fit in a standard egg carton?", "How many Emmy Awards did Johnny Carson win?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Egg carton-8"]], [["Johnny Carson-1"]], ["operation"]], [[["Egg carton-8"]], [["Johnny Carson-1"]], ["operation"]], [[["Egg as food-11"]], [["Johnny Carson-1"]], ["operation"]]], "golden_sentence": [["Standard egg cartons have room for 10 or 12 eggs, but they can come in a variety of sizes, holding from three to 24 eggs."], ["Carson received six Emmy Awards, the Television Academy's 1980 Governor's Award, and a 1985 Peabody Award."]]}, {"qid": "1cd8ac17e2558d438100", "term": "Mercedes-Benz", "description": "automobile brand of Daimler AG", "question": "Was Mercedes-Benz associated with the Nazis?", "answer": true, "facts": ["During the 1930s, Mercedes-Benz produced the 770 model.", "The 770 was popular with Nazis, and Adolf Hitler used them as his personal vehicle."], "decomposition": ["Which Mercedes-Benz model was made during the 1930s?", "Was #1 popular among the Nazis?"], "evidence": [[[["Mercedes-Benz-6"]], ["operation"]], [[["Mercedes-Benz 770-1"]], [["Mercedes-Benz 770-1"]]], [[["Mercedes-Benz 770-6"]], [["Mercedes-Benz-6"], "operation"]]], "golden_sentence": [["Throughout the 1930s, Mercedes-Benz produced the 770 model, a car that was popular during Germany's Nazi period."]]}, {"qid": "86bebdbb554d72683123", "term": "Buzz Aldrin", "description": "American astronaut; second person to walk on the Moon", "question": "Could Buzz Aldrin have owned a computer?", "answer": true, "facts": ["Buzz Aldrin was born in 1930 and is still alive in 2020. ", "Home computers were first available for sale in 1977. "], "decomposition": ["When were personal computers made available to the public?", "When was Buzz Aldrin born?", "Is #2 well before #1?"], "evidence": [[[["Personal computer-10"]], [["Buzz Aldrin-1"]], [["Buzz Aldrin-1"], "operation"]], [[["Personal computer-15"]], [["Buzz Aldrin-1"]], ["operation"]], [[["Personal computer-7"]], [["Buzz Aldrin-1"]], ["operation"]]], "golden_sentence": [["It was built starting in 1972, and a few hundred units were sold."], ["Buzz Aldrin (/\u02c8\u0254\u02d0ldr\u026an/; born Edwin Eugene Aldrin Jr.; January 20, 1930) is an American engineer, former astronaut and fighter pilot."], [""]]}, {"qid": "96819553e068db04fd99", "term": "Force", "description": "Any action that tends to maintain or alter the motion of an object", "question": "Can a cheetah generate enough force to topple Big Show?", "answer": true, "facts": ["Big Show is a professional wrestler that weighs 383 pounds.", "Force is equal to mass times acceleration.", "An adult Cheetah weighs around 160 pounds.", "An adult Cheetah can run up to 58 MPH."], "decomposition": ["How much does Big Show weigh?", "How much does a cheetah weigh?", "How fast can a cheetah run?", "Is the force produced by a mass of #2 and a speed of #3 enough to knock over something that weighs #1?"], "evidence": [[["no_evidence"], [["Cheetah-1"]], [["Cheetah-1"]], ["operation"]], [[["Big Show-4"], "no_evidence"], [["Cheetah-1"]], [["Cheetah-1"]], [["Acceleration-13"], "operation"]], [[["Big Show-4"], "no_evidence"], [["Cheetah-1"]], [["Cheetah-15"]], ["operation"]]], "golden_sentence": [["Adults typically weigh between 20 and 65\u00a0kg (44 and 143\u00a0lb)."], ["It is the fastest land animal, capable of running at 80 to 128\u00a0km/h (50 to 80\u00a0mph), and as such has several adaptations for speed, including a light build, long thin legs and a long tail."]]}, {"qid": "a228973e0f8f76f0414d", "term": "Elijah", "description": "Biblical prophet", "question": "Is Elijah part of a Jewish holiday?", "answer": true, "facts": ["The Jewish holiday Passover involves a traditional ceremonial dinner.", "During the ceremony, it is customary to fill an extra cup with wine and put it at the center of the table.", "The door is then opened so the prophet Elijah can visit."], "decomposition": ["How is Elijah venerated according to Jewish custom?", "Does #1 include venerating Elijah at a holiday?"], "evidence": [[[["Elijah-49"], "no_evidence"], [["Elijah-43", "Elijah-44"], "operation"]], [[["Elijah-49"]], ["no_evidence"]], [[["Passover Seder-62"]], ["operation"]]], "golden_sentence": [[""], ["", ""]]}, {"qid": "e4339612a48fb7599675", "term": "J. Edgar Hoover", "description": "American law enforcement officer and first director of the FBI", "question": "Did J. Edgar Hoover take his calls in Langley, Virginia?", "answer": false, "facts": ["J. Edgar Hoover was the director of the FBI", "The FBI is headquartered in Washington, D.C.", "Langley, Virginia is the headquarters of the C.I.A."], "decomposition": ["What government agency was J. Edgar Hoover the head of?", "Where are the headquarters of #1?", "Is #2 in Langley, Virginia?"], "evidence": [[[["J. Edgar Hoover-1"]], [["Federal Bureau of Investigation-4"]], [["Federal Bureau of Investigation-4"], "operation"]], [[["J. Edgar Hoover-1"]], [["J. Edgar Hoover Building-1"]], [["J. Edgar Hoover Building-1"]]], [[["J. Edgar Hoover-1"]], [["Federal Bureau of Investigation-4"]], ["operation"]]], "golden_sentence": [["John Edgar Hoover (January 1, 1895\u00a0\u2013 May 2, 1972) was the first Director of the Federal Bureau of Investigation (FBI) of the United States and an American law enforcement administrator."], ["The FBI headquarters is the J. Edgar Hoover Building, located in Washington, D.C."], [""]]}, {"qid": "ac4c9d21cda72c255118", "term": "Kangaroo", "description": "\u0441ommon name of family of marsupials", "question": "Does a kangaroo incubate its offspring?", "answer": false, "facts": ["Incubation is the process of hatching offspring from eggs", "Kangaroos are mammals", "Mammals give birth to live offspring"], "decomposition": ["Incubation is required for what method of embryonic development?", "What infraclass do kangaroos belong to?", "What method of embryonic development do #2 employ?", "Is #1 the same as #3?"], "evidence": [[[["Egg incubation-1"]], [["Kangaroo-1"]], [["Marsupial-15"]], ["operation"]], [[["Incubator (culture)-1"]], [["Kangaroo-1"]], [["Mammalian reproduction-3"]], ["operation"]], [[["Egg incubation-1"]], [["Marsupial-1"]], [["Kangaroo-34", "Mammalian reproduction-11"]], ["operation"]]], "golden_sentence": [["Incubation is the process by which certain oviparous (egg-laying) animals hatch their eggs; it also refers to the development of the embryo within the egg under favorable environmental condition."], [""], [""]]}, {"qid": "0595b6ac43aa2268fac9", "term": "Basil", "description": "species of plant", "question": "Is basil safe from Hypervitaminosis D?", "answer": true, "facts": ["Hypervitaminosis D is a rare disease caused by having too much vitamin D.", "Basil contains many vitamins including Vitamin A, B, C, E, and K."], "decomposition": ["Hypervitaminosis D is caused by eating too much of what vitamin?", "Does basil contain #1?", "Is it safe to avoid #1 by eating #2?"], "evidence": [[[["Hypervitaminosis D-1"]], ["no_evidence"], ["operation"]], [[["Hypervitaminosis D-1"]], [["Vitamin D-52"]], ["operation"]], [[["Hypervitaminosis D-1"]], ["no_evidence"], ["operation"]]], "golden_sentence": [["Hypervitaminosis D is a state of vitamin D toxicity."]]}, {"qid": "b697e6bd0f828941991f", "term": "Garfield", "description": "Comic strip created by Jim Davis", "question": "Is Garfield known for hating italian cuisine?", "answer": false, "facts": ["Garfield is well known for loving lasagna.", "Lasagna is a traditional Italian dish."], "decomposition": ["What food is Garfield known for loving?", "What country does #1 come from?", "Is #2 where Italian cuisine comes from?"], "evidence": [[[["Garfield (character)-2"]], [["Lasagne-4"]], [["Lasagne-2"]]], [[["Garfield-2"]], [["Lasagne-3"]], [["Italian cuisine-1"]]], [[["Garfield (character)-1"]], [["Garfield (character)-2"], "no_evidence"], ["operation"]]], "golden_sentence": [["As a kitten, he develops a taste for lasagna, which would become his favorite food."], ["Lasagne al forno, layered with a thicker rag\u00f9 and B\u00e9chamel sauce, and corresponding to the most common version of the dish outside Italy, are traditionally associated with the Emilia-Romagna region of Italy."], [""]]}, {"qid": "e6b6560bef62b5d1aa0a", "term": "Northern Mariana Islands", "description": "American-dependent insular area in the western Pacific", "question": "Is Mark Cuban able to visit Northern Mariana Islands without a passport?", "answer": true, "facts": ["Citizens of the United States can visit Northern Mariana Islands without a passport.", "Mark Cuban is an American citizen."], "decomposition": ["Citizens of what countries can visit the Northern Mariana Islands without a passport?", "What country is Mark Cuban a citizen of?", "Is #2 included in #1?"], "evidence": [[[["Northern Mariana Islands-1"], "no_evidence"], [["Brian Cuban-2"], "no_evidence"], ["operation"]], [[["Northern Mariana Islands-50"], "no_evidence"], [["Mark Cuban-3"]], ["no_evidence", "operation"]], [[["Northern Mariana Islands-1"]], [["Mark Cuban-1"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "6678470f7135a2456ff2", "term": "Apollo 13", "description": "A failed crewed mission to land on the Moon", "question": "Were deaths from Apollo 13 mission eclipsed by other space missions?", "answer": true, "facts": ["Apollo 13 was a failed Moon landing in which the crew had faulty oxygen resources.", "All of the Apollo 13 crew returned safely to earth.", "The Challenger was a space shuttle that malfunctioned, killing all seven passengers.", "The Space Shuttle Columbia disaster had seven casualties."], "decomposition": ["How many astronauts died on the Apollo 13 mission?", "How many astronauts died during the launch of the Challenger space shuttle?", "How many astronauts died aboard the Columbia space shuttle?", "Are #2 and #3 both greater than #1?"], "evidence": [[[["Apollo 13-3"]], [["Space Shuttle Challenger disaster-1"]], [["Space Shuttle Columbia disaster-1"]], ["operation"]], [[["Apollo 13-55"]], [["Space Shuttle Challenger disaster-1"]], [["Space Shuttle Columbia disaster-1"]], ["operation"]], [[["Apollo 13-52"]], [["Space Shuttle Challenger-1"]], [["Space Shuttle Columbia-24"]], ["operation"]]], "golden_sentence": [[""], ["The Space Shuttle Challenger disaster was a fatal incident in the United States space program that occurred on Tuesday, January 28, 1986, when the Space Shuttle Challenger (OV-099) broke apart 73 seconds into its flight, killing all seven crew members aboard."], ["The Space Shuttle Columbia disaster was a fatal incident in the United States space program that occurred on February 1, 2003, when the Space Shuttle Columbia (OV-102) disintegrated as it reentered the atmosphere, killing all seven crew members."]]}, {"qid": "37259245681ee45e246f", "term": "Final Fantasy VI", "description": "1994 video game", "question": "Is Final Fantasy VI closer to beginning than end of its franchise?", "answer": true, "facts": ["Final Fantasy VI is the sixth entry into the Final Fantasy series of video games.", "There are 15 total games in the main Final Fantasy series of video games as of 2020."], "decomposition": ["How many releases have been made in the Final Fantasy franchise?", "Final Fantasy VI comes in what position in the series?", "What is the absolute difference between #1 and the cardinal value of #2?", "What is the absolute difference between 1 and the cardinal value of #2?", "Is #3 greater than #4?"], "evidence": [[[["Final Fantasy-9"]], ["operation"], ["operation"], ["operation"], ["operation"]], [[["Final Fantasy-1"]], [["Final Fantasy-6"]], ["operation"], ["operation"], ["operation"]], [[["Final Fantasy-1"]], [["Final Fantasy VI-1"]], ["operation"], ["operation"], ["operation"]]], "golden_sentence": [["It was also the first game released in Chinese and High Definition along with being released on two consoles at once."]]}, {"qid": "d905a257d61d28b505b0", "term": "Secretary", "description": "occupation", "question": "Is Tange Sazen hypothetically an ideal choice for a secretary job?", "answer": false, "facts": ["Secretaries are required to type and also read copious amounts of notes.", "Tange Sazen is a one-eyed, one-armed swordsman in Japanese literature."], "decomposition": ["What physical characteristics is Tange Sazen known to have?", "What type of skill is secretary supposed to have?", "Would it be easy to do #2 when having #1?"], "evidence": [[[["Tange Sazen-1"]], [["Secretary-3"]], [["Secretary-3", "Tange Sazen-1"]]], [[["Tange Sazen-1"]], [["Secretary-3"]], ["operation"]], [[["Tange Sazen-1"]], [["Secretary-13"]], ["operation"]]], "golden_sentence": [["The one-eyed, one-armed swordsman Tange Sazen (\u4e39\u4e0b \u5de6\u81b3) is a fictional character in Japanese literature, cinema and TV."], ["A secretary, also known as a personal assistant (PA) or administrative assistant, has many administrative duties."], ["", ""]]}, {"qid": "5587bfd31bc8f008ee61", "term": "Charles Manson", "description": "American criminal, cult leader", "question": "Has Don King killed more people than Charles Manson did with his own hands in 1971?", "answer": true, "facts": ["Charles Manson is famous for a series of murders in 1971.", "Charles Manson's cult was responsible for seven deaths in 1971 but he was not present during the murders.", "Boxing promoter Don King has been charged with killing two people in incidents 13 years apart and settled out of court.."], "decomposition": ["How many people did Charles Manson actually kill?", "Don King has been charged with killing how many people?", "Is #2 larger than #1?"], "evidence": [[[["Charles Manson-1"]], [["Don King (boxing promoter)-3"]], [["Charles Manson-1", "Don King (boxing promoter)-3"], "operation"]], [[["Charles Manson-1"]], [["Don King (boxing promoter)-1"]], ["operation"]], [[["Charles Manson-1"], "no_evidence"], [["Don King (boxing promoter)-3"]], ["operation"]]], "golden_sentence": [["In 1971, he was convicted of first-degree murder and conspiracy to commit murder for the deaths of seven people, including the film actress Sharon Tate."], ["King has been charged with killing two people in incidents 13 years apart."], ["", ""]]}, {"qid": "47deb4827e74ac5ce72b", "term": "Sea turtle", "description": "superfamily of reptiles", "question": "Are sea turtles enjoying life during quarantine?", "answer": true, "facts": ["Sea turtles nest on beaches", "Quarantine due to COVID has resulted in far fewer people using beaches", "More sea turtles have been able to nest and reproduce on beaches during quarantine"], "decomposition": ["What elements comprise \"enjoying life\" for a sea turtle?", "Where do the elements in #1 occur?", "How prevalent were humans in the areas in #2 pre-Covid-19?", "In the areas in #2, are humans less prevalent now than in #3?"], "evidence": [[[["Sea turtle-11"], "no_evidence"], [["Sea turtle-45"]], [["Sea turtle-13"]], ["operation"]], [[["Sea turtle-14"], "no_evidence"], [["Sea turtle migration-6"], "no_evidence"], ["no_evidence"], [["Sea turtle-13"], "no_evidence", "operation"]], [[["Green sea turtle-30"]], [["Sea turtle-12"]], [["Tourism-47"], "no_evidence"], [["Sea turtle-13"], "operation"]]], "golden_sentence": [[""], [""], [""]]}, {"qid": "fd3db4c7b04b2223af82", "term": "Big Ben", "description": "Bell within the clock tower at the Palace of Westminster in London, England", "question": "Can a blind person tell time by Big Ben?", "answer": true, "facts": ["Big Ben is a striking clock", "A blind person can hear the time the bell tolls"], "decomposition": ["What type of clock is Big Ben?", "How does #1 indicate a new hour?", "Can a blind person hear #2?"], "evidence": [[[["Big Ben-1"]], [["Striking clock-8"]], [["Striking clock-9"]]], [[["Big Ben-1"]], [["Big Ben-47"]], ["operation"]], [[["Big Ben-1"]], [["Big Ben-50"]], ["operation"]]], "golden_sentence": [[""], [""], [""]]}, {"qid": "5dd71d65b4720ebf22d0", "term": "Tom and Jerry", "description": "Hanna Barbera cartoon series", "question": "Are Tom and Jerry featured in a ride at Disneyland?", "answer": false, "facts": ["Tom and Jerry were created by Hanna-Barbera and are currently owned by Warner Bros. and Turner Entertainment", "Disneyland is a theme park which features characters owned by the Walt Disney Company"], "decomposition": ["What company made Tom and Jerry?", "What company owns Disneyland?", "Are #1 and #2 the same?"], "evidence": [[[["Tom and Jerry-2"]], [["Disneyland-4"]], ["operation"]], [[["Tom and Jerry-1"]], [["The Walt Disney Company-2"]], ["operation"]], [[["Metro-Goldwyn-Mayer-35"]], [["Disneyland Resort-1"]], ["operation"]]], "golden_sentence": [["In its original run, Hanna and Barbera produced 114 Tom and Jerry shorts for MGM from 1940 to 1958."], [""]]}, {"qid": "5f2f6b15f39dfe00bafc", "term": "Emulator", "description": "system that emulates a real system such that the behavior closely resembles the behavior of the real system", "question": "Would downloading Mario 64 on an emulator be legal?", "answer": false, "facts": ["Mario 64 is licenced to Nintendo Entertainment.", "Nintendo holds a copyright on all of their Mario games."], "decomposition": ["What company is Mario 64 licensed to?", "What does #1 hold on Mario 64?", "Is it legal to download Mario 64 if it is #2?"], "evidence": [[[["Super Mario 64-1"]], [["Copyright-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Super Mario 64-1"]], [["Super Mario 64-21"], "no_evidence"], [["Copyright infringement-4"], "no_evidence", "operation"]], [[["Super Mario 64-2"]], [["Super Mario 64-2"]], [["Video game console emulator-5"]]]], "golden_sentence": [[""], [""]]}, {"qid": "febc65867089ac6a79e7", "term": "Johnny Carson", "description": "American talk show host and comedian", "question": "Could Johnny Carson's children fill out a water polo team?", "answer": false, "facts": ["Johnny Carson had 3 children.", "Water polo teams consist of 7 players."], "decomposition": ["How many children does Johnny Carson have?", "How many people are needed to fill out a water polo team?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Johnny Carson-55"]], [["Water polo-1"]], ["operation"]], [[["Johnny Carson-55"]], [["Water polo-1"]], ["operation"]], [[["Johnny Carson-55"]], [["Water polo-9"]], ["operation"]]], "golden_sentence": [["Carson had three sons with his first wife: Christopher, Cory, and Richard."], ["Each team is made up of six field players and one goalkeeper."]]}, {"qid": "56188d87684b9bb63109", "term": "Fear", "description": "Basic emotion induced by a perceived threat", "question": "Could someone in a coma experience fear?", "answer": false, "facts": ["Fear is induced when an individual feels threatened by something or someone.", "A person in a coma is unconscious and therefore cannot perceive their surroundings."], "decomposition": ["What kind of feeling is fear?", "Can a person in a coma experience #1?"], "evidence": [[[["Fear-1"]], [["Coma-18"], "no_evidence", "operation"]], [[["Emotion-54"]], [["Coma-1"]]], [[["Fear-1"]], [["Coma-1"]]]], "golden_sentence": [["The fear response arises from the perception of danger leading to confrontation with or escape from/avoiding the threat (also known as the fight-or-flight response), which in extreme cases of fear (horror and terror) can be a freeze response or paralysis."], [""]]}, {"qid": "0adad4e36465b678ce0a", "term": "Alan Rickman", "description": "British actor", "question": "Do many fans of J.K Rowling know who Alan Rickman is?", "answer": true, "facts": ["J.K Rowling wrote the Harry Potter series.", "Alan Rickman performed the role of Severus Snape throughout all 8 Harry Potter films."], "decomposition": ["What is JK Rowling most famous for?", "What characters has actor Alan Rickman played?", "What characters appear in #1?", "Is at least one character from #2 also listed in #3?"], "evidence": [[[["J. K. Rowling-1"]], [["Alan Rickman-2"]], [["Harry Potter (film series)-30"], "no_evidence"], ["operation"]], [[["J. K. Rowling-1"]], [["Alan Rickman-2"]], [["Severus Snape-3"]], ["operation"]], [[["J. K. Rowling-1"]], [["Alan Rickman-15"]], [["Severus Snape-47"]], ["operation"]]], "golden_sentence": [["She is best known for writing the Harry Potter fantasy series, which has won multiple awards and sold more than 500\u00a0million copies, becoming the best-selling book series in history."], [""], [""]]}, {"qid": "50e89d5c8cb95848e909", "term": "Month", "description": "unit of time", "question": "Are months based on the solar cycle?", "answer": false, "facts": ["The solar cycle is measured by the year: the length of one orbit, as well as by day: the length of one of Earth's rotation about its axis.", "However, months are based on the lunar cycle, how the shadow of the Earth on the moon causes it to appear to grow and shrink and change shape over the course of four weeks."], "decomposition": ["What units of time depend on solar cycles?", "Is months one of #1?"], "evidence": [[[["Solar cycle-1"]], ["operation"]], [[["Solar cycle-32"], "no_evidence"], [["Month-17", "Solar cycle-32"], "operation"]], [[["Solar cycle-3"]], ["operation"]]], "golden_sentence": [["Solar activity, driven both by the sunspot cycle and transient aperiodic processes govern the environment of the Solar System planets by creating space weather and impact space- and ground-based technologies as well as the Earth's atmosphere and also possibly climate fluctuations on scales of centuries and longer."]]}, {"qid": "44afb4740e093b10d11f", "term": "Billionaire", "description": "person who has a net worth of at least one billion (1,000,000,000) units of a given currency", "question": "Would a 900,000 pound net worth person be an American billionaire if they exchange currency June 2020?", "answer": true, "facts": ["The exchange rate in June of 2020 between dollars and pounds is 1 Euro= 1.23 dollar.", "900000 pounds is equal to about 1,107,000.00"], "decomposition": ["What is the minimum amount one must have to be called a billionaire?", "As of  June 2020, how many dollars make a pound?", "Is #2 times 900000 at least equal to #1?"], "evidence": [[[["Billionaire-1"]], [["Pound sterling-62"], "no_evidence"], ["operation"]], [[["Billionaire-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Billionaire-1"]], [["Pound sterling-62"]], ["operation"]]], "golden_sentence": [["Additionally, a centibillionaire (or centi-billionaire) has been deemed applicable to a billionaire worth one hundred billion dollars (100,000,000,000), a mark first achieved in 2017 by Amazon founder Jeff Bezos, with a net worth of $112 billion in a report issued in early 2018."], [""]]}, {"qid": "e7cabc6bdf3e28f09b35", "term": "Missionary", "description": "member of a religious group sent into an area to do evangelism", "question": "Is there a popular Broadway character who is a missionary?", "answer": true, "facts": ["The Book of Mormon is a popular Broadway musical.", "The two main characters are Elder Price and Elder Cunningham.", "They are Mormon missionaries sent to Africa to share their religion with the villagers."], "decomposition": ["Who are the two main characters in the popular Broadway musical \"Book of Mormon\"?", "Are #1 missionaries?"], "evidence": [[[["The Book of Mormon (musical)-3"]], ["operation"]], [[["The Book of Mormon (musical)-3"]], ["operation"]], [[["The Book of Mormon (musical)-32"]], [["The Book of Mormon (musical)-3"]]]], "golden_sentence": [[""]]}, {"qid": "714a31779140b1051f5d", "term": "Reiki", "description": "Pseudoscientific healing technique", "question": "Would somebody leave reiki with bruises?", "answer": false, "facts": ["Bruises are caused by blunt trauma to the body.", "Reiki is performed without touching the recipient. "], "decomposition": ["What are the processes involved in Reiki?", "Does any of #1 involve physical contact with the body?"], "evidence": [[[["Reiki-1"]], [["Reiki-15"]]], [[["Reiki-1"]], [["Reiki-1"]]], [[["Reiki-1"]], ["operation"]]], "golden_sentence": [["Reiki practitioners use a technique called palm healing or hands-on healing through which a \"universal energy\" is said to be transferred through the palms of the practitioner to the patient in order to encourage emotional or physical healing."], ["Several authors have pointed to the vitalistic energy which reiki is claimed to treat, with one saying, \"Ironically, the only thing that distinguishes reiki from Therapeutic touch is that it [reiki] involves actual touch,\" and others stating that the International Center for Reiki Training \"mimic[s] the institutional aspects of science\" seeking legitimacy but holds no more promise than an alchemy society."]]}, {"qid": "555bbc44f44d0d18592c", "term": "Tower of London", "description": "A historic castle on the north bank of the River Thames in central London", "question": "Would Robert Stack have been interested in Tower of London during 1400s for his 14 season show?", "answer": true, "facts": ["Robert Stack was an actor best known for Unsolved Mysteries which lasted for 14 seasons before being rebooted by Netflix.", "The Tower of London is a historic building in London.", "Unsolved Mysteries explored unexplained phenomenon and mysterious events.", "The heirs of Edward IV mysteriously vanished from the Tower of London in the 1400s and were presumed muredered."], "decomposition": ["What is the defining feature of Robert Stack's 14 season show?", "What events happened at the Tower of London in the 1400's?", "Do any of the events in #2 have the characteristic in #1?"], "evidence": [[[["Robert Stack-35"]], [["Tower of London-2", "Tower of London-3"]], ["operation"]], [[["Robert Stack-1"]], [["Tower of London-34"]], ["operation"]], [[["Unsolved Mysteries-1"]], [["Princes in the Tower-2"]], ["operation"]]], "golden_sentence": [[""], ["", ""]]}, {"qid": "a5f6d7d936fa90104083", "term": "Almond", "description": "Species of plant", "question": "Would a stool be useful for a Lusotitan to reach the top of an almond tree?", "answer": false, "facts": ["Almond trees can grow up to 10 meters high", "The Lusotitan adult was over 10 meters high"], "decomposition": ["How tall would a typical almond tree grow to be?", "How tall was the typical Lusotitan adult?", "Is #1 larger than #2?"], "evidence": [[[["Almond-3"]], [["Lusotitan-4"], "no_evidence"], [["Brachiosaurus-3"], "no_evidence", "operation"]], [[["Almond-3"]], [["Lusotitan-4"], "no_evidence"], ["operation"]], [[["Almond-3"]], [["Lusotitan-4"]], ["operation"]]], "golden_sentence": [["The almond is a deciduous tree, growing 4\u201310\u00a0m (13\u201333\u00a0ft) in height, with a trunk of up to 30\u00a0cm (12\u00a0in) in diameter."], ["It has been estimated that Lusotitan was 25 meters (82 feet) long."], [""]]}, {"qid": "9738e1d805b31d334721", "term": "Tony Bennett", "description": "American singer", "question": "Could ancient Tony Bennett have a baby in 2020?", "answer": true, "facts": ["Tony Bennett is a legendary singer who will turn 94 years old in August 2020.", "Ramjit Raghav, the oldest man to have a baby, had his first child at age 94.", "Ramjit Raghav had his second child at age 96."], "decomposition": ["How old was Tony Bennett in 2020?", "How old was the oldest man to father a child?", "Is #1 less than #2?"], "evidence": [[[["Tony Bennett-1"]], [["Ramjit Raghav-1"]], ["operation"]], [[["Tony Bennett-1"]], [["Ramjit Raghav-1"]], ["operation"]], [[["Tony Bennett-1"]], [["Ramjit Raghav-4"]], ["operation"]]], "golden_sentence": [[""], ["He claimed to have had his first child with his wife at age 94."]]}, {"qid": "4e7877a14de6b741529d", "term": "Eagle", "description": "large carnivore bird", "question": "Are eagles and young bears both used as labels for skills-training youth groups?", "answer": true, "facts": ["A young bear is a cub.", "Boy Scouts is a skill-training youth group that includes divisions such as Cub Scouts and Eagle Scouts"], "decomposition": ["What is the name of a young bear?", "What is a popular skill training group for boys? ", "Are #1 and eagles names for groups in #2?"], "evidence": [[[["Bear-27"]], [["Boy Scouts of America-2"]], [["Boy Scouts of America-22", "Boy Scouts of America-26"], "operation"]], [[["Bear-37"]], [["Boy Scouts of America-1", "Scouting-1"]], [["Cub Scout-1", "Eagle Scout (Boy Scouts of America)-1"]]], [[["Bear-27"]], [["Scout (Scouting)-1"]], [["Eagle Scout (Boy Scouts of America)-1", "Scout (Scouting)-5"], "operation"]]], "golden_sentence": [[""], ["For younger members, the Scout method is part of the program to instill typical Scouting values such as trustworthiness, good citizenship, and outdoors skills, through a variety of activities such as camping, aquatics, and hiking."], ["", ""]]}, {"qid": "c1466851e3381d6fd882", "term": "Nerd", "description": "Descriptive term, often used pejoratively, indicating that a person is overly intellectual, obsessive, or socially impaired", "question": "Do movies always show nerds as the losers?", "answer": false, "facts": ["Superbad features two main characters that are nerds on a quest for love, and ends with them being victorious.", "The Social Network is a film about a University Nerd who took his website global and became one of the most powerful people in the world."], "decomposition": ["How did the quests of the nerds in Superbad turn out at the end?", "How did the project of the nerd in Social Network turn out at the end?", "Was #1 or #2 a negative outcome for the nerds?"], "evidence": [[[["Superbad (film)-3", "Superbad (film)-7"]], [["Facebook-3", "The Social Network-1"]], [["Facebook-3", "Superbad (film)-7"]]], [[["Superbad (film)-7"]], [["The Social Network-7"]], ["operation"]], [[["Superbad (film)-7"]], [["The Social Network-6"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["", ""], ["", ""], ["", ""]]}, {"qid": "cf06573b2020b631daa6", "term": "HIV", "description": "Human retrovirus, cause of AIDS", "question": "Is it safe to share silverware with an HIV positive person?", "answer": true, "facts": ["HIV is transmitted through blood and mucous membrane contact, not saliva.", "Silverware is used in the mouth and contacts saliva but not other bodily fluids. "], "decomposition": ["How is HIV transmitted?", "What comes in contact with silverware when you use it?", "Is there any overlap between #1 and #2?"], "evidence": [[[["HIV-1"]], [["Household silver-3"], "no_evidence"], ["operation"]], [[["HIV-1"]], [["Eating utensil etiquette-5"]], [["Management of HIV/AIDS-63"], "operation"]], [[["HIV-1"]], [["Cutlery-1"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["In most cases, HIV is a sexually transmitted infection and occurs by contact with or transfer of blood, pre-ejaculate, semen, and vaginal fluids."], [""]]}, {"qid": "f947868fccb2c56aff6c", "term": "Brussels sprout", "description": "vegetable", "question": "Would many meals heavy in brussels sprouts benefit someone on Coumadin?", "answer": false, "facts": ["Brussels sprouts are high in vitamin K", "Eating lots of foods high in vitamin K is potentially harmful to people taking anticoagulants ", "Coumadin is an anticoagulant "], "decomposition": ["In which class of drugs is Coumadin in?", "Which nutrients are people taking #1 cautioned against getting too much of?", "Are Brussels sprouts low in #2?"], "evidence": [[[["Warfarin-1"]], [["Vitamin K-19"]], [["Brussels sprout-12"]]], [[["Warfarin-1"]], [["Warfarin-11"], "no_evidence"], [["Cruciferous vegetables-1"], "operation"]], [[["Warfarin-1"]], [["Warfarin-2"]], [["Vitamin K-33", "Warfarin-2"]]]], "golden_sentence": [[""], ["The proper anticoagulant action of the drug is a function of vitamin K intake and drug dose, and due to differing absorption must be individualized for each patient."], [""]]}, {"qid": "ea4cd41d64f667c720dc", "term": "Baptism", "description": "Christian rite of admission and adoption, almost invariably with the use of water", "question": "Can Immersion Baptism lead to a death like Jeff Buckley's?", "answer": true, "facts": ["Immersion Baptism is the practice of submerging people underwater for a religious ritual.", "Jeff Buckley was an acclaimed singer that died of drowning in 1997.", "A baby in Moldova died from Immersion Baptism in 2010."], "decomposition": ["How did Jeff Buckley die?", "How is immersion baptism performed?", "Are the circumstances surrounding #1 similar to that of #2?"], "evidence": [[[["Jeff Buckley-3"]], [["Immersion baptism-1"]], ["operation"]], [[["Jeff Buckley-36"]], [["Immersion baptism-4"]], [["Immersion baptism-4"], "operation"]], [[["Jeff Buckley-3"]], [["Immersion baptism-2"]], [["Drowning-1", "Swimming-1"], "operation"]]], "golden_sentence": [["On May 29, 1997, while awaiting the arrival of his band from New York, he drowned during a spontaneous evening swim, fully clothed, in the Mississippi River when he was caught in the wake of a passing boat; his body was found on June 4."], ["Immersion baptism (also known as baptism by immersion or baptism by submersion) is a method of baptism that is distinguished from baptism by affusion (pouring) and by aspersion (sprinkling), sometimes without specifying whether the immersion is total or partial, but very commonly with the indication that the person baptized is immersed completely."]]}, {"qid": "68eec9d02b6892ac8ac6", "term": "The Great Gatsby", "description": "1925 novel by F. Scott Fitzgerald", "question": "When Hugh Jackman was a teacher, would he have taught The Great Gatsby?", "answer": false, "facts": ["The Great Gatsby is often taught in high school English classes. ", "Hugh Jackman worked as a school gym teacher before he was an actor."], "decomposition": ["What classes did Hugh Jackman teach?", "In what classes is The Great Gatsby taught?", "Are any of the classes listed in #1 also listed in #2?"], "evidence": [[[["Hugh Jackman-4"], "no_evidence"], [["The Great Gatsby-23"], "no_evidence"], ["no_evidence"]], [[["Hugh Jackman-4"]], [["The Great Gatsby-1", "The Great Gatsby-3"], "no_evidence"], ["no_evidence", "operation"]], [[["Hugh Jackman-4"]], [["English studies-1"]], ["operation"]]], "golden_sentence": [["Following graduation, he spent a gap year working at Uppingham School in England as a Physical Education teacher."], [""]]}, {"qid": "aea99d6215d3275f1375", "term": "Arnold Schwarzenegger", "description": "Austrian-American actor, businessman, bodybuilder and politician", "question": "Would Arnold Schwarzenegger be unable to run for President of the US?", "answer": true, "facts": ["Arnold Schwarzenegger was born in Austria.", "One requirement of running for the US Presidency is to have been born in the USA."], "decomposition": ["Where was Arnold Schwarzenegger born?", "Is #1 in the US?"], "evidence": [[[["Arnold Schwarzenegger-11"]], ["operation"]], [[["Arnold Schwarzenegger-6"]], [["Thal, Styria-1"], "operation"]], [[["Arnold Schwarzenegger-6"]], [["Styria-1"]]]], "golden_sentence": [[""]]}, {"qid": "4c75383949dd3c442888", "term": "Capsaicin", "description": "chemical compound", "question": "Is capsaicin associated with cooking?", "answer": true, "facts": ["Capsaicin occurs naturally in chilies and other peppers.", "It is the substance that makes the peppers spicy.", "Peppers are routinely used in cooking in many cultures."], "decomposition": ["What is capsaicin a highly active component of?", "Is #1 used in cooking? "], "evidence": [[[["Capsaicin-1"]], [["Chili pepper-1"]]], [[["Capsaicin-1"]], [["Chili pepper-1"]]], [[["Capsaicin-2"]], [["Capsicum-1"]]]], "golden_sentence": [["Capsaicin (8-methyl-N-vanillyl-6-nonenamide) is an active component of chili peppers, which are plants belonging to the genus Capsicum."], ["The chili pepper (also chile, chile pepper, chilli pepper, or chilli), from Nahuatl ch\u012blli (Nahuatl pronunciation:\u00a0[\u02c8t\u0361\u0283i\u02d0l\u02d0i] (listen)), is the fruit of plants from the genus Capsicum which are members of the nightshade family, Solanaceae."]]}, {"qid": "905fd9400f657c015041", "term": "Bartender", "description": "person who serves usually alcoholic beverages behind the bar in a licensed establishment", "question": "Does a person need a college degree to become a bartender?", "answer": false, "facts": ["College degrees require at least 2 years of study to obtain.", "Bartender training generally takes 40 hours."], "decomposition": ["How much hours of training does it take to become a bartender?", "How many years does it take to get the lowest college degree?", "Is #2 less than #1?"], "evidence": [[[["Bartending school-2"]], [["Associate degree-1"]], [["Year-57"], "operation"]], [[["Bartender-12"]], [["Associate degree-17"]], ["operation"]], [[["Bartender-12"], "no_evidence"], [["Associate degree-1", "Bachelor's degree-131"]], ["operation"]]], "golden_sentence": [["Some offer only a few hours of instruction, others offer up to 40 hours."], [""], [""]]}, {"qid": "14da89a58fca45b586fb", "term": "German Shepherd", "description": "Dog breed", "question": "Would a German Shepherd be welcome in an airport?", "answer": true, "facts": ["Airports in the US must be compliant with the ADA allowing for service dogs as medical equipment.", "Police security often use dogs like German Shepherds to search for drugs at airports.", "Some airlines have special travel accommodations for dogs."], "decomposition": ["What measures are used to provide security in airports?", "Which of #1 involve the use of animals?", "Are German Shepherds used as #2?"], "evidence": [[[["Airport security-2"]], [["Detection dog-1"]], [["German Shepherd-2"], "operation"]], [[["Airport security-7"]], [["Airport security-7"]], ["operation"]], [[["Airport security-7"]], [["Police dog-1"]], [["Police dog-11"]]]], "golden_sentence": [["Aviation security is a combination of human and material resources to safeguard civil aviation against unlawful interference."], ["A detection dog or sniffer dog is a dog that is trained to use its senses to detect substances such as explosives, illegal drugs, wildlife scat, currency, blood, and contraband electronics such as illicit mobile phones."], [""]]}, {"qid": "ad02a003ee8b5bd201e8", "term": "Mathematician", "description": "person with an extensive knowledge of mathematics", "question": "Would Hodor hypothetically be a good math mathematician?", "answer": false, "facts": ["Mathematicians are expert students of mathematics.", "Hodor was a dimwitted giant of a man that served House Stark in Game of Thrones.", "Hodor worked in the stables and could only utter the only word he ever said was his own name.", "Mathematicians frequently publish articles on theories and need to be able to read and write."], "decomposition": ["How proficient is Hodor at reading/writing and general intelligence?", "What skills would be required to be good at math?", "Could #1 satisfy #2?"], "evidence": [[[["Hodor (disambiguation)-1"], "no_evidence"], [["Mathematician-1", "Mathematician-12"]], ["operation"]], [["no_evidence"], [["Mathematics-2"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], [["Mathematics-2"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["", ""]]}, {"qid": "06d5867dc8a62e32a97a", "term": "Moustache", "description": "Facial hair grown on the upper lip", "question": "Would a Rockette look odd with a moustache? ", "answer": true, "facts": ["The Rockettes are an American dance group made up of all women.", "In America, it is uncommon to see a woman with a moustache."], "decomposition": ["What groups of people make up the Rockettes?", "Would #1 look odd with a moustache?"], "evidence": [[[["The Rockettes-2"], "no_evidence"], ["operation"]], [[["The Rockettes-5"]], [["Facial hair-5"]]], [[["The Rockettes-1"], "no_evidence"], [["Facial hair-5"]]]], "golden_sentence": [["As Markert would later recall, \"If I ever got a chance to get a group of American girls who would be taller and have longer legs and could do really complicated tap routines and eye-high kicks, they'd really knock your socks off.\""]]}, {"qid": "2af8d67b0d05467e2963", "term": "Audi", "description": "Automotive manufacturing subsidiary of Volkswagen Group", "question": "Is sound barrier too much for Audi R8 V-10 Plus to break?", "answer": true, "facts": ["Audi R8 V-10 Plus is the fastest car produced by Audi.", "The Audi R8 V-10 Plus has a top speed of 205 MPH.", "To break the sound barrier, a jet must reach a speed of 770 MPH."], "decomposition": ["What is the top speed of an Audi R8 V-10 Plus?", "What speed must be obtained to break the sound barrier?", "Is #2 greater than #1?"], "evidence": [[[["Audi R8 (Type 4S)-11"]], [["Sound barrier-2"]], [["Audi R8 (Type 4S)-11", "Speed of sound-1"]]], [[["Audi S8-5"], "no_evidence"], [["Sound barrier-11"]], ["operation"]], [[["Audi R8-1"], "no_evidence"], [["Speed of sound-1"]], ["operation"]]], "golden_sentence": [["Top speed for the V10 Performance quattro is 330\u00a0km/h (205\u00a0mph)."], ["In 1947 it was demonstrated that safe flight at the speed of sound was achievable in purpose-designed aircraft thereby breaking the barrier."], ["", "At 20\u00a0\u00b0C (68\u00a0\u00b0F), the speed of sound in air is about 343 metres per second (1,235\u00a0km/h; 1,125\u00a0ft/s; 767\u00a0mph; 667\u00a0kn), or a kilometre in 2.9 s or a mile in 4.7 s. It depends strongly on temperature as well as the medium through which a sound wave is propagating."]]}, {"qid": "bb803e47318e09ddf4e2", "term": "Kanji", "description": "adopted logographic Chinese characters used in the modern Japanese writing system", "question": "Can printing books in kanji instead of the Roman alphabet save trees?", "answer": true, "facts": ["The Kanji writing system uses less space to communicate ideas than an alphabet system.", "Trees are killed to make pages for books."], "decomposition": ["What type of writing system is kanji?", "Does #1 take up less space than an alphabet?"], "evidence": [[[["Logogram-1"]], [["Logogram-2"], "operation"]], [[["Kanji-1"]], [["Logogram-1"], "no_evidence"]], [[["Kanji-1"]], [["Kanji-49", "Kanji-80"], "no_evidence", "operation"]]], "golden_sentence": [["The use of logograms in writing is called logography, and a writing system that is based on logograms is called a logographic system."], [""]]}, {"qid": "19b952ddd9d1dbe8c573", "term": "Chinchilla", "description": "Rodent genus", "question": "Is a Chinchilla breed of felis catus a type of rodent?", "answer": false, "facts": ["A Chinchilla is a rodent native to the Andes mountains.", "Felis catus is the scientific name for a cat.", "The Chinchilla breed of cats is named for its plush coat which shares similarities to the Chinchilla.", "The Chinchilla cat is really a variant of the Persian breed of cats."], "decomposition": ["Which species are named felis catus?", "What is the most notable feature of the Chinchilla?", "Are Chinchilla breed of #1 so named because they have #2?", "Given that #3 is positive, does that make #1 rodents?"], "evidence": [[[["Cat-1"]], [["Chinchilla-2"]], [["Persian cat-28"], "no_evidence"], ["operation"]], [[["Cat-1"]], [["Chinchilla-2"]], ["no_evidence"], ["no_evidence"]], [[["Cat-13"]], [["Chinchilla-10"], "no_evidence"], [["Persian cat-28", "Rodent-1"], "operation"], ["operation"]]], "golden_sentence": [["The cat (Felis catus) is a domestic species of small carnivorous mammal."], ["The chinchilla has the densest fur of all mammals that live on land."], [""]]}, {"qid": "3eff833a877d73ef5c7d", "term": "Watermelon", "description": "A large fruit with a smooth hard rind, of the gourd family", "question": "Is watermelon safe for people with a tricarboxylic acid allergy?", "answer": true, "facts": ["Tricarboxylic acid as an acid that manifests itself in fruits as citric acid.", "Citric acid can be found in citrus fruits such as oranges and lemon.", "Watermelon is not a citrus fruit."], "decomposition": ["What is the most common example of a tricarboxylic acid?", "Which kind of fruits is #1 usually present in?", "Is watermelon excluded from #2?"], "evidence": [[[["Tricarboxylic acid-1"]], [["Citric acid-1"]], [["Citrus-1", "Watermelon-2"], "operation"]], [[["Tricarboxylic acid-2"]], [["Citric acid-14"]], [["Citric acid-14", "Watermelon-8"], "operation"]], [[["Tricarboxylic acid-1"]], [["Citric acid-14"]], ["operation"]]], "golden_sentence": [["The best-known example of a tricarboxylic acid is citric acid."], ["It occurs naturally in citrus fruits."], ["", ""]]}, {"qid": "3c1eee5f322c97e3b949", "term": "Dalai Lama", "description": "Tibetan Buddhist spiritual teacher", "question": "Can the Dalai Lama fit in a car?", "answer": true, "facts": ["The Dalai Lama is a person.", "Cars are designed for people to sit in them."], "decomposition": ["What type of being is the Dalai Lama?", "Who are cars designed for?", "Is #1 the same as #2?"], "evidence": [[[["Dalai Lama-1"]], [["Car controls-23"]], ["operation"]], [[["Dalai Lama-1"]], [["Car-42"]], ["operation"]], [[["Dalai Lama-1", "Person-1"]], [["Car-1"]], ["operation"]]], "golden_sentence": [["Dalai Lama (UK: /\u02c8d\u00e6la\u026a \u02c8l\u0251\u02d0m\u0259/, US: /\u02c8d\u0251\u02d0la\u026a \u02c8l\u0251\u02d0m\u0259/; Standard Tibetan: \u0f4f\u0f71\u0f0b\u0f63\u0f60\u0f72\u0f0b\u0f56\u0fb3\u0f0b\u0f58\u0f0b, T\u0101 la'i bla ma, [t\u00e1\u02d0l\u025b\u02d0 l\u00e1ma]) is a title given by the Tibetan people for the foremost spiritual leader of the Gelug or \"Yellow Hat\" school of Tibetan Buddhism, the newest of the classical schools of Tibetan Buddhism."], [""]]}, {"qid": "3729445084dad7b551af", "term": "Vulcan (mythology)", "description": "Ancient Roman god of fire, volcanoes, and metalworking", "question": "Does the Roman god Vulcan have a Greek equivalent?", "answer": true, "facts": ["Vulcan is the Roman god of fire and metalworking.", "Hephaestus is the Greek god of fire and metalworking.", "They are the same mythological figure, one of many characters the Romans borrowed from the Greeks and changed their names."], "decomposition": ["What is the Roman god Vulcan god of?", "Is there a god of #1 in Greek mythology?"], "evidence": [[[["Vulcan (mythology)-17"]], [["Helios-13"]]], [[["Vulcan (mythology)-1"]], [["Vulcan (mythology)-1"]]], [[["Vulcan (mythology)-17"]], [["Hephaestus-1"]]]], "golden_sentence": [["The Roman concept of the god seems to associate him to both the destructive and the fertilizing powers of fire."], ["In Plato's Republic (516\u00a0B), Helios, the Sun, is the symbolic offspring of the idea of the Good."]]}, {"qid": "cac7831b62b4b08f66b4", "term": "Christians", "description": "people who adhere to Christianity", "question": "Does Hammurabi's Code violate Christians Golden Rule?", "answer": true, "facts": ["The Golden Rule of Christianity states to do unto others as you would want them to do to you.", "Hammurabi's Code states an eye for an eye and a tooth for a tooth."], "decomposition": ["What is the golden rule in Christianity? ", "What does the Code of Hammurabi state?", "Is #1 the same meaning as #2?"], "evidence": [[[["Golden Rule-1", "Golden Rule-20"]], [["Code of Hammurabi-15", "Shofetim (parsha)-26"], "no_evidence"], ["operation"]], [[["Golden Rule-1"]], [["Code of Hammurabi-1"]], [["Eye for an eye-1"], "operation"]], [[["Golden Rule-21"]], [["Code of Hammurabi-2"]], ["operation"]]], "golden_sentence": [["The Golden Rule is the principle of treating others as you want to be treated.", "The \"Golden Rule\" of Leviticus 19:18 was quoted by Jesus of Nazareth (Matthew 7:12; see also Luke 6:31) and described by him as the second great commandment."], ["", "The Code of Hammurabi provided that if a man destroyed the eye of another man, they were to destroy his eye."]]}, {"qid": "641584a23042362fd6db", "term": "Chicago \"L\"", "description": "rapid transit system in Chicago, Illinois, operated by the CTA", "question": "Would the fastest tortoise win a race against a Chicago \"L\"?", "answer": false, "facts": ["Top speed of  Chicago \"L\" is 55 mph (89 km/h).", "The Guinness Book of World Records maintains the record for fastest tortoise: the tortoise ran at an average speed of 0.63 miles per hour."], "decomposition": ["What is the top speed of a Chicago \"L\"?", "What is the top speed of a tortoise?", "Is #2 greater than #1?"], "evidence": [[[["Chicago \"L\"-37"], "no_evidence"], [["Turtle-5"], "no_evidence"], ["operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Chicago \"L\"-58"], "no_evidence"], [["Turtle racing-6"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "df2994ef74bf7b2d678b", "term": "Unicode", "description": "Character encoding standard", "question": "Did Malcolm X use Unicode?", "answer": false, "facts": ["Malcolm X died in 1965. ", "Unicode did not become a standard until 1991. "], "decomposition": ["When did Malcolm X die?", "When was Unicode established?", "Is #2 before #1?"], "evidence": [[[["Malcolm X-96"]], [["Unicode-12"]], [["Malcolm X-96", "Unicode-12"], "operation"]], [[["Malcolm X-1"]], [["Unicode-18"]], ["operation"]], [[["Malcolm X-1"]], [["Unicode-12"]], ["operation"]]], "golden_sentence": [[""], ["Based on experiences with the Xerox Character Code Standard (XCCS) since 1980, the origins of Unicode date to 1987, when Joe Becker from Xerox with Lee Collins and Mark Davis from Apple, started investigating the practicalities of creating a universal character set."], ["", ""]]}, {"qid": "3bad5588e52fce297f10", "term": "Jukebox musical", "description": "stage or film musical compiled from pre-existing songs", "question": "Is there a jukebox musical about a sweet transvestite from Transexual, Transylvania?", "answer": false, "facts": ["Jukebox musicals feature songs that have already been released.", "Rocky Horror Picture Show is about a sweet transvestite from Transexual, Transylvania", "Rocky Horror Picture Show contains songs written specifically for itself"], "decomposition": ["What is characteristic of songs in a jukebox musical?", "What musical is about a sweet transvestite from Transexual, Transylvania?", "Does #2 contain #1?"], "evidence": [[[["Jukebox musical-6"]], [["The Rocky Horror Picture Show-5"]], ["operation"]], [[["Jukebox musical-1"]], [["The Rocky Horror Show-1"]], ["operation"]], [[["Jukebox musical-1"]], [["The Rocky Horror Picture Show-5"]], [["The Rocky Horror Picture Show-31"], "no_evidence", "operation"]]], "golden_sentence": [[""], ["They are soon swept into the world of Dr. Frank-N-Furter, a self-proclaimed \"sweet transvestite from Transsexual, Transylvania\"."]]}, {"qid": "97b9e50aec36e4c76657", "term": "Cactus", "description": "Family of mostly succulent plants, adapted to dry environments", "question": "Would an aerodynamic cactus benefit from more frequently closed stomata?", "answer": true, "facts": ["Cactus spines help the plant retain water by reducing air flow around the plant", "Aerodynamic objects have smooth surfaces ", "Crassulacean acid metabolism is used by cactuses ", "Crassulacean acid metabolism is when a plant's stomata stay closed during daylight or times of drought to prevent water loss"], "decomposition": ["What helps cacti conserve water?", "Of #1, what methods do not involve protrusions that might restrict air flow?", "Are closed stoma one of #2?"], "evidence": [[[["Cactus-28", "Cactus-29"]], [["Cactus-34"]], ["operation"]], [[["Cactus-1"]], [["Cactus-35"]], ["operation"]], [[["Cactus-13", "Cactus-14"], "no_evidence"], [["Cactus-13", "Cactus-14"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["", "Spines provide protection from herbivores and camouflage in some species, and assist in water conservation in several ways."], [""]]}, {"qid": "391d1a54b4d82e0ddf71", "term": "Drummer", "description": "percussionist who creates and accompanies music using drums", "question": "Do drummers need spare strings?", "answer": false, "facts": ["Drummers usually work with guitarists or other stringed instrumentalists.", "However, drum sets do not contain strings.", "Musicians usually change their own strings as necessary, so it is their band mates, not the drummer, who carries the spare strings."], "decomposition": ["What instruments require strings in order to be played?", "What instrument do drummers play?", "Is there any overlap between #2 and #1?"], "evidence": [[[["String instrument-1"]], [["Drum-1"]], ["operation"]], [[["String instrument-4"]], [["Drum-1"]], [["Drum-1"], "operation"]], [[["String instrument-4"]], [["Drummer-2"]], ["operation"]]], "golden_sentence": [["String instruments, stringed instruments, or chordophones are musical instruments that produce sound from vibrating strings when the performer plays or sounds the strings in some manner."], ["Drums consist of at least one membrane, called a drumhead or drum skin, that is stretched over a shell and struck, either directly with the player's hands, or with a percussion mallet, to produce sound."]]}, {"qid": "7a902578b41720e9d9f7", "term": "Parsifal", "description": "opera in three acts by Richard Wagner", "question": "Was the subject of Parsifal taken from British folklore?", "answer": true, "facts": ["Parsifal was loosely based on a poem about Percival", "Percival was a Knight of the Round Table", "King Arthur and the Knights of the Round Table were products of British folklore"], "decomposition": ["What was the opera 'Parsifal' based on?", "Who is the main character in #1?", "Which group is #2 part of?", "Did #3 originate from British folklore?"], "evidence": [[[["Parsifal-1"]], [["Parzival-1"]], [["Percival-1"]], [["Knights of the Round Table-1"]]], [[["Parsifal-1"]], [["Percival-1"]], [["Knights of the Round Table-1"]], ["operation"]], [[["Parsifal-5"]], [["Parsifal-34"]], [["Parsifal-33"]], [["Knight-4"]]]], "golden_sentence": [["It is loosely based on Parzival by Wolfram von Eschenbach, a 13th-century epic poem of the Arthurian knight Parzival (Percival) and his quest for the Holy Grail (12th century)."], ["The poem, commonly dated to the first quarter of the 13th century, centers on the Arthurian hero Parzival (Percival in English) and his long quest for the Holy Grail following his initial failure to achieve it."], [""], [""]]}, {"qid": "aa13ff9827c81b307984", "term": "Parsley", "description": "species of plant, herb", "question": "Is it normal to find parsley in multiple sections of the grocery store?", "answer": true, "facts": ["Parsley is available in both fresh and dry forms.", "Fresh parsley must be kept cool.", "Dry parsley is a shelf stable product."], "decomposition": ["What forms of parsley are consumed?", "What areas of the grocery store carry each of #1?", "Does #2 include places separate from one another?"], "evidence": [[[["Parsley-13", "Parsley-15"]], [["Produce-2"], "no_evidence"], ["operation"]], [[["Parsley-18", "Parsley-3"]], [["Supermarket-3"], "no_evidence"], ["no_evidence"]], [[["Parsley-16", "Parsley-18"], "no_evidence"], [["Canning-1", "Produce-2"], "no_evidence"], ["operation"]]], "golden_sentence": [["", "Parsley is widely used in Middle Eastern, Mediterranean, Brazilian, and American cuisine."], ["Produce is the main product sold by greengrocers (UK, Australia) and farmers' markets."]]}, {"qid": "c219d1bf5a4f102c5ed2", "term": "Guam", "description": "Island territory of the United States of America", "question": "Was Raphael's paintings influenced by the country of Guam?", "answer": false, "facts": ["Raphael was a European painter that died in 1520.", "Magellan's 1521 voyage was Europe's first trip to Guam."], "decomposition": ["When did the painter Raphael die?", "When did Europeans first visit Guam?", "Was #1 after #2?"], "evidence": [[[["Raphael-38"]], [["Guam-11"]], [["Guam-11", "Raphael-38"], "operation"]], [[["Raphael-1"]], [["Guam-11"]], ["operation"]], [[["Raphael-1"]], [["Guam-3"]], ["operation"]]], "golden_sentence": [["Raphael died on Good Friday (April 6, 1520), which was possibly his 37th birthday, Vasari says that Raphael had also been born on a Good Friday, which in 1483 fell on March 28, and that the artist died from exhaustion brought on by unceasing romantic interests while he was working on the Loggia."], ["The first European to travel to Guam was Portuguese navigator Ferdinand Magellan, sailing for the King of Spain, when he sighted the island on March 6, 1521, during his fleet's circumnavigation of the globe."], ["", ""]]}, {"qid": "370d640506b1871317ac", "term": "Dosa", "description": "Thin pancakes originating from South India", "question": "Would someone on a keto diet be able to eat Dosa?", "answer": false, "facts": ["Dosa's main ingredients are rice and black gram,", "The ketogenic diet is a high-fat, adequate-protein, low-carbohydrate diet.", "Rice is high in carbohydrates."], "decomposition": ["Which food nutrients are minimally consumed in a keto diet?", "Which food nutrients does Dosa promarily contain?", "Is #2 excluded from #1?"], "evidence": [[[["Ketogenic diet-1"]], [["Dosa-1"], "no_evidence"], ["operation"]], [[["Ketogenic diet-1"]], [["Dosa-1"]], ["operation"]], [[["Ketogenic diet-1"]], [["Dosa-1", "Rice-17"]], ["operation"]]], "golden_sentence": [["The diet forces the body to burn fats rather than carbohydrates."], ["Its main ingredients are rice and black gram, ground together in a fine, smooth batter with a dash of salt."]]}, {"qid": "a0142de1c51416a654b1", "term": "Kayak", "description": "small boat propelled with a double-bladed paddle", "question": "Are kayaks used at the summit of Mount Everest?", "answer": true, "facts": ["Kayaks are used to transport people in water.", "The summit of Mount Everest has no running water."], "decomposition": ["What are kayaks typically used on?", "Does the summit of Mount Everest have any #1?"], "evidence": [[[["Kayak-1"]], [["Mount Everest-122"], "operation"]], [[["Whitewater kayaking-1"]], [["Dudh Koshi-2"]]], [[["Kayak-1"]], [["Mount Everest-122"], "no_evidence"]]], "golden_sentence": [[""], ["The summit of Everest has been described as \"the size of a dining room table\"."]]}, {"qid": "65b1fe3cb23fbd0c7bb8", "term": "Lullaby", "description": "soothing song, usually sung to young children before they go to sleep", "question": "Is an inappropriate lullaby Love Song from November 11, 2000?", "answer": true, "facts": ["Lullabies are often simple and repetitive.", "Lullabies are used to soothe young children, usually to go to sleep.", "Love Song was a song released by Marilyn Manson on his November 11, 2000 record.", "Marilyn Manson is a heavy metal band.", "The lyrics to Love Song has curse words and scary imagery."], "decomposition": ["Why are lullabies usually sung?", "What were the features of The Love Song by Marilyn Manson?", "Would #2 be unsuitable to achieve #1?"], "evidence": [[[["Lullaby-1"]], [["Holy Wood (In the Shadow of the Valley of Death)-1", "Holy Wood (In the Shadow of the Valley of Death)-21"]], ["operation"]], [[["Lullaby-1"]], ["no_evidence"], ["no_evidence"]], [[["Lullaby-1"]], [["Marilyn Manson-8"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["As a result, the music is often simple and repetitive."], ["", "The song is driven by keyboards, synthesizers and a synth bass."]]}, {"qid": "c7a6afc2254e51ecc7cf", "term": "Drum", "description": "type of musical instrument of the percussion family", "question": "Would a vegan prefer a natural bongo drum over a synthetic one?", "answer": false, "facts": ["Natural bongo drums are made with leather.", "Synthetic bongo drums are made with plastic or leather substitutes.", "Vegans do not use or consume any animal products."], "decomposition": ["Which kind of products would a vegan avoid?", "What are natural Bongo drums made with?", "What are synthetic Bongo drums made with?", "Are #2 included in #1 and #3 excluded?"], "evidence": [[[["Veganism-1"]], [["Drumhead-3"]], [["Drumhead-5"]], ["operation"]], [[["Veganism-1"]], ["no_evidence"], ["no_evidence"], ["operation"]], [[["Veganism-1"]], [["Bongo drum-6"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Another term is \"environmental veganism\", which refers to the avoidance of animal products on the premise that the industrial farming of animals is environmentally damaging and unsustainable."], ["Originally, drumheads were made from animal hide and were first used in early human history, long before records began."], ["In recent years, companies such as Remo have begun manufacturing synthetic counterparts (most notably Fiberskin) for certain hand drums such as congas, and also banjos."]]}, {"qid": "6fbffb56e95e01397a1d", "term": "Bern", "description": "Place in Switzerland", "question": "Is Bern a poor choice for a xenophobic Swiss citizen to live?", "answer": false, "facts": ["Xenophobic people do not like people from other countries, such as tourists.", "Bern Switzerland was once described by CNN as being a relatively tourist free area.", "Zurich and Geneva get the most tourist traffic out of any city in Switzerland."], "decomposition": ["Who do xenophobic people want to avoid?", "What was Bern Switzerland once described by CNN as?", "Is #1 the same as #2?"], "evidence": [[[["Xenophobia-1"]], [["Bern-39", "Canton of Bern-64"], "no_evidence"], ["operation"]], [[["Xenophobia-1"]], [["Bern-39"], "no_evidence"], ["no_evidence", "operation"]], [[["Xenophobia-1"]], [["Bern-17"], "no_evidence"], ["operation"]]], "golden_sentence": [["It is an expression of perceived conflict between an ingroup and an outgroup and may manifest in suspicion by the one of the other's activities, a desire to eliminate their presence, and fear of losing national, ethnic or racial identity."], ["", ""]]}, {"qid": "cd04d5b73eedb452a591", "term": "People's Volunteer Army", "description": "Communist Chinese forces during the Korean War", "question": "Could all People's Volunteer Army hypothetically be transported on Symphony of the Seas?", "answer": false, "facts": ["The People's Volunteer Army had 780,000 troops.", "The cruise ship, Symphony of the Seas, has a capacity of 5,518 people."], "decomposition": ["How many people were in the  People's Volunteer Army?", "How many people can the Symphony of the Seas carry?", "Is #2 greater than #1?"], "evidence": [[[["People's Volunteer Army-1"]], [["Symphony of the Seas-4"]], ["operation"]], [[["People's Volunteer Army-1"]], [["Symphony of the Seas-4"]], ["operation"]], [[["People's Volunteer Army-1"], "no_evidence"], [["Symphony of the Seas-4"]], ["operation"]]], "golden_sentence": [["The initial (October 25 \u2013 November 5, 1950) units in the PVA included 38th, 39th, 40th, 42nd, 50th, 66th Corps totalling 250,000 men, and eventually about 3 million Chinese civilian and military personnel served in Korea by July 1953."], ["She is able to accommodate 5,518 passengers at double occupancy up to a maximum capacity of 6,680 passengers, as well as a 2,200-person crew."]]}, {"qid": "f5cb261cc87e16177618", "term": "LinkedIn", "description": "Social networking website for people in professional occupations", "question": "Did Kim Il-sung network on LinkedIn?", "answer": false, "facts": ["LinkedIn was launched in 2003.", "Kim Il-sung died in 1994."], "decomposition": ["When was LinkedIn launched?", "When did Kim Il-Sung die?", "Did #1 happen before #2?"], "evidence": [[[["LinkedIn-1"]], [["Kim Il-sung-40"]], ["operation"]], [[["LinkedIn-1"]], [["Death and state funeral of Kim Il-sung-1"]], ["operation"]], [[["LinkedIn-1"]], [["Kim Il-sung-1"]], ["operation"]]], "golden_sentence": [["Launched on May 5, 2003, it is mainly used for professional networking, including employers posting jobs and job seekers posting their CVs."], ["After several hours, the doctors from Pyongyang arrived, but despite their efforts to save him, Kim Il-sung died later that day at the age of 82."]]}, {"qid": "2342fbfa3e6b123d872d", "term": "Christopher Nolan", "description": "British\u2013American film director, screenwriter, and producer", "question": "Is Christopher Nolan indebted to Bob Kane?", "answer": true, "facts": ["Christopher Nolan rose to fame in large part because of his trilogy of Batman movies released from 2005 to 2012", "Bob Kane was the original artist and co-creator of Batman"], "decomposition": ["Who created the Batman?", "To what films was Christopher Nolan's Hollywood success attributed to from the 2000s to 2010s?", "Did #1 provide the source material to #2?"], "evidence": [[[["Batman-1"]], [["Christopher Nolan-16"]], ["operation"]], [[["Batman-1"]], [["Christopher Nolan-14"]], ["operation"]], [[["Batman-1"]], [["Bruce Wayne (The Dark Knight trilogy)-6"]], ["operation"]]], "golden_sentence": [["The character was created by artist Bob Kane and writer Bill Finger, and first appeared in Detective Comics #27 in 1939."], [""]]}, {"qid": "6072178730630aad75ae", "term": "Cane toad", "description": "World's largest toad", "question": "Would the average Hawaiian male experience more days on Earth compared to a wild cane toad?", "answer": true, "facts": ["Cane toads have a life expectancy of 10 to 15 years in the wild.", "The average life expectancy of a male born in Hawaii is 79.3 years as of 2018."], "decomposition": ["What is the average lifespan of an average Hawaiian male?", "What is the average lifespan of a wild cane toad?", "Is #1 greater than #2?"], "evidence": [[[["Human-58"], "no_evidence"], [["Cane toad-10"]], ["operation"]], [["no_evidence"], [["Cane toad-10"]], ["operation"]], [[["Life expectancy-12"], "no_evidence"], [["Cane toad-10"]], ["operation"]]], "golden_sentence": [["For various reasons, including biological/genetic causes, women live on average about four years longer than men\u2014as of 2013[update] the global average life expectancy at birth of a girl is estimated at 70.2 years compared to 66.1 for a boy."], ["They have a life expectancy of 10 to 15\u00a0years in the wild, and can live considerably longer in captivity, with one specimen reportedly surviving for 35\u00a0years."]]}, {"qid": "05bb3acd980eee7cb505", "term": "Suicide", "description": "Intentional act of causing one's own death", "question": "Is slitting your wrists an unreliable suicide method?", "answer": true, "facts": ["Wrist slitting has only a 6% mortality rate.", "Many people cannot complete the action of slitting their wrists due to pain or shock."], "decomposition": ["How often do people survive attempts to commit suicide by wrist-slitting?", "Does #1 indicate a high chance of survival?"], "evidence": [[["no_evidence"], ["no_evidence"]], [["no_evidence"], ["no_evidence"]], [[["Suicide methods-5"], "no_evidence"], ["operation"]]], "golden_sentence": []}, {"qid": "26cdd836701b0c75b486", "term": "Johnny Cash", "description": "American singer-songwriter and actor", "question": "Was it typical to see Johnny Cash on stage in a rainbow-colored outfit?", "answer": false, "facts": ["Johnny Cash regularly performed dressed all in black, wearing a long, black, knee-length coat.", "A rainbow-colored outfit would consist of the colors red, orange, yellow, green, blue, indigo, and violet."], "decomposition": ["What colors are in a rainbow?", "What color did Johnny Cash always wear on stage?", "Is #2 in the list of #1?"], "evidence": [[[["Rainbow-15"]], [["Johnny Cash-47"]], ["operation"]], [[["Rainbow flag-1", "Rainbow-15"]], [["Johnny Cash-47", "Johnny Cash-50"]], ["operation"]], [[["Rainbow-6"]], [["Johnny Cash-2"]], ["operation"]]], "golden_sentence": [["Newton, who admitted his eyes were not very critical in distinguishing colours, originally (1672) divided the spectrum into five main colours: red, yellow, green, blue and violet."], ["He regularly performed dressed all in black, wearing a long, black, knee-length coat."]]}, {"qid": "867b18b420a5928740d5", "term": "Woodrow Wilson", "description": "28th president of the United States", "question": "Was Woodrow Wilson sandwiched between two presidents from the opposing party?", "answer": true, "facts": ["Woodrow Wilson was a Democratic president and was between Taft and Harding.", "President William Howard Taft was a Repubican.", "President Warren G. Harding was a Republican."], "decomposition": ["Who was president before Woodrow Wilson?", "Who was president after Woodrow Wilson?", "What is the party of #1?", "What is the party of #2?", "Are #3 and #4 the same as each other and not the same as Wilson's party?"], "evidence": [[[["William Howard Taft-1"]], [["Warren G. Harding-1"]], [["William Howard Taft-3"]], [["Warren G. Harding-1"]], [["Woodrow Wilson-1"], "operation"]], [[["William Howard Taft-1"]], [["Presidency of Warren G. Harding-2"]], [["William Howard Taft-1"]], [["Warren G. Harding-26"]], [["Woodrow Wilson-30"], "operation"]], [[["Woodrow Wilson-2"]], [["Warren G. Harding-1", "Woodrow Wilson-1"]], [["William Howard Taft-3"]], [["Warren G. Harding-1"]], ["operation"]]], "golden_sentence": [["Taft was elected president in 1908, the chosen successor of Theodore Roosevelt, but was defeated for re-election by Woodrow Wilson in 1912 after Roosevelt split the Republican vote by running as a third-party candidate."], ["Warren Gamaliel Harding (November 2, 1865 \u2013 August 2, 1923) was the 29th president of the United States from 1921 until his death in 1923."], ["His administration was filled with conflict between the conservative wing of the Republican Party, with which Taft often sympathized, and the progressive wing, toward which Roosevelt moved more and more."], ["A member of the Republican Party, he was one of the most popular U.S. presidents to that point."], [""]]}, {"qid": "de40b948fcd6bab1b0d9", "term": "Holy Land", "description": "Term used by Jews, Christians, and Muslims to describe the Land of Israel and Palestine", "question": "Do worshipers of Shiva make a pilgrimage to the Holy Land?", "answer": false, "facts": ["The Holy Land is sacred to Judaism, Islam and Christianity", "Worshipers of Shiva are adherents of Hinduism"], "decomposition": ["Which group of religions have the Holy Land as a pilgrimage destination?", "Which religious group worships Shiva?", "Is #2 the same as any of #1?"], "evidence": [[[["Holy Land-1"]], [["Shiva-1"]], ["operation"]], [[["Pilgrimage-23"]], [["Shiva-10"]], ["operation"]], [[["Holy Land-4"], "no_evidence"], [["Shiva Puja-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["Jews, Christians, and Muslims all regard it as holy."], [""]]}, {"qid": "26a74da576e26181a6ce", "term": "Earth's magnetic field", "description": "Magnetic field that extends from the Earth\u2019s inner core to where it meets the solar wind", "question": "Would a compass attuned to Earth's magnetic field be a bad gift for a Christmas elf??", "answer": true, "facts": ["Christmas elves work in Santa's workshop on the North Pole", "Magnetic compasses point to the North Pole ", "If you are on the North Pole a compass will not work"], "decomposition": ["What do compasses do?", "Where do Christmas elves work?", "Would #1 not be effective if you are already at #2?"], "evidence": [[[["Compass-1"]], [["Christmas elf-1"]], [["North Magnetic Pole-1"]]], [[["Compass-1"]], [["North Pole-61"]], ["operation"]], [[["Compass-19", "Compass-20", "Compass-7"]], [["Christmas elf-1", "Christmas elf-7"]], [["Compass-35"], "operation"]]], "golden_sentence": [["Compasses often display markings for angles in degrees in addition to (or sometimes instead of) the rose."], ["In American, Canadian, Irish, and British cultures, a Christmas elf is a diminutive elf that lives with Santa Claus at the North Pole and acts as his helper."], [""]]}, {"qid": "ed17a0408fb1ebdb40d0", "term": "RoboCop", "description": "1987 science fiction film directed by Paul Verhoeven", "question": "Is RoboCop director from same country as Gaite Jansen?", "answer": true, "facts": ["Robocop was directed by Paul Verhoeven.", "Paul Verhoeven was born in Amsterdam, Netherlands.", "Gaite Jansen is an actress known for Jett and Peaky Blinders and was born in Rotterdam, Netherlands."], "decomposition": ["Who is the director of the movie RoboCop?", "Where was #1 born?", "Where was Gaite Jansen born?", "Are #2 and #3 the same?"], "evidence": [[[["RoboCop-1"]], [["Paul Verhoeven-1", "Paul Verhoeven-4"]], [["Gaite Jansen-1"]], ["operation"]], [[["RoboCop-1"]], [["Paul Verhoeven-4"]], [["Gaite Jansen-1"]], ["operation"]], [[["RoboCop-13"]], [["Paul Verhoeven-4"]], [["Gaite Jansen-1"]], ["operation"]]], "golden_sentence": [["RoboCop is a 1987 American satirical science fiction action film directed by Paul Verhoeven and written by Edward Neumeier and Michael Miner."], ["", "Paul Verhoeven was born in Amsterdam on 18 July 1938, the son of a school teacher, Wim Verhoeven, and a hat maker, Nel van Schaardenburg."], ["Gaite Sara Kim Jansen (born 25 December 1991) is a Dutch actress born in Rotterdam, the Netherlands."]]}, {"qid": "e74cec4cca23b22140ff", "term": "Mercury (element)", "description": "Chemical element with atomic number 80", "question": "Does Mercury make for good Slip N Slide material?", "answer": false, "facts": ["The Slip N Slide was an outdoor water slide toy.", "Mercury is a thick liquid at room temperature.", "Mercury is poisonous and used to kill hatters that lined their hats with the substance."], "decomposition": ["Who are Slip N Slides made for?", "Is Mercury safe for #1 to be around?"], "evidence": [[[["Slip 'N Slide-4"]], [["Mercury poisoning-27"]]], [[["Slip 'N Slide-4"]], [["Mercury poisoning-1", "Mercury poisoning-27"]]], [[["Slip 'N Slide-2"]], [["Mercury (element)-3", "Mercury (element)-5"], "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "b9fa863748cf12de345a", "term": "Torah", "description": "First five books of the Hebrew Bible", "question": "Does Happy Gilmore Productions CEO own a Torah?", "answer": true, "facts": ["The CEO of Happy Gilmore Productions is Adam Sandler.", "Adam Sandler's religious beliefs are Judaism. ", "The Torah is the first part of the bible in Judaism."], "decomposition": ["Who is the CEO of Happy Gilmore Productions?", "What religion does #1 follow?", "What religion uses the Torah?", "Is #2 the same as #3?"], "evidence": [[[["Happy Madison Productions-1"], "no_evidence"], [["Adam Sandler-26"]], [["Torah-1"]], ["operation"]], [[["Happy Madison Productions-1"], "no_evidence"], [["Adam Sandler-5"], "no_evidence"], [["Torah-1", "Torah-1"]], ["operation"]], [["no_evidence"], ["no_evidence"], [["Jews-1", "Torah-1"]], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["Titone had converted to Sandler's religion, Judaism."], [""]]}, {"qid": "a0a5b82832186a674ef1", "term": "ZIP Code", "description": "numeric postal code used in the United States", "question": "Do most people only memorize slightly over half of their ZIP code?", "answer": true, "facts": ["ZIP codes in the US are 9 digits in length. ", "Most forms in the US only require and have space for the first 5 digits of a ZIP code?"], "decomposition": ["How long are zip codes in the US?", "When forms ask for zip codes, how many spaces do they typically request?", "Is #2 less than #1?"], "evidence": [[[["ZIP Code-1"]], [["ZIP Code-10"], "no_evidence"], ["operation"]], [[["ZIP Code-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["ZIP Code-1"]], [["ZIP Code-10"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], ["A ZIP+4 Code uses the basic five-digit code plus four additional digits to identify a geographic segment within the five-digit delivery area, such as a city block, a group of apartments, an individual high-volume receiver of mail, a post office box, or any other unit that could use an extra identifier to aid in efficient mail sorting and delivery."]]}, {"qid": "b2bbfbeaffdb88c25c28", "term": "Honey badger", "description": "species of mammal", "question": "Would a snake have reasons to fear a honey badger?", "answer": true, "facts": ["Snakes are considered prey to Honey Badgers.", "Honey Badgers have sharp teeth and are carnivorous. "], "decomposition": ["What prey do Honey badgers hunt?", "Are snakes listed in #1?"], "evidence": [[[["Honey badger-21"]], [["Honey badger-21"]]], [[["Honey badger-21"]], ["operation"]], [[["Honey badger-21"]], ["operation"]]], "golden_sentence": [["Honey badgers studied in Kgalagadi Transfrontier Park preyed largely on geckos and skinks (47.9% of prey species), gerbils and mice (39.7% of prey)."], [""]]}, {"qid": "b30421b6b1c8bd490f81", "term": "Snowshoe", "description": "Footwear for walking easily across snow", "question": "Has Burger King  contributed to a decrease in need for snowshoes?", "answer": true, "facts": ["Burger king serves beef", "Beef farming is associated with increased global temperatures and decreased snowfall"], "decomposition": ["What is the main food item that burger king sells?", "What kind of meat is in #1?", "What does farming for #2 do to the global temperature?", "If #3 occurs, are people less lilely to need snowshoes?"], "evidence": [[[["Burger King-1"]], [["Pork-2"]], ["no_evidence"], ["no_evidence"]], [[["Burger King-1"]], [["Hamburger-1"]], [["Environmental impact of meat production-19"], "no_evidence"], [["Snowshoe-1"], "operation"]], [[["Burger King-1"]], [["Meat-9"]], [["Cattle-90"]], [["Global warming-3"]]]], "golden_sentence": [["Burger King (BK) is an American multinational chain of hamburger fast food restaurants."], ["Charcuterie is the branch of cooking devoted to prepared meat products, many from pork."]]}, {"qid": "598a86ec030c6dec5f75", "term": "Pablo Escobar", "description": "Colombian drug lord (1949\u20131993)", "question": "Did Pablo Escobar's nickname collection outshine Robert Moses Grove's?", "answer": true, "facts": ["Robert Moses Grove was a baseball player nicknamed Lefty Grove.", "Pablo Escobar had several nicknames including: Don Pablo, El Padrino, and El Patr\u00f3n."], "decomposition": ["How many nicknames did Pablo Escobar have?", "How many nicknames did Robert Moses Grove have?", "Is #1 greater than #2?"], "evidence": [[[["Pablo Escobar-28"]], [["Lefty Grove-1"]], [["Lefty Grove-1", "Pablo Escobar-28"]]], [[["Pablo Escobar-1", "Pablo Escobar-28"], "no_evidence"], [["Lefty Grove-1"]], ["operation"]], [[["Pablo Escobar-28"], "no_evidence"], [["Lefty Grove-1"]], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""], ["", ""]]}, {"qid": "4fbca4542b511a9a8426", "term": "Cell biology", "description": "Scientific Discipline that Studies Cells", "question": "Does cell biology teach about the life cycle of Al Qaeda?", "answer": false, "facts": ["Cell biology is a subdiscipline of biology that deals with the structure and function of cells in living organisms", "Al Qaeda is made up of terrorist cells", "Terrorist cells are small groups of terrorists acting semi-independently for the same cause"], "decomposition": ["What is the main topic that people learn about in Cell biology?", "What is Al Qaeda made up of?", "Is #1 the same as #2?"], "evidence": [[[["Cell biology-1"]], [["Al-Qaeda-2"]], ["operation"]], [[["Cell biology-1"]], [["Al-Qaeda-1"]], ["operation"]], [[["Cell biology-1"]], [["Al-Qaeda-2"]], ["operation"]]], "golden_sentence": [["Cell biology encompasses both prokaryotic and eukaryotic cells and can be divided into many sub-topics which may include the study of cell metabolism, cell communication, cell cycle, and cell composition."], ["Al-Qaeda operates as a network of Islamic extremists and Salafist jihadists."]]}, {"qid": "273a2c55dc03f60f44b9", "term": "Godzilla", "description": "Giant monster or kaiju", "question": "Could Godzilla have been killed by the Tohoku earthquake?", "answer": false, "facts": ["The Tohoku earthquake led to the Fukushima Daiichi nuclear power plant meltdown", "Nuclear meltdowns lead to a release of deadly levels of radiation", "Godzilla draws power from radiation and is not hurt by it"], "decomposition": ["What major accident was caused by the Tohoku Earthquake?", "What was released into the environment by #1?", "Does #2 cause harm to Godzilla?"], "evidence": [[[["2011 T\u014dhoku earthquake and tsunami-90"]], [["2011 T\u014dhoku earthquake and tsunami-90"]], [["Godzilla-2"], "operation"]], [[["2011 T\u014dhoku earthquake and tsunami-9"]], [["Fukushima Daiichi nuclear disaster-3"]], [["Godzilla-2"]]], [[["2011 T\u014dhoku earthquake and tsunami-9"]], [["2011 T\u014dhoku earthquake and tsunami-93"]], [["Godzilla-2"], "operation"]]], "golden_sentence": [[""], ["Officials from the Japanese Nuclear and Industrial Safety Agency reported that radiation levels inside the plant were up to 1,000 times normal levels, and that radiation levels outside the plant were up to eight times normal levels."], [""]]}, {"qid": "037df3b69b4a03cbd0f1", "term": "Great Depression", "description": "20th-century worldwide economic depression", "question": "Can a person be diagnosed with a Great Depression?", "answer": false, "facts": ["The Great Depression was a severe worldwide economic depression that took place mostly during the 1930s, beginning in the United States.", "Major depressive disorder (MDD), also known simply as depression, is a mental disorder characterized by at least two weeks of low mood that is present across most situations."], "decomposition": ["What was the Great Depression?", "What is depression that people suffer from?", "Are #1 and #2 the same?"], "evidence": [[[["Great Depression-1"]], [["Minor depressive disorder-2"]], [["Great Depression-1", "Minor depressive disorder-2"], "operation"]], [[["Great Depression-1"]], [["Depression (mood)-1"]], ["operation"]], [[["Great Depression-1"]], [["Depression (mood)-8", "Major depressive disorder-30"]], ["operation"]]], "golden_sentence": [["The Great Depression was a severe worldwide economic depression that took place mostly during the 1930s, beginning in the United States."], ["A person is considered to have minor depressive disorder if they experience 2 to 4 depressive symptoms, with one of them being either depressed mood or loss of interest or pleasure, during a 2-week period."], ["", ""]]}, {"qid": "5b17d5c729a227c106bb", "term": "Marco Rubio", "description": "United States Senator from Florida", "question": "Could Marco Rubio ride the Candymonium roller coaster at Hershey Park?", "answer": true, "facts": ["The Candymonium roller coaster is restricted to park visitors over 54\" tall (4'6\").", "Marco Rubio is 5'9\" tall."], "decomposition": ["What is the height limit for the Candymonium roller coaster?", "How tall is Marco Rubio?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Candymonium-6"], "no_evidence"], ["no_evidence"], ["operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Candymonium-1"], "no_evidence"], [["Marco Antonio Rubio-1", "Marco Rubio-1"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["The ride will be 4,636 feet (1,413\u00a0m) long with a maximum speed of 76 miles per hour (122\u00a0km/h) and an initial drop of 210 feet (64\u00a0m)."]]}, {"qid": "fd7e75103b9f1847d019", "term": "Spinach", "description": "species of plant", "question": "Was the amount of spinach Popeye ate unhealthy?", "answer": true, "facts": ["Popeye was a cartoon character that ate whole cans of spinach to maintain his fighting strength.", "Spinach is high in oxalates which can lead to kidney stones.", "Too much spinach can lead to bloating, gas, fever, and diarrhea."], "decomposition": ["What is spinach high in?", "What does eating too much of #1 do to a body?", "Are #2's bad for a body?"], "evidence": [[[["Spinach-1"]], [["Oxalate-10"]], [["Kidney stone disease-1", "Oxalate-10"]]], [[["Spinach-7"]], [["Oxalate-10"]], ["operation"]], [[["Spinach-7"]], [["Vitamin A-13"]], [["Vitamin A-16"]]]], "golden_sentence": [[""], [""], ["", ""]]}, {"qid": "ee405969de174f7a45d2", "term": "Aladdin", "description": "Middle Eastern folk tale", "question": "Is the voice of the Genie from Disney's Aladdin still alive?", "answer": false, "facts": ["The Genie was voiced by comedian Robin Williams.", "Robin Williams died in 2014."], "decomposition": ["Who provided the voice of the Genie in Aladdin?", "Is #1 still alive?"], "evidence": [[[["Robin Williams-33"]], [["Robin Williams-1"], "operation"]], [[["Aladdin (1992 Disney film)-1", "Robin Williams-33"]], [["Robin Williams-60"]]], [[["Robin Williams-33"]], [["Robin Williams-3"]]]], "golden_sentence": [[""], [""]]}, {"qid": "e2e4a6066e36b90edc4d", "term": "Snowy owl", "description": "species of bird", "question": "Could a snowy owl survive in the Sonoran?", "answer": false, "facts": ["The Snowy owl is a bird native to the Arctic regions of North America.", "Temperatures in the North American Arctic range from -36.4F to 50F.", "The Sonoran is one of the hottest deserts in the world.", "The Sonoran Desert can get as hot as 118F."], "decomposition": ["Where are Snowy owls found?", "What is the temperature range of #1?", "How hot does it get in the Sonoran Desert?", "Is #3 within #2?"], "evidence": [[[["Snowy owl-1"]], [["Climate of the Arctic-40"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Snowy owl-1"]], [["Arctic-4"]], [["Sonoran Desert-18"]], ["operation"]], [[["Snowy owl-1"]], [["Arctic-4"]], [["Sonoran Desert-18"]], ["operation"]]], "golden_sentence": [["Snowy owls are native to Arctic regions in North America and the Palearctic."], ["It shows the average temperature in the coldest months is in the \u221230s, and the temperature rises rapidly from April to May; July is the warmest month, and the narrowing of the maximum and minimum temperature lines shows the temperature does not vary far from freezing in the middle of summer; from August through December the temperature drops steadily."]]}, {"qid": "8dc79ebc511bbe644d03", "term": "Transport", "description": "Human-directed movement of things or people between locations", "question": "Can any person with a driver's license work in transport of aviation fuel?", "answer": false, "facts": ["A vehicle operator must possess a Commercial Driver's License to work in the delivery of fuel.", "The process for getting a CDL is much lengthier than that of getting a Driver's License. "], "decomposition": ["What certification is required of a driver to work in transport of aviation fuel?", "Does #1 consist only of a regular driver's license?"], "evidence": [[[["Truck driver-63"]], ["operation"]], [[["Dangerous goods-33"]], ["operation"]], [[["Pilot licensing and certification-2"]], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "7653461a4e16787f8ead", "term": "Astronaut", "description": "Person who commands, pilots, or serves as a crew member of a spacecraft", "question": "Has every astronaut survived their space journey?", "answer": false, "facts": ["In 1986, the space shuttle Challenger exploded during launch, killing all astronauts aboard.", "In 2003, the space shuttle Columbia also exploded, again killing its entire crew.", "Various other space flights have resulted in fatal disasters."], "decomposition": ["How many astronauts have died during a mission?", "Is #1 equal to zero?"], "evidence": [[[["Astronaut-59"]], ["operation"]], [[["Astronaut-59"]], [["Astronaut-59"], "operation"]], [[["Astronaut-59"]], ["operation"]]], "golden_sentence": [["Eighteen astronauts (fourteen men and four women) have lost their lives during four space flights."]]}, {"qid": "055518206578e9530184", "term": "Hypothermia", "description": "A human body core temperature below 35.0\u00b0C", "question": "Would hypothermia be a concern for a human wearing zoot suit on Triton?", "answer": true, "facts": ["A zoot suit was a man's suit of an exaggerated style popular in the 1940s.", "Triton is one of the coldest planets in the solar system.", "Triton is located about 2.8 billion miles from the warmth of the sun.", "Triton has an average temperature of -235.0\u00b0C", "A zoot suit is made of thin material such as cloth."], "decomposition": ["What is the average temperature on Triton?", "What material are zoot suits made of?", "Below which body temperature will hypothermia set in?", "Would clothes made of #2 be unable to keep body temperature above #3 in ambient temperature of #1?"], "evidence": [[[["Triton (moon)-3"]], [["Zoot Suit Riots-2"]], [["Hypothermia-1"]], ["operation"]], [[["Triton (moon)-3"]], [["Zoot Suit Riots-12", "Zoot Suit Riots-2"]], [["Human body temperature-30"]], ["operation"]], [[["Triton (moon)-3"]], [["Zoot suit-15"], "no_evidence"], [["Hypothermia-1"]], ["operation"]]], "golden_sentence": [["During its 1989 flyby of Triton, Voyager 2 found surface temperatures of \u2212235\u00a0\u00b0C and also discovered active geysers; Voyager 2 remains the only spacecraft to visit Triton."], ["American servicemen and white Angelenos attacked and stripped children, teenagers, and youths who wore zoot suits, ostensibly because they considered the outfits, which were made from a lot of fabric, to be unpatriotic during World War II."], ["Hypothermia is defined as a body core temperature below 35.0\u00a0\u00b0C (95.0\u00a0\u00b0F) in humans."]]}, {"qid": "6981f7dbe2159d320fc9", "term": "Spice Girls", "description": "British girl group", "question": "Could the Spice Girls compete against \u017dRK Kumanovo?", "answer": false, "facts": ["The Spice Girls had 5 members.", "\u017dRK Kumanovo is a women's handball club from Kumanovo in the Republic of Macedonia.", "Handball is a sport played by two teams of seven players each."], "decomposition": ["How many members did the Spice Girls have?", "What sport does  \u017dRK Kumanovo compete in?", "How many people are on a team in #2?", "Is #1 greater than or equal to #3?"], "evidence": [[[["Spice Girls-1"]], [["\u017dRK Kumanovo-1"]], [["Handball-1"]], ["operation"]], [[["Spice Girls-1"]], [["\u017dRK Kumanovo-1"]], [["Handball-1"]], ["operation"]], [[["Spice Girls-1"]], [["\u017dRK Kumanovo-1"]], [["Handball-1"]], ["operation"]]], "golden_sentence": [[""], ["\u017dRK Kumanovo competes in the Super Liga."], ["Play media Handball (also known as team handball, European handball or Olympic handball) is a team sport in which two teams of seven players each (six outcourt players and a goalkeeper) pass a ball using their hands with the aim of throwing it into the goal of the other team."]]}, {"qid": "535c4019093c150247f8", "term": "Amazon (company)", "description": "American electronic commerce and cloud computing company", "question": "Was Amazon involved in the lunar landing?", "answer": false, "facts": ["The lunar landing occurred in 1969.", "Amazon was founded in 1994."], "decomposition": ["When did the lunar landing take place?", "When was the company Amazon founded?", "Is #2 before #1?"], "evidence": [[[["Apollo 11-1"]], [["Amazon (company)-3"]], ["operation"]], [[["Moon landing-2"]], [["Amazon (company)-3"]], ["operation"]], [[["Apollo 11-1"]], [["Amazon (company)-3"]], ["operation"]]], "golden_sentence": [["Commander Neil Armstrong and lunar module pilot Buzz Aldrin formed the American crew that landed the Apollo Lunar Module Eagle on July 20, 1969, at 20:17 UTC."], ["Amazon was founded by Jeff Bezos in Bellevue, Washington, in July 1994."]]}, {"qid": "b84ec951677290d8cac8", "term": "Tahiti", "description": "Largest island of French Polynesia", "question": "Could all Tahiti hotels hypothetically accommodate US D-Day troops?", "answer": false, "facts": ["Tahiti has 47 hotels with around 3,000 rooms.", "The US D-Day force consisted of:  23,250 on Utah Beach, 34,250 on Omaha Beach, and 15,500 airborne troops."], "decomposition": ["How many hotel rooms are there in Tahiti?", "How many people can comfortably share a hotel room?", "What is #1 multiplied by #2?", "How many troops were in the US D-Day force?", "Is #3 greater than or equal to #4?"], "evidence": [[[["Tahiti-75"], "no_evidence"], ["no_evidence"], ["operation"], [["Operation Overlord-1"], "no_evidence"], ["operation"]], [["no_evidence"], ["no_evidence"], ["operation"], [["Normandy landings-39"]], ["operation"]], [[["Tahiti-75"]], [["Hotel-42"], "no_evidence"], ["no_evidence", "operation"], [["Normandy landings-3"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "0a123041a228e4355ba2", "term": "Bengal fox", "description": "species of mammal", "question": "Is a bengal fox likely to see the Superbowl?", "answer": false, "facts": ["The Superbowl is the championship game of the National Football League", "The National Football League is a sports league for American football", "American football enjoys the majority of its popularity in the United States", "The bengal fox is found exclusively on the Indian subcontinent"], "decomposition": ["Where is the Super Bowl usually held?", "Where is the Bengal Fox mostly found?", "Are #1 and #2 within the same country?"], "evidence": [[[["National Football League-1", "Super Bowl-1"]], [["Bengal fox-1"]], [["India-1", "United States-1"]]], [[["American football-52"]], [["Bengal fox-1"]], ["operation"]], [[["Super Bowl-65"]], [["Bengal fox-1"]], ["operation"]]], "golden_sentence": [["Following the conclusion of the regular season, seven teams from each conference (four division winners and three wild card teams) advance to the playoffs, a single-elimination tournament culminating in the Super Bowl, which is usually held on the first Sunday in February and is played between the champions of the NFC and AFC.", ""], ["The Bengal fox (Vulpes bengalensis), also known as the Indian fox, is a fox endemic to the Indian subcontinent and is found from the Himalayan foothills and Terai of Nepal through southern India and from southern and eastern Pakistan to eastern India and southeastern Bangladesh."], ["Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east.", ""]]}, {"qid": "b81deb7e4ecac54307d7", "term": "Harvey Milk", "description": "American politician who became a martyr in the gay community", "question": "Did Harvey Milk ever run for governor?", "answer": false, "facts": ["In 1977 Harvey Milk was elected to the San Francisco Board of Supervisors.", "Less than a year later, he was assassinated before he could run for higher offices."], "decomposition": ["What were Harvey Milk's political campaigns?", "Does #1 include a gubernatorial campaign?"], "evidence": [[[["Harvey Milk-1"]], [["Harvey Milk-1"]]], [[["Harvey Milk-2", "Jim Foster (activist)-4"]], ["operation"]], [[["Harvey Milk-1"]], [["Governor-4"], "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "34d039bc48151f291554", "term": "Nine Inch Nails", "description": "American industrial rock band", "question": "Is Nine Inch Nails a good guest for students in earliest grade to take Iowa tests?", "answer": false, "facts": ["The Iowa test is administered to students in kindergarten through eighth grade.", "Nine Inch Nails is a heavy industrial rock band formed in 1988.", "Nine Inch Nails albums are stamped with the explicit warning label."], "decomposition": ["Who were the Nine Inch Nails?", "What are #1's albums rated as?", "What is the age range for students who have to take the Iowa test?", "Is it safe to show kids that are #3 things rated #2?"], "evidence": [[[["Nine Inch Nails-1"]], [["Nine Inch Nails-10"], "no_evidence"], [["Iowa Tests of Educational Development-1"], "no_evidence"], ["operation"]], [[["Nine Inch Nails-1"]], [["Nine Inch Nails-21", "Parental Advisory-1"], "no_evidence"], [["Iowa Assessments-2", "Iowa Assessments-3"], "no_evidence"], ["operation"]], [[["Nine Inch Nails-1"]], [["Nine Inch Nails-17"], "no_evidence"], [["Iowa Assessments-2"]], ["operation"]]], "golden_sentence": [[""], [""], ["The Iowa Tests of Educational Development (ITED) are a set of standardized tests given annually to high school students in many schools in the United States, covering Grades 9 to 12."]]}, {"qid": "82df7c824e18b8b27bbb", "term": "Disco", "description": "music genre", "question": "Did the Beatles write any music in the Disco genre?", "answer": false, "facts": ["The Beatles were active from 1960 until 1969.", "Disco began to appear around 1972."], "decomposition": ["When were the Beatles active as a full group?", "When did disco start?", "Is #2 before #1?"], "evidence": [[[["Break-up of the Beatles-1", "The Beatles-1"]], [["Disco-1"]], ["operation"]], [[["The Beatles-1"]], [["Disco-1"]], ["operation"]], [[["The Beatles-1", "The Beatles-3"]], [["Disco-1"]], ["operation"]]], "golden_sentence": [["The Beatles were an English rock band consisting of John Lennon, Paul McCartney, George Harrison and Ringo Starr from August 1962 to September 1969.", ""], ["Disco is a genre of dance music and a subculture that emerged in the 1970s from the United States\u2019 urban nightlife scene."]]}, {"qid": "3cfd6b7e1a8981379710", "term": "Coca", "description": "group of plant varieties cultivated for coca production", "question": "Are leaves from coca good for gaining weight?", "answer": false, "facts": ["People who want to gain weight seek to increase caloric intake ", "Coca leaf contains chemicals that suppress hunger and thirst"], "decomposition": ["What kinds of foods do people who want to gain weight look for?", "Are coca leaves #1?"], "evidence": [[[["Weight gain-2"], "no_evidence"], [["Coca-4", "Stimulant-21"], "no_evidence", "operation"]], [[["Weight gain-8"], "no_evidence"], [["Coca-4", "Cocaine-43"], "operation"]], [[["Weight gain-2"]], [["Coca-18"]]]], "golden_sentence": [[""], ["", ""]]}, {"qid": "c5d5564b5685d2febd54", "term": "Funeral", "description": "ceremony for a person who has died", "question": "Do embalmed bodies feel different at funerals?", "answer": true, "facts": ["Embalming fluid fixates into the bodily tissues and replaces the bodily fluid.", "Bodies that have not been embalmed tend to feel soft.", "When embalming fluid fills the body, the body becomes firm."], "decomposition": ["What does Embalming a body do to it?", "Does #1 make a body hard?", "What does a non embalmed body feel like", "Is #2 different from #3?"], "evidence": [[[["Embalming-4"]], ["operation"], ["no_evidence"], ["operation"]], [[["Embalming-37"], "no_evidence"], ["no_evidence", "operation"], ["no_evidence"], ["no_evidence", "operation"]], [[["Embalming-1"]], [["Embalming chemicals-14"]], [["Natural burial-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["They did so by removing organs, ridding the body of moisture, and covering the body with natron."]]}, {"qid": "35bce43f87a89a120169", "term": "Stone Cold Steve Austin", "description": "American professional wrestler", "question": "Did Stone Cold Steve Austin wrestle in three different centuries?", "answer": false, "facts": ["A century is a period of 100 years.", "Stone Cold Steve Austin made his wrestling debut on September 30, 1989.", "Stone Cold Steve Austin retired on March 30, 2003.", "The 20th (twentieth) century was a century that began on January 1, 1901 and ended on December 31, 2000.", "The 21st century began on January 1, 2001, and will end on December 31, 2100."], "decomposition": ["When did Stone Cold Steve Austin start wrestling?", "When did Stone Cold Steve Austin stop wrestling?", "In what century is #1?", "In what century is #2?", "Is #4 minus #3 greater than 1?"], "evidence": [[[["Stone Cold Steve Austin-6"]], [["Stone Cold Steve Austin-45"]], ["no_evidence"], ["no_evidence"], ["operation"]], [[["Stone Cold Steve Austin-6"]], [["Stone Cold Steve Austin-4"]], [["20th century-2"]], [["21st century-1"]], ["operation"]], [[["Stone Cold Steve Austin-6"]], [["Stone Cold Steve Austin-41"]], ["operation"], ["operation"], ["operation"]]], "golden_sentence": [["His first lesson in that came from Tony Falk, the referee in his 1989 televised WCCW debut against Frogman LeBlanc, who called the spots to lead him to a pinfall and a $40 payday."], [""]]}, {"qid": "7efbf0d0d6264e96b66f", "term": "Kane (wrestler)", "description": "American professional wrestler, actor, businessman, and politician", "question": "Can Kane challenge Joe Biden in this year's primaries?", "answer": false, "facts": ["Kane is a member of the Republican Party", "Joe Biden is a member of the Democratic Party", "Primaries are conducted between members of the same political party"], "decomposition": ["Primaries are held within what?", "What #1 does Joe Biden belong to?", "What #1 does Kane belong to?", "Are #2 and #3 the same?"], "evidence": [[[["Primary election-1"]], [["Joe Biden-1"]], [["Kane (wrestler)-1"]], ["operation"]], [[["Primary election-5"]], [["Joe Biden-1"]], [["Kane (wrestler)-1"]], ["operation"]], [[["Primary election-1"]], [["Joe Biden-1"]], [["Kane (wrestler)-1"]], ["operation"]]], "golden_sentence": [["Depending on the country and administrative divisions within the country, voters might consist of the general public in what is called an open primary, or solely the members of a political party in what is called a closed primary."], ["Joseph Robinette Biden Jr. (/\u02ccr\u0252b\u026a\u02c8n\u025bt \u02c8ba\u026ad\u0259n/; born November 20, 1942) is an American politician who served as the 47th vice president of the United States from 2009 to 2017 and represented Delaware in the U.S. Senate from 1973 to 2009."], [""]]}, {"qid": "2f742aa96a9ac1d5d7e8", "term": "Richard III of England", "description": "15th-century King of England", "question": "Did Richard III know his grandson?", "answer": false, "facts": ["Richard III died in battle at age 32.", "He had only one son, who died during childhood, and therefore had no grandchildren.", "Even if he did have grandchildren, he would have been dead long before they were born based on his age at death."], "decomposition": ["Did Richard III have any grandchildren?"], "evidence": [[[["Richard III of England-71"], "no_evidence"]], [[["Richard III of England-43"], "operation"]], [[["Richard III of England-40"], "operation"]]], "golden_sentence": [["The paternal side, however, demonstrated some variance from what had been expected, with the DNA showing no links to the purported descendants of Richard's great-great-grandfather Edward III of England through Henry Somerset, 5th Duke of Beaufort."]]}, {"qid": "8e1318615369b26ba15c", "term": "Ringo Starr", "description": "British musician, drummer of the Beatles", "question": "Has Ringo Starr been in a relatively large number of bands?", "answer": false, "facts": ["RIngo Starr has been in three bands besides the Beatles.", "Mike Patton, lead singer of Faith No More, has been in at least 12 bands.", "Dave Grohl, lead singer of the Foo Fighters, has played in over 10 bands."], "decomposition": ["How many bands has Ringo Starr been part of?", "How many bands has Mike Patton been part of?", "How many bands has Dave Grohl been part of?", "Is #1 larger than #2 or #3?"], "evidence": [[[["Plastic Ono Band-2", "Ringo Starr & His All-Starr Band-1", "Rory Storm-1", "The Beatles-1"]], [["Mike Patton-1"]], [["Dave Grohl-1", "Scream (band)-5", "Teenage Time Killers-1"]], ["operation"]], [[["Ringo Starr-13"]], [["Mike Patton-18"]], [["Dave Grohl-28"]], ["operation"]], [[["Ringo Starr-3"], "no_evidence"], [["Mike Patton-1"]], [["Dave Grohl-1"]], ["operation"]]], "golden_sentence": [["", "", "", "With a line-up comprising John Lennon, Paul McCartney, George Harrison and Ringo Starr, they are regarded as the most influential band of all time."], [""], ["", "", ""]]}, {"qid": "1029c52f64026e7c17e1", "term": "Pancake", "description": "Thin, round cake made of eggs, milk and flour", "question": "Are pancakes typically prepared in a pot?", "answer": false, "facts": ["Pancakes are usually fried on a shallow flat surface.", "Pots typically have high walls.", "Griddles and skillets are low, shallow flat pans appropriate for pancakes."], "decomposition": ["What kind of surface are pancakes usually made on?", "Does a pot have #1?"], "evidence": [[[["Pancake-1", "Pancake-57"], "no_evidence"], [["Cookware and bakeware-52"], "operation"]], [[["Pancake-1"]], [["Pancake-1"]]], [[["Pancake-1"]], [["Cookware and bakeware-4"], "operation"]]], "golden_sentence": [["A pancake (or hotcake, griddlecake, or flapjack, not to be confused with oat bar flapjacks) is a flat cake, often thin and round, prepared from a starch-based batter that may contain eggs, milk and butter and cooked on a hot surface such as a griddle or frying pan, often frying with oil or butter.", ""], ["Generally within the classic batterie de cuisine a vessel designated \"pot\" is round, has \"ear\" handles in diagonal opposition, with a relatively high height to cooking surface ratio, and is intended for liquid cooking such as stewing, stocking, brewing or boiling."]]}, {"qid": "9a48326a1e2daeb57c42", "term": "Sunday", "description": "day of the week", "question": "Is Christmas always celebrated on a Sunday?", "answer": false, "facts": ["Christmas is always celebrated on December 25.", "A specific date on the calendar rotates to the following day of the week each year.", "Christmas can therefore be any day of the week."], "decomposition": ["What date does Christmas fall on each year?", "Does #1 always fall on a Sunday?"], "evidence": [[[["Christmas-1"]], [["Sunday-1"], "operation"]], [[["Christmas-3"]], [["Christmas-3"]]], [[["Christmas-1"]], [["Christmas Sunday-2"]]]], "golden_sentence": [["Christmas (or Feast of the Nativity) is an annual festival commemorating the birth of Jesus Christ, observed primarily on December 25 as a religious and cultural celebration among billions of people around the world."], [""]]}, {"qid": "94dfa1922f47b71a213b", "term": "Toronto Star", "description": "Newspaper in Toronto, Ontario, Canada", "question": "Would someone in Boston not receive the Toronto Star?", "answer": true, "facts": ["The Toronto Star is only distributed in Canada.", "Boston is located in the United States."], "decomposition": ["Where is the Toronto Star distributed?", "Where is Boston located?", "Is #2 not in #1?"], "evidence": [[[["Toronto Star-1"]], [["Boston-1"]], ["operation"]], [[["Toronto Star-1"]], [["Boston-1"]], ["operation"]], [[["Toronto Star-1"]], [["Boston-1"]], ["operation"]]], "golden_sentence": [[""], ["Boston (UK: /\u02c8b\u0252st\u0259n/, US: /\u02c8b\u0254\u02d0st\u0259n/) is the capital and most populous city of the Commonwealth of Massachusetts in the United States, and the 21st most populous city in the United States."]]}, {"qid": "7d524bb9bd987516f9d9", "term": "Saint", "description": "one who has been recognized for having an exceptional degree of holiness, sanctity, and virtue", "question": "Will Lhamo Thondup be considered by Catholic Church to be a saint?", "answer": false, "facts": ["Lhamo Thondup is the current Dalai Lama.", "The Dalai Lama is the spiritual leader of Tibetan Buddhism.", "Catholic saints must fill specific criteria, including being devout Christians."], "decomposition": ["What religion must one belong to as part of the criteria to be a Catholic saint?", "Which religion does Lhamo Thondup practice?", "Is #2 the same as #1?"], "evidence": [[[["Saint-1"]], [["14th Dalai Lama-1"]], [["14th Dalai Lama-1", "Saint-1"]]], [[["Saint-7"], "no_evidence"], [["14th Dalai Lama-1"]], ["no_evidence", "operation"]], [[["Saint-1"]], [["14th Dalai Lama-1"]], ["operation"]]], "golden_sentence": [["In Catholic, Eastern Orthodox, Anglican, Oriental Orthodox, and Lutheran doctrine, all of their faithful deceased in Heaven are considered to be saints, but some are considered worthy of greater honor or emulation; official ecclesiastical recognition, and consequently veneration, is given to some saints through the process of canonization in the Catholic Church or glorification in the Eastern Orthodox Church."], ["Dalai Lamas are important monks of the Gelug school, the newest school of Tibetan Buddhism, which was formally headed by the Ganden Tripas."], ["", ""]]}, {"qid": "6b2d9e23d23bfaa0f351", "term": "Cinnamon", "description": "spice obtained from the inner bark of several trees from the genus Cinnamomum", "question": "Is a spice grinder ueseless for the cheapest cinnamon sticks?", "answer": true, "facts": ["Different species of cinnamon require different grinding methods to make the spice", "Indonesian cinnamon quills are thick and capable of damaging a spice or coffee grinder.", "Indonesian cinnamon is the most common and cheapest cinnamon in the USA"], "decomposition": ["What type of cinnamon sticks are the cheapest in the US?", "What is the size of #1?", "Because of #2, would it cause damage to a typical spice grinder?"], "evidence": [[[["Cinnamon-2"]], [["Cinnamomum cassia-2"]], ["operation"]], [[["Cinnamon-2"]], [["Cinnamomum cassia-2"]], [["Herb grinder-1"], "operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["The tree grows to 10\u201315\u00a0m (33\u201349\u00a0ft) tall, with greyish bark and hard, elongated leaves that are 10\u201315\u00a0cm (3.9\u20135.9\u00a0in) long and have a decidedly reddish colour when young."]]}, {"qid": "f4b2da9e93d90d7921fc", "term": "Sea of Japan", "description": "Marginal sea between Japan, Russia and Korea", "question": "Is the Sea of Japan landlocked within Japan?", "answer": false, "facts": ["The sea of Japan touches Japan, Russia and the Koreas", "Japan has no landlocked sea"], "decomposition": ["Which countries have a shoreline that touches the Sea of Japan?", "Is Japan the only item in #1?"], "evidence": [[[["Sea of Japan-1"]], ["operation"]], [[["Sea of Japan-13"]], ["operation"]], [[["Sea of Japan-1"]], ["operation"]]], "golden_sentence": [["It is bordered by Japan, Korea (North and South) and Russia."]]}, {"qid": "9f2941e1ffe2e67394bc", "term": "Hundred Years' War", "description": "Series of conflicts and wars between England and France during the 14th and 15th-century", "question": "Did the first Duke of Valentinois play a key role in the Hundred Years' War?", "answer": false, "facts": ["The Hundred Years' War was a conflict between England and France from 1337-1453", "Cesare Borgia, the son of Pope Alexander VI, was the first Duke of Valentinois.", "Cesare Borgia was born in 1475."], "decomposition": ["When did the Hundred Years' War end?", "Who was the first Duke of Valentinois?", "When was #2 born?", "Is #3 before #1?"], "evidence": [[[["Hundred Years' War (1415\u20131453)-1"]], [["Duke of Valentinois-6"]], [["Honor\u00e9 II, Prince of Monaco-1"]], ["operation"]], [[["Hundred Years' War-1"]], [["Cesare Borgia-7"]], [["Cesare Borgia-1"]], ["operation"]], [[["Hundred Years' War-1"]], [["Cesare Borgia-7"]], [["Cesare Borgia-1"]], ["operation"]]], "golden_sentence": [["It followed a long period of peace from the end of the Caroline War in 1389."], ["However, since the title's inheritance was restricted to male heirs, and because Antoine had only daughters and no sons, it was due to pass his brother, Fran\u00e7ois-Honor\u00e9 Grimaldi, but became extinct on 22 July 1715 when Fran\u00e7ois-Honor\u00e9 forfeited his right to succeed Antoine."], ["Honor\u00e9 II (24 December 1597 \u2013 10 January 1662) was Prince of Monaco from 1604 to 1662."]]}, {"qid": "299d237ba3e3bf7f1824", "term": "Dustin Hoffman", "description": "American actor and director", "question": "Can you substitute the pins in a bowling alley lane with Dustin Hoffman's Oscars?", "answer": false, "facts": ["There are ten pins in a bowling alley lane", "Dustin Hoffman has won two Oscars"], "decomposition": ["How many pins are on a bowling alley lane?", "How many Oscars has Dustin Hoffman won?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Bowling-1"]], [["Dustin Hoffman-35", "Dustin Hoffman-45"]], ["operation"]], [[["Ten-pin bowling-1"]], [["Dustin Hoffman-5"]], ["operation"]], [[["Ten-pin bowling-1"]], [["Dustin Hoffman-5"]], ["operation"]]], "golden_sentence": [["In the U.S. and Canada, the term bowling usually refers to ten-pin bowling; in the U.K. and Commonwealth Countries, however, the term bowling could also refer to lawn bowls."], ["", "Levinson, Hoffman and Cruise worked for two years on the film, and Hoffman's performance gained him his second Academy Award."]]}, {"qid": "18a0f3129bcaa3a9adc6", "term": "Whole genome sequencing", "description": "A process that determines the complete DNA sequence of an organism's genome at a single time", "question": "Did Rosalind Franklin contribute to work that led to Whole Genome Sequencing?", "answer": true, "facts": ["Rosalind Franklin used specialized photography to capture the first photos of the double helix.", "The double helix is the form that DNA takes.", "Without understanding the structure of DNA, genome sequencing would be impossible."], "decomposition": ["Rosalind Franklin capture the first photo of what?", "What takes the form of #1?", "Is understanding #2 essential to genome sequencing?"], "evidence": [[[["Rosalind Franklin-19"]], [["DNA-1"]], [["Whole genome sequencing-1"], "operation"]], [[["Rosalind Franklin-3"]], [["Rosalind Franklin-3"]], [["Whole genome sequencing-1"], "operation"]], [[["Rosalind Franklin-3"]], [["Rosalind Franklin-3"]], [["Rosalind Franklin-3", "Whole genome sequencing-3"]]]], "golden_sentence": [[""], ["Alongside proteins, lipids and complex carbohydrates (polysaccharides), nucleic acids are one of the four major types of macromolecules that are essential for all known forms of life."], [""]]}, {"qid": "0a274f46de0b80a406fe", "term": "Blue", "description": "A primary colour between purple and green", "question": "Is the most expensive color in the world Blue?", "answer": true, "facts": ["Blue is a primary color.", "Blue is between violet and green on the visible light spectrum.", "Lapis Lazuli is used to make ultramarine. ", "Ultramarine is a pigment of Blue", "Processing Lapis Lazuli into Ultramarine is the most expensive of color processes."], "decomposition": ["What was the most expensive pigment used by Renaissance painters?", "Is #1 a shade of the color blue?"], "evidence": [[[["Ultramarine-2"]], [["Ultramarine-1"], "operation"]], [[["Blue-2"]], [["Ultramarine-2"], "operation"]], [[["Ultramarine-2"]], [["Ultramarine-2"]]]], "golden_sentence": [["Ultramarine was the finest and most expensive blue used by Renaissance painters."], [""]]}, {"qid": "1b8761c1be344913edcc", "term": "French Revolution", "description": "Revolution in France, 1789 to 1798", "question": "Was the French Revolution televised?", "answer": false, "facts": ["The french revolution occurred during the 1700's.", "Television was invented in 1927. "], "decomposition": ["When did the French Revolution occur?", "When did televisions become common?", "Is #2 before or within #1?"], "evidence": [[[["French Revolution-1"]], [["Television-2"]], ["operation"]], [[["French Revolution-1"]], [["Television-2"]], ["operation"]], [[["French Revolution-1"]], [["Television-2"]], ["operation"]]], "golden_sentence": [["The French Revolution (French: R\u00e9volution fran\u00e7aise [\u0281ev\u0254lysj\u0254\u0303 f\u0281\u0251\u0303s\u025b\u02d0z]) was a period of far-reaching social and political upheaval in France and its colonies beginning in 1789 and ending in 1799."], ["Television became available in crude experimental forms in the late 1920s, but it would still be several years before the new technology would be marketed to consumers."]]}, {"qid": "ddec9ded2c4a361713fd", "term": "Eskimo", "description": "Name used to describe Indigenous people from the circumpolar region", "question": "Do the Eskimos sunbathe frequently?", "answer": false, "facts": ["Sunbathing requires a high amount of sunshine.", "The Eskimos live in regions that receive very little sunshine.", "The Eskimos live in very cold regions, which would make it dangerous to be exposed to outside temperatures without clothes."], "decomposition": ["What is necessary for sunbathing?", "Where do Eskimos live?", "Is #1 frequently present in #2?"], "evidence": [[[["Sun tanning-1"]], [["Eskimo-1"]], [["Climate of the Arctic-1"], "operation"]], [[["Sun tanning-1"], "no_evidence"], [["Eskimo-1"]], [["Arctic-5"], "no_evidence", "operation"]], [[["Sun tanning-1"]], [["Igloo-2"]], ["operation"]]], "golden_sentence": [[""], ["Eskimo (/\u02c8\u025bsk\u026amo\u028a/ ESS-kih-moh) or Eskimos are the indigenous circumpolar peoples who have traditionally inhabited the northern circumpolar region from eastern Siberia (Russia) to Alaska (United States), Canada, and Greenland."], [""]]}, {"qid": "2e4d95b36f60496ca6db", "term": "Small intestine", "description": "part of the digestive tract, following the stomach and followed by the large intestine", "question": "Will the small intenstine break down a cotton ball?", "answer": false, "facts": ["The small intestine does not digest cellulose ", "Cotton fiber is 90% cellulose"], "decomposition": ["What compound is cotton mostly made up of?", "Can the small intestine digest #1?"], "evidence": [[[["Cotton-1"]], [["Cellulose-3"]]], [[["Cotton pad-1"]], [["Bezoar-1"], "no_evidence", "operation"]], [[["Cotton-1"]], [["Cellulose-3"]]]], "golden_sentence": [["The fiber is almost pure cellulose."], [""]]}, {"qid": "6f07f8282cf3b9292100", "term": "University of Pittsburgh", "description": "American state-related research university located in Pittsburgh, Pennsylvania", "question": "Is University of Pittsburgh easier to enter than FBI?", "answer": true, "facts": ["The University of Pittsburgh has around a 60% acceptance rate.", "The FBI estimated accepting 900 agents out of 16000 applicants in 2019."], "decomposition": ["What percent of applicants does University of Pittsburgh accept?", "How many applications did the FBI get in 2019?", "Out of #2, how many were accepted?", "What is #3 divided by #2?", "Is #1 greater than #4?"], "evidence": [[["no_evidence"], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"], ["no_evidence", "operation"]], [[["University of Pittsburgh-2"], "no_evidence"], [["Federal Bureau of Investigation-59"], "no_evidence"], ["no_evidence"], ["operation"], ["operation"]], [[["University of Pittsburgh-35"], "no_evidence"], [["Federal Bureau of Investigation-61"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"], ["no_evidence", "operation"]]], "golden_sentence": []}, {"qid": "def5f2765e8f6f941346", "term": "Pregnancy", "description": "time when children develop inside the mother's body before birth", "question": "Do women often need new shoes during their pregnancy?", "answer": true, "facts": ["Pregnancy can cause swelling in the feet and legs.", "For safety and comfort, one needs to get new shoes if the size of their feet change."], "decomposition": ["Which signs and symptoms of pregnancy in women affect the lower extremities?", "Do #1 lead to a change in size of affected areas?"], "evidence": [[[["Pregnancy-9"]], ["operation"]], [[["Pregnancy-9"]], ["operation"]], [[["Inferior vena cava syndrome-3"]], [["Edema-1"], "operation"]]], "golden_sentence": [["Can be caused by inferior vena cava syndrome resulting from compression of the inferior vena cava and pelvic veins by the uterus leading to increased hydrostatic pressure in lower extremities."]]}, {"qid": "b99fab7b62f0b5a6153d", "term": "Depression (mood)", "description": "state of low mood and fatigue", "question": "Would Seroquel be the first treatment recommended by a doctor to someone with depression?", "answer": false, "facts": ["Seroquel is a powerful drug that is prescribed for bipolar disorder.", "Seroquel has sedating effects and can increase feelings of depression.", "Depression is usually treated by SSRI's. ", "Seroquel is an atypical antipsychotic."], "decomposition": ["What is Seroquel typically used for?", "What are the side effects of taking #1?", "Would #2 be helpful for someone with depression"], "evidence": [[[["Quetiapine-32"]], [["Quetiapine-17"]], ["no_evidence"]], [[["Quetiapine-1"]], [["Quetiapine-2"]], ["operation"]], [[["Quetiapine-1"]], [["Quetiapine-11"]], [["Quetiapine-12"], "operation"]]], "golden_sentence": [["However, Seroquel XR has become available in U.S. pharmacies only after the FDA approved Seroquel XR for use as maintenance treatment for schizophrenia, in addition to acute treatment of the illness, on November 16, 2007."], ["Extended release (XR) formulations tend to produce less sedation, dose-by-dose than the immediate release formulations) Common (1\u201310% incidence) adverse effects High blood pressure Orthostatic hypotension High pulse rate High blood cholesterol Elevated serum triglycerides Abdominal pain Constipation Increased appetite Vomiting Increased liver enzymes Backache Asthenia Insomnia Lethargy Tremor Agitation Nasal congestion Pharyngitis Fatigue Pain Dyspepsia (Indigestion) Peripheral oedema Dysphagia Extrapyramidal disease: quetiapine and clozapine are noted for their relative lack of extrapyramidal side effects Weight gain: SMD 0.43\u00a0kg when compared to placebo."]]}, {"qid": "a8264c32511efba03274", "term": "QR code", "description": "trademark for a type of matrix barcode", "question": "Do you have to put on glasses to read a QR code?", "answer": false, "facts": ["Glasses are used to improve one's vision capabilities.", "QR codes are not readable by humans and have to be read by machines or programs."], "decomposition": ["Can a human read QR codes?"], "evidence": [[["no_evidence", "operation"]], [[["QR code-5"]]], [[["QR code-1"]]]], "golden_sentence": []}, {"qid": "cd06afe78198e22d3fa6", "term": "Anchovy", "description": "Family of fishes", "question": "Can an anchovy born in 2020 survive 25th US census?", "answer": false, "facts": ["The US Census takes place every ten years.", "The 24th US Census took place in 2020.", "The 25th US Census will take place in 2030.", "The average lifespan of an anchovy is five years."], "decomposition": ["What is the ordinal number of the 2020 U.S. Census?", "How many years after #1 wll the 25th census occur?", "What is the maximum life span of an anchovy?", "Is #3 greater than #2?"], "evidence": [[[["2020 United States Census-1"]], [["United States Census-1"]], [["Japanese anchovy-1"], "no_evidence"], ["operation"]], [[["2020 United States Census-1"]], [["United States Census Bureau-4"]], [["European anchovy-7", "Japanese anchovy-1"]], ["operation"]], [[["2020 United States Census-1"]], [["United States Census-1"]], [["European anchovy-7"]], ["operation"]]], "golden_sentence": [["The United States Census of 2020 is the twenty-fourth United States Census."], ["The actual Enumeration shall be made within three years after the first meeting of the Congress of the United States, and within every subsequent Term of ten Years\"."], ["They live up to 2\u20133 years, similar to European anchovy."]]}, {"qid": "34db1b7b265b7c192b47", "term": "Double-slit experiment", "description": "Physics experiment, showing light can be modelled by both waves and particles", "question": "Can a minor replicate the double-slit experiment?", "answer": true, "facts": ["A minor is a human child.", "The double-slit experiment can theoretically be replicated by any human."], "decomposition": ["What species of living things does 'minor' refer to?", "Can #1 replicate the double-slit experiment?"], "evidence": [[[["Minor (law)-1"]], [["Double-slit experiment-2"]]], [[["Minor (law)-1"]], [["Double-slit experiment-1"], "operation"]], [[["Age of majority-1"]], [["Double-slit experiment-2"], "operation"]]], "golden_sentence": [[""], ["However, such experiments demonstrate that particles do not form the interference pattern if one detects which slit they pass through."]]}, {"qid": "47573814fdd27602a871", "term": "Alan Greenspan", "description": "13th Chairman of the Federal Reserve in the United States", "question": "Do Squidward Tentacles and Alan Greenspan have different musical passions?", "answer": false, "facts": ["Squidward Tentacles plays the clarinet.", "Alan Greenspan played clarinet and saxophone along with Stan Getz.", "Alan Greenspan studied clarinet at the Juilliard School from 1943 to 1944."], "decomposition": ["What musical instruments does Squidward Tentacles play?", "What musical instruments does Alan Greenspan play?", "Is at least one instrument in #1 also found in #2?"], "evidence": [[[["Squidward Tentacles-5"]], [["Alan Greenspan-5"]], ["operation"]], [[["Squidward Tentacles-5"]], [["Alan Greenspan-5"]], ["operation"]], [[["Squidward Tentacles-5"]], [["Alan Greenspan-5"]], ["operation"]]], "golden_sentence": [["Squidward lives in a constant state of self-pity and misery; he is unhappy with his humdrum lifestyle and yearns for celebrity status, wealth, hair, and a glamorous and distinguished career as a musician or painter with a passion for art and playing the clarinet."], ["He played clarinet and saxophone along with Stan Getz."]]}, {"qid": "9a2571faa1554261fa1f", "term": "Infinitive", "description": "grammatical form", "question": "Is Shakespeare famous because of the infinitive form?", "answer": true, "facts": ["Shakespeare wrote the play Hamlet", "Hamlet contains one of Shakespeare's most famous passages, Hamlet's soliloquy", "Hamlet's soliloquy begins with the line 'To be or not to be', which uses the infinitive form"], "decomposition": ["The use of the infinitive form in \"To be or not to be\" appears in which popular soliloquy?", "Which book contained #1", "Did Williams Shakespeare write #2?"], "evidence": [[[["To be, or not to be-1"]], [["Hamlet-1"]], [["Hamlet-2"], "operation"]], [[["To be, or not to be-1"]], [["To be, or not to be-1"]], [["To be, or not to be-1"]]], [[["To be, or not to be-1"]], [["Hamlet-1"]], ["operation"]]], "golden_sentence": [["\"To be, or not to be\" is the opening phrase of a soliloquy uttered by Prince Hamlet in the so-called \"nunnery scene\" of William Shakespeare's play Hamlet, Act 3, Scene 1."], [""], [""]]}, {"qid": "4b4383deebfc06d6b37b", "term": "Television", "description": "Telecommunication medium for transmitting and receiving moving images", "question": "Did Gandhi watch the television show Bonanza?", "answer": false, "facts": ["Bonanza was a television show that aired from  September 12, 1959 until January 16, 1973.", "Gandhi was assassinated on January 30, 1948."], "decomposition": ["How long ago did Bonanza first air?", "How long ago did Gandhi die?", "Is #1 greater than #2?"], "evidence": [[[["Bonanza-1"]], [["Mahatma Gandhi-1"]], ["operation"]], [[["Bonanza-1"]], [["Family of Mahatma Gandhi-1"]], ["operation"]], [[["Bonanza-1"]], [["Mahatma Gandhi-90"]], ["operation"]]], "golden_sentence": [["Bonanza is an American western television series that ran on NBC from September 12, 1959, to January 16, 1973."], ["Mohandas Karamchand Gandhi (/\u02c8\u0261\u0251\u02d0ndi, \u02c8\u0261\u00e6ndi/; 2 October 1869\u00a0\u2013 30 January 1948) was an Indian lawyer,"]]}, {"qid": "b1fc0daa3010d9c6f360", "term": "Farmer", "description": "person that works in agriculture", "question": "Do you need a farmer to make a circuit board?", "answer": false, "facts": ["Farmers cultivate and produce crops and/or livestock for sale or consumption", "Circuit boards contain various man made materials as well as metals", "Metals are produced from the earth by miners"], "decomposition": ["What do farmers produce?", "What are the things needed to make a circuit board?", "Is any of #1 part of #2?"], "evidence": [[[["Farmer-1"]], [["Printed circuit board-1"]], ["operation"]], [[["Farmer-9"]], [["Stamped circuit board-4"]], ["operation"]], [[["Farmer-1"]], [["Printed circuit board-1"]], ["operation"]]], "golden_sentence": [["A farmer (also called an agriculturer) is a person engaged in agriculture, raising living organisms for food or raw materials."], ["A printed circuit board (PCB) mechanically supports and electrically connects electrical or electronic components using conductive tracks, pads and other features etched from one or more sheet layers of copper laminated onto and/or between sheet layers of a non-conductive substrate."]]}, {"qid": "aa2cd67fdfda410739a4", "term": "Yuri Gagarin", "description": "Soviet pilot and cosmonaut, first human in space", "question": "Would LeBron James hypothetically glance upwards at Yuri Gagarin?", "answer": false, "facts": ["LeBron James is 6 feet 9 inches tall.", "Yuri Gagarin was 5 feet 2 inches tall.", "Typically shorter individuals look up at taller individuals when they are speaking as it is polite to look face to face at someone when you are speaking to them."], "decomposition": ["How tall is LeBron James?", "How tall was Yuri Gagarin?", "Is #1 lesser than #2?"], "evidence": [[[["LeBron James-42"], "no_evidence"], [["Yuri Gagarin-9"], "no_evidence"], ["no_evidence", "operation"]], [[["LeBron James-42"]], [["Yuri Gagarin-9"]], ["operation"]], [[["LeBron James-42"]], [["Yuri Gagarin-9"]], ["operation"]]], "golden_sentence": [["Standing 6\u00a0feet 9\u00a0inches (2.06\u00a0m) and weighing 250 pounds (113\u00a0kg), James has mainly played at the small forward position but has also started at point guard, shooting guard and power forward, but he can also play the other position."], ["The chief engineer of the programme Sergei Korolev also specified that candidates, to fit in the limited space in the Vostok capsule, should weigh less than 72\u00a0kg (159\u00a0lb) and be no taller than 1.70 metres (5\u00a0ft 7\u00a0in) in height; Gagarin was 1.57 metres (5\u00a0ft 2\u00a0in) tall."]]}, {"qid": "6104bd58a6033b5560a7", "term": "Ludacris", "description": "American rapper and actor", "question": "Is Ludacris in same music genre as 2000's Binaural?", "answer": false, "facts": ["Ludacris is a rapper, particularly in the southern rap style.", "Binaural was a 2000 album released by Pearl Jam.", "Pearl Jam is a grunge rock band formed in Seattle."], "decomposition": ["What genre does Ludacris produce music in?", "Who recorded the 2000 album Binaural?", "What genre does #2 produce music in?", "Is #1 the same as #3?"], "evidence": [[[["Hip hop music-1", "Ludacris-1"]], [["Binaural (album)-1"]], [["Binaural (album)-1"]], ["operation"]], [[["Ludacris-1"]], [["Binaural (album)-1"]], [["Pearl Jam-1"]], ["operation"]], [[["Ludacris-1"]], [["Binaural (album)-1"]], [["Pearl Jam-1"]], ["operation"]]], "golden_sentence": [["Hip hop music, also called hip-hop or rap music, is a genre of popular music developed in the United States by inner-city African Americans and Latino Americans in the Bronx borough of New York City in the 1970s.", ""], ["Binaural is the sixth studio album by U.S. alternative rock band Pearl Jam, released May 16, 2000 through Epic Records."], [""]]}, {"qid": "88925173d5fe5cbdb2d8", "term": "Seismology", "description": "The scientific study of earthquakes and propagation of elastic waves through a planet", "question": "Did Brad Peyton need to know about seismology?", "answer": true, "facts": ["Brad Peyton directed the movie San Andreas", "San Andreas is an earthquake disaster film", "Seismology is the science of earthquakes and related phenomena"], "decomposition": ["What does the study of seismology involve?", "What was the movie San Andreas primarily about?", "Did Brad Peyton direct San Andreas and is #2 included in #1?"], "evidence": [[[["Seismology-1"]], [["San Andreas (film)-1"]], [["San Andreas (film)-1"], "operation"]], [[["Seismology-1"]], [["San Andreas (film)-1"]], [["Brad Peyton-1"], "operation"]], [[["Seismology-1"]], [["San Andreas (film)-1"]], ["operation"]]], "golden_sentence": [[""], ["Its plot centers on an earthquake caused by the San Andreas Fault devastating Los Angeles and the San Francisco Bay Area."], ["San Andreas is a 2015 American disaster film directed by Brad Peyton and written by Carlton Cuse, with Andre Fabrizio and Jeremy Passmore receiving story credit."]]}, {"qid": "84a74bd879365dafc965", "term": "Breast cancer", "description": "cancer that originates in the mammary gland", "question": "Is someone more likely to survive having breast cancer in Japan than in Sweden?", "answer": false, "facts": ["84.70% of people in Japan with breast cancer survive", "86.20% of people in Sweden with breast cancer survive"], "decomposition": ["What percentage of people survive breast cancer in Japan?", "What percentage of people survive breast cancer in Sweden?", "Is #1 more than #2?"], "evidence": [[[["Breast cancer-4"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""]]}, {"qid": "924f21073276224fcf84", "term": "2009", "description": "Year", "question": "Could $1 for each 2009 eclipse buy a copy of TIME magazine in 2020?", "answer": true, "facts": ["The 2020 Newsstand price of TIME magazine is $5.99.", "There were six eclipses in 2009 including 2 solar and 4 lunar eclipses."], "decomposition": ["What was the price of a single issue of TIME magazine in 2020?", "How many solar eclipses were there in 2009?", "How many lunar eclipses were there in 2009?", "What is #2 plus #3?", "Is #4 greater than or equal to #1?"], "evidence": [[[["Time (magazine)-5"], "no_evidence"], [["July 2009 lunar eclipse-6"], "no_evidence"], [["July 2009 lunar eclipse-4"], "no_evidence"], ["operation"], ["operation"]], [[["Time (magazine)-5"], "no_evidence"], [["Solar eclipse of January 26, 1990-1", "Solar eclipse of July 22, 2009-1"]], [["August 2009 lunar eclipse-1"]], ["operation"], ["operation"]], [[["Time (magazine)-22", "Time (magazine)-5"], "no_evidence"], [["Solar eclipse-3"], "no_evidence"], [["Lunar eclipse-23"], "no_evidence"], ["no_evidence", "operation"], ["no_evidence", "operation"]]], "golden_sentence": [["The cover price was 15\u00a2 (equivalent to $2.25 in 2019)."], ["This eclipse is one of five lunar eclipses in a short-lived series."], ["Third eclipse this season: 6 August 2009 Penumbral Lunar Eclipse"]]}, {"qid": "cd73ab95b74d450677b0", "term": "Olympia, Washington", "description": "State capital and city in Washington, United States", "question": "Does Olympia Washington share name with Hephaestus's workshop location?", "answer": true, "facts": ["Olympia Washington, is named after Mount Olympus.", "Mount Olympus is a mountain range in Washington named after the ancient Greek Mount Olympus.", "Hephaestus was the ancient Greek god of the forge and had a workshop on Mount Olympus."], "decomposition": ["Where did Hephaestus have his workshop?", "Olympia, Washington derived it's name from what mountain?", "Is #2 the same as #1?"], "evidence": [[[["Hephaestus-5"]], [["Olympia, Washington-3"]], ["operation"]], [[["Hephaestus-2"]], [["Mount Olympus-1"]], ["operation"]], [[["Hephaestus-5"], "operation"], [["Hephaestus-5"], "operation"], ["operation"]]], "golden_sentence": [["Hephaestus had his own palace on Olympus, containing his workshop with anvil and twenty bellows that worked at his bidding."], ["In 1850, the town settled on the name Olympia, at the suggestion of local resident Colonel Isaac N. Ebey, because of its view of the Olympic Mountains to the Northwest."]]}, {"qid": "8da5b467084dfdd65137", "term": "Saudi Aramco", "description": "Saudi Arabian petroleum and natural gas company", "question": "Was Saudi Aramco started due to an assassination?", "answer": true, "facts": ["Saudi Aramco was formed in response to oil shortages during World War I", "The origins of World War I can be traced to the assassination of Archduke Franz Ferdinand in Sarajevo"], "decomposition": ["Saudi Aramco was formed in response to oil shortages during what major conflict?", "What event is widely acknowledged to have started #1?", "Is #2 an assassination?"], "evidence": [[[["Saudi Aramco-7"]], [["Paris in the Belle \u00c9poque-96"]], [["Assassination-1"]]], [[["Saudi Aramco-7"]], [["Assassination of Archduke Franz Ferdinand-1"]], ["operation"]], [[["Saudi Aramco-7"]], [["Assassination of Archduke Franz Ferdinand-1"]], ["operation"]]], "golden_sentence": [["Saudi Aramco's origins trace to the oil shortages of World War\u00a0I and the exclusion of American companies from Mesopotamia by the United Kingdom and France under the San Remo Petroleum Agreement of 1920."], [""], ["It is an act that may be done for financial gain, to avenge a grievance, from a desire to acquire fame or notoriety, or because of a military, security, insurgent or secret police group's command to carry out the assassination."]]}, {"qid": "7b5517301e4aebae3192", "term": "Order of the British Empire", "description": "British order of chivalry", "question": "Is Hermione Granger eligible for the Order of the British Empire?", "answer": false, "facts": ["The Order of the British Empire is awarded to people that have made significant contributions to the United Kingdom", "Hermione Granger is a fictional character from the Harry Potter series of books"], "decomposition": ["What criteria makes one eligible for the Order of the British Empire?", "Does Hermione Granger meet #1?"], "evidence": [[[["Order of the British Empire-2"]], [["Hermione Granger-1"], "no_evidence"]], [[["Order of the British Empire-1"], "no_evidence"], [["Hermione Granger-3"], "no_evidence", "operation"]], [[["Order of the British Empire-2"]], [["Hermione Granger-1"], "operation"]]], "golden_sentence": [["It was established on 4 June 1917 by King George V and comprises five classes across both civil and military divisions, the most senior two of which make the recipient either a knight if male or dame if female."], [""]]}, {"qid": "05367cae08b8e2829de9", "term": "Carl Linnaeus", "description": "Swedish botanist, physician, and zoologist", "question": "Did Linnaeus edit Darwin's draft of Origin of Species?", "answer": false, "facts": ["Linnaeus died in 1778", "Origin of Species was published in 1859"], "decomposition": ["When did Carl Linnaeus pass away?", "When was Origin of Species first published?", "Is #2 before #1?"], "evidence": [[[["Carl Linnaeus-1"]], [["On the Origin of Species-1"]], ["operation"]], [[["Carl Linnaeus-1"]], [["On the Origin of Species-1"]], ["operation"]], [[["Carl Linnaeus-1"]], [["On the Origin of Species-1"]], ["operation"]]], "golden_sentence": [["Carl Linnaeus (/l\u026a\u02c8ni\u02d0\u0259s, l\u026a\u02c8ne\u026a\u0259s/; 23 May 1707 \u2013 10 January 1778), also known after his ennoblement as Carl von Linn\u00e9 (Swedish pronunciation:\u00a0[\u02c8k\u0251\u02d0\u026d f\u0254n l\u026a\u02c8ne\u02d0] (listen)), was a Swedish botanist, zoologist, and physician who formalised binomial nomenclature, the modern system of naming organisms."], ["On the Origin of Species (or, more completely, On the Origin of Species by Means of Natural Selection, or the Preservation of Favoured Races in the Struggle for Life), published on 24 November 1859, is a work of scientific literature by Charles Darwin which is considered to be the foundation of evolutionary biology."]]}, {"qid": "47e8c7dd545f450e462a", "term": "Riksdag", "description": "Legislative body of Sweden", "question": "Is the Riksdag a political entity in Scandinavia?", "answer": true, "facts": ["The Riksdag is the legislative branch of the Swedish government.", "Sweden is part of Scandinavia."], "decomposition": ["What country does the Riksdag belong to?", "Which countries are part of Scandinavia?", "Is #1 included in #2?"], "evidence": [[[["Riksdag-1"]], [["Scandinavia-1"]], ["operation"]], [[["Riksdag-1"]], [["Scandinavia-1"]], ["operation"]], [[["Riksdag-1"]], [["Scandinavia-1"]], ["operation"]]], "golden_sentence": [["The Riksdag (Swedish: riksdagen or Sveriges riksdag) is the national legislature and the supreme decision-making body of Sweden."], ["The term Scandinavia in local usage covers the three kingdoms of Denmark, Norway, and Sweden."]]}, {"qid": "6aacc3a3eee1fc818e7e", "term": "Heart", "description": "organ for the circulation of blood in animal circulatory systems", "question": "Is a jellyfish safe from atherosclerosis?", "answer": true, "facts": ["Atherosclerosis is a condition in which the arteries to the heart are blocked.", "Jellyfish use their guts to circulate nutrients because they do not have hearts."], "decomposition": ["What structures are affected by atherosclerosis?", "What bodily system does #1 contribute to?", "What structures are found in the jellyfish #2?", "Are there structures in common in both #1 and #3?"], "evidence": [[[["Atherosclerosis-1"]], [["Circulatory system-1"]], [["Jellyfish-18"]], ["operation"]], [[["Atherosclerosis-1"]], [["Artery-2"]], [["Jellyfish-18"]], ["operation"]], [[["Atherosclerosis-1"]], [["Artery-2"]], [["Jellyfish-18"]], ["operation"]]], "golden_sentence": [["When severe, it can result in coronary artery disease, stroke, peripheral artery disease, or kidney problems, depending on which arteries are affected."], ["The circulatory system, also called the cardiovascular system or the vascular system, is an organ system that permits blood to circulate and transport nutrients (such as amino acids and electrolytes), oxygen, carbon dioxide, hormones, and blood cells to and from the cells in the body to provide nourishment and help in fighting diseases, stabilize temperature and pH, and maintain homeostasis."], ["Nematocysts, which deliver the sting, are located mostly on the tentacles; true jellyfish also have them around the mouth and stomach."]]}, {"qid": "64efb44a01b6f4fd97cc", "term": "Quran", "description": "The central religious text of Islam", "question": "Would Dave Chappelle pray over a Quran?", "answer": true, "facts": ["Dave Chappelle converted to Islam in 1998.", "Dave Chappelle has not deviated from his religious beliefs since 1998 and is a practicing Muslim.", "Practicing Muslims pray often."], "decomposition": ["Which group uses the Quran as their religious text?", "Does Dave Chappelle belong to #1?"], "evidence": [[[["Quran-1"]], [["Dave Chappelle-57"], "operation"]], [[["Quran-1"]], [["Dave Chappelle-57"]]], [[["Quran-1"]], [["Dave Chappelle-57"], "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "f8a08334b9e15db086a8", "term": "Justin Timberlake", "description": "American singer, record producer, and actor", "question": "Can Justin Timberlake ride Shipwreck Falls at Six Flags?", "answer": true, "facts": ["Shipwreck Falls is a boat ride at Six Flags", "The minimum height for Shipwreck Falls is 42\"", "Justin Timberlake is 73\" tall"], "decomposition": ["What is Shipwreck Falls?", "What is the minimum height required to ride #1?", "How tall is Justin Timberlake?", "Is #3 bigger than #2?"], "evidence": [[[["Shipwreck Falls-1"]], ["no_evidence"], [["Justin Timberlake-1"], "no_evidence"], ["operation"]], [[["Shipwreck Falls-1"]], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Shipwreck Falls-1"]], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Shipwreck Falls is a Shoot-the-Chute water ride at many Herschend and Six Flags theme parks, in the United States."], [""]]}, {"qid": "872e0f9c1f072b343406", "term": "Dementia", "description": "long-term brain disorders causing impaired memory, reasoning, and normal function together with personality changes", "question": "Can dementia be cured with a cast?", "answer": false, "facts": ["Dementia refers to various disorders of the brain.", "Casts are used to help treat broken bones.", "The brain does not contain any bones."], "decomposition": ["What part of the body does Dementia affect?", "What do cast help fix?", "Are there any #2 in #1?"], "evidence": [[[["Dementia-21"]], [["Bone fracture-22"]], [["Bone fracture-22", "Dementia-21"], "operation"]], [[["Dementia-1"]], [["Orthopedic cast-1"]], ["operation"]], [[["Dementia-1"]], [["Orthopedic cast-1"]], [["Brain-1"]]]], "golden_sentence": [["The part of the brain most affected by Alzheimer's is the hippocampus."], ["Occasionally smaller bones, such as phalanges of the toes and fingers, may be treated without the cast, by buddy wrapping them, which serves a similar function to making a cast."], ["", ""]]}, {"qid": "c78f6f8236a28f549f43", "term": "D\u00fcsseldorf", "description": "Place in North Rhine-Westphalia, Germany", "question": "Does D\u00fcsseldorf have only a small number of smoggy days each year?", "answer": true, "facts": ["Mercer's 2012 Quality of Living survey ranked D\u00fcsseldorf the sixth most livable city in the world.", "Clean air is an important attribute for a livable city.", "Smog is a term for air pollution."], "decomposition": ["What is another term for smog?", "What is D\u00fcsseldorf ranked as in Mercer's 2012 Quality of Living survey?", "To be #2, does a country need to have limited #1 days a year?"], "evidence": [[[["Smog-1"]], [["Global Liveability Ranking-2"], "no_evidence"], ["operation"]], [[["Smog-1"]], [["D\u00fcsseldorf-2"]], ["operation"]], [[["Smog-1"]], [["D\u00fcsseldorf-2"]], [["Most livable cities-7"]]]], "golden_sentence": [["The word \"smog\" was coined in the early 20th century, and is a contraction (portmanteau) of the words smoke and fog to refer to smoky fog; its opacity, and odor."], [""]]}, {"qid": "4bff0cabe59ec582d403", "term": "Saint", "description": "one who has been recognized for having an exceptional degree of holiness, sanctity, and virtue", "question": "Can a false pope become a saint?", "answer": true, "facts": ["A false pope, or antipope, is someone that tries to claim they are the true pope but the church rejects them.", "Hippolytus (c. 170\u2013235 AD) headed a schismatic group as a rival to the Bishop of Rome, thus becoming an antipope.", "Hippolytus (c. 170\u2013235 AD) was named a saint in the Roman Catholic Church."], "decomposition": ["Which actions could make the Catholic church consider one a false pope or antipope?", "What role did Hippolytus (c. 170\u2013235 AD) play in the schismatic group against the Bishop of Rome?", "Is #2 a form of #1 and he still became saint?"], "evidence": [[[["Antipope-1"]], [["Hippolytus of Rome-1"]], [["Hippolytus of Rome-24"], "operation"]], [[["Antipope-1"], "no_evidence"], [["Hippolytus of Rome-1"]], [["Hippolytus of Rome-2"], "operation"]], [[["Antipope-1"]], [["Antipope-4"]], [["Antipope-16", "Saint-7"], "no_evidence"]]], "golden_sentence": [[""], ["One older theory asserts he came into conflict with the popes of his time and seems to have headed a schismatic group as a rival to the Bishop of Rome, thus becoming an Antipope."], [""]]}, {"qid": "2d354dff1748979b5bb1", "term": "Snowshoe", "description": "Footwear for walking easily across snow", "question": "Can a snake wear a snowshoe?", "answer": false, "facts": ["Snowshoes are worn by attaching them to the wearer's feet.", "Snakes do not have feet."], "decomposition": ["Which part of the body are snowshoes worn on?", "Do snakes have #1?"], "evidence": [[[["Snowshoe-1"]], [["Snake-1"]]], [[["Snowshoe-1"]], [["Snake-1"], "operation"]], [[["Footwear-1", "Snowshoe-1"]], [["Snake-1"]]]], "golden_sentence": [["Snowshoes work by distributing the weight of the person over a larger area so that the person's foot does not sink completely into the snow, a quality called \"flotation\"."], ["Legless lizards resemble snakes, but several common groups of legless lizards have eyelids and external ears, which snakes lack, although this rule is not universal (see Amphisbaenia, Dibamidae, and Pygopodidae)."]]}, {"qid": "699fa782768709819795", "term": "Starch", "description": "glucose polymer used as energy store in plants", "question": "Can a wheelbarrow full of starch kill hyperglycemics?", "answer": true, "facts": ["Hyperglycemia is a condition in which people have higher than normal blood glucose levels.", "Starch is a compound made by plants that is made of numerous glucose units.", "An excess of glucose can lead to diabetic complications and can result ind death.", "The average wheelbarrow can hold up to 1200 pounds."], "decomposition": ["What is hyperglycemia?", "What is starch made of?", "How much can the average wheelbarrow hold?", "Could #3 of #2 potentially be fatal to someone who has #1?"], "evidence": [[[["Hyperglycemia-1"]], [["Starch-1"]], [["Wheelbarrow-2"]], [["Hyperglycemia-21"]]], [[["Hyperglycemia-1"]], [["Starch-1"]], [["Wheelbarrow-2"]], [["Hyperglycemia-2"], "operation"]], [[["Hyperglycemia-1"]], [["Starch-1"]], [["Wheelbarrow-2"]], ["operation"]]], "golden_sentence": [["Hyperglycemia is a condition in which an excessive amount of glucose circulates in the blood plasma."], ["Starch or amylum is a polymeric carbohydrate consisting of numerous glucose units joined by glycosidic bonds."], [""], ["1) Dysfunction of the thyroid, adrenal, and pituitary glands 2) Numerous diseases of the pancreas 3) Severe increases in blood glucose may be seen in sepsis and certain infections 4) Intracranial diseases (frequently overlooked) can also cause hyperglycemia."]]}, {"qid": "a1f4d97f793484e4c298", "term": "Giraffe", "description": "Tall African ungulate", "question": "Do giraffes require special facilities at zoos?", "answer": true, "facts": ["Giraffes are much taller than other land animals.", "Giraffe shelters at zoos must be built larger than shelters for other animals to accommodate their height."], "decomposition": ["What is the most distinctive feature of a giraffe?", "Does #1 make it necessary for them to have different facilities from other animals at a zoo?"], "evidence": [[[["Giraffe-16"]], [["Giraffe-16"]]], [[["Giraffe-2"]], [["West African giraffe-4"], "no_evidence", "operation"]], [[["Giraffe-2"]], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "43d9cf9db10e871abb54", "term": "Camel", "description": "Genus of mammals", "question": "Is Bactrian Camel most impressive animal when it comes to number of humps?", "answer": false, "facts": ["The Bactrian Camel is a camel with two humps native to Central Asia.", "Three humped camels were discovered on the Arabian peninsula in 2019."], "decomposition": ["How many humps does the Bactrian Camel have?", "What is the most number of humps seen on a camel?", "Is #1 greater than #2?"], "evidence": [[[["Bactrian camel-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Camel-1"]], ["no_evidence"], ["operation"]], [[["Bactrian camel-1"]], [["Dromedary-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["It has two humps on its back, in contrast to the single-humped dromedary camel."]]}, {"qid": "f79ffe7cc45658c6dfd2", "term": "Bartender", "description": "person who serves usually alcoholic beverages behind the bar in a licensed establishment", "question": "Would a responsible bartender make a drink for Millie Bobby Brown?", "answer": false, "facts": ["Millie Bobby Brown is currently 16 years old.", "In the United States, the minimum legal age to purchase any alcohol beverage is 21 years old.", "Bartenders are usually responsible for confirming that customers meet the legal drinking age requirements before serving them alcoholic beverages. "], "decomposition": ["How old is Millie Bobby Brown?", "What is the minimum legal age one must be to be served alcohol in the US?", "Is #1 larger than #2?"], "evidence": [[[["Millie Bobby Brown-1"]], [["National Minimum Drinking Age Act-1"]], ["operation"]], [[["Millie Bobby Brown-1"]], [["Legal drinking age-6"]], ["operation"]], [[["Millie Bobby Brown-1"]], [["Legal drinking age-6"]], ["operation"]]], "golden_sentence": [["Millie Bobby Brown (born 19 February 2004) is a British actress."], ["The act would punish any state that allowed persons under 21 years to purchase and publicly possess alcoholic beverages by reducing its annual federal highway apportionment by 10 percent."]]}, {"qid": "1b8f8cb4420dab15d758", "term": "Salsa music", "description": "Latin American dance music genre", "question": "Would Ibn Saud tolerate salsa music?", "answer": false, "facts": ["Ibn Saud was the first ruler of Saudi Arabia and adhered to Wahhabism.", "Wahhabism is an ultra conservative sect of Islam that prohibits dancing.", "Salsa is a popular Latin American music genre that is heavily connected to dance."], "decomposition": ["Which religion(s) did Ibn Saud practice?", "Did #1 permit its adherents to listen to or play music during Ibn Saud's lifetime?"], "evidence": [[[["Ibn Saud-3"]], [["Islamic music-28"], "operation"]], [[["Ibn Saud-3"]], [["Wahhabism-1", "Wahhabism-52"], "operation"]], [[["Ibn Saud-3"]], [["Najd-23"], "no_evidence"]]], "golden_sentence": [["Under the influence and inspiration of Wahhabi Islam, the Saudis had previously attempted to control much of the Arabian peninsula in the form of the Emirate of Diriyah, the \"First Saudi State\", until its destruction by an Ottoman army in the Ottoman\u2013Wahhabi War in the early nineteenth century."], ["[citation needed] However, those who argue that music is halal (permitted) state that this hadith relates to usage\u2014at the time the polytheists used music and musical instruments as part of their worship- and does not apply to all music."]]}, {"qid": "c8aae4666877bbddb27c", "term": "House of Lords", "description": "upper house in the Parliament of the United Kingdom", "question": "Was Aristotle a member of the House of Lords?", "answer": false, "facts": ["Aristotle died in 322 BC.", "The House of Lords is grown out of the Model Parliament, which was the first English Parliament.", "The Model Parliament was held in 1295."], "decomposition": ["When did Aristotle die?", "Where did the House of Lords originate from?", "When did #2 occur?", "Did #3 happen before #1?"], "evidence": [[[["Aristotle-1"]], [["House of Commons of the United Kingdom-17"]], [["House of Commons of the United Kingdom-17"]], ["operation"]], [[["Aristotle-10"], "no_evidence"], [["House of Lords-8"]], [["House of Lords-8"]], ["operation"]], [[["Aristotle-69"], "no_evidence"], [["House of Lords-18"], "no_evidence"], ["no_evidence"], ["no_evidence"]]], "golden_sentence": [["Aristotle (/\u02c8\u00e6r\u026ast\u0252t\u0259l/; Greek: \u1f08\u03c1\u03b9\u03c3\u03c4\u03bf\u03c4\u03ad\u03bb\u03b7\u03c2 Aristot\u00e9l\u0113s, pronounced\u00a0[aristot\u00e9l\u025b\u02d0s]; 384\u2013322\u00a0BC) was a Greek philosopher and polymath during the Classical period in Ancient Greece."], ["The British parliament of today largely descends, in practice, from the Parliament of England, although the 1706 Treaty of Union, and the Acts of Union that ratified the Treaty, created a new Parliament of Great Britain to replace the Parliament of England and the Parliament of Scotland, with the addition of 45 MPs and sixteen Peers to represent Scotland."], [""]]}, {"qid": "c947e0c37c3bf769b8e1", "term": "Krishna", "description": "Major deity in Hinduism", "question": "Was Krishna skilled at using the bow?", "answer": true, "facts": ["Lord Krishna was known as the eighth manifestation of the god Vishnu.", "Vishnu had a trove of weapons including the Sudarshana Chakra and Sharanga.", "Sharanga was a celestial bow and a favored weapon of Vishnu."], "decomposition": ["Which Hindu god was Krishna known to be a manifestation of?", "Which weapons belonging to #1 were among his favorite?", "Is the bow included in #2?"], "evidence": [[[["Krishna-1"]], [["Sharanga-1"]], [["Sharanga-1"]]], [[["Krishna-1"]], [["Sharanga-1"]], ["operation"]], [[["Krishna-1"]], [["Krishna-24", "Krishna-36", "Sharanga-1"]], ["operation"]]], "golden_sentence": [["He is worshipped as the eighth avatar of the god Vishnu and also as the supreme God in his own right."], ["Other weapons of Vishnu include the Sudarshana Chakra, the Narayanastra, the Vaishnavastra, the Kaumodaki mace, Nandaka sword."], [""]]}, {"qid": "88e2ae66964d2cd67517", "term": "Dessert", "description": "A course that concludes a meal; usually sweet", "question": "Is dessert eaten before breakfast?", "answer": false, "facts": ["Desserts are sweets.", "Meals generally begin with savory foods, and sweets eaten after."], "decomposition": ["What is a dessert?", "Are #1 usually sweet or salty?", "Do meals generally begin with foods that are #2?"], "evidence": [[[["Dessert-1"]], [["Dessert-1"]], ["no_evidence", "operation"]], [[["Dessert-1"]], [["Dessert-1"]], ["operation"]], [[["Dessert-1"]], [["Dessert-2"]], [["Breakfast-85"], "no_evidence"]]], "golden_sentence": [["Dessert (/d\u026a\u02c8z\u025c\u02d0rt/) is a course that concludes a meal."], [""]]}, {"qid": "b9cfeab1d1d3d8f99e78", "term": "Alice in Wonderland (1951 film)", "description": "1951 American animated musical fantasy film produced by Walt Disney Productions", "question": "Does Disney's Alice in Wonderland involve the celebration of a holiday?", "answer": true, "facts": ["In the movie, Alice meets the Mad Hatter.", "The Mad Hatter is having a tea party to celebrate his Unbirthday.", "The Unbirthday is a holiday which happens every day of the year which is not the subject's actual birthday."], "decomposition": ["What celebrations were featured in the Disney movie Alice in Wonderland?", "Is any of #1 an holiday?"], "evidence": [[[["Alice in Wonderland (1951 film)-7"]], ["operation"]], [[["Alice in Wonderland (franchise)-14"]], [["Birthday-1"], "operation"]], [[["Alice in Wonderland (1951 film)-9"], "no_evidence"], ["operation"]]], "golden_sentence": [["She encounters both, along with the Dormouse, at the Hare's house having a mad tea party and celebrating their \"unbirthdays\"."]]}, {"qid": "484d95ba0830017fb0cb", "term": "Bob Marley", "description": "Jamaican singer-songwriter", "question": "Could Bob Marley's children hypothetically win tug of war against Kublai Khan's children?", "answer": false, "facts": ["Bob Marley had 9 children.", "Kublai Khan had 23 children.", "Many of Bob Marley's children became singers, and followed his themes of peace and love.", "The children of Kublai Khan followed in his footsteps and were fierce warlords."], "decomposition": ["How many children did Bob Marley have?", "How many children did Kublai Khan have?", "Is #1 greater than #2?"], "evidence": [[[["Bob Marley-42"]], [["Kublai Khan-71", "Toghon (son of Kublai)-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Bob Marley-42"]], [["Kublai Khan-71", "Kublai Khan-76"], "no_evidence"], ["operation"]], [[["Bob Marley-42"]], [["Kublai Khan-76"]], ["operation"]]], "golden_sentence": [["The official Bob Marley website acknowledges 11 children."], ["Only two of Kublai's daughters are known by name; he may have had others.", "Toghon (Mongolian: \u0422\u043e\u0433\u043e\u043e\u043d, Chinese: \u812b\u6b61, Vietnamese: Tho\u00e1t Hoan,\u00a0?\u20131301), also Toghan or Togon, was the ninth son of Kublai Khan, founder of the Yuan dynasty."]]}, {"qid": "59f0d8ce9edf63429e60", "term": "Separation of church and state", "description": "principle to separate religious and civil institutions", "question": "Does USA fail separation of church and state in multiple ways?", "answer": true, "facts": ["Separation of church ad state refers to keeping God and religion out of state matters.", "Presidents of the United States are sworn in by placing their hand on a bible.", "The US currency contains the words, \"In God We Trust.\"", "The Pledge of Allegiance states, \"One Nation Under God.\""], "decomposition": ["How are US Presidents sworn in?", "What is the inscription on the US currency?", "What does the Pledge of Allegiance state?", "Do #1, #2 and #3 contain references to religion/the chuch?"], "evidence": [[[["President of the United States-46"]], [["In God We Trust-2"]], [["Pledge of Allegiance-1"]], ["operation"]], [[["United States presidential inauguration-23"]], [["In God We Trust-15"]], [["Pledge of Allegiance-1"]], ["operation"]], [[["Oath of office of the President of the United States-12"]], [["In God We Trust-3"]], [["Pledge of Allegiance-43"]], ["operation"]]], "golden_sentence": [["Although the oath may be administered by any person authorized by law to administer oaths, presidents are traditionally sworn in by the chief justice of the United States."], ["84\u2013140) and approved by President Dwight Eisenhower on July 30, 1956, requires that \"In God We Trust\" appear on American currency."], [""]]}, {"qid": "50a32bd0c2599ed65733", "term": "Star Wars", "description": "Epic science fantasy space opera franchise", "question": "Do Star Wars fans say \"beam me up\" often?", "answer": false, "facts": ["Beam me up is an expression from Star Trek.", "Much to the annoyance of fans, Star Trek and Star Wars are often confused for one another. "], "decomposition": ["Where does the expression beam me up come from?", "Is the answer to #1 the same as Star Wars?"], "evidence": [[[["Beam me up, Scotty-1"]], ["operation"]], [[["Beam me up, Scotty-1"]], ["operation"]], [[["Beam me up, Scotty-1"]], ["operation"]]], "golden_sentence": [["\"Beam me up, Scotty\" is a catchphrase that made its way into popular culture from the science fiction television series Star Trek: The Original Series."]]}, {"qid": "0a151ecbf4b74bc603f1", "term": "Gorilla", "description": "Genus of mammals", "question": "Are gorillas closely related to humans?", "answer": true, "facts": ["Gorillas are part of the animal family Hominidae.", "Hominidae also includes the genus Homo, which only contains the human species."], "decomposition": ["What animal family are Gorillas part of?", "Are humans also part of #1?"], "evidence": [[[["Hominidae-1"]], ["operation"]], [[["Gorilla-1"]], [["Primate-2"]]], [[["Hominidae-1"]], [["Hominidae-1"]]]], "golden_sentence": [["The Hominidae (/h\u0252\u02c8m\u026an\u026adi\u02d0/), whose members are known as great apes or hominids (/\u02c8h\u0252m\u026an\u026adz/), are a taxonomic family of primates that includes eight extant species in four genera: Pongo, the Bornean, Sumatran and Tapanuli orangutan; Gorilla, the eastern and western gorilla; Pan, the common chimpanzee and the bonobo; and Homo, of which only modern humans remain."]]}, {"qid": "9f4a0cc7c58f3bd3a1e9", "term": "Goat", "description": "domesticated mammal raised primarily for its milk", "question": "Are goats found on abyssal plains?", "answer": false, "facts": ["An abyssal plain is typically located between 10,000 and 20,000 feet below the surface of the ocean", "A goat is a mammal that lives on land and cannot intake oxygen from underwater environments"], "decomposition": ["What things do goats need to live?", "Where are abyssal plains located?", "Is everything in #1 also found in #2?"], "evidence": [[[["Goat-1"], "no_evidence"], [["Abyssal plain-1"]], ["operation"]], [[["Goat-5"]], [["Abyssal plain-1"]], ["operation"]], [[["Goat-23"]], [["Abyssal plain-1"]], ["operation"]]], "golden_sentence": [["Goats are one of the oldest domesticated species of animal, and have been used for milk, meat, fur and skins across much of the world."], ["Lying generally between the foot of a continental rise and a mid-ocean ridge, abyssal plains cover more than 50% of the Earth's surface."]]}, {"qid": "ce663767413f5acaec66", "term": "Julian calendar", "description": "solar calendar in use from imperial Rome until after the Reformation", "question": "Did Saint Augustine use the Julian calendar?", "answer": true, "facts": ["The Julian calendar was in use from 45 BC to the late 16th century AD", "Saint Augustine lived from 354 AD to 430 AD"], "decomposition": ["During what years was the Julian calendar used?", "When did Saint Augustine live?", "Is #2 during the time period listed in #1?"], "evidence": [[[["Julian calendar-1", "Julian calendar-2"]], [["Augustine of Hippo-1"]], ["operation"]], [[["Julian calendar-1", "Julian calendar-2"]], [["Augustine of Hippo-1"]], ["operation"]], [[["Julian calendar-2"]], [["Augustine of Hippo-1"]], ["operation"]]], "golden_sentence": [["", ""], ["Augustine of Hippo (/\u0254\u02d0\u02c8\u0261\u028cst\u026an/; Latin: Aurelius Augustinus Hipponensis; 13 November 354 \u2013 28 August 430 AD), also known as Saint Augustine, was a Roman African, Manichaean, early Christian theologian, doctor of the Church, and Neoplatonic philosopher from Numidia whose writings influenced the development of the Western Church and Western philosophy, and indirectly all of Western Christianity."]]}, {"qid": "8e6bc9125d53376e938e", "term": "New York Harbor", "description": "harbor in the New York City, U.S.A. metropolitan area", "question": "Did Donald Trump come up with the idea for the New York Harbor?", "answer": false, "facts": ["The New York Harbor is at the mouth of the Hudson River", "A harbor is a sheltered body of water where boats and ships can be docked.", "The New York Harbor has been used since colonial era of the 1500s.", "Donald Trump is a failed business man and 2016 president elect.", "Donald Trump makes outrageous deceitful claims "], "decomposition": ["When was the New York Harbor built?", "When was Donald Trump born?", "Did #2 come before #1?"], "evidence": [[[["New York Harbor-2"]], [["Donald Trump-1"]], ["operation"]], [[["New York Harbor-2"]], [["Donald Trump-1"]], ["operation"]], [[["New York Harbor-4"]], [["Donald Trump-1"]], ["operation"]]], "golden_sentence": [["The original population of the 16th century New York Harbor, the Lenape, used the waterways for fishing and travel."], ["Donald John Trump (born June 14, 1946) is the 45th and current president of the United States."]]}, {"qid": "114463691b4c12772599", "term": "Wheat", "description": "Cereal grain", "question": "Can a woman on average have a baby before wheat seed blooms?", "answer": false, "facts": ["The average time it takes for a woman to give birth is 9 months.", "Wheat takes between 7 to 8 months to harvest."], "decomposition": ["How long does pregnancy typically last in humans?", "How long does it typically take to grow and harvest wheat?", "Is #1 shorter than #2?"], "evidence": [[[["Pregnancy-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Pregnancy-1"]], [["Intensive crop farming-11"]], ["operation"]], [[["Pregnancy-1"]], [["Intensive crop farming-11"]], ["operation"]]], "golden_sentence": [["This is just over nine\u00a0months, where each month averages 31 days."]]}, {"qid": "aecc67d36e5dbacb91d7", "term": "Jackfruit", "description": "species of plant", "question": "Would it be safe to have a jackfruit thrown at your head?", "answer": false, "facts": ["Jackfruit can weigh between 22-55 lbs. ", "Jackfruit are covered in small spikes."], "decomposition": ["How much do jackfruit weigh?", "Is #1 light enough to not hurt you?"], "evidence": [[[["Jackfruit-2"]], [["Jackfruit-2"], "no_evidence"]], [[["Jackfruit-2"]], ["operation"]], [[["Jackfruit-2"]], ["operation"]]], "golden_sentence": [["It bears the largest fruit of all trees, reaching as much as 55\u00a0kg (120\u00a0lb) in weight, 90\u00a0cm (35\u00a0in) in length, and 50\u00a0cm (20\u00a0in) in diameter."], [""]]}, {"qid": "7f9673a262dfd5cc9132", "term": "Vice President of the United States", "description": "Second highest executive office in United States", "question": "Was the first Vice President of the United States an Ottoman descendant?", "answer": false, "facts": ["The first Vice President of the United States was John Adams.", "The Ottomans were a Turkic group that conquered Constantinople in 1453.", "John Adams was descended from English Puritans."], "decomposition": ["Who was the first Vice President of the United States?", "Which group of people was #1 a descendant of?", "Is #2 the same as Ottoman?"], "evidence": [[[["John Adams-1"]], [["John Adams-5"], "no_evidence"], [["Ottoman dynasty-1"], "operation"]], [[["Vice President of the United States-52"]], [["John Adams-6"], "no_evidence"], [["Christianity in the modern era-12"], "operation"]], [[["John Adams-1"]], [["John Adams-5"]], ["operation"]]], "golden_sentence": [["Adams was a dedicated diarist and regularly corresponded with many important figures in early American history, including his wife and adviser Abigail Adams, and Thomas Jefferson."], [""], [""]]}, {"qid": "7db47fc3b0b666e44271", "term": "Aldi", "description": "Germany-based supermarket chain", "question": "Are all United States Aldi locations owned by the same company?", "answer": false, "facts": ["Aldi is actually two German-based supermarket chains, Aldi Nord and Aldi Sud.", "Both companies operate internationally, but the United States is the only country other than Germany where both Aldi chains operate."], "decomposition": ["Which country is Aldi based in?", "How many chains of Aldi operate in #1?", "Are each of #2 owned by different organizations?", "How many chains of Aldi operate in the US?", "Is #3 negative or #4 different than #2?"], "evidence": [[[["Aldi-1"]], [["Aldi-2"]], [["Aldi-13"]], [["Aldi-2"]], ["operation"]], [[["Aldi-1"]], [["Aldi-1"]], ["no_evidence", "operation"], [["Aldi-2"]], ["operation"]], [[["Aldi-1"]], [["Aldi-1"]], [["Aldi-1"]], [["Aldi-17", "Aldi-2"]], ["operation"]]], "golden_sentence": [["Based in Germany, the chain was founded by brothers Karl and Theo Albrecht in 1946 when they took over their mother's store in Essen."], ["Aldi's German operations consist of Aldi Nord's 35 individual regional companies with about 2,500 stores in western, northern, and eastern Germany, and Aldi S\u00fcd's 32 regional companies with 1,600 stores in western and southern Germany."], [""], ["Both Aldi Nord (as Trader Joe's) and Aldi S\u00fcd (as Aldi) also operate in the United States with 1,600 stores between them as of 2017 (and the U.S. is the only country to have both Aldi companies operating outside of Germany)."]]}, {"qid": "dd24b8386e0c04552eb5", "term": "Saint Peter", "description": "apostle and first pope", "question": "Could Saint Peter watch television?", "answer": false, "facts": ["Saint Peter died in 64 BC.", "The television was invented in 1900."], "decomposition": ["When was television invented?", "When did Saint Peter die?", "Is #1 before #2?"], "evidence": [[[["History of television-15"]], [["Saint Peter-55"]], ["operation"]], [[["Television-12"], "no_evidence"], [["Saint Peter-57"], "no_evidence"], ["operation"]], [[["Television-2"]], [["Saint Peter-1"]], ["operation"]]], "golden_sentence": [["Herbert E. Ives and Frank Gray of Bell Telephone Laboratories gave a dramatic demonstration of mechanical television on April 7, 1927."], ["Theologians Donald Fay Robinson and Warren M. Smaltz have suggested that the incident in Acts 12:1\u201317, where Peter is \"released by an angel\" and goes to \"another place\", really represents an idealized account of his death, which may have occurred in a Jerusalem prison in as early as 44 AD."]]}, {"qid": "98384220d83a28d56003", "term": "Universal Music Group", "description": "American music corporation", "question": "Will NY Stock Exchange closing bell be heard in Universal Music Group's headquarters?", "answer": false, "facts": ["The New York Stock Exchange is located in New York, USA.", "Universal Music Group's headquarters is located in Santa Monica, California.", "Santa Monica is about 2800 miles from New York.", "A shout can be heard up to 100 meters away."], "decomposition": ["Where is the New York Stock Exchange located?", "Where is Universal Music Group's headquarters located?", "What is the distance between #1 and #2?", "Is #3 a reasonable distance within which a bell's chime can be heard?"], "evidence": [[[["New York Stock Exchange-1"]], [["Universal Music Group-1"]], ["no_evidence", "operation"], ["operation"]], [[["New York Stock Exchange-1"]], [["Universal Music Group-1"]], ["no_evidence"], ["no_evidence"]], [[["New York Stock Exchange-1"]], [["Universal Music Group-1"]], ["no_evidence", "operation"], [["Bell-32"], "operation"]]], "golden_sentence": [["The New York Stock Exchange (NYSE, nicknamed \"The Big Board\") is an American stock exchange located at 11 Wall Street, Lower Manhattan, New York City, New York."], ["UMG's global corporate headquarters are located in Santa Monica, California."]]}, {"qid": "cd739be90f85ae1b0fd4", "term": "Super Mario", "description": "platform video game series from Nintendo's Mario franchise", "question": "Does Super Mario protagonist hypothetically not need continuing education classes in Illinois?", "answer": false, "facts": ["Mario, the protagonist of Super Mario, is a plumber by profession.", "Continuing education classes are required for certain professions in certain jurisdictions.", "Plumbers are required in Illinois to take continuing education classes."], "decomposition": ["Who is the protagonist of Super Mario?", "What is #1's profession? ", "In Illinois, can #2's avoid taking continuing education classes?"], "evidence": [[[["Super Mario-2"]], [["Kill the Plumber-2"]], ["no_evidence", "operation"]], [[["Super Mario Bros.-4"]], ["no_evidence"], [["Plumber-9"], "operation"]], [[["Mario-1"]], [["Mario-1"]], [["Continuing education-9"], "no_evidence", "operation"]]], "golden_sentence": [["The games have simple plots, typically with Mario rescuing the kidnapped Princess Peach from the primary antagonist, Bowser."], [""]]}, {"qid": "0c90e711c2cd393908d3", "term": "Samsung Galaxy", "description": "series of Android mobile computing devices", "question": "Can you save every HD episode of Game of Thrones on Samsung Galaxy A10e?", "answer": false, "facts": ["The Samsung Galaxy A10e has 32GB of storage.", "The average storage requirement of an HD episode of Game of Thrones is 600MB", "There are 60 total episodes of Game of Thrones.", "There are 1000MB in one GB."], "decomposition": ["How much storage does a Samsung Galaxy A10e have?", "What is #1 multiplied by 1000?", "What is the average storage requirement for an HD episode of Game of Thrones?", "How many episodes are the of Game of Thrones?", "Is #2 greater than or equal to #3 multiplied by #4?"], "evidence": [[[["Samsung Galaxy A10-1"]], ["operation"], ["no_evidence"], [["The Iron Throne (Game of Thrones)-1"]], ["no_evidence", "operation"]], [[["Samsung Galaxy-1"], "no_evidence"], ["no_evidence", "operation"], [["High-definition video-18"], "no_evidence"], [["Game of Thrones-1"]], ["no_evidence", "operation"]], [[["Samsung Galaxy A10-1"]], ["operation"], [["Game of Thrones (season 1)-26"], "no_evidence"], [["Game of Thrones-1"]], ["operation"]]], "golden_sentence": [["It comes with Android 9 (Pie) with One UI, 32GB internal storage, and a 3400 mAh battery."], ["It is the sixth episode of the eighth season and the 73rd overall episode of the series."]]}, {"qid": "4b266847e05b4c36bb21", "term": "Tenth Amendment to the United States Constitution", "description": "says powers not Constitutionally granted to the Federal Government belong to States or the People", "question": "Was the tenth Amendment to the Constitution written using Pitman shorthand?", "answer": false, "facts": ["Pitman shorthand was invented in 1837.", "The tenth Amendment to the Constitution was added in 1791."], "decomposition": ["When was Pitman shorthand invented?", "When was the  tenth Amendment to the Constitution added?", "Did #1 happen before #2?"], "evidence": [[[["Pitman shorthand-1"]], [["Tenth Amendment to the United States Constitution-1"]], ["operation"]], [[["Pitman shorthand-5"]], [["Tenth Amendment to the United States Constitution-1"]], ["operation"]], [[["Pitman shorthand-1"]], [["Tenth Amendment to the United States Constitution-1"]], ["operation"]]], "golden_sentence": [["Pitman shorthand is a system of shorthand for the English language developed by Englishman Sir Isaac Pitman (1813\u20131897), who first presented it in 1837."], ["The Tenth Amendment (Amendment X) to the United States Constitution, which is part of the Bill of Rights, was ratified on December 15, 1791."]]}, {"qid": "e02a1f8a3a8f39ccac1d", "term": "Skype", "description": "telecommunications software service", "question": "Are the founders of Skype from Asia?", "answer": false, "facts": ["Skype was created by the Niklas Zennstr\u00f6m and the Dane Janus Friis.", "Niklas Zennstr\u00f6m and Dane Janus Friis are from Sweden.", "Sweden is located in Europe, not Asia. "], "decomposition": ["Who are the founders of Skype?", "What country are #1 from?", "What continent is #2 on?", "Is #3 Asia?"], "evidence": [[[["Skype-4"]], [["Skype-4"]], [["Outline of Denmark-2", "Sweden-1"]], ["operation"]], [[["Skype-9"]], [["Skype-9"]], [["Denmark-1"]], ["operation"]], [[["Skype-9"]], [["Skype-9"]], [["Denmark-1", "Sweden-1"]], ["operation"]]], "golden_sentence": [["First released in August 2003, Skype was created by the Swede Niklas Zennstr\u00f6m and the Dane Janus Friis, in cooperation with Ahti Heinla, Priit Kasesalu, and Jaan Tallinn, Estonians who developed the backend that was also used in the music-sharing application Kazaa."], ["First released in August 2003, Skype was created by the Swede Niklas Zennstr\u00f6m and the Dane Janus Friis, in cooperation with Ahti Heinla, Priit Kasesalu, and Jaan Tallinn, Estonians who developed the backend that was also used in the music-sharing application Kazaa."], ["Denmark \u2013 country located in Scandinavia of Northern Europe.", "Sweden (/\u02c8swi\u02d0.d\u0259n/; Swedish: Sverige [\u02c8sv\u00e6\u030crj\u025b] (listen)), officially the Kingdom of Sweden (Swedish: Konungariket Sverige [\u02c8k\u00f4\u02d0n\u0275\u014ba\u02ccri\u02d0k\u025bt \u02c8sv\u00e6\u030crj\u025b] (listen)), is a Nordic country in Northern Europe."]]}, {"qid": "5dad8755c2eabbfe389b", "term": "Astronaut", "description": "Person who commands, pilots, or serves as a crew member of a spacecraft", "question": "Can actress Danica McKellar skip astronaut education requirements?", "answer": true, "facts": ["Astronaut's are required to have a bachelor's degree in engineering, biological science, physical science, computer science, or mathematics.", "Actress Danica McKellar graduated summa cum laude from UCLA with a degree in Mathematics."], "decomposition": ["Astronauts can have any one of which degrees?", "What degree does Danica McKellar have?", "Is #2 included in #1?"], "evidence": [[[["NASA Astronaut Corps-10"]], [["Danica McKellar-5"]], ["operation"]], [[["NASA Astronaut Corps-10"]], [["Danica McKellar-5"]], ["operation"]], [[["NASA Astronaut Corps-10"]], [["Danica McKellar-5"]], ["operation"]]], "golden_sentence": [["Candidates must have a master's degree from an accredited institution in engineering, biological science, physical science or mathematics."], ["McKellar studied at the University of California, Los Angeles where she earned a Bachelor of Science (BS) Degree summa cum laude in Mathematics in 1998."]]}, {"qid": "a4845b83e5be06494092", "term": "Robert Downey Jr.", "description": "American actor", "question": "Did Robert Downey Jr. possess same caliber gun as Resident Evil's Barry Burton?", "answer": true, "facts": ["Robert Downey Jr. was arrested in 1996 n drug and weapons charges and possessed a .357 Magnum.", "Barry Burton, a character in the Resident Evil series, used a Colt Python.", "The Colt Python is a type of .357 Magnum revolver."], "decomposition": ["What type of gun did Robert Downey Jr. have when he was arrested?", "What gun does Barry Burton use?", "What type of gun is #2?", "Is #1 the same as #3?"], "evidence": [[[["Robert Downey Jr.-13"]], [["Resident Evil 3: Nemesis-10"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Robert Downey Jr.-13"]], [["Resident Evil 3: Nemesis-10"], "no_evidence"], [["Colt Python-1"]], ["operation"]], [[["Robert Downey Jr.-13"]], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["In April 1996, Downey was arrested for possession of heroin, cocaine, and an unloaded .357 Magnum handgun while he was speeding down Sunset Boulevard."], [""]]}, {"qid": "192c589ad94135320512", "term": "Daily Mirror", "description": "British daily tabloid newspaper owned by Reach plc.", "question": "Can a copy of The Daily Mirror sustain a campfire?", "answer": true, "facts": ["The Daily Mirror is a British tabloid made of paper.", "Kindling helps sustain a fire because easily combustible help a fire keep igniting.", "Paper is capable of igniting and burning easily."], "decomposition": ["What kind of product is The Daily Mirror?", "Is #1 made of combustible material?"], "evidence": [[[["Daily Mirror-1"]], ["no_evidence"]], [[["Daily Mirror-1"]], [["Newsprint-1"], "no_evidence", "operation"]], [[["Daily Mirror-1"]], [["Combustibility and flammability-2"]]]], "golden_sentence": [["The Daily Mirror is a British national daily tabloid newspaper founded in 1903."]]}, {"qid": "1237cd2789b9742da102", "term": "Porsche", "description": "automotive brand manufacturing subsidiary of Volkswagen", "question": "Could a Porsche 992 Turbo S defeat Usain Bolt in a 100 meter sprint?", "answer": true, "facts": ["The Porsche 992 Turbo S can accelerate to 62 mph in 2.7 seconds.", "Usain Bolt's top speed ever measured is 27.79 mph."], "decomposition": ["What is the max speed of a Porsche 992 Turbo S?", "What is Bolt's top speed?", "Is #1 faster than #2?"], "evidence": [[[["Porsche 992-8"]], [["Usain Bolt-106"]], ["operation"]], [[["Porsche 992-8"]], [["Footspeed-4"]], ["operation"]], [[["Porsche 992-8"]], [["Usain Bolt-106"]], ["operation"]]], "golden_sentence": [[""], ["Bolt's top speed, based on his split time of 1.61 s for the 20 metres from the 60- to 80-metre marks (made during the 9.58 WR at 100m), is 12.42"]]}, {"qid": "99faf2010e5f30083e15", "term": "Cuban Revolution", "description": "Revolution in Cuba between 1953 and 1959", "question": "During the Cuban revolution, did the US experience a population boom?", "answer": true, "facts": ["After WWII, the US experienced a baby boom.", "WWII ended in 1945."], "decomposition": ["When was the Cuban Revolution?", "When did the United States experience a rapid growth in its population?", "Does some or all of #2 overlap with #1?"], "evidence": [[[["Cuban Revolution-8"]], [["Baby boom-3"]], [["Baby boom-3", "Cuban Revolution-8"], "operation"]], [[["Cuban Revolution-1"]], [["Mid-twentieth century baby boom-1", "Mid-twentieth century baby boom-3"]], ["operation"]], [[["Cuban Revolution-1"]], [["Mid-twentieth century baby boom-12"]], ["operation"]]], "golden_sentence": [[""], ["From 1941 to 1961, more than 65 million children were born in the United States."], ["", ""]]}, {"qid": "0fbab3ff8e948d3ec5ec", "term": "Meatball", "description": "dish made from ground meat rolled into a small ball-like form", "question": "Can you buy furniture and meatballs in the same store?", "answer": true, "facts": ["IKEA is well known for selling cheap, modern furniture.", "IKEA is famous for serving meatballs at their in-store restaurants."], "decomposition": ["What is IKEA known for selling?", "What are some delicacies IKEA is known to serve at their in-store restaurants?", "Is meatballs included in #2 and #1 furniture?"], "evidence": [[[["IKEA-1"]], [["IKEA-15"]], [["IKEA-15"]]], [[["IKEA-1"]], [["IKEA-12"]], ["operation"]], [[["IKEA-1"]], [["IKEA-12"]], ["operation"]]], "golden_sentence": [["IKEA (/a\u026a\u02c8ki\u02d0\u0259/ eye-KEE-\u0259, Swedish:\u00a0[\u026a\u02c8k\u00ea\u02d0a]) is a Swedish-origin Dutch-headquartered multinational group that designs and sells ready-to-assemble furniture, kitchen appliances and home accessories, among other useful goods and occasionally home services."], [""], [""]]}, {"qid": "17d2d0256d12afee9440", "term": "2000", "description": "Year", "question": "Was there fear leading up to the year 2000?", "answer": true, "facts": ["Many computer programs were not designed with the year 2000 in mind.", "People were worried that computers would crash all over the world when the year 2000 arrived.", "Financial and electrical systems require computers to function.", "Without financial and electrical systems there could be global chaos."], "decomposition": ["What concerns did people have about computing systems as 2000 approached?", "Did #1 involve a widespread fear of malfunction?"], "evidence": [[[["Year 2000 problem-1"]], [["Year 2000 problem-23"], "operation"]], [[["Year 2000 problem-1"]], [["Year 2000 problem-15"], "operation"]], [[["Year 2000 problem-1"]], [["Year 2000 problem-23"]]]], "golden_sentence": [["Problems were anticipated, and arose, because many programs represented four-digit years with only the final two digits \u2013 making the year 2000 indistinguishable from 1900."], [""]]}, {"qid": "24e766b24a686c124e21", "term": "Leonardo da Vinci", "description": "15th and 16th-century Italian Renaissance polymath", "question": "Did Leonardo da Vinci lack contemporary peers in his home city?", "answer": false, "facts": ["Leonardo da Vinci was born in Anchiano, a town in the city of Florence.", "Da Vinci lived during the 15th and 16th century.", "Sandro Boticelli was a Florentine artist 15th and 16th century.", "Donatello was a Florentine artist during the 15th century."], "decomposition": ["Which period did Leonardo da Vinci live through and where was his home city?", "When did Sandro Boticelli live through and where was his home city?", "Where was Donatello's home city and what period did he live through?", "Are #1, #2 and #3 different from one another?"], "evidence": [[[["Leonardo da Vinci-1", "Leonardo da Vinci-2"]], [["Sandro Botticelli-1", "Sandro Botticelli-2"]], [["Donatello-1"]], ["operation"]], [[["Leonardo da Vinci-1"]], [["Sandro Botticelli-1"]], [["Donatello-1"]], ["operation"]], [[["Leonardo da Vinci-3", "Leonardo da Vinci-7"]], [["Sandro Botticelli-1", "Sandro Botticelli-5"]], [["Donatello-1"]], ["operation"]]], "golden_sentence": [["Leonardo di ser Piero da Vinci (Italian:\u00a0[leo\u02c8nardo di \u02ccs\u025br \u02c8pj\u025b\u02d0ro da (v)\u02c8vint\u0283i] (listen); 14/15 April 1452\u00a0\u2013\u00a02 May 1519), known as Leonardo da Vinci (English: /\u02ccli\u02d0\u0259\u02c8n\u0251\u02d0rdo\u028a d\u0259 \u02c8v\u026ant\u0283i, \u02ccli\u02d0o\u028a\u02c8-, \u02ccle\u026ao\u028a\u02c8-/ LEE-\u0259-NAR-doh d\u0259 VIN-chee, LEE-oh-, LAY-oh-), was an Italian polymath of the Renaissance whose areas of interest included invention, drawing, painting, sculpture, architecture, science, music, mathematics, engineering, literature, anatomy, geology, astronomy, botany, paleontology, and cartography.", ""], ["", ""], ["Born in Florence, he studied classical sculpture and used this to develop a complete Renaissance style in sculpture, whose periods in Rome, Padua and Siena introduced to other parts of Italy a long and productive career."]]}, {"qid": "45c08eea9af474867f4a", "term": "Nickel", "description": "Chemical element with atomic number 28", "question": "Is nickel dominant material in US 2020 nickels?", "answer": false, "facts": ["Nickels have been made of various materials including silver in the 1940s.", "Nickels in 2020 are made from a mix of copper and nickel.", "2020 nickels are 25% nickel and 75% copper."], "decomposition": ["What is the composition of the US 2020 nickel?", "Of the elements listed in #1, do any of them make up more than 50% of the US 2020 Nickel?", "If #2 is yes, is that element nickel?"], "evidence": [[[["Jefferson nickel-14"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Nickel (United States coin)-1"], "no_evidence"], [["Nickel (United States coin)-1"]], ["operation"]], [[["Nickel-5"]], ["operation"], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "9118e38d12207ecec3a5", "term": "Bee", "description": "Clade of insects", "question": "Can Africanized bees be considered multicultural?", "answer": true, "facts": ["Multicultural refers to a blend of several cultures within one organism.", "Africanized bees, also killer bees are a result of crossbreeding.", "Africanized bees are a mix of East African lowland honey bees and European honey bee subspecies such as the Italian honey bee and the Iberian honey bee. "], "decomposition": ["What is the definition of multicultural?", "What are Africanized bees a result of?", "What types of bees were part of #2?", "Is #3 an example of #1?"], "evidence": [[[["Multiculturalism-1"]], [["Africanized bee-1"]], [["Africanized bee-1"]], [["Africanized bee-1", "Multiculturalism-1"]]], [[["Multiracial people-1"]], [["Honey bee-21"]], [["Africanized bee-1"]], ["operation"]], [[["Multiculturalism-1"]], [["Africanized bee-1"]], [["African bee-1", "Western honey bee-1"]], ["operation"]]], "golden_sentence": [["In sociology and in everyday usage, it is a synonym for \"ethnic pluralism\", with the two terms often used interchangeably, for example, a cultural pluralism in which various ethnic groups collaborate and enter into a dialogue with one another without having to sacrifice their particular identities."], ["The Africanized bee, also known as the Africanized honey bee, and known colloquially as the \"killer bee\", is a hybrid of the western honey bee species (Apis mellifera), produced originally by cross-breeding of the East African lowland honey bee (A.\u00a0m.\u00a0scutellata) with various European honey bees such as the Italian honey bee A.\u00a0m.\u00a0ligustica and the Iberian honey bee A.\u00a0m.\u00a0iberiensis."], [""], ["", ""]]}, {"qid": "ee1a733e4c839680853f", "term": "Heracles", "description": "divine hero in Greek mythology, son of Zeus and Alcmene", "question": "Were all of Heracles's children present for his funeral pyre?", "answer": false, "facts": ["Heracles killed his children by his first wife Megara.", "They were not returned to life prior to his death."], "decomposition": ["What did Heracles do to his children by his first wife?", "Are people who have been #1 able to come back to life?"], "evidence": [[[["Heracles-21"], "no_evidence"], [["Death-11"], "operation"]], [[["Megara (mythology)-4"]], [["Death (disambiguation)-1"], "operation"]], [[["Megara (mythology)-4"]], ["operation"]]], "golden_sentence": [["In a fit of madness, induced by Hera, Heracles killed his children and Megara."], [""]]}, {"qid": "030d314ad94420dc8b4a", "term": "Phobos (moon)", "description": "natural satellite of Mars", "question": "Is Phobos part of the Andromeda galaxy?", "answer": false, "facts": ["Phobos orbits around Mars.", "Mars is a planet in Earth's solar system.", "The solar system is in the Milky Way galaxy."], "decomposition": ["What planet does Phobos orbit around?", "What solar system is #1 part of?", "What galaxy is #2 part of?", "Is #3 the same as the Andromeda galaxy?"], "evidence": [[[["Phobos (moon)-1"]], [["Mars-1"]], [["Milky Way-1"]], [["Andromeda Galaxy-1"]]], [[["Phobos (moon)-1"]], [["Solar System-2"]], [["Milky Way-1"]], ["operation"]], [[["Phobos (moon)-16"]], [["Solar System-37"]], [["Solar System-73"]], ["operation"]]], "golden_sentence": [["Phobos /\u02c8fo\u028ab\u0252s/ (systematic designation: Mars I) is the innermost and larger of the two natural satellites of Mars, the other being Deimos."], ["Mars is a terrestrial planet with a thin atmosphere, with surface features reminiscent of the impact craters of the Moon and the valleys, deserts and polar ice caps of Earth."], ["The Milky Way is the galaxy that contains the Solar System, with the name describing the galaxy's appearance from Earth: a hazy band of light seen in the night sky formed from stars that cannot be individually distinguished by the naked eye."], [""]]}, {"qid": "c7ce9bed72052a7cbc32", "term": "Lobster", "description": "family of crustaceans", "question": "Can lobster breathe in the desert?", "answer": false, "facts": ["Lobsters use gills to breathe.", "Gills require water to breathe.", "There is no water in the desert. "], "decomposition": ["Which part of their body do lobsters breathe with?", "Where does #1 obtain oxygen from?", "Is #2 easily found in the desert?"], "evidence": [[[["Gill-1", "Lobster-14"]], [["Aquatic respiration-2"]], [["Desert-3"], "operation"]], [[["Lobster-14"], "no_evidence"], [["Lobster-2"], "no_evidence"], [["Desert-1"], "operation"]], [[["Gill-1", "Lobster-14"]], [["Gill-1"]], [["Desert-1", "Precipitation-1"]]]], "golden_sentence": [["The gills of some species, such as hermit crabs, have adapted to allow respiration on land provided they are kept moist.", "Symbiotic animals of the genus Symbion, the only member of the phylum Cycliophora, live exclusively on lobster gills and mouthparts."], ["Most fish exchange gases using gills on either side of the pharynx (throat), forming the Splanchnocranium; the Splanchnocranium being the portion of the skeleton where the cartilage of the cranium converges into the cartilage of the pharynx and its associated parts."], [""]]}, {"qid": "85fd15af2553c013b7b5", "term": "Parody", "description": "Imitative work created to mock, comment on or trivialise an original work", "question": "Are parodies of the President of the United States illegal?", "answer": false, "facts": ["Parody in the US is protected under fair use in regards to copyright.", "Criticism of political leaders is protected under the 1st Amendment."], "decomposition": ["Is parody illegal in the US?", "Is criticism of the government against the US constitution?", "Is #1 or #2 positive?"], "evidence": [[[["Fair use-1", "Parody-30"], "no_evidence"], [["Freedom of speech in the United States-1", "Human rights in the United States-2"]], ["operation"]], [[["Parody-30"]], [["Freedom of speech in the United States-1", "Freedom of speech in the United States-34"]], ["operation"]], [[["Parody-30"]], [["First Amendment to the United States Constitution-1"]], ["operation"]]], "golden_sentence": [["", ""], ["", ""]]}, {"qid": "f705abefaf0d869bc13b", "term": "Oyster", "description": "salt-water bivalve mollusc", "question": "Can oysters be used in guitar manufacturing?", "answer": true, "facts": ["Oysters produce nacre", "Nacre is also known as mother of pearl", "Mother of pearl is commonly used as an inlay on guitar fretboards, headstocks, and soundboards"], "decomposition": ["What non-food products are derived from oysters?", "Which of #1 are used for decoration?", "What materials are used to decorate a guitar?", "Is there overlap between #2 and #3?"], "evidence": [[[["Nacre-1", "Nacre-17"], "no_evidence"], [["Oyster-2"], "no_evidence"], [["Guitar-45"], "no_evidence"], ["operation"]], [[["Oyster-9"]], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Oyster-6"]], [["Oyster-2"]], [["Inlay (guitar)-1"]], [["Inlay (guitar)-1", "Oyster-2"], "operation"]]], "golden_sentence": [["", ""], ["Windowpane oysters are harvested for their translucent shells, which are used to make various kinds of decorative objects."], ["Some older or high-end instruments have inlays made of mother of pearl, abalone, ivory, colored wood or other exotic materials and designs."]]}, {"qid": "46ae76b89c366c460313", "term": "Veto", "description": "legal power to unilaterally stop an official action, especially the enactment of legislation", "question": "Can the US branch of government that has power over the military also have the power to veto?", "answer": true, "facts": ["The US President is the commander in chief of the US military.", "The executive branch of the USA includes the President.", "The President has power to veto."], "decomposition": ["What US branch has power over the military?", "Who has the power to veto?", "Is #2 part of #1?"], "evidence": [[[["Article Two of the United States Constitution-1", "Article Two of the United States Constitution-4", "Executive (government)-5"]], [["Federal government of the United States-18", "Veto-1"]], ["operation"]], [[["Federal government of the United States-17"]], [["Federal government of the United States-17", "Federal government of the United States-18"]], [["Federal government of the United States-17"]]], [[["Federal jurisdiction (United States)-4"]], [["Veto-34"]], ["operation"]]], "golden_sentence": [["Article Two of the United States Constitution establishes the executive branch of the federal government, which carries out and enforces federal laws.", "", ""], ["The president may sign legislation passed by Congress into law or may veto it, preventing it from becoming law unless two-thirds of both houses of Congress vote to override the veto.", "A veto (Latin for \"I forbid\") is the power (used by an officer of the state, for example) to unilaterally stop an official action, especially the enactment of legislation."]]}, {"qid": "0e85f8b8c5d74d88e4ff", "term": "Kane (wrestler)", "description": "American professional wrestler, actor, businessman, and politician", "question": "Have any murderers outlasted Kane's Royal Rumble record?", "answer": true, "facts": ["The longest Kane lasted in the Royal Rumble was 53:46 in 2001.", "Chris Benoit lasted over an hour in the 2004 Royal Rumble.", "Chris Benoit murdered his own wife and son in 2007."], "decomposition": ["What is Kane's Royal Rumble record?", "Which wrestlers have a Royal Rumble record longer than #1?", "Are any of the wrestlers listed in #2 a murderer?"], "evidence": [[[["Royal Rumble (2001)-13"], "no_evidence"], [["Royal Rumble (2004)-19"]], [["Chris Benoit-3"], "operation"]], [[["Royal Rumble (2014)-36"]], [["Royal Rumble match-30"]], [["Chris Benoit-3"]]], [[["Kane (wrestler)-3"]], [["Chris Benoit-2"], "no_evidence"], [["Chris Benoit-3"], "no_evidence", "operation"]]], "golden_sentence": [[""], ["Goldberg punched several wrestlers including Angle, Jericho, RVD, Cena and Haas."], [""]]}, {"qid": "c4476e2b912f164cf963", "term": "Euro", "description": "European currency", "question": "Will a 2 Euro coin float across the Red Sea?", "answer": false, "facts": ["A 2 Euro coin is made of a mix of copper and brass.", "Objects float if their density is less than water.", "Ancient bronze metal ingots were found on the sea floor off the coast of Italy in 2015."], "decomposition": ["What are the material constituents of a 2 Euro coin?", "#1 belong to which family of materials?", "Can non hollow forms of #2 float on water?"], "evidence": [[[["2 euro coin-1"]], [["Metal-1"]], ["operation"]], [[["2 euro coin-1"]], [["Copper-2"]], [["Metal-9"], "no_evidence"]], [[["2 euro coin-8"]], [["Metal-35"]], ["no_evidence"]]], "golden_sentence": [["The coin is made of two alloys: the inner part of nickel brass, the outer part of copper-nickel."], [""]]}, {"qid": "ab38b7acffca34fa5642", "term": "Sloth", "description": "tree dwelling animal noted for slowness", "question": "Could a sloth hypothetically watch an entire episode of Scrubs underwater?", "answer": true, "facts": ["Sloths can hold their breath underwater for up to 40 minutes.", "The running time of a Scrubs episode is between 20-23 minutes."], "decomposition": ["How long can sloths hold their breath underwater?", "How long is an episode of Scrubs?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Sloth-20"]], [["Scrubs (TV series)-69"]], ["operation"]], [[["Sloth-20"]], ["no_evidence"], ["no_evidence"]], [[["Sloth-20"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Sloths can reduce their already slow metabolism even further and slow their heart rate to less than a third of normal, allowing them to hold their breath underwater for up to 40 minutes."], ["At the 59th Primetime Emmy Awards, the episode \"My Musical\" was nominated for five awards in four categories: Outstanding Directing for a Comedy Series (Will Mackenzie), Outstanding Music Direction (Jan Stevens) and Outstanding Original Music and Lyrics (\"Everything Comes Down to Poo\" and \"Guy Love\"); while sharing the award for Outstanding Sound Mixing for a Comedy or Drama Series (Half-Hour) And Animation (Joe Foglia, Peter J. Nusbaum, and John W. Cook II) with Entourage."]]}, {"qid": "52550625430e7be7c82a", "term": "Marco Polo", "description": "Italian explorer and merchant noted for travel to central and eastern Asia", "question": "Did Marco Polo travel with Christopher Columbus?", "answer": false, "facts": ["Marco Polo died in 1324.", "Christopher Columbus was born in 1451."], "decomposition": ["When did Marco Polo die?", "When was Columbus born?", "Was #1 after #2?"], "evidence": [[[["Marco Polo-1"]], [["Christopher Columbus-1"]], ["operation"]], [[["Marco Polo-24"]], [["Christopher Columbus-5"]], ["operation"]], [[["Marco Polo-24"]], [["Christopher Columbus-5"]], ["operation"]]], "golden_sentence": [["Marco Polo (/\u02c8m\u0251\u02d0rko\u028a \u02c8po\u028alo\u028a/ (listen), Venetian:\u00a0[\u02c8ma\u027eko \u02c8polo], Italian:\u00a0[\u02c8marko \u02c8p\u0254\u02d0lo], 1254\u00a0\u2013 January 8\u20139, 1324) was an Italian merchant, explorer, and writer who travelled through Asia along the Silk Road between 1271 and 1295."], ["Christopher Columbus (/k\u0259\u02c8l\u028cmb\u0259s/; before 31 October 1451\u00a0\u2013 20 May 1506) was an Italian explorer and colonizer who completed four voyages across the Atlantic Ocean that opened the New World for conquest and permanent European colonization of the Americas."]]}, {"qid": "d0ecc32bf6330b6a6074", "term": "Presidency of Bill Clinton", "description": "1993\u20132001 U.S. presidential administration", "question": "Did the Presidency of Bill Clinton conclude with his impeachment?", "answer": false, "facts": ["Bill Clinton was impeached in 1998.", "Bill Clinton remained in office until 2001."], "decomposition": ["In what year was Bill Clinton impeached?", "In what year did Bill Clinton's presidency end?", "Is #1 the same as #2?"], "evidence": [[[["Bill Clinton-61"]], [["Bill Clinton-1", "Impeachment of Bill Clinton-29"]], ["operation"]], [[["Impeachment of Bill Clinton-16"]], [["Bill Clinton-1"]], ["operation"]], [[["Bill Clinton-61"]], [["Bill Clinton-61"]], ["operation"]]], "golden_sentence": [["Clinton was impeached on December 19, 1998 by the House of Representatives."], ["William Jefferson Clinton (n\u00e9 Blythe III; born August 19, 1946) is an American politician who served as the 42nd president of the United States from 1993 to 2001.", ""]]}, {"qid": "3659f94a4c4095e84665", "term": "Canary Islands", "description": "Archipelago in the Atlantic and autonomous community of Spain", "question": "Could someone in the Canary Islands fish for largemouth bass?", "answer": false, "facts": ["The Canary Islands are located in the Atlantic Ocean", "The Atlantic Ocean is a body of salt water", "Largemouth bass live in fresh water"], "decomposition": ["What kind of water do largemouth bass live in?", "In what body of water are the Canary Islands located?", "What kind of water is found in #2?", "Is #1 the same as #3?"], "evidence": [[[["Largemouth bass-1"]], [["Canary Islands-1"]], [["Atlantic Ocean-31"]], ["operation"]], [[["Largemouth bass-1"]], [["Canary Islands-1"]], [["Saline water-1"]], ["operation"]], [[["Largemouth bass-1"]], [["Canary Islands-1"]], [["Archipelago-1"]], ["operation"]]], "golden_sentence": [["The largemouth bass (Micropterus salmoides) is a carnivorous freshwater gamefish in the Centrarchidae (sunfish) family, a species of black bass generally native to eastern and central North America, in Canada, United States and northern Mexico, but widely introduced elsewhere."], ["The Canary Islands (/k\u0259\u02c8n\u025b\u0259ri/; Spanish: Islas Canarias, pronounced\u00a0[\u02c8islas ka\u02c8na\u027ejas]), also known informally as the Canaries, are a Spanish archipelago and the southernmost autonomous community of Spain located in the Atlantic Ocean, in a region known as Macaronesia, 100 kilometres (62 miles) west of Morocco at the closest point."], ["Although the lowest salinity values are just north of the equator (because of heavy tropical rainfall), in general, the lowest values are in the high latitudes and along coasts where large rivers enter."]]}, {"qid": "2f484b8eddb4e08f1f4e", "term": "Referee", "description": "person of authority, in a variety of sports, who is responsible for presiding over the game from a neutral point of view", "question": "Is the referee at a soccer match highly visible against the field?", "answer": true, "facts": ["Referees uniforms for soccer matches are usually bright neon colors.", "An alternative referee uniform color is black and white stripes."], "decomposition": ["What are the typically colors of a referee's uniform?", "Is #1 easy to see from afar?"], "evidence": [[[["Referee (association football)-6"]], ["operation"]], [[["Referee (association football)-6"]], ["no_evidence"]], [[["Kit (association football)-12"]], [["Black-3"], "operation"]]], "golden_sentence": [["FIFA allows referees to wear five colours: black, red, yellow, green and blue."]]}, {"qid": "3c23de919cb06f46d8ae", "term": "Intellectual disability", "description": "Generalized neurodevelopmental disorder", "question": "Is dyslexia the most common intellectual disability in US?", "answer": false, "facts": ["An intellectual disability is reflected in below-average IQ and a lack of skills needed for daily living.", "Learning disabilities are weaknesses in certain academic skills. usually, Reading, writing and math.", "Dyslexia is characterized by difficulties with accurate and/or fluent word recognition and by poor spelling and decoding abilities.", "Thomas Jefferson, George Washington, and John F. Kennedy were successful presidents while being dyslexic."], "decomposition": ["What are the practical effects of an intellectual disability?", "What are the practical effects of dyslexia?", "Is #2 within the scope of #1?"], "evidence": [[[["Intellectual disability-1"]], [["Dyslexia-1"]], [["Dyslexia-1", "Intellectual disability-1"]]], [[["Intellectual disability-1"]], [["Dyslexia-1"]], ["operation"]], [[["Intellectual disability-1"], "no_evidence"], [["Dyslexia-1"]], ["operation"]]], "golden_sentence": [[""], [""], ["", ""]]}, {"qid": "7218e2c69a2549fd489f", "term": "Ada Lovelace", "description": "English mathematician, computer programming pioneer", "question": "Did Ada Lovelace die tragically young for her era?", "answer": false, "facts": ["Ada Lovelace died at the age of 36 in 1852.", "The life expectancy in the 1800s was between 35 and 39 years old.", "Tuberculosis was one of the leading causes of death in the 1800s and a vaccine was not made until the 1900s."], "decomposition": ["How old was Ada Lovelace when she died?", "In what year did Ada Lovelace die?", "What was the average life expectancy range around #2?", "Is #1 not is #3?"], "evidence": [[[["Ada Lovelace-6"]], [["Ada Lovelace-1"]], [["Life expectancy-11"], "no_evidence"], ["operation"]], [[["Ada Lovelace-6"]], [["Ada Lovelace-6"]], ["no_evidence"], ["operation"]], [[["Ada Lovelace-20"]], [["Ada Lovelace-20"]], [["Life expectancy-10"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["She died of uterine cancer in 1852 at the age of 36."], ["Augusta Ada King, Countess of Lovelace (n\u00e9e Byron; 10 December 1815\u00a0\u2013 27 November 1852) was an English mathematician and writer, chiefly known for her work on Charles Babbage's proposed mechanical general-purpose computer, the Analytical Engine."], [""]]}, {"qid": "0c8ce64882b7257750ca", "term": "Constitution of the Philippines", "description": "Supreme law of the Republic of the Philippines", "question": "Does the Constitution of the Philippines copy text from the British constitution?", "answer": false, "facts": ["The Constitution of the Philippines is a document ratified in 1987", "The British constitution is not an actual document, but a collection of legal statutes, precedent, political custom and social convention"], "decomposition": ["What was the British Constitution?", "What kind of document was the Constitution of the Philippines?", "Can #1 copy something from #2"], "evidence": [[[["Constitution of the United Kingdom-1"]], [["Constitution of the Philippines-1"]], ["operation"]], [[["Constitution of the United Kingdom-1"]], [["Constitution of the Philippines-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Constitution of the United Kingdom-5"]], [["Constitution of the Philippines-1"]], ["no_evidence"]]], "golden_sentence": [["In England in 1215, Magna Carta required the King to call \"common counsel\" or Parliament, hold courts in a fixed place, guarantee fair trials, guarantee free movement of people, free the church from the state, and enshrined the rights of \"common\" people to use the land."], ["The Constitution of the Philippines (Filipino: Saligang Batas ng Pilipinas or Konstitusyon ng Pilipinas, Spanish: Constituci\u00f3n de la Rep\u00fablica de Filipinas) is the constitution or supreme law of the Republic of the Philippines."]]}, {"qid": "7adf8127893067701a70", "term": "Argon", "description": "Chemical element with atomic number 18", "question": "Is Argon near Neon on the periodic table of elements?", "answer": true, "facts": ["Argon is a noble gas.", "Neon is a noble gas. ", "The noble gases are all clumped together on the periodic table of elements."], "decomposition": ["What group of the periodic table is argon in?", "What group of the periodic table is neon in?", "Is #1 the same as #2?"], "evidence": [[[["Noble gas-1"]], [["Noble gas-1"]], ["operation"]], [[["Argon-1"]], [["Noble gas-1", "Noble gas-2"]], ["operation"]], [[["Argon-1"]], [["Neon-21"]], [["Group (periodic table)-5"]]]], "golden_sentence": [["The noble gases (historically also the inert gases; sometimes referred to as aerogens) make up a group of chemical elements with similar properties; under standard conditions, they are all odorless, colorless, monatomic gases with very low chemical reactivity."], ["The noble gases (historically also the inert gases; sometimes referred to as aerogens) make up a group of chemical elements with similar properties; under standard conditions, they are all odorless, colorless, monatomic gases with very low chemical reactivity."]]}, {"qid": "541fd46c0135502dbbd3", "term": "Leaf", "description": "organ of a vascular plant, composing its foliage", "question": "Do oak trees have leaves during winter?", "answer": false, "facts": ["Oak trees are deciduous.", "Deciduous trees lose their leaves during autumn, and they grow back during spring.", "Winter is between autumn and spring."], "decomposition": ["When do oak trees lose their leaves?", "When do oak trees leaves grow back", "Is winter not the season between #1 and #2?"], "evidence": [[[["Deciduous-3", "Oak-1"]], [["Deciduous-9"]], ["operation"]], [[["Oak-2"]], [["Oak-2"]], [["Winter-1"]]], [[["Oak-2"]], [["Oak-2"], "no_evidence"], [["Winter-6"], "operation"]]], "golden_sentence": [["In some cases leaf loss coincides with winter\u2014namely in temperate or polar climates.", ""], ["This layer is formed in the spring during active new growth of the leaf; it consists of layers of cells that can separate from each other."]]}, {"qid": "a6a9efa56b587c1b586c", "term": "Ronda Rousey", "description": "American professional wrestler, actress, author, mixed martial artist and judoka", "question": "Does Ronda Rousey avoid BBQ restaraunts?", "answer": true, "facts": ["Ronda Rousey is a professional athlete in MMA.", "Ronda Rousey is a vegan.", "BBQ is a style of restaurant that predominantly serves cooked meat.", "Meat consumption is opposed and avoided by vegans.", "Vegans don't eat meat."], "decomposition": ["What kind of food is served at BBQ restaurants?", "What dietary restrictions does Ronda Rousey follow?", "Would #2 avoid #1?"], "evidence": [[[["Ribs (food)-4"]], [["Ronda Rousey-73"]], [["Veganism-24"]]], [[["Barbecue restaurant-10"]], [["Ronda Rousey-73"]], [["Paleolithic diet-11"], "operation"]], [[["Barbecue-3"]], [["Ronda Rousey-73"]], [["Veganism-1"], "operation"]]], "golden_sentence": [["They are served as a rack of meat which diners customarily tear apart by hand, then eat the meat from the bone."], [""], ["The British Vegan Society will certify a product only if it is free of animal involvement as far as possible and practical, including animal testing, but \"recognises that it is not always possible to make a choice that avoids the use of animals\", an issue that was highlighted in 2016 when it became known that the UK's newly-introduced \u00a35 note contained tallow."]]}, {"qid": "88d23df6d791f5136fad", "term": "Amtrak", "description": "Intercity rail operator in the United States", "question": "Does Amtrak run from NYC directly to the Moai location?", "answer": false, "facts": ["Amtrak is a series of railways that transport people to various locations.", "The Moai are ancient stone statue faces that are a popular tourist destination.", "The Moai are located on Easter Island, an island in the Pacific ocean, near Chile."], "decomposition": ["Which major regions does Amtrak's passenger railroad service cover?", "Where are the Moai located?", "Is #2 located within any of #1?"], "evidence": [[[["Amtrak-1"]], [["Moai-8"]], ["operation"]], [[["Amtrak-3"]], [["Rapa Nui people-9"]], ["operation"]], [[["Amtrak-1"]], [["Moai-1"]], [["Polynesia-1"]]]], "golden_sentence": [["The National Railroad Passenger Corporation, doing business as Amtrak (reporting marks AMTK, AMTZ), is a passenger railroad service that provides medium and long-distance intercity service in the contiguous United States and to nine Canadian cities."], [""]]}, {"qid": "3242ddaaea44e389322c", "term": "Supreme Court of the United States", "description": "Highest court in the United States", "question": "Is Supreme Court of the United States analogous to High Courts of Justice of Spain?", "answer": false, "facts": ["The Supreme Court of the United States is the final court ad has final say in judicial matters.", "The High Courts of Justice in Spain rule over single communities.", "The Supreme Court of Spain is the highest court in Spain and can overrule lesser courts."], "decomposition": ["What is the extent of the jurisdiction of The Supreme Court of the United States?", "Do the High courts of justice (Spain) have the same jurisdiction as #1?"], "evidence": [[[["Supreme Court of the United States-1"]], [["High Courts of Justice of Spain-1", "Judiciary of Spain-7"]]], [[["Supreme Court of the United States-1"]], [["High Courts of Justice of Spain-1"], "operation"]], [[["Supreme Court of the United States-60"]], ["operation"]]], "golden_sentence": [["It has ultimate (and largely discretionary) appellate jurisdiction over all federal and state court cases that involve a point of federal law, and original jurisdiction over a narrow range of cases, specifically \"all Cases affecting Ambassadors, other public Ministers and Consuls, and those in which a State shall be Party\"."], ["The Superior Courts of Justice (Spanish: Tribunales Superiores de Justicia), or High Courts of Justice, are courts within the judicial system of Spain, whose territorial scope covers an Autonomous Community, as laid down in the Organic Law of Judicial Power (Ley Org\u00e1nica del Poder Judicial).", "Composed of five chambers, it has cognizance of all jurisdictional orders and its rulings cannot be appealed, except to the Constitutional Court, when one of the parties claims that their constitutional rights have been infringed."]]}, {"qid": "130fb60a6a86457cb6ea", "term": "Polymath", "description": "Individual whose knowledge spans a significant number of subjects", "question": "Would Tony Stark be considered a polymath?", "answer": true, "facts": ["A polymath is a person who has knowledge in a wide variety of subjects.", "Tony Stark is considered a genius in mathematics, engineering, computer science, and physics, as well as demonstrating skills in metalworking, engine design, and genetics."], "decomposition": ["What does one have to have to be considered a polymath?", "Does Tony Stark have #1?"], "evidence": [[[["Polymath-1"]], [["Iron Man-2"], "operation"]], [[["Polymath-1"]], [["Tony Stark (Marvel Cinematic Universe)-1"]]], [[["Polymath-1"]], [["Iron Man-71"], "operation"]]], "golden_sentence": [["A polymath (Greek: \u03c0\u03bf\u03bb\u03c5\u03bc\u03b1\u03b8\u03ae\u03c2, polymath\u0113s, \"having learned much\"; Latin: homo universalis, \"universal man\") is an individual whose knowledge spans a significant number of subjects, known to draw on complex bodies of knowledge to solve specific problems."], [""]]}, {"qid": "c70766f9cf9fd466ee91", "term": "Rand Paul", "description": "American politician, ophthalmologist, and United States Senator from Kentucky", "question": "Did Rand Paul frequently swim in Lake Michigan during his undergraduate years?", "answer": false, "facts": ["Rand Paul joined the swim team when he attended Baylor University.", "Baylor University is located in Waco, Texas.", "Lake Michigan is nearly 1,000 miles from Waco, Texas."], "decomposition": ["Where did Rand Paul do his undergraduate studies?", "In what state is #1?", "Is Lake Michigan near #2?"], "evidence": [[[["University of Pittsburgh School of Medicine-26"]], [["Baylor University-1"]], ["operation"]], [[["Rand Paul-2"]], [["Baylor University-1"]], ["operation"]], [[["Rand Paul-9"]], [["Baylor University-1"]], [["Lake Michigan-2"]]]], "golden_sentence": [[""], ["Chartered in 1845 by the last Congress of the Republic of Texas, it is the oldest continuously operating university in Texas and one of the first educational institutions west of the Mississippi River in the United States."]]}, {"qid": "fb2bdbdeb31dd37ad4a2", "term": "Retail", "description": "Sale of goods and services from individuals or businesses to the end-user", "question": "Is SnapCap an example of a retail store?", "answer": false, "facts": ["SnapCap specializes in small business loans.", "Retail stores sell products to individual consumers. ", "Small businesses are not individual consumers."], "decomposition": ["What does SnapCap specialize in?", "Who do #1's sell their products to?", "Who do retail stores sell their products to?", "Is #2 the same as #3?"], "evidence": [[[["LendingTree-8"], "no_evidence"], ["no_evidence"], [["Retail-6"]], ["operation"]], [[["LendingTree-8"], "no_evidence"], [["LendingTree-1"], "no_evidence"], [["Retail-1"]], ["operation"]], [[["Payday loan-1"], "no_evidence"], [["Payday loan-1"]], [["Retail-1"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], ["Retail refers to the activity of selling goods or services directly to consumers or end-users."]]}, {"qid": "c16a7ebf4aeb20f5b6e6", "term": "Hair", "description": "protein filament that grows from follicles found in the dermis, or skin", "question": "Can furniture be made of hair?", "answer": true, "facts": ["Hair is a protein filament that grows from living bodies.", "Hair is durable when woven together. ", "Furniture cushions can be maid from horse hair. "], "decomposition": ["What is hair?", "Can #1 be woven together securely?"], "evidence": [[[["Hair-2"]], ["no_evidence", "operation"]], [[["Hair-2"]], [["Alpha-keratin-4"], "operation"]], [[["Hair-2"]], [["Braid-2", "Cushion-1"], "no_evidence"]]], "golden_sentence": [["Most common interest in hair is focused on hair growth, hair types, and hair care, but hair is also an important biomaterial primarily composed of protein, notably alpha-keratin."]]}, {"qid": "85c4a7dd9d6d3e315872", "term": "March", "description": "third month in the Julian and Gregorian calendars", "question": "Is March named after Jupiter's son in Roman mythology?", "answer": true, "facts": ["March is named after the Roman god Mars.", "Mars was the son of the Roman gods Jupiter and Juno."], "decomposition": ["Who are the sons of Jupiter in Roman mythology?", "Who is the month of March named after?", "Is #2 included in #1?"], "evidence": [[[["Hercules-1", "Mars (mythology)-1", "Vulcan (mythology)-41"]], [["March-1"]], ["operation"]], [[["Jupiter (mythology)-106"]], [["Apollo-25"], "no_evidence"], ["operation"]], [[["Mars (mythology)-7"]], [["Martius (month)-1"]], ["operation"]]], "golden_sentence": [["He was the Roman equivalent of the Greek divine hero Heracles, who was the son of Zeus (Roman equivalent Jupiter) and the mortal Alcmene.", "", "He was the son of Jupiter and Juno, and the husband of Maia and Aphrodite (Venus)."], ["January February March April May June July August September October November December March is the third month of the year and named after Mars in both the Julian and Gregorian calendars."]]}, {"qid": "27fa24f93ae380de7e1b", "term": "Cultural hegemony", "description": "Marxist notion of cultural dominance", "question": "Can the theory of cultural hegemony explain global warming?", "answer": false, "facts": ["Cultural hegemony is a theory of social and cultural dominance rooted in Marxism", "Marxism is a philosophy with applications in the social sciences and humanities", "Global warming is a phenomenon dealt with by environmental science"], "decomposition": ["The theory of cultural hegemony is rooted in which philosophy?", "Which branch of science does #1 have applications in?", "Which branch of science does global warming concern?", "Is #2 the same as #3?"], "evidence": [[[["Cultural hegemony-1"]], [["Marxism-3"]], [["Global warming-71", "Svante Arrhenius-1"], "no_evidence"], ["operation"]], [[["Cultural hegemony-1"]], [["Cultural hegemony-2"], "no_evidence"], [["Global warming-19", "Scientific consensus on climate change-43"]], ["operation"]], [[["Cultural hegemony-1"]], [["Marxist philosophy-1"]], [["Atmospheric chemistry-2"]], ["operation"]]], "golden_sentence": [["In Marxist philosophy, cultural hegemony is the domination of a culturally diverse society by the ruling class who manipulate the culture of that society \u2014 the beliefs and explanations, perceptions, values, and mores \u2014 so that the imposed, ruling-class worldview becomes the accepted cultural norm; the universally valid dominant ideology, which justifies the social, political, and economic status quo as natural and inevitable, perpetual and beneficial for every social class, rather than as artificial social constructs that benefit only the ruling class."], ["Marxism has had a profound impact on global academia and has influenced many fields such as archaeology, art history, anthropology, media studies, science studies, political science, theater, history, sociology, art history and theory, cultural studies, education, economics, ethics, criminology, geography, literary criticism, aesthetics, film theory, critical psychology and philosophy."], ["", "Originally a physicist, but often referred to as a chemist, Arrhenius was one of the founders of the science of physical chemistry."]]}, {"qid": "38d2c97da9f6e1d70c41", "term": "Pig Latin", "description": "secret language game", "question": "Is it impossible for pigs to use pig latin?", "answer": true, "facts": ["Pig latin is a language game played by rearranging parts of words to disguise them", "Pigs are ungulates and incapable of speech using human languages"], "decomposition": ["What is referred to as pig latin?", "Which species are capable of using #1?", "Are pigs excluded from #2?"], "evidence": [[[["Pig Latin-1"]], [["English language-1", "Human-1", "Language-15"]], ["operation"]], [[["Pig Latin-1"]], [["Language-1"]], ["operation"]], [[["Pig Latin-1"]], [["Great ape language-1", "Language-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["The reference to Latin is a deliberate misnomer; Pig Latin is simply a form of argot or jargon unrelated to Latin, and the name is used for its English connotations as a strange and foreign-sounding language."], ["", "", ""]]}, {"qid": "1f4e75706228246754a3", "term": "J. D. Salinger", "description": "American writer", "question": "Is J.D. Salinger's most successful work influential to killers?", "answer": true, "facts": ["J.D. Salinger's most popular work was Catcher in the Rye.", "John Hinckley Jr. tried to assassinate Ronald Reagan after reading Catcher in the Rye.", "Mark David Chapman had a copy of Catcher in the Rye when he assassinated John Lennon.", "Robert John Bardo carried a copy of Catcher in the Rye when he murdered actress Rebecca Schaeffer."], "decomposition": ["Which of J.D. Salinger's novels was most popular?", "has #1 been associated with inspiring murder?"], "evidence": [[[["J. D. Salinger-1"]], [["The Catcher in the Rye-26"]]], [[["J. D. Salinger-1"]], [["The Catcher in the Rye-26"]]], [[["J. D. Salinger-1"]], [["The Catcher in the Rye-26"]]]], "golden_sentence": [["Jerome David Salinger (/\u02c8s\u00e6l\u026and\u0292\u0259r/; January 1, 1919\u00a0\u2013 January 27, 2010) was an American writer best known for his novel The Catcher in the Rye."], ["Several shootings have been associated with Salinger's novel, including Robert John Bardo's murder of Rebecca Schaeffer and John Hinckley Jr.'s assassination attempt on Ronald Reagan."]]}, {"qid": "eaae55177d9a00d86d93", "term": "Ice", "description": "water frozen into the solid state", "question": "Did Ice make people rich?", "answer": true, "facts": ["Trading ice was common in the 1800s.", "People created industries harvesting and selling ice.", "Some ice sellers became extremely rich. "], "decomposition": ["In the 1800's, what item was commonly traded?", "Did some people become rich off of selling #1?"], "evidence": [[[["Ice trade-1"]], [["Ice trade-10"]]], [[["Ice-48"], "no_evidence"], [["Ice-49"], "operation"]], [[["Ice trade-1"]], [["Ice trade-2"]]]], "golden_sentence": [["Networks of ice wagons were typically used to distribute the product to the final domestic and smaller commercial customers."], ["Where Tudor had a strong market share, he would respond to competition from passing traders by lowering his prices considerably, selling his ice at the unprofitable rate of one cent ($0.20) per pound (0.5\u00a0kg); at this price, competitors would typically be unable to sell their own stock at a profit: they would either be driven into debt or if they declined to sell, their ice would melt away in the heat."]]}, {"qid": "577dc7d321b43faa3a17", "term": "Justin Bieber", "description": "Canadian singer-songwriter and actor", "question": "Does Justin Bieber vote in October?", "answer": true, "facts": ["Justin Bieber is a Canadian citizen", "Canadian elections are held on the third Monday in October"], "decomposition": ["What country is Justin Bieber a citizen of?", "When does #1 hold its national elections?", "Is #2 October?"], "evidence": [[[["Justin Bieber-1", "Justin Bieber-51"]], [["Fixed election dates in Canada-13"]], ["operation"]], [[["Justin Bieber-1"]], [["Elections in Canada-23"]], ["operation"]], [[["Justin Bieber-1"]], [["Elections in Canada-13"]], ["operation"]]], "golden_sentence": [["Justin Drew Bieber (/\u02c8bi\u02d0b\u0259r/; born March 1, 1994) is a Canadian singer, songwriter and actor.", "Bieber has said he is not interested in obtaining US citizenship, and has praised Canada as being \"the best country in the world\", citing its mostly government-funded health care system as a model example."], ["This amendment will result in the next provincial election occurring on June 2, 2022."]]}, {"qid": "1e628c1171e197713797", "term": "Flying fish", "description": "Family of marine fish that can make powerful, self-propelled leaps out of water", "question": "Do flying fish have good eyesight?", "answer": true, "facts": ["Flying fish  are commonly found in the epipelagic zone, the top layer of the ocean to a depth of about 200 m (656 ft). ", "The epipelagic zone is the illuminated zone at the surface of the sea where enough light is available for photosynthesis. ", "Good eyesight is a necessary survival trait for animals living in well-lit areas."], "decomposition": ["Which layer of the ocean are flying fish usually found?", "What are the lighting conditions characteristic of #1?", "Would good eyesight be necessary for organisms in #2 environment?"], "evidence": [[[["Flying fish-6"]], [["Photic zone-1"]], [["Photic zone-3"], "no_evidence"]], [[["Flying fish-6"]], [["Photic zone-1"]], [["Photic zone-1"], "operation"]], [[["Flying fish-6"]], [["Photic zone-1"]], ["operation"]]], "golden_sentence": [["They are commonly found in the epipelagic zone, the top layer of the ocean to a depth of about 200 m (656 ft)."], [""], [""]]}, {"qid": "15c1c5825e1f5469c9b7", "term": "Carl Linnaeus", "description": "Swedish botanist, physician, and zoologist", "question": "Does Carl Linnaeus share the same final resting place as Michael Jackson?", "answer": false, "facts": ["Carl Linnaeus is buried in the Uppsala Cathedral.", "Michael Jackson is entombed at the Forest Lawn Memorial Park."], "decomposition": ["Where is Carl Linnaeus buried?", "Where is Michael Jackson entombed?", "Is #1 the same as #2?"], "evidence": [[[["Uppsala Cathedral-3"], "no_evidence"], [["Forest Lawn Memorial Park (Glendale)-13"]], ["operation"]], [[["Uppsala Cathedral-29"]], [["Michael Jackson memorial service-17"]], [["Michael Jackson memorial service-17", "Uppsala Cathedral-29"]]], [[["Uppsala Cathedral-3"]], [["Michael Jackson-70"]], ["operation"]]], "golden_sentence": [[""], ["In 2009 Forest Lawn Glendale became the focus of intense media interest surrounding the private interment of Michael Jackson in the privacy of Holly Terrace in the Great Mausoleum."]]}, {"qid": "2c3f1f6879f5f2aa7b39", "term": "Xenophobia", "description": "dislike of that which is perceived to be foreign or strange", "question": "Is xenophobia hypothetically unimportant between Saladin and Ali Askari?", "answer": true, "facts": ["Xenophobia is the dislike of someone that is foreign or from a different background.", "Saladin was a Kurdish leader that became sultan of Egypt.", "Ali Askari was a Kurdish politician."], "decomposition": ["Which relation between two parties could lead bring about xenophobia?", "What was Saladin's ethnicity?", "What was Ali Askari's ethnicity?", "Does the relation between #2 and #3 fail to describe #1?"], "evidence": [[[["Xenophobia-1"]], [["Saladin-1"]], [["Ali Askari-3"]], ["operation"]], [[["Xenophobia-1"]], [["Saladin-1"]], [["Ali Askari-2"]], ["operation"]], [[["Xenophobia-1"]], [["Saladin-1"]], [["Ali Askari-3"]], [["In-group and out-group-1"]]]], "golden_sentence": [["It is an expression of perceived conflict between an ingroup and an outgroup and may manifest in suspicion by the one of the other's activities, a desire to eliminate their presence, and fear of losing national, ethnic or racial identity."], ["A Sunni Muslim of Kurdish ethnicity, Saladin led the Muslim military campaign against the Crusader states in the Levant."], [""]]}, {"qid": "b28a377ad4264c7393aa", "term": "Diary", "description": "Written record with discrete entries arranged by date", "question": "Can a dolphin keep a diary?", "answer": false, "facts": ["A diary is a written record.", "Dolphins cannot write."], "decomposition": ["What is a diary?", "What does one need to do in order to keep #1?", "Can a dolphin do #2?"], "evidence": [[[["Diary-1"]], [["Writing-1"]], ["operation"]], [[["Diary-1"]], [["Hand-1"]], [["Dolphin-20"]]], [[["Diary-1"]], [["Diary-19"]], [["Dolphin-1"], "operation"]]], "golden_sentence": [["A diary is a record (originally in handwritten format) with discrete entries arranged by date reporting on what has happened over the course of a day or other period."], [""]]}, {"qid": "a0cb40b94f3f44f437a5", "term": "Tourism", "description": "travel for recreational or leisure purposes", "question": "Is the Jurassic era a tourist destination?", "answer": false, "facts": ["The Jurassic era is a period of time in the past.", "Time travel does not currently exist. "], "decomposition": ["When did the Jurassic era occur?", "Can tourist travel to #1?"], "evidence": [[[["Jurassic-1"]], [["Time travel-2"]]], [[["Jurassic-1"]], ["operation"]], [[["Jurassic-1"]], ["operation"]]], "golden_sentence": [["The Jurassic (/d\u0292\u028a\u02c8r\u00e6s.\u026ak/ juu-RASS-ik; from the Jura Mountains) is a geologic period and system that spanned 56 million years from the end of the Triassic Period 201.3 million years ago (Mya) to the beginning of the Cretaceous Period 145 Mya."], ["It is uncertain if time travel to the past is physically possible."]]}, {"qid": "21283ac756a83f563c8b", "term": "CAPTCHA", "description": "computer test to discriminate human users from spambots", "question": "Are any of the words that CAPTCHA stands for palindromes?", "answer": false, "facts": ["A palindrome is a word that reads the same backwards and forwards like madam.", "CAPTCHA stands for: Completely Automated Public Turing test to tell Computers and Humans Apart."], "decomposition": ["What words does CAPTCHA stand for?", "What is the characteristic of a palindrome?", "Does any word from #1 have the characteristic of #2?"], "evidence": [[[["CAPTCHA-1"]], [["Palindrome-1"]], ["operation"]], [[["CAPTCHA-1"]], [["Palindrome-1"]], ["operation"]], [[["CAPTCHA-1"]], [["Palindrome-1"]], ["operation"]]], "golden_sentence": [["A CAPTCHA (/k\u00e6p.t\u0283\u0259/, a contrived acronym for \"completely automated public Turing test to tell computers and humans apart\") is a type of challenge\u2013response test used in computing to determine whether or not the user is human."], ["A palindrome is a word, number, phrase, or other sequence of characters which reads the same backward as forward, such as madam, racecar."]]}, {"qid": "fef17bdef486a3e8bea2", "term": "The Young and the Restless", "description": "television series", "question": "Would a binge watch of entire Young and the Restless take longer than a leap year?", "answer": true, "facts": ["A leap year has 366 total days.", "As of March 19th, 2018, every episode of the Young and the Restless would take 467 days and 2 hours to watch."], "decomposition": ["How many days are in a leap year?", "How long would it take to watch every episode of the Young and the Restless?", "Is #2 greater than #1?"], "evidence": [[[["Leap year-2"]], [["The Young and the Restless-1", "The Young and the Restless-12"], "no_evidence"], ["operation"]], [[["Leap year-1", "Leap year-6"]], [["The Young and the Restless-1", "The Young and the Restless-12"], "no_evidence"], ["operation"]], [[["Leap year-2"]], [["The Young and the Restless-1"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["For example, in the Gregorian calendar, each leap year has 366 days instead of 365, by extending February to 29 days rather than the common 28."], ["First broadcast on March 26, 1973, The Young and the Restless was originally broadcast as half-hour episodes, five times a week.", "On March 20, 2020, after 32 years and over 1,500 consecutive weeks, The Young and the Restless was no longer the number one soap opera in the United States."]]}, {"qid": "a46b255d6f83cb5b62aa", "term": "Metroid", "description": "Video game series", "question": "Was the original Metroid groundbreaking for its polygons?", "answer": false, "facts": ["Metroid was a 1986 video game released on the Nintendo Entertainent System.", "Polygons were a graphics style that became prominent in the 1990s on the Sony Playstation.", "Nintendo Entertainment System games had a 2-D pixel-art style.", "Metroid is hailed as being the first mainstream game with a playable female protagonist."], "decomposition": ["When was video game Metroid originally released?", "What does polygons in video gaming represent?", "When did #2 become prominent?", "Is #1 after #3?"], "evidence": [[[["Metroid (video game)-1"]], [["Polygon (computer graphics)-1"]], [["Computer graphics-39"]], ["operation"]], [[["Metroid (video game)-1"]], [["3D computer graphics-1"], "no_evidence"], [["Fifth generation of video game consoles-1"], "no_evidence"], ["operation"]], [[["Metroid (video game)-13"]], [["Polygon (website)-1"]], [["Polygon (website)-1"]], ["operation"]]], "golden_sentence": [["The first installment in the Metroid series, it was originally released in Japan for the Family Computer Disk System peripheral in August 1986."], [""], ["The Sega Model 2 in 1993 and Sega Model 3 in 1996 subsequently pushed the boundaries of commercial, real-time 3D graphics."]]}, {"qid": "c3a441feb8d421d0616e", "term": "Preventive healthcare", "description": "Prevent and minimize the occurrence of diseases", "question": "Do you need to schedule separate preventive healthcare and sickness visits? ", "answer": true, "facts": ["Preventive healthcare options are typically covered at no charge by health insurance.", "Sick visits to the doctor are billed separately from preventive healthcare visits.", "Sick visits and preventive healthcare visits are generally given different time allotments. "], "decomposition": ["How are preventive healthcare visits billed to insurance companies?", "How are sick visits to the doctor billed to insurance companies?", "Is #1 different from #2?"], "evidence": [[[["Preventive healthcare-60"], "no_evidence"], ["no_evidence"], ["operation"]], [[["Health care-11"], "no_evidence"], [["Health care-15"], "no_evidence"], ["operation"]], [[["Preventive healthcare-54"], "no_evidence"], [["Managed care-38"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Healthcare insurance companies are willing to pay for preventive care despite the fact that patients are not acutely sick in hope that it will prevent them from developing a chronic disease later on in life."]]}, {"qid": "64253dd8d8bcca944516", "term": "Black", "description": "The darkest shade, resulting from the absence or complete absorption of light. Like white and grey, it has no hue", "question": "Is Anakin Skywalker from Star Wars associated with the color black?", "answer": true, "facts": ["As a Jedi during the Clone Wars, Anakin Skywalker often wore black robes.", "After he was burned and transformed into the cyborg Darth Vader, he received a distinctive and famous all-black outfit including a black mask."], "decomposition": ["What is the color of most outfits worn by Star Wars' Anakin Skywalker?", "Is #1 the same as black?"], "evidence": [[[["Darth Vader-1", "Darth Vader-15"]], ["operation"]], [[["Darth Vader-15"]], ["operation"]], [[["Darth Vader-1", "Darth Vader-15"]], ["operation"]]], "golden_sentence": [["", ""]]}, {"qid": "7802bba0b11f8458ca1e", "term": "Whole genome sequencing", "description": "A process that determines the complete DNA sequence of an organism's genome at a single time", "question": "Can whole genome sequencing be used for COVID-19?", "answer": false, "facts": ["Whole genome sequencing is used to analyze DNA", "RNA viruses do not have DNA", "COVID-19 is an RNA virus."], "decomposition": ["What does the whole genome sequencing process determine?", "Which virus is responsible for COVID-19?", "Does #2 have #1?"], "evidence": [[[["Whole genome sequencing-6"]], [["Coronavirus-1"]], [["Coronavirus-1"], "operation"]], [[["Whole genome sequencing-1"]], [["Coronavirus-1"]], [["RNA virus-1", "RNA virus-2"], "operation"]], [[["Whole genome sequencing-1"]], [["Coronavirus disease-8"]], [["Positive-sense single-stranded RNA virus-1"]]]], "golden_sentence": [[""], ["Mild illnesses include some cases of the common cold (which is caused also by certain other viruses, predominantly rhinoviruses), while more lethal varieties can cause SARS, MERS, and COVID-19."], [""]]}, {"qid": "720c98f230cc71cb5517", "term": "Viscosity", "description": "Resistance of a fluid to shear deformation", "question": "Do people with swallowing disorders need high viscosity drinks?", "answer": true, "facts": ["Swallowing disorders can make thin liquids like water dangerous to drink.", "Liquid thickeners are marketed towards people with difficulty drinking."], "decomposition": ["If a person has a swallowing disorder, what types of liquids are dangerous for them to drink?", "Are high viscosity drinks the opposite of #1?"], "evidence": [[[["Thickened fluids-1"]], [["Viscosity-1"], "operation"]], [[["Dysphagia-2"], "no_evidence"], [["Viscosity-1"], "operation"]], [[["Oropharyngeal dysphagia-2"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Individuals with difficulty swallowing may find liquids cause coughing, spluttering or even aspiration and thickening drinks enables them to swallow safely."], ["For liquids, it corresponds to the informal concept of \"thickness\": for example, syrup has a higher viscosity than water."]]}, {"qid": "3c141411b060b3677e98", "term": "Blues", "description": "Musical form and music genre", "question": "Were Depeche Mode heavily influenced by blues music?", "answer": false, "facts": ["Blues incorporated spirituals, work songs, field hollers, shouts, chants, and rhymed simple narrative ballads and was derived from African-Americans.", "Blues music uses instruments like slide guitar, harmonica, piano, and bass drums.", "Depeche Mode are a British pop synth group.", "Depeche Mode uses computer synthesizers to create their unique sound as well as heavy rock guitars.", "Depeche Mode was influenced by The Cure, and Ultravox, new wave rock bands."], "decomposition": ["What kind of songs and instruments are associated with Blues?", "What kind of musical instruments does the Depeche Mode use to create music?", "Is #2 very similar to #1?"], "evidence": [[[["Blues-37"]], [["Depeche Mode-35"]], ["operation"]], [[["Blues-1"]], [["Depeche Mode-1"]], ["operation"]], [[["Blues-1"], "no_evidence"], [["Depeche Mode-6"]], ["operation"]]], "golden_sentence": [["Performers such as Frank Stokes, Sleepy John Estes, Robert Wilkins, Joe McCoy, Casey Bill Weldon and Memphis Minnie used a variety of unusual instruments such as washboard, fiddle, kazoo or mandolin."], ["In 1993, Songs of Faith and Devotion, again with Flood producing, saw them experimenting with arrangements based as much on heavily distorted electric guitars and live drums (played by Alan Wilder, whose debut as a studio drummer had come on the Violator track \"Clean\") as on synthesizers."]]}, {"qid": "7a6b9595ecba68ad2dbe", "term": "Mail carrier", "description": "employee of the post office or postal service, who delivers mail to residences and businesses", "question": "Was being a mail carrier considered one of the most dangerous jobs?", "answer": true, "facts": ["The Pony Express was one of the first mail carrier services.", "The Pony Express operated form 1860 to 1861", "Pony Express riders would have to travel hundreds of miles on horse back through extreme weather and terrain. ", "The Pony Express sought to hire young expert horse riders willing to risk death."], "decomposition": ["What was the name of the mail carrier service that operated from 1860 to 1861?", "What would riders on #1 have to endure?", "Is #2 considered very dangerous?"], "evidence": [[[["Pony Express-1"]], [["Pony Express-31"]], ["operation"]], [[["Pony Express-1"]], [["Pony Express-38", "Pony Express-40"], "no_evidence"], ["operation"]], [[["Pony Express-1"]], [["Pony Express-30", "Pony Express-31"]], [["Pony Express-30", "Pony Express-31"]]]], "golden_sentence": [["The Pony Express was a mail service delivering messages, newspapers, and mail using relays of horse-mounted riders that operated from April 3, 1860 to October 24, 1861 between Missouri and California in the United States of America."], [""]]}, {"qid": "b12e912c02a0365b574a", "term": "Leopard seal", "description": "Species of mammal", "question": "Is Sea World hazardous to leopard seal's health?", "answer": true, "facts": ["Leopard seals have only one natural predator, the killer whale.", "Sea World is an aquatic show that involves many water animals.", "Killer Whales, such as Tilikum, are headliners at Sea World."], "decomposition": ["What is the leopard seals's predator?", "Would one find a #1 at Sea World?"], "evidence": [[[["Leopard seal-1"]], [["Kamogawa Sea World-15"]]], [[["Leopard seal-1"]], [["Shamu-1"], "operation"]], [[["Killer whale-7", "Leopard seal-1"]], [["SeaWorld-1"], "operation"]]], "golden_sentence": [["Its only natural predator is the killer whale."], [""]]}, {"qid": "766046991ab45420847e", "term": "Compact disc", "description": "Optical disc for storage and playback of digital audio", "question": "Would a compact disc melt in magma?", "answer": true, "facts": ["Magma is the molten material beneath the earth's surface.", "The temperature of magma ranges between 1300F and 2400F degrees.", "A compact disc is made of polycarbonate plastic.", "Polycarbonate plastic melts at 311F degrees."], "decomposition": ["What material is a compact disc made of?", "At what temperature does #1 melt", "What is the typical temperature range of magma?", "Is #2 less than or within #3"], "evidence": [[[["Compact disc-23"]], [["Polycarbonate-25"], "no_evidence"], [["Magma-6"]], ["no_evidence", "operation"]], [[["CD-ROM-4"]], [["Plastic-19"]], [["Magma-6"]], ["operation"]], [[["CD-R-7"]], [["Polycarbonate-10"]], [["Magma-6"]], ["operation"]]], "golden_sentence": [["A CD is made from 1.2-millimetre (0.047\u00a0in) thick, polycarbonate plastic and weighs 15\u201320 grams."], [""], ["Ultramafic (picritic) SiO2 < 45% Fe\u2013Mg > 8% up to 32%MgO Temperature: up to 1500\u00b0C Viscosity: Very Low Eruptive behavior: gentle or very explosive (kimberilites) Distribution: divergent plate boundaries, hot spots, convergent plate boundaries; komatiite and other ultramafic lavas are mostly Archean and were formed from a higher geothermal gradient and are unknown in the present Mafic (basaltic) SiO2 < 50% FeO and MgO typically < 10 wt% Temperature: up to ~1300\u00b0C Viscosity: Low Eruptive behavior: gentle Distribution: divergent plate boundaries, hot spots, convergent plate boundaries Intermediate (andesitic) SiO2 ~ 60% Fe\u2013Mg: ~ 3%th Temperature: ~1000\u00b0C Viscosity: Intermediate Eruptive behavior: explosive or effusive Distribution: convergent plate boundaries, island arcs Felsic (rhyolitic) SiO2 > 70% Fe\u2013Mg: ~ 2% Temperature: < 900\u00b0C Viscosity: High Eruptive behavior: explosive or effusive Distribution: common in hot spots in continental crust (Yellowstone National Park) and in continental rifts Temperatures of most magmas are in the range 700\u00a0\u00b0C to 1300\u00a0\u00b0C (or 1300\u00a0\u00b0F to 2400\u00a0\u00b0F), but very rare carbonatite magmas may be as cool as 490\u00a0\u00b0C, and komatiite magmas may have been as hot as 1600\u00a0\u00b0C."]]}, {"qid": "eb324efb728182a2faa9", "term": "Monarch", "description": "Person at the head of a monarchy", "question": "Does Canada have a relationship with a monarch?", "answer": true, "facts": ["Canada is a constitutional monarchy.", "The head of the monarchy that rules Canada is Queen Elizabeth."], "decomposition": ["What system of government does Canada follow?", "Who is the head of #1?", "Is #2 a monarch?"], "evidence": [[[["Government of Canada-6"]], ["no_evidence"], [["Records of heads of state-7"]]], [[["By the Grace of God-10", "Constitutional monarchy-1"], "no_evidence"], [["Monarchy-1"]], ["operation"]], [[["Government of Canada-1"]], [["Government of Canada-1"]], [["Government of Canada-1"]]]], "golden_sentence": [["As per the Constitution Acts of 1867 and 1982, Canada is a constitutional monarchy, wherein the role of the reigning sovereign is both legal and practical, but not political."], [""]]}, {"qid": "53fe86634a7f704c4492", "term": "Modern Family", "description": "American comedy TV series", "question": "Did Modern Family win a Slammy award?", "answer": false, "facts": ["Modern Family is a television sitcom", "The Slammy Awards were presented to people involved in professional wrestling"], "decomposition": ["What television genre is Modern Family?", "What genre are the Slammy Awards given to?", "Is #1 and #2 the same?"], "evidence": [[[["Modern Family-1"]], [["Slammy Award-1"]], ["operation"]], [[["Modern Family-1"]], [["Slammy Award-1"]], ["operation"]], [[["Modern Family-1"]], [["Slammy Award-1"]], ["operation"]]], "golden_sentence": [["Modern Family is an American television mockumentary family sitcom created by Christopher Lloyd and Steven Levitan for the American Broadcasting Company."], ["The Slammy Awards was a concept used by WWE, where awards, similar to the Academy and Grammy Awards, are given to professional wrestlers and other individuals within WWE, such as commentators and managers."]]}, {"qid": "0a96694091242cad10d0", "term": "Grey seal", "description": "species of seal", "question": "Can a grey seal swim in the same water as the subject of Moby Dick?", "answer": true, "facts": ["The range of gray seals is limited to parts of the northern hemisphere bordered by the Atlantic ocean", "The subject of Moby Dick was a sperm whale", "Sperm whales can be found in the north Atlantic, in addition to most other bodies of water on earth."], "decomposition": ["What kind of whale was Moby Dick?", "What is the range of #1?", "What is the range of gray seals?", "Is there an overlap between #2 and #3?"], "evidence": [[[["Moby-Dick-1"]], [["Sperm whale-2"], "no_evidence"], [["Grey seal-1"]], ["no_evidence", "operation"]], [[["Moby-Dick-1"]], [["Sperm whale-2"]], [["Grey seal-1"]], ["operation"]], [[["Moby-Dick-1"]], [["Sperm whale-2"]], [["Grey seal-7"]], ["operation"]]], "golden_sentence": [["The book is the sailor Ishmael's narrative of the obsessive quest of Ahab, captain of the whaling ship Pequod, for revenge on Moby Dick, the giant white sperm whale that on the ship's previous voyage bit off Ahab's leg at the knee."], [""], [""]]}, {"qid": "c745a00646afa56f7843", "term": "C-SPAN", "description": "American pay television network", "question": "Does the name C-SPAN refer to a form of telecommunications that utilizes outer space?", "answer": true, "facts": ["The S in C-SPAN refers to Satellite.", "Satellite communications require communicating with satellites that orbit the Earth in outer space."], "decomposition": ["What does C-SPAN's transmission equipment consist of, according to its full meaning?", "Is any of #1 located in outer space?"], "evidence": [[[["Cable television-2"]], [["Satellite-1"], "operation"]], [[["C-SPAN-14"]], [["Technology of television-4"]]], [[["C-SPAN-1"]], [["Satellite-1"]]]], "golden_sentence": [[""], [""]]}, {"qid": "417f753762126ee50c31", "term": "Johann Sebastian Bach", "description": "German composer", "question": "Did Johann Sebastian Bach influence heavy metal?", "answer": true, "facts": ["Johann Sebastian Bach was a classical German composer born in 1685.", "Lead singer of heavy metal band Skid Row, Sebastian Bach, took his name from German composer Johann Sebastian Bach.", "Heavy Metal band Metallica released a live album with the San Francisco Symphony.", "Deep Purple, n English hard rock/heavy metal band has cited classical musicians as their inspiration.", "Deep Purple's keyboard and guitar solos on \"Highway Star,\" have been called Bach-like in harmonic progression and virtuosic arpeggio figuration."], "decomposition": ["Who is the lead singer of \"Skid Row\"?", "Who did #1 name himself after?", "Which classic musician's work have Deep Purple's solo on \"Highway Star\" been compared with?", "Are #2 and #3 Johann Sebastian Bach and both bands heavy metal?"], "evidence": [[[["Sebastian Bach-1"]], [["Johann Sebastian Bach-1"], "no_evidence"], [["Highway Star (song)-4"]], [["Deep Purple-1", "Skid Row (American band) discography-2"], "operation"]], [[["Skid Row (American band)-1"]], [["Johann Sebastian Bach-1"]], [["Highway Star (song)-3"]], ["operation"]], [[["Sebastian Bach-1"]], [["Johann Sebastian Bach-1"]], [["Highway Star (song)-3"]], [["Deep Purple-1", "Skid Row (American band)-1"], "operation"]]], "golden_sentence": [["Sebastian Philip Bierk (born April 3, 1968), known professionally as Sebastian Bach, is a Canadian singer-songwriter who achieved mainstream success as frontman of Skid Row from 1987 to 1996."], ["Johann Sebastian Bach (31 March\u00a0[O.S."], [""], ["", ""]]}, {"qid": "a5562939e8315e41d631", "term": "Hello", "description": "salutation or greeting", "question": "Can your psychologist say hello to you while you are out at the supermarket?", "answer": false, "facts": ["Therapists are bound by confidentiality in all areas of their work. ", "It would violate the standard set by the APA for a therapist to acknowledge any client outside of a therapeutic setting."], "decomposition": ["What kind of code are therapists bound by in all areas of their work?", "Would it be in conformity to #1 to acknowledge a client outside a therapeutic setting?"], "evidence": [[[["Psychotherapy-16"]], ["no_evidence"]], [[["Hippocratic Oath-1"]], [["Confidentiality-13"], "operation"]], [[["APA Ethics Code-1"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""]]}, {"qid": "b599c6ded512ac11e797", "term": "Dustin Hoffman", "description": "American actor and director", "question": "Will Dustin Hoffman likely vote for Trump in 2020?", "answer": false, "facts": ["Dustin Hoffman is a liberal and has long supported the Democratic Party and Ralph Nader.", "Donald Trump belongs to the Republican Party."], "decomposition": ["What is Dustin Hoffman's political party affiliation?", "What party is Donald Trump affiliated with?", "Is someone from #1 likely to vote for a candidate from #2?"], "evidence": [[[["Dustin Hoffman-81"]], [["Donald Trump-3"]], [["Political parties in the United States-4"], "no_evidence"]], [[["Dustin Hoffman-81"]], [["Donald Trump-3"]], [["Dustin Hoffman-81", "Two-party system-4"]]], [[["Dustin Hoffman-81"]], [["Donald Trump-3"]], ["operation"]]], "golden_sentence": [["A liberal, Hoffman has long supported the Democratic Party and Ralph Nader."], ["Trump entered the 2016 presidential race as a Republican and defeated 16 other candidates in the primaries."], [""]]}, {"qid": "41f383fb82719fe84c20", "term": "Mood disorder", "description": "(psychology) Any of various disorders characterised by disturbance in an individual's mood", "question": "Do people with mood disorders need permanent institutionalization?", "answer": false, "facts": ["Most mood disorders can be treated in the outpatient setting.", "Many people with mood disorders do not get diagnosed at all."], "decomposition": ["Do most mood disorders need the patient to leave their homes to get treatment?"], "evidence": [[[["Mood disorder-21"], "operation"]], [[["Mood disorder-21"], "no_evidence"]], [[["Bipolar disorder-44", "Mood (psychology)-17"], "no_evidence"]]], "golden_sentence": [[""]]}, {"qid": "7f5b7d9363dc28a08b27", "term": "Sahara", "description": "desert in Africa", "question": "Can Poland Spring make money in the Sahara?", "answer": true, "facts": ["The Sahara is the largest hot desert", "Deserts are dry regions that receive little precipitation", "Poland Spring sells bottled water"], "decomposition": ["What does Poland Spring produce and sell?", "What is the weather condition in the Sahara?", "Would #2 make #1 highly desirable?"], "evidence": [[[["Poland Spring-1"]], [["Sahara-14", "Sahara-15"]], [["Thirst-1"]]], [[["Poland Spring-2"]], [["Sahara desert (ecoregion)-5"]], [["Sahara-12"]]], [[["Poland Spring-1"]], [["Sahara-1", "Sahara-20"]], ["operation"]]], "golden_sentence": [["Poland Spring is a brand of bottled water, produced in Poland, Maine, named after the original natural spring in the town of Poland, Maine it was drawn from."], ["Indeed, the extreme aridity of the Sahara is not only explained by the subtropical high pressure: the Atlas Mountains of Algeria, Morocco and Tunisia also help to enhance the aridity of the northern part of the desert.", "The harsh climate of the Sahara is characterized by: extremely low, unreliable, highly erratic rainfall; extremely high sunshine duration values; high temperatures year-round; negligible rates of relative humidity; a significant diurnal temperature variation; and extremely high levels of potential evaporation which are the highest recorded worldwide."], [""]]}, {"qid": "2da09b35036f729969b9", "term": "Ivan the Terrible", "description": "Grand Prince of Moscow and 1st Tsar of Russia", "question": "Was 847 Pope Leo same iteration of his name as Ivan the Terrible?", "answer": true, "facts": ["Pope Leo in 847 AD was the fourth Leo to have that name and was called Leo IV.", "Ivan the Terrible was the 4th Tsar to have the name Ivan and was known as Ivan IV Vasilyevich."], "decomposition": ["Which Pope Leo is associated with the year 847 AD?", "How many similarly named popes were before #1?", "What was Ivan the Terrible's title as a ruler?", "How many similarly named #3 ruled before him?", "Is #2 equal to #4?"], "evidence": [[[["Pope Leo IV-1"]], [["Pope Leo I-1", "Pope Leo II-1", "Pope Leo III-1"]], [["Ivan the Terrible-1"]], [["Ivan I of Moscow-1", "Ivan II of Moscow-1", "Ivan III of Russia-1"]], ["operation"]], [[["Pope Leo IV-1"]], ["operation"], [["Ivan the Terrible-1"]], ["operation"], ["operation"]], [[["Pope Leo IV-1"]], ["operation"], [["Ivan the Terrible-1"]], ["operation"], ["operation"]]], "golden_sentence": [["Pope Leo IV (790 \u2013 17 July 855) was the bishop of Rome and ruler of the Papal States from 10 April 847 to his death."], ["", "", ""], ["18 March]\u00a01584), commonly known as Ivan the Terrible (Russian: \u0418\u0432\u0430\u0301\u043d \u0413\u0440\u043e\u0301\u0437\u043d\u044b\u0439\u200b\u00a0(help\u00b7info) Ivan Grozny; \"Ivan the Formidable\" or \"Ivan the Fearsome\", Latin: Ioannes Severus ), was the Grand Prince of Moscow from 1533 to 1547 and the first Tsar of Russia from 1547 to 1584."], ["", "Until that date, he had ruled the towns of Ruza and Zvenigorod.", ""]]}, {"qid": "7b25f66f3226d4ad1585", "term": "The Powerpuff Girls", "description": "American animated television series", "question": "Could the Powepuff Girls make the background to the Azerbaijani flag?", "answer": true, "facts": ["The national flag of the Republic of Azerbaijan is a horizontal tricolour featuring three equally sized fesses of blue, red, and green", "Each of the Powerpuff Girls creates a trail of a different color when she flies: Bubbles makes blue, Blossom makes red, and Buttercup makes green."], "decomposition": ["What colors are present on the Azerbaijani flag?", "What colors are the Powerpuff Girls?", "Is #1 the same as #2?"], "evidence": [[[["Flag of Azerbaijan-1"]], [["The Powerpuff Girls-5"]], ["operation"]], [[["Flag of Azerbaijan-1"]], [["The Powerpuff Girls-9"]], ["operation"]], [[["Flag of Azerbaijan-1"]], [["The Powerpuff Girls-5"]], ["operation"]]], "golden_sentence": [["The national flag of the Republic of Azerbaijan (Azerbaijani: Az\u0259rbaycan bayra\u011f\u0131) is a horizontal tricolour featuring three equally sized fesses of blue, red, and green, with a white crescent and an eight-pointed star in the center."], ["The show revolves around the adventures of three kindergarten aged girls with an array of various superpowers: Blossom (pink), Bubbles (blue), and Buttercup (green)."]]}, {"qid": "44f2d4eca4708b948dec", "term": "Florence", "description": "Capital and most populous city of the Italian region of Tuscany", "question": "Is there a Harry Potter character named after Florence?", "answer": true, "facts": ["Firenze is the native Italian form of the name Florence.", "There is a centaur who appars as a minor character in the Harry Potter series named Firenze.", "Firenze appears in three of the Harry Potter books but only one movie."], "decomposition": ["What is the native Italian form for the name Florence?", "What is the name of the centaur who appears  in the Harry Potter series?", "Is #1 the same as #2?"], "evidence": [[[["Florence (given name)-5"]], [["Magical creatures in Harry Potter-65"]], [["Florence (given name)-5", "Magical creatures in Harry Potter-65"], "operation"]], [[["Florence-1"]], [["Magical creatures in Harry Potter-65"]], ["operation"]], [[["Florence-1"]], [["Magical creatures in Harry Potter-65"]], ["operation"]]], "golden_sentence": [["Alternate forms include: Florance (English) Florentia (German) Fiorentina, Fiorenza (Italian) Florencia, Florencita, Floriana, Florinia (Spanish) English nicknames for Florence include:"], ["Firenze is a centaur and, after Order of the Phoenix, a Divination teacher at Hogwarts."], ["", ""]]}, {"qid": "a63d87687a21c65b1ef3", "term": "Snakebite", "description": "Injury caused by a bite from a snake", "question": "Would a snakebite hypothetically be a threat to T-1000?", "answer": false, "facts": ["Snakebites are dangerous because they inject venom into blood streams.", "The T-1000 is an android from the movie series Terminator.", "Androids are machines made of wires and computer parts."], "decomposition": ["Where does the injurous action of a snakebite happen?", "What kind of entity is a T-1000?", "Does a #2 have a #1?"], "evidence": [[[["Snakebite-32"], "no_evidence"], [["T-1000-2"]], [["T-1000-7"], "no_evidence", "operation"]], [[["Skin-1"]], [["T-1000-10"]], ["operation"]], [[["Venomous snake-1"]], [["T-1000-2"]], ["operation"]]], "golden_sentence": [[""], ["A shapeshifting android assassin, the T-1000 is the main antagonist of Terminator 2: Judgment Day, as well as a minor antagonist in Terminator Genisys and the theme park attraction T2 3-D: Battle Across Time."], [""]]}, {"qid": "0a1d558e57eb06d15c21", "term": "Ham", "description": "Pork from a leg cut that has been preserved by wet or dry curing, with or without smoking", "question": "Did Malcolm X avoid eating ham?", "answer": true, "facts": ["Malcolm X was a practicing Muslim", "Muslims are prohibited from eating foods derived from pigs"], "decomposition": ["What religion did Malcolm X practice?", "Does #1 forbid its believers eating pig products?"], "evidence": [[[["Malcolm X-1"]], ["operation"]], [[["Malcolm X-1"]], [["Islamic culture-45"]]], [[["Malcolm X-50"]], [["Islamic culture-45"]]]], "golden_sentence": [["El-Hajj Malik El-Shabazz (Arabic: \u0671\u0644\u0652\u062d\u064e\u0627\u062c\u0651 \u0645\u064e\u0627\u0644\u0650\u0643 \u0671\u0644\u0634\u064e\u0651\u0628\u064e\u0627\u0632\u0651\u200e, romanized:\u00a0al-\u1e24\u0101jj M\u0101lik ash-Shab\u0101zz; born Malcolm Little; May 19, 1925 \u2013 February 21, 1965), better known as Malcolm X, was an American Muslim minister and human rights activist who was a popular figure during the civil rights movement."]]}, {"qid": "8fdbf64daea4734f18ca", "term": "New Year's Eve", "description": "holiday celebrated on 31 December", "question": "Would New Year's Eve hypothetically be Bacchus's favorite holiday?", "answer": true, "facts": ["Bacchus was the Roman god of wine and revelry.", "One of the main New Year's Eve traditions is drinking a toast to the new year.", "New Year\u2019s Eve is the biggest day of the year for liquor stores in terms of sales."], "decomposition": ["What was Bacchus the Roman god of?", "Do people tend to celebrate with #1 on New Year's Eve?"], "evidence": [[[["Dionysus-1", "Dionysus-2"]], [["New Year's Eve-15"], "operation"]], [[["Dionysus-1", "Dionysus-2"]], [["New Year's Eve-136", "New Year's Eve-97"]]], [[["Dionysus-1"]], [["New Year's Eve-15"], "operation"]]], "golden_sentence": [["", "He is also known as Bacchus (/\u02c8b\u00e6k\u0259s/ or /\u02c8b\u0251\u02d0k\u0259s/; Greek: \u0392\u03ac\u03ba\u03c7\u03bf\u03c2, B\u00e1kkhos), the name adopted by the Romans; the frenzy he induces is bakkheia."], ["People wish each other a happy New Year, and sometimes share a toast with neighbours."]]}, {"qid": "a2613af1019057ed2885", "term": "Zebra", "description": "Black and white striped animals in the horse family", "question": "Are black and white prison uniforms made to resemble a zebra?", "answer": false, "facts": ["Prison stripes are made of parallel lines.", "Zebra stripes are jagged in appearance. "], "decomposition": ["What is the design on a prison uniform?", "What is the pattern on a zebra?", "Is #1 the same as #2?"], "evidence": [[[["Prison uniform-28"], "no_evidence"], [["Plains zebra-13"]], ["operation"]], [[["Prison uniform-2"], "no_evidence"], [["Zebra-2"]], ["no_evidence", "operation"]], [[["Prison uniform-24", "Prison uniform-26"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Prison uniforms are red and grey."], ["The purpose of the bold black-and-white striping of zebras has been a subject of debate among biologists for over a century."]]}, {"qid": "e1573bcba91b7d2c680e", "term": "Jack Dempsey", "description": "American boxer", "question": "Did Jack Dempsey ever witness Conor McGregor's fights?", "answer": false, "facts": ["Jack Dempsey died in 1983.", "Conor McGregor's first MMA fight was in 2008."], "decomposition": ["In what year did Jack Dempsey die?", "In what year was Conor McGregor's first fight?", "Is #2 before #1?"], "evidence": [[[["Jack Dempsey-1"]], [["Conor McGregor-7"]], ["operation"]], [[["Jack Dempsey-1"]], [["Conor McGregor-2"]], ["operation"]], [[["Jack Dempsey-1"]], [["Conor McGregor-7"]], ["operation"]]], "golden_sentence": [["William Harrison \"Jack\" Dempsey (June 24, 1895 \u2013 May 31, 1983), nicknamed Kid Blackie, and The Manassa Mauler, was an American professional boxer who competed from 1914 to 1927, and reigned as the world heavyweight champion from 1919 to 1926."], ["On 9 March 2008, McGregor had his first professional MMA bout, as a lightweight, defeating Gary Morris with a second-round TKO."]]}, {"qid": "665a56c656c5bf6a035c", "term": "Immersion (virtual reality)", "description": "perception of being physically present in a non-physical world", "question": "Is immersion in virtual reality testable on cnidarians before humans?", "answer": false, "facts": ["Perceptions are interpreted in the brain via the nervous system", "Hypothetical ways of interfacing with the central nervous system are being explored for immersive virtual reality", "Cnidarians have a nervous system, but not a central nervous system"], "decomposition": ["Which system of the human body will immersive virtual reality interact with?", "Do cnidarians have #1?"], "evidence": [[[["Motion sickness-8", "Virtual reality-58"], "no_evidence"], [["Cnidaria-25"], "operation"]], [[["Immersion (virtual reality)-9"]], [["Ctenophora-5"]]], [[["Immersion (virtual reality)-16"]], [["Cnidaria-3"], "no_evidence", "operation"]]], "golden_sentence": [["Motion sickness is caused by a conflict between signals arriving in the brain from the inner ear, which forms the base of the vestibular system, the sensory apparatus that deals with movement and balance, and which detects motion mechanically.", "When the vestibular system, the body's internal balancing system, does not experience the motion that it expects from visual input through the eyes, the user may experience VR sickness."], ["Cnidarians are generally thought to have no brains or even central nervous systems."]]}, {"qid": "4049be89602f10859810", "term": "Holy Land", "description": "Term used by Jews, Christians, and Muslims to describe the Land of Israel and Palestine", "question": "Is the Holy Land important to Eastern religions?", "answer": false, "facts": ["Eastern religions include Hinduism, Buddhism, and Shintoism.", "Hinduism recognizes seven Holy Cities which are Ayodhya, Mathura, Haridwar, Varanasi, Kanchipuram, Dvaraka and Ujjain.", "Bodh Gaya: (in the current Mahabodhi Temple, Bihar, India), is the most important religious site and place of pilgrimage for Buddhists.", "The most sacred Shinto shrine is located in the city of Ise, within the Shima Peninsula of Japan."], "decomposition": ["What are some typical Eastern religions?", "Which place is referred to as the Holy Land?", "Which places do some of #1 consider sacred or holy?", "Is #2 included in #3?"], "evidence": [[[["Eastern religions-1"]], [["Holy Land-1"]], ["no_evidence"], ["no_evidence"]], [[["Eastern religions-1"]], [["Holy Land-1"]], ["no_evidence"], ["no_evidence"]], [[["Eastern religions-1"]], [["Holy Land-1", "Holy Land-4"]], [["Ganga in Hinduism-1"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["This includes the East Asian religions (Shintoism, Sindoism, Taoism and Confucianism), Indian religions (Hinduism, Buddhism, Sikhism and Jainism) as well as animistic indigenous religions."], ["The Holy Land (Hebrew: \u05d0\u05b6\u05e8\u05b6\u05e5 \u05d4\u05b7\u05e7\u05bc\u05d5\u05b9\u05d3\u05b6\u05e9\u05c1 Eretz HaKodesh, Latin: Terra Sancta; Arabic: \u0627\u0644\u0623\u0631\u0636 \u0627\u0644\u0645\u0642\u062f\u0633\u0629 Al-Ar\u1e0d Al-Muqaddasah or \u0627\u0644\u062f\u064a\u0627\u0631 \u0627\u0644\u0645\u0642\u062f\u0633\u0629 Ad-Diyar Al-Muqaddasah) is an area roughly located between the Jordan River and the Mediterranean Sea that also includes the Eastern Bank of the Jordan River."]]}, {"qid": "7eab1c4ae348696ccbac", "term": "Crucifixion", "description": "Method of capital punishment in which the victim is tied or nailed to a large wooden beam and left to hang until eventual death", "question": "Is Home Depot a one stop shop for crucifixion supplies?", "answer": true, "facts": ["A one stop shop is a store where multiple items are supplied.", "Crucifixion is a form of punishment in which a person is nailed to a wooden cross.", "Home Depot sells numerous supplies including: hammers, nails, and wood."], "decomposition": ["What is the definition of a one stop shop?", "What tools are necessary for Crucifixion?", "Is Home Depot a #1 for all of #2?"], "evidence": [[[["One stop shop-1"]], [["Crucifixion-1"]], ["operation"]], [[["One stop shop-1"]], [["Descriptions in antiquity of the execution cross-6"], "no_evidence"], [["The Home Depot-1"], "operation"]], [[["One stop shop-1"]], [["Crucifixion-1"]], [["The Home Depot-1"], "operation"]]], "golden_sentence": [["A one-stop shop, one-stop store or one-stop source is a business or office where multiple services are offered; i.e., customers can get all they need in just \"one stop\"."], ["Crucifixion is a method of punishment or capital punishment in which the victim is tied or nailed to a large wooden beam and left to hang, perhaps for several days, until eventual death from exhaustion and asphyxiation.It has been used as a punishment in parts of the world as recently as the twentieth century."]]}, {"qid": "2b27dab8365bd5394da5", "term": "Chrysler", "description": "Automotive brand manufacturing subsidiary of Fiat Chrysler Automobiles", "question": "Can you carry a Chrysler in a laptop bag?", "answer": false, "facts": ["Chrysler manufactures automobiles, which weigh several thousand pounds", "Laptop bags are designed to hold laptop computers, which typically weigh under ten pounds"], "decomposition": ["What kind of object is \"Chrysler\" referring to?", "How much do #1's typically weigh?", "What object is a laptop bag designed to carry?", "How much does #3 typically weigh?", "Is #4 greater than or equal to #2?"], "evidence": [[[["Chrysler-1"]], [["Car-40"], "no_evidence"], [["Laptop-43"]], [["Laptop-13"], "no_evidence"], ["operation"]], [[["Chrysler-1"]], ["no_evidence"], [["Laptop-48"], "no_evidence"], [["Laptop-13"], "no_evidence"], ["operation"]], [[["Chrysler-1"]], [["Car-40"]], [["Backpack-23"]], [["Laptop-7"]], ["operation"]]], "golden_sentence": [[""], ["During the late 20th and early 21st century cars increased in weight due to batteries, modern steel safety cages, anti-lock brakes, airbags, and \"more-powerful\u2014if more-efficient\u2014engines\" and, as of 2019[update], typically weigh between 1 and 3 tonnes."], [""], [""]]}, {"qid": "ac29ee14b3ca9e41e604", "term": "Bodybuilding", "description": "use of progressive resistance exercise to control and develop musculature", "question": "Would a bodybuilder enjoy wearing a cast for several weeks?", "answer": false, "facts": ["Casts encase a limb and prevent it from moving.", "Movement of limbs under resistance promote muscle growth.", "An absence of limb movement will result in decreased muscle size.", "The goal of bodybuilding is to increase the size of your muscles.", "Individuals are not happy when they are prevented from pursuing their goals."], "decomposition": ["What does a bodybuilder need to do on a daily basis?", "What does a cast limit freedom of?", "Does the limit on #2 make #1 possible?"], "evidence": [[[["Bodybuilding-1"]], [["Orthopedic cast-1"]], [["Muscle atrophy-1", "Muscle atrophy-7"]]], [[["Bodybuilding-1"]], [["Orthopedic cast-1"]], ["no_evidence"]], [[["Bodybuilding-37"], "no_evidence"], [["Orthopedic cast-1"]], ["operation"]]], "golden_sentence": [[""], [""], ["", ""]]}, {"qid": "757d37aaa83e71f884f1", "term": "Charlemagne", "description": "King of the Franks, King of Italy, and Holy Roman Emperor", "question": "Would Temujin hypothetically be jealous of Charlemagne's conquests?", "answer": false, "facts": ["Temujin was the birth name of Genghis Khan.", "Genghis Khan founded the Mongol Empire which was the largest land empire in world history.", "Charlemagne, King of the Franks, conquered most of Western Europe.", "At its peak, the Mongol Empire had 110 million people.", "Charlemagne's empire had around 20 million people at its height."], "decomposition": ["Temujin was the name of which leader?", "How many people did #1's empire have at its peak?", "How many people did Charlemagne's empire have at its peak?", "Is #3 greater than #2?"], "evidence": [[[["Genghis Khan-1"]], ["no_evidence"], [["Carolingian Empire-1", "Carolingian Empire-3"]], ["no_evidence", "operation"]], [[["Genghis Khan-1"]], [["Mongol Empire-1"], "no_evidence"], [["Carolingian Empire-3", "Charlemagne-1"]], ["no_evidence", "operation"]], [[["Genghis Khan-1"]], [["Mongol Empire-109"], "no_evidence"], [["Carolingian Empire-1", "Carolingian Empire-3"]], ["operation"]]], "golden_sentence": [["Due to his exceptional military successes, Genghis Khan is often considered to be the greatest conqueror of all time."], ["", "The size of the empire at its inception was around 1,112,000 square kilometres (429,000\u00a0sq\u00a0mi), with a population of between 10 and 20\u00a0million people."]]}, {"qid": "6b8e649fed3203e612d4", "term": "Brooklyn", "description": "Borough in New York City and county in New York state, United States", "question": "Can DRL Racer X drone get across Brooklyn Bridge in 18 seconds?", "answer": false, "facts": ["The Brooklyn Bridge is 1.1 miles long.", "The DRL Racer X drone can fly at a top speed of 179.6 MPH.", "The DRL Racer X drone can cover around 3 miles a minute."], "decomposition": ["What is the top speed of the  DRL Racer X drone?", "How long is the Brooklyn Bridge?", "What is #2 multiplied by 60 and then divided by #1?", "is #3 less than or equal to 18?"], "evidence": [[[["Drone Racing League-22"]], [["Brooklyn Bridge-1"]], ["operation"], ["operation"]], [[["Drone Racing League-22"]], [["Brooklyn Bridge-5", "Mile-1"]], ["operation"], ["operation"]], [[["Drone Racing League-22"]], [["Brooklyn Bridge-1"]], ["no_evidence", "operation"], ["operation"]]], "golden_sentence": [["Weighing less than two pounds, the RacerX hit a top speed of 179.6 miles per hour, the official record-breaking speed was recorded as 163.5 miles per hour."], ["It was also the longest suspension bridge in the world at the time, with a main span of 1,595.5 feet (486.3\u00a0m) and a deck height of 127\u00a0ft (38.7\u00a0m) above mean high water."]]}, {"qid": "1ef9bfdef465ee8ebcf1", "term": "Hyena", "description": "family of mammal", "question": "Would a human following a hyena diet be unwelcome at a vegan festival?", "answer": true, "facts": ["A hyena is a carnivorous mammal that feeds on the flesh of other animals.", "Vegans are people that stick to a strict diet that does not include animals or animal products."], "decomposition": ["What does the hyena diet consist mainly of?", "What do people on a vegan diet eat?", "Is there an overlap between #1 and #2?"], "evidence": [[[["Hyena-1", "Hyena-3"]], [["Veganism-1"]], ["operation"]], [[["Hyena-21"]], [["Veganism-1"]], ["operation"]], [[["Striped hyena-15"]], [["Veganism-31"]], ["operation"]]], "golden_sentence": [["", ""], ["Dietary vegans (also known as \"strict vegetarians\") refrain from consuming meat, eggs, dairy products, and any other animal-derived substances."]]}, {"qid": "da9177a4ca2dcedddffc", "term": "Red panda", "description": "Mammal of the family Ailuridae", "question": "Is it normal to see a red panda in Shanghai outside of a zoo?", "answer": false, "facts": ["The red panda is endemic to the temperate forests of the Himalayas, and ranges from the foothills of western Nepal to China in the east.", "The red panda lives between 2,200 and 4,800 m (7,200 and 15,700 ft) altitude, inhabiting areas of moderate temperature between 10 and 25 \u00b0C (50 and 77 \u00b0F) with little annual change.", "Shanghai is located on an alluvial plain, as such the vast majority of its land area is flat, with an average elevation of 4 m (13 ft)."], "decomposition": ["At what elevations are red pandas found?", "What is the elevation of Shanghai?", "Is there an overlap between #1 and #2?"], "evidence": [[[["Red panda-7"]], [["Shanghai-27"]], ["operation"]], [[["Red panda-8"], "no_evidence"], [["Shanghai-27"], "no_evidence"], ["no_evidence", "operation"]], [[["Red panda-7"]], [["Shanghai-27"]], ["operation"]]], "golden_sentence": [["The red panda lives between 2,200 and 4,800\u00a0m (7,200 and 15,700\u00a0ft) altitude, inhabiting areas of moderate temperature between 10 and 25\u00a0\u00b0C (50 and 77\u00a0\u00b0F) with little annual change."], ["As such, the vast majority of its 6,340.5\u00a0km2 (2,448.1\u00a0sq\u00a0mi) land area is flat, with an average elevation of 4\u00a0m (13\u00a0ft)."]]}, {"qid": "31f815c0944d6b334d02", "term": "Mayor", "description": "head of municipal government such as a town or city", "question": "Are Mayors safe from harm from the federal government?", "answer": false, "facts": ["The Mayor of Portland is Ted Wheeler.", "Ted Wheeler was tear-gassed by federal troops sent to his state."], "decomposition": ["Who is the mayor of Portland?", "Has #1 been able to avoid harm when federal troops were sent to his state"], "evidence": [[[["Ted Wheeler-1"]], ["no_evidence", "operation"]], [[["Government of Portland, Oregon-3"], "no_evidence"], ["no_evidence"]], [[["Ted Wheeler-1"]], ["no_evidence"]]], "golden_sentence": [["Edward Tevis Wheeler (born August 31, 1962) is an American politician who has served as the mayor of Portland, Oregon, since 2017."]]}, {"qid": "c5945cab1b4feaa04243", "term": "Snickers", "description": "brand name chocolate bar made by Mars, Incorporated", "question": "Would 2019 Natalie Portman avoid a Snickers bar due to her diet?", "answer": true, "facts": ["Actress Natalie Portman resumed her vegan diet in 2018 after giving birth, and has been vegan ever since.", "Vegans do not eat animal products.", "Snickers contains egg whites as an ingredient."], "decomposition": ["What foods has Natalie Portman avoided since 2018?", "What are the ingredients in a Snickers bar?", "Is anything from #2 also in #1?"], "evidence": [[[["Natalie Portman-30", "Veganism-1"]], [["Snickers-1"]], [["Nougat-1", "Types of chocolate-6"], "operation"]], [[["Natalie Portman-27", "Veganism-1"]], [["Snickers-1"]], ["operation"]], [[["Natalie Portman-27"], "no_evidence"], [["Snickers-7"]], ["operation"]]], "golden_sentence": [["", ""], ["Snickers is a brand name chocolate bar made by the American company Mars, Incorporated, consisting of nougat topped with caramel and peanuts that has been enrobed in milk chocolate."], ["", "It is pale ivory colour, and lacks many of the compounds found in milk and dark chocolates."]]}, {"qid": "776ad2811920dcc26e93", "term": "Year", "description": "Orbital period of the Earth around the Sun", "question": "Can you listen to the entire Itunes song catalog in one year?", "answer": false, "facts": ["Itunes has around 43 million songs as of 2017.", "The average length of a song is 3 minutes.", "There are 525,600 minutes in a year."], "decomposition": ["How many songs are on iTunes?", "What is the average song length?", "What is #1 multiplies by #2?", "How many minutes are in a year?", "Is #4 greater than #3?"], "evidence": [[[["ITunes Store-2"]], [["Popular music-19"]], ["operation"], [["Year-57"]], ["operation"]], [[["ITunes-20"]], ["no_evidence"], ["no_evidence", "operation"], [["Seasons of Love-1"]], ["no_evidence", "operation"]], [[["ITunes Store-2"]], [["Justin Bieber-29"], "no_evidence"], ["operation"], [["Year-19"], "no_evidence"], ["operation"]]], "golden_sentence": [["As of January 2017, iTunes offered over 35\u201340 million songs, 2.2 million apps, 25,000 TV shows, and 65,000 films."], ["The average song length in 2018 was 3 minutes and 30 seconds, 20 seconds shorter than the average in 2014."], ["An average Gregorian year is 365.2425 days (52.1775 weeks, 8765.82 hours, 525949.2 minutes or 31556952 seconds)."]]}, {"qid": "112a793c2ef8cd268ba3", "term": "Sesame", "description": "species of plant", "question": "Could white rice go rancid before sesame seeds?", "answer": false, "facts": ["Sesame seeds should last 6-12 months unopened.", "White rice can last 4-5 years in a pantry."], "decomposition": ["What is the shelf life of sesame seeds?", "What is the shelf life of white rice?", "Is #2 shorter than #1?"], "evidence": [[["no_evidence"], [["White rice-1"], "no_evidence"], ["operation"]], [[["Sesame-17"], "no_evidence"], [["Rice-84"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], ["no_evidence"], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "a68c7239b41bc99a27e5", "term": "Taco Bell", "description": "American fast-food chain", "question": "Does the Taco Bell kitchen contain cinnamon?", "answer": true, "facts": ["Taco Bell serves churros.", "Cinnamon is an ingredient in churros."], "decomposition": ["What dough pastry based snack does Taco Bell serve?", "Does #1 contain Cinnamon?"], "evidence": [[[["Taco Bell-1"]], ["no_evidence", "operation"]], [[["Taco Bell-21"]], [["Cinnabon-3"]]], [[["Taco Bell-21"]], [["Cinnabon-3"], "no_evidence"]]], "golden_sentence": [[""]]}, {"qid": "2f9d28d38381bbe4f5ae", "term": "Santa Claus", "description": "Folkloric figure, said to deliver gifts to children on Christmas Eve", "question": "Are most mall Santa Claus actors white?", "answer": true, "facts": ["In 2016, a black man playing Santa Claus at the Mall of America made national headlines.", "There are map websites dedicated to locating black Santa Claus mall actors."], "decomposition": ["What is the ethnicity of the man who made headlines for playing Santa Claus at the Mall of America in 2016?", "Does #1 imply that black Santas are a rare occurrence?"], "evidence": [[["no_evidence"], ["no_evidence"]], [[["Santa Claus-2"], "no_evidence"], ["operation"]], [["no_evidence"], [["Santa Claus-50"], "no_evidence", "operation"]]], "golden_sentence": []}, {"qid": "f90f095bb094fc6caff3", "term": "Lip", "description": "Visible body part at the mouth", "question": "Does having lip piercings lead to more expensive dental bills?", "answer": true, "facts": ["Lip piercings can rub the enamel on your teeth and can cause tissue damage to the gums.", "Tooth enamel protects the teeth from decay."], "decomposition": ["What is the function of Tooth Enamel?", "Can Lip piercing cause damage to #1", "Will #2 cost you more expensive dental bills?"], "evidence": [[[["Tooth enamel-1"]], [["Tooth enamel-26"], "no_evidence"], ["no_evidence"]], [[["Tooth enamel-1"]], [["Lip piercing-5"], "no_evidence"], ["no_evidence", "operation"]], [[["Tooth enamel-21"]], [["Body piercing-42", "Lip piercing-4"]], [["Tooth enamel-39"], "no_evidence"]]], "golden_sentence": [[""], ["The truly destructive forces are the parafunctional movements, as found in bruxism, which can cause irreversible damage to the enamel."]]}, {"qid": "1cc011f3fa56a2f590b5", "term": "Bipolar disorder", "description": "mental disorder that causes periods of depression and abnormally elevated mood", "question": "Are you more likely to find bipolar disorder in a crowd than diabetes?", "answer": false, "facts": ["Bipolar disorder is a condition that effects around 1% of the population.", "It is estimated that around 10% of the population suffers from diabetes."], "decomposition": ["What percent of the population has bipolar disorder?", "What percent of the population has diabetes?", "Is #1 greater than #2?"], "evidence": [[[["Bipolar disorder-4"]], [["Diabetes-4"]], [["Diabetes-4"]]], [[["Bipolar disorder-4"]], [["Diabetes-4"]], ["operation"]], [[["Bipolar disorder-4"]], [["Diabetes-4"]], ["operation"]]], "golden_sentence": [["Bipolar disorder occurs in approximately 1% of the global population."], ["As of 2017[update], an estimated 425 million people had diabetes worldwide, with type 2 diabetes making up about 90% of the cases."], [""]]}, {"qid": "16b201c5a48615f7aeff", "term": "Heart", "description": "organ for the circulation of blood in animal circulatory systems", "question": "Do anatomical and symbolic hearts look remarkably different?", "answer": true, "facts": ["Symbolic hearts are sharply pointed at the bottom and feature a sharp valley between the bumps at the top.", "Anatomical hearts are rounded, have numerous vascular tubes entering and exiting them, and do not feature sharp angles."], "decomposition": ["What are the dimensions of the heart symbol?", "Do anatomical hearts lack the dimensions of #1?"], "evidence": [[[["Heart symbol-1"]], [["Heart-6", "Heart-7"]]], [[["Heart symbol-1"], "no_evidence"], [["Heart-2"], "no_evidence", "operation"]], [[["Heart symbol-3"]], [["Heart-8"]]]], "golden_sentence": [["The \"wounded heart\" indicating lovesickness came to be depicted as a heart symbol pierced with an arrow (Cupid's), or heart symbol \"broken\" in two or more pieces."], ["", ""]]}, {"qid": "dbd45bf31dd7b3e59322", "term": "Gulf of Mexico", "description": "An Atlantic Ocean basin extending into southern North America", "question": "Are fossil fuels reducing jobs in the Gulf of Mexico?", "answer": true, "facts": ["An oil spill is still polluting the Gulf of Mexico", "Workers such as fishermen are out of work due to pollution"], "decomposition": ["What are the consequences of fossil fuel presence in the Gulf of Mexico?", "Is #1 putting some people out of job?"], "evidence": [[[["Deepwater Horizon oil spill-2", "Taylor oil spill-2"]], [["Deepwater Horizon oil spill-71", "Deepwater Horizon oil spill-72"], "operation"]], [[["Gulf of Mexico-38"]], [["Gulf of Mexico-38"], "no_evidence"]], [[["Gulf of Mexico-42"]], ["no_evidence"]], [[["Gulf of Mexico-42"]], [["Gulf of Mexico-36"], "no_evidence", "operation"]]], "golden_sentence": [["", "The reserves are likely sufficient for the spill to continue for up to 100 years if it is not contained."], ["", "Local officials in Louisiana expressed concern that the offshore drilling moratorium imposed in response to the spill would further harm the economies of coastal communities as the oil industry directly or indirectly employs about 318,000 Louisiana residents (17% of all jobs in the state)."]]}, {"qid": "17e4a35e275106b5a871", "term": "Fair trade", "description": "form of trade", "question": "Did Medieval English lords engage in fair trade with peasants?", "answer": false, "facts": ["Fair trade is a system in which fair prices are paid to the producers of a product.", "English lords had peasants working on their manors and the peasants were indentured servants.", "The peasants had few rights, were unpaid, and had to even ask their lord for permission to marry."], "decomposition": ["What is fair trade?", "Are peasants able to participate in #1 with Lords?"], "evidence": [[[["Fair trade-1"]], [["Peasant-1"], "no_evidence"]], [[["Fair trade-1"], "no_evidence"], [["Peasant-8"], "no_evidence", "operation"]], [[["Fair trade-1"]], ["no_evidence", "operation"]]], "golden_sentence": [["Fair trade is an arrangement designed to help producers in developing countries achieve good trading."], [""]]}, {"qid": "fd36aed8b22a74b7f546", "term": "Small Solar System body", "description": "object in the Solar System that is neither a planet, nor a dwarf planet, nor a satellite", "question": "Is the name of a mythical creature also the name of a Small Solar System body?", "answer": true, "facts": ["A centaur is a kind of Small Solar System body with characteristics of both asteroids and comets.", "A centaur is also a mythical creature that has the body of a horse and the head of a man."], "decomposition": ["What is a mythical creature with the body of a horse and the head of a man called?", "Is any Small Solar System named after #1?"], "evidence": [[[["Centaur-1"]], [["Centaur (small Solar System body)-1"], "operation"]], [[["Centaur-5"]], [["Centaurus-18"]]], [[["Centaur-1"]], [["Centaur (small Solar System body)-1"]]]], "golden_sentence": [["A centaur (/\u02c8s\u025bnt\u0254\u02d0r/; Greek: \u03ba\u03ad\u03bd\u03c4\u03b1\u03c5\u03c1\u03bf\u03c2, k\u00e9ntauros, Latin: centaurus), or occasionally hippocentaur, is a creature from Greek mythology with the upper body of a human and the lower body and legs of a horse."], ["Centaurs typically behave with characteristics of both asteroids and comets."]]}, {"qid": "5a78b1602250e5b0dd2e", "term": "Astronomer", "description": "Scientist who studies celestial bodies", "question": "Do astronomers write horoscopes?", "answer": false, "facts": ["Astronomer study the actual science of the stars.", "Horoscopes are written by astrologers, not astronomers."], "decomposition": ["Which field of science do horoscopes fall under?", "Which science field do astronomers study?", "Is #1 the same as #2?"], "evidence": [[[["Astrology-1"]], [["Astronomy-1"]], ["operation"]], [[["Astrology-1"]], [["Astronomer-1"]], ["operation"]], [[["Horoscope-1"]], [["Astronomer-1"]], ["operation"]]], "golden_sentence": [["Astrology has been dated to at least the 2nd millennium BCE, and has its roots in calendrical systems used to predict seasonal shifts and to interpret celestial cycles as signs of divine communications."], ["Astronomy (from Greek: \u1f00\u03c3\u03c4\u03c1\u03bf\u03bd\u03bf\u03bc\u03af\u03b1) is a natural science that studies celestial objects and phenomena."]]}, {"qid": "b49c5e4392e168f05851", "term": "Crustacean", "description": "subphylum of arthropods", "question": "Could a Diwali celebration feature a crustacean?", "answer": true, "facts": ["Diwali is an important Hindu holiday which includes feasting.", "Crustaceans are shelled, mainly aquatic animals that include shrimp, lobster, and crabs.", "Hindus are allowed to eat any food besides beef, since cows are sacred.", "Hindus are allowed to eat "], "decomposition": ["What religion celebrates Diwali with feasting?", "What foods are on #1 s forbidden list?", "Is lobster part of #2?"], "evidence": [[[["Diwali-4"]], [["Buddhist cuisine-9"]], ["operation"]], [[["Diwali-1"]], [["Hinduism-94"]], [["Lobster-2"]]], [[["Diwali-15"]], [["Diet in Hinduism-27"]], ["operation"]]], "golden_sentence": [["The Jains observe their own Diwali which marks the final liberation of Mahavira, the Sikhs celebrate Bandi Chhor Divas to mark the release of Guru Hargobind from a Mughal Empire prison, while Newar Buddhists, unlike other Buddhists, celebrate Diwali by worshipping Lakshmi, while the Bengali Hindus generally celebrate Diwali, by worshipping Goddess Kali."], [""]]}, {"qid": "b92655118371a588ec26", "term": "Chives", "description": "edible species of plant", "question": "Are there any chives hypothetically good for battling vampires?", "answer": true, "facts": ["Vampires in folklore have a weakness to garlic.", "Chives, an edible plant species, come in a number of varieties.", "Garlic chives are a variant of chives first found in China thousands of years ago."], "decomposition": ["What items are used to ward off vampires according to folklore?", "What are the varieties of chives that exist?", "Is any of #1 included in #2?"], "evidence": [[[["Garlic-61"]], [["Garlic-1"]], [["Garlic-1"], "operation"]], [[["Vampire-16"]], [["Chives-1"]], ["operation"]], [[["Garlic-61"]], [["Allium-1"]], ["operation"]]], "golden_sentence": [["To ward off vampires, garlic could be worn, hung in windows, or rubbed on chimneys and keyholes."], ["Its close relatives include the onion, shallot, leek, chive, and Chinese onion."], [""]]}, {"qid": "d387018edb6d1dc2e58f", "term": "Golden Gate Bridge", "description": "suspension bridge on the San Francisco Bay", "question": "Can you make an MP3 from the Golden Gate Bridge?", "answer": true, "facts": ["MP3 is a file compression format for audio recordings", "The Golden Gate Bridge has been reported emitting sounds when the wind passes through its sidewalk railing slats"], "decomposition": ["What is an MP3 a compressed file of?", "Does the Golden Gate Bridge produce #1?"], "evidence": [[[["MP3-1"]], ["no_evidence", "operation"]], [[["MP3-1"]], ["no_evidence", "operation"]], [[["MP3-1"]], [["Golden Gate Bridge-12"]]]], "golden_sentence": [[""]]}, {"qid": "5bf08bbcd69a48445d4e", "term": "Ubuntu", "description": "Linux distribution based on Debian", "question": "If you were at an Apple store, would most of the computers be running Ubuntu?", "answer": false, "facts": ["Apple stores stock only Mac brand computers.", "Mac computers come preinstalled with the latest iOS."], "decomposition": ["Which operating system do Apple computers run on?", "Is #1 the same as Ubuntu?"], "evidence": [[[["Operating system-40"]], [["Ubuntu-1"]]], [[["MacOS-1"]], ["operation"]], [[["MacOS-1"]], ["operation"]]], "golden_sentence": [["Unlike its predecessor, macOS is a UNIX operating system built on technology that had been developed at NeXT through the second half of the 1980s and up until Apple purchased the company in early 1997."], [""]]}, {"qid": "667fcd10c51c874a9472", "term": "Charles Darwin", "description": "\"British naturalist, author of \"\"On the Origin of Species, by Means of Natural Selection\"\"\"", "question": "Did Lamarck and Darwin agree about the origin of species diversity?", "answer": false, "facts": ["Darwin theorized that evolution was driven by the fittest animals surviving and passing their genes on.", "Lamarck theorized that animals' responses to needs in their life would influence the growth of their offspring."], "decomposition": ["What was Darwin's theory about the origins of species diversity?", "What was the theory of Lamarck regarding the origins of different species?", "Are the theories of #1 and #2 the same?"], "evidence": [[[["On the Origin of Species-1"]], [["Lamarckism-1"]], ["operation"]], [[["Charles Darwin-1"]], [["Lamarckism-1"]], ["operation"]], [[["Charles Darwin-2"]], [["Jean-Baptiste Lamarck-1"]], ["operation"]]], "golden_sentence": [["It presented a body of evidence that the diversity of life arose by common descent through a branching pattern of evolution."], ["This paints a false picture of the history of biology, as Lamarck did not originate the idea of soft inheritance, which was known from the classical era onwards, and it was not the primary focus of Lamarck's theory of evolution."]]}, {"qid": "ed5adee30bcf8df0bcd5", "term": "Saint Vincent and the Grenadines", "description": "Country in the Caribbean", "question": "Was Saint Vincent and the Grenadines named by an Italian explorer?", "answer": true, "facts": ["Christopher Columbus, an Italian explorer, was the first European to discover the islands.", "He named them after St. Vincent because he first saw the island on the saint's feast day, and the Spanish city of Granada."], "decomposition": ["Who discovered Saint Vincent and the Grenadines?", "Was #1 from Italy?"], "evidence": [[[["Saint Vincent and the Grenadines-7"]], [["Christopher Columbus-1"], "operation"]], [[["Saint Vincent and the Grenadines-7"]], [["Christopher Columbus-1"]]], [[["Saint Vincent and the Grenadines-7"]], [["Christopher Columbus-1"]]]], "golden_sentence": [["citation needed] It is thought that Christopher Columbus sighted the island in 1498, giving it the name St Vincent."], ["Christopher Columbus (/k\u0259\u02c8l\u028cmb\u0259s/; before 31 October 1451\u00a0\u2013 20 May 1506) was an Italian explorer and colonizer who completed four voyages across the Atlantic Ocean that opened the New World for conquest and permanent European colonization of the Americas."]]}, {"qid": "078f8b6b508dced28cc0", "term": "Alice in Wonderland (1951 film)", "description": "1951 American animated musical fantasy film produced by Walt Disney Productions", "question": "Was milliner in Alice in Wonderland (1951 film) likely in need of succimer?", "answer": true, "facts": ["A milliner is someone who makes hats.", "The character of the Mad Hatter was a milliner in the 1951 Alice in Wonderland film.", "The phrase, Mad as a Hatter, comes from the fact that hat makers used mercury to line their hats and often suffered mercury poisoning.", "Succimer is a chemical that is used to treat lead, mercury, and arsenic poisoning."], "decomposition": ["What does a milliner do?", "Which Alice in Wonderland (1951 film) character did #1?", "Which element did #2 use for work that could be harmful to their mental health?", "Is succimer useful for treatment of the effects of #3?"], "evidence": [[[["Hatmaking-1"]], [["Alice in Wonderland (1951 film)-7"]], [["Erethism-1"]], [["Dimercaptosuccinic acid-1"], "operation"]], [[["Hatmaking-1"]], [["Hatter (Alice's Adventures in Wonderland)-1"]], [["Hatter (Alice's Adventures in Wonderland)-5"]], [["Dimercaptosuccinic acid-1"]]], [[["Hatmaking-1"]], [["Hatter (Alice's Adventures in Wonderland)-1"]], [["Hatter (Alice's Adventures in Wonderland)-5"]], [["Dimercaptosuccinic acid-1"]]]], "golden_sentence": [[""], [""], ["Associated physical problems may include a decrease in physical strength, \"headaches, general pain, and tremors after exposure to metallic mercury\" as well as an irregular heartbeat."], [""]]}, {"qid": "20cebdb76d46d9fe8ff6", "term": "Red hair", "description": "Hair color", "question": "If you have black hair and want red hair, do you need bleach?", "answer": true, "facts": ["You cannot dye hair to be lighter than the starting color.", "To make hair a color lighter than the starting color, you need to bleach the hair."], "decomposition": ["Why would someone need bleach when dying their hair?", "Is red hair #1 than black hair?"], "evidence": [[[["Hair coloring-11"]], [["Hair coloring-11"]]], [[["Bleach-22"], "no_evidence"], [["Red hair-2"], "operation"]], [[["Hair coloring-26"]], ["no_evidence", "operation"]]], "golden_sentence": [["For lightening, the hair sometimes has to be bleached before coloring."], [""]]}, {"qid": "0ddef4e1d78330cd7955", "term": "Reddit", "description": "Online news aggregator", "question": "Can you buy Reddit at Walmart?", "answer": false, "facts": ["Reddit is an online social networking forum and community", "Walmart sells tangible goods and services"], "decomposition": ["What is Reddit?", "Is #1 tangible?", "Does Walmart sell tangible items??", "Are #2 and #3 the same?"], "evidence": [[[["Reddit-1"]], ["operation"], [["Walmart-1"], "no_evidence"], ["operation"]], [[["Reddit-5"]], [["Reddit-5"]], [["Walmart-5"]], [["Walmart-5"]]], [[["Reddit-1"]], ["operation"], [["Walmart-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["Reddit (/\u02c8r\u025bd\u026at/, stylized in its logo as reddit) is an American social news aggregation, web content rating, and discussion website."], [""]]}, {"qid": "35f5639802bf53c4eeb3", "term": "Management", "description": "Coordinating the efforts of people", "question": "In order to work in district management, does one need a car?", "answer": true, "facts": ["District managers are responsible for supervising many stores within an area.", "District managers must travel to the various stores they supervise to ensure peak performance."], "decomposition": ["What is the main responsibility of district managers?", "In order to do #1 efficiently, is a car needed? "], "evidence": [[["no_evidence"], ["no_evidence", "operation"]], [[["Account manager-1"], "no_evidence"], [["Account manager-9"], "operation"]], [[["District Programme Manager-1"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": []}, {"qid": "4a417cac6aae535b9ab6", "term": "Asparagus", "description": "species of plant", "question": "Are slime lilies in a different scientific family than asparagus?", "answer": false, "facts": ["Asparagus is a species of plants of the Asparagaceae family.", "Slime lilies are the common name for the flowering albuca plant.", "The albuca plant belongs to the scientific family of Asparagaceae."], "decomposition": ["Which family does the asparagus belong to?", "Which plants are commonly referred to as slime lilies?", "Which family does #2 belong to?", "Is #1 different from #3?"], "evidence": [[[["Asparagaceae-1"]], [["Albuca-1"]], [["Albuca-1"]], ["operation"]], [[["Asparagus-2"]], [["Albuca-1"]], [["Albuca-1"]], ["operation"]], [[["Asparagaceae-1"]], [["Albuca-1"]], [["Albuca-1"]], ["operation"]]], "golden_sentence": [[""], [""], ["Albuca is a genus of flowering plants in the family Asparagaceae, subfamily Scilloideae."]]}, {"qid": "4bc6ea48a0ec3766e5ad", "term": "Forbidden City", "description": "Art museum, Imperial Palace, Historic site in Beijing, China", "question": "Is the Forbidden City host to a wooden rollercoaster?", "answer": false, "facts": ["Wooden rollercoasters are relatively modern.", "The Forbidden City is an ancient historic site."], "decomposition": ["When were wooden rollercoasters first built?", "What is the Forbidden City?", "When was #2 built?", "Did #3 come before #1?"], "evidence": [[[["Wooden roller coaster-3"]], [["Forbidden City-1"]], [["Forbidden City-2"]], ["operation"]], [[["Wooden roller coaster-3"]], [["Forbidden City-1"]], [["Forbidden City-2"]], ["operation"]], [[["History of the roller coaster-8"]], [["Forbidden City-1"]], [["Forbidden City-2"]], ["operation"]]], "golden_sentence": [[""], ["The Forbidden City (Chinese: \u6545\u5bab; pinyin: G\u00f9g\u014dng) is a palace complex in central Beijing, China."], ["Constructed from 1406 to 1420, the complex consists of 980 buildings and covers 72 hectares (over 180 acres)."]]}, {"qid": "1f21168791621d1aa386", "term": "Celery", "description": "species of plant", "question": "Did any cultures associate celery with death?", "answer": true, "facts": ["Ancient Greeks used garlands of celery leafs to bury their dead.", "Ancient Greece was considered a culture. "], "decomposition": ["What are the various forms of cultural depictions of celery?", "Is any of #1 associated with death?"], "evidence": [[[["Celery-44"]], ["operation"]], [[["Celery-44"]], ["operation"]], [[["Celery-44"]], [["Celery-44"]]]], "golden_sentence": [[""]]}, {"qid": "9a054ef9666e008bca60", "term": "Butter", "description": "dairy product", "question": "Would toast for a vegan have margarine instead of butter?", "answer": true, "facts": ["Margarine is typically made without the use of dairy ingredients.", "Vegans do not eat any animal products, including dairy and eggs."], "decomposition": ["Which products are avoided in vegan diet?", "Is margarine free of #1?"], "evidence": [[[["Veganism-1"]], [["Margarine-36"]]], [[["Veganism-1"]], [["Margarine-2"], "operation"]], [[["Veganism-1"]], [["Margarine-2"]]]], "golden_sentence": [["Veganism is the practice of abstaining from the use of animal products, particularly in diet, and an associated philosophy that rejects the commodity status of animals."], ["However, margarine that strictly does not contain animal products also exists."]]}, {"qid": "3e1b5bcaf9e67663d710", "term": "Michael Crichton", "description": "American author, screenwriter, film director", "question": "Was Michael Crichton ever in danger of flunking out of Harvard as an undergraduate?", "answer": false, "facts": ["Scholastic probation or academic dismissal, sometimes known as flunking out, is the termination of students at a higher educational institution as the result of poor academic achievement.", "Michael Crichton obtained his bachelor's degree in biological anthropology summa cum laude in 1964.", "Summa cum laude is the highest distinction a person can achieve in college for academic success.", "Someone who achieves summa cum laude cannot have even a single semester of poor grades."], "decomposition": ["What grade is considered flunking in US colleges?", "What honors did Michael Crichton graduate with?", "Can someone achieve #2 with grades of #1?"], "evidence": [[[["Academic grading in the United States-18"]], [["Michael Crichton-7"]], [["Academic grading in the United States-18", "Michael Crichton-7"]]], [[["Grading systems by country-216"], "no_evidence"], [["Michael Crichton-7"]], [["Latin honors-5"], "operation"]], [[["Grading systems by country-216"]], [["Michael Crichton-7"]], [["Latin honors-5"], "operation"]]], "golden_sentence": [["In some places, .25 or .3 instead of .33 is added for a + grade and subtracted for a \u2212 grade."], ["He obtained his bachelor's degree in biological anthropology summa cum laude in 1964 and was initiated into the Phi Beta Kappa Society."], ["A = 4 B = 3 C = 2 D = 1 F = 0 This allows grades to be easily averaged.", ""]]}, {"qid": "a52ee4756c5646f76c06", "term": "Pyrenees", "description": "Range of mountains in southwest Europe", "question": "Would Jolly Green Giant's largest monument look impressive next to Pyrenees?", "answer": false, "facts": ["The Jolly Green Giant monument in Blue Earth, Minnesota is 55.5 feet tall.", "The Pyrenees mountains are 11,168 feet high.", "The Pyrenees mountains are 305 miles wide."], "decomposition": ["How tall is the tallest monument to the Jolly Green Giant?", "How high is the tallest of the Pyrenees?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Green Giant-10"]], [["Pyrenees-1"]], ["operation"]], [[["Green Giant-10"]], [["Pyrenees-1"]], ["operation"]], [[["Green Giant-10"]], [["Pyrenees-13"]], ["operation"]]], "golden_sentence": [["Sixty miles (97\u00a0km) further south on US 169, in the city of Blue Earth, Minnesota, stands a 55-foot (17\u00a0m) fiberglass statue of the Jolly Green Giant."], ["Reaching a height of 3,404 metres (11,168\u00a0ft) altitude at the peak of Aneto, it extends for about 491\u00a0km (305\u00a0mi) from its union with Cantabrian Mountains to the Mediterranean Sea (Cap de Creus)."]]}, {"qid": "8bf07721f124b89b96d1", "term": "Lecturer", "description": "tenure-track or tenured position at a university or similar institution", "question": "Would Quiet from Metal Gear be a poor hypothetical choice for lecturer at Haub?", "answer": true, "facts": ["Quiet is an assassin from the Metal Gear video game series that does not speak.", "Haub is a school at Pace University that has annual lectures.", "Haub is a law school that has annual lectures on topics in the law field."], "decomposition": ["Who is Quiet?", "What is #1 unable to do?", "How does one convey information as a lecturer?", "Is #2 the same as #3?"], "evidence": [[[["Quiet (Metal Gear)-1"]], [["Quiet (Metal Gear)-5"]], [["Lecture-1"]], ["operation"]], [[["Quiet (Metal Gear)-1"]], [["Quiet (Metal Gear)-7"]], [["Lecture-1"]], ["operation"]], [[["Quiet (Metal Gear)-1"]], [["Quiet (Metal Gear)-7"]], [["Lecture-1"]], ["operation"]]], "golden_sentence": [["Created by Hideo Kojima, designed by Yoji Shinkawa, and based on and played by Stefanie Joosten, Quiet appears in the 2015 action-adventure stealth game, Metal Gear Solid V: The Phantom Pain as one of the game bosses opposing the protagonist Venom Snake, but can also become an ally who can go on missions with him."], [""], ["Lectures are used to convey critical information, history, background, theories, and equations."]]}, {"qid": "de651aef079ec8d3e543", "term": "Morphine", "description": "Pain medication of the opiate family", "question": "Could morphine cure HIV?", "answer": false, "facts": ["Morphine is an opioid that is used to treat pain.", "HIV is a virus that has no known cure, but can be treated with anti-retroviral drugs."], "decomposition": ["What is morphine used to treat?", "What type of system is affected by contraction of HIV?", "Will treatment of #1 cure HIV-infected #2?"], "evidence": [[[["Morphine-1"]], [["HIV-2"]], [["HIV-49"], "operation"]], [[["Morphine-1"]], [["HIV-1"]], ["operation"]], [[["Morphine-1"]], [["HIV-2"]], [["HIV-49"], "operation"]]], "golden_sentence": [["It is frequently used for pain from myocardial infarction and during labor."], ["HIV infects vital cells in the human immune system, such as helper T cells (specifically CD4+ T cells), macrophages, and dendritic cells."], [""]]}, {"qid": "4b161b1393f76bc7d10e", "term": "1960", "description": "Year", "question": "Could you buy Hershey's Kisses in red foil with farthings after 1960?", "answer": false, "facts": ["The British farthing was made obsolete at the end of 1960", "In 1962, Hershey's Kisses began to be sold in colored wrappers (such as red foil)"], "decomposition": ["When was the British farthing made obsolete?", "When did Hershey's Kisses begin selling candy sold in colored wrappers?", "Is #2 before #1?"], "evidence": [[[["Farthing (British coin)-1"]], [["Hershey's Kisses-10"]], ["operation"]], [[["Farthing (British coin)-1"]], [["Hershey's Kisses-10"]], ["operation"]], [[["Farthing (British coin)-1"]], [["Hershey's Kisses-11"]], ["operation"]]], "golden_sentence": [["It featured two different designs on its reverse during its 100 years in circulation: from 1860 until 1936, the image of Britannia; and from 1937 onwards, the image of a wren."], [""]]}, {"qid": "60c6a1fc466cf31fe759", "term": "Ammonia", "description": "Chemical compound of nitrogen and hydrogen", "question": "Is an ammonia fighting cleaner good for pet owners?", "answer": true, "facts": ["Ammonia is a component in pet urine.", "Ammonia has a very pungent and unpleasant odor."], "decomposition": ["What unsanitary substances contain ammonia?", "Is animal waste included in #1?"], "evidence": [[[["Ammonia-32"]], ["operation"]], [[["Ammonia-32"]], [["Urination-3"]]], [[["Ammonia-32"]], [["Ammonia-32"]]]], "golden_sentence": [[""]]}, {"qid": "bc42bc5e27aa2fbc8c91", "term": "Orange County, California", "description": "County in California, United States", "question": "Did the founders of the biggest city in Orange County, California speak Italian?", "answer": false, "facts": ["Anaheim is the biggest city in Orange County, California", "Anaheim was founded by fifty German families", "People from Germany speak German"], "decomposition": ["What is the biggest city in Orange County, California?", "Who founded #1?", "Did #2's speak Italian? "], "evidence": [[[["Anaheim, California-1"]], [["Anaheim, California-5"]], [["Anaheim, California-5"]]], [[["Anaheim, California-1"], "no_evidence"], [["Anaheim, California-2"]], ["no_evidence", "operation"]], [[["Anaheim, California-1"]], [["Anaheim, California-5"]], [["German language-1", "Italian language-1"], "operation"]]], "golden_sentence": [["Anaheim is the second-largest city in Orange County in terms of land area, and is known for being the home of the Disneyland Resort, the Anaheim Convention Center, and two major sports teams: the Anaheim Ducks ice hockey club and the Los Angeles Angels baseball team."], ["The city of Anaheim was founded in 1857 by 50 German-Americans who were residents of San Francisco and whose families had originated in Rothenburg ob der Tauber, Franconia in Bavaria."], [""]]}, {"qid": "b5838f73d61564fcdd88", "term": "Isaac", "description": "Biblical character", "question": "Did Isaac's father almost commit similar crime as Marvin Gay Sr.?", "answer": true, "facts": ["Filicide is the act of killing a son or a daughter.", "Marvin Gay Sr. committed filicide in 1984 when he shot his son, singer Marvin Gaye.", "Isaac's father Abraham, was commanded by God to sacrifice his son Isaac, but was spared by an angel."], "decomposition": ["What crime did Marvin Gay Sr commit?", "Was the Biblical Abraham going to commit #1?"], "evidence": [[[["Marvin Gaye-36"]], [["Binding of Isaac-3"]]], [[["Marvin Gay Sr.-1"]], [["Binding of Isaac-1"]]], [[["Marvin Gay Sr.-7"], "operation"], ["no_evidence"]]], "golden_sentence": [["Gay Sr. was initially charged with first degree murder, but the charges were reduced to voluntary manslaughter following a diagnosis of a brain tumor."], [""]]}, {"qid": "38d1d9d7c7309a957ee0", "term": "The Jackson 5", "description": "American pop music family group", "question": "Could the Jackson 5 play a full game of rugby with each other?", "answer": false, "facts": ["The Jackson 5 consisted of five members.", "A full game of rugby is played between 2 teams of 15 players each."], "decomposition": ["How many members are in the Jackson 5?", "How many players are there in a full game of rugby?", "Is #1 greater than or equal to #2?"], "evidence": [[[["The Jackson 5-1"]], [["Rugby union-1"]], ["operation"]], [[["The Jackson 5-1"]], [["Rugby union-1"]], ["operation"]], [[["The Jackson 5-1"]], [["Rugby league positions-1"]], ["operation"]]], "golden_sentence": [[""], ["In its most common form, a game is played between two teams of 15 players using an oval-shaped ball on a rectangular field called a pitch."]]}, {"qid": "16b89a754c59f2f87e9a", "term": "Snickers", "description": "brand name chocolate bar made by Mars, Incorporated", "question": "Is Snickers helpful for weight loss?", "answer": false, "facts": ["Weight loss is best achieved through watching the calories and sugar in the food you eat.", "Snickers is high in fat, sugar, and calories, while being low in nutritional value."], "decomposition": ["What must you avoid to best achieve weight loss?", "Are snickers avoid of those #1?"], "evidence": [[[["Dieting-1"]], [["Snickers-8"], "operation"]], [[["Dieting-1"]], [["Snickers-1", "Snickers-11"], "no_evidence", "operation"]], [[["Weight loss-12"]], [["Snickers-10"], "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "a7f5ac0080a365154437", "term": "Fair trade", "description": "form of trade", "question": "Is the United States the largest exporter of Fair Trade products?", "answer": false, "facts": ["Fair trade is an arrangement designed to help producers in developing countries achieve good trading.", "The United States is not considered a developing country."], "decomposition": ["What countries can use the designation \"fair trade\" for their goods? ", "Does the US have the designation in #1?"], "evidence": [[[["European Fair Trade Association-1"], "no_evidence"], ["operation"]], [[["Fair trade-5"], "no_evidence"], ["no_evidence", "operation"]], [[["Fair trade-1"]], [["Developed country-3", "Developing country-1"]]]], "golden_sentence": [["It regroups 11 fair trade importers in 9 European countries: Austria, Belgium, France, Germany, Italy, Netherlands, Spain, Switzerland and the United Kingdom."]]}, {"qid": "efac307a979039899630", "term": "The Matrix", "description": "1999 science fiction action film directed by the Wachowskis", "question": "Was Harry Potter a better investment than The Matrix for Warner Bros.?", "answer": true, "facts": ["Warner Bros. distributes several movie franchises including The Matrix, Harry Potter, and The Dark Knight.", "The Matrix had 2 sequels.", "Harry Potter had 7 sequels and several spin-offs.", "Harry Potter and the Deathly Hallows \u2013 Part 2 is Warner Bros. highest grossing film worldwide with a box office gross of $1,342,932,398."], "decomposition": ["How much did the Harry Potter (film series) gross?", "How much did the The Matrix (franchise) gross?", "Is #1 greater than #2?"], "evidence": [[[["Harry Potter (film series)-4"]], [["The Matrix (franchise)-4"]], ["operation"]], [[["Harry Potter (film series)-4"]], [["The Matrix (franchise)-4"]], ["operation"]], [[["Harry Potter-3"], "no_evidence"], [["The Matrix-36"]], ["operation"]]], "golden_sentence": [["Without inflation adjustment, it is the third highest-grossing film series with $7.7 billion in worldwide receipts."], [""]]}, {"qid": "df93f677a15bf99f04af", "term": "Lemon", "description": "citrus fruit", "question": "Does Lemon enhance the flavor of milk?", "answer": false, "facts": ["When milk becomes acidic, the water and fats separate from each other.", "When the water and fats separate in milk, it becomes clumpy and has a bad texture.", "Lemon is highly acidic."], "decomposition": ["What is the effect of acid on milk?", "Does #1 make milk more desirable?", "Is Lemon acidic?", "Is #2 or #3 negative?"], "evidence": [[[["Curdling-2"], "no_evidence"], ["operation"], [["Lemon-2"]], ["operation"]], [[["Soured milk-1"]], [["Soured milk-1"]], [["Lemon-21"]], ["operation"]], [[["Curdling-2", "Curdling-3"]], [["Curdling-2"], "no_evidence"], [["Lemon-13", "Lemon-15"]], ["operation"]]], "golden_sentence": [["This is what happens when milk curdles, as the pH drops and becomes more acidic, the protein (casein and others) molecules attract one another and become \"curdles\" floating in a solution of translucent whey."], ["The juice of the lemon is about 5% to 6% citric acid, with a pH of around 2.2, giving it a sour taste."]]}, {"qid": "3f43c58682b0ad647045", "term": "Tom Cruise", "description": "American actor and producer", "question": "Would Tom Cruise ever insult L. Ron Hubbard?", "answer": false, "facts": ["Tom Cruise is an outspoken advocate for the Church of Scientology and its associated social programs.", "The Church of Scientology was founded by L. Ron Hubbard.", "L. Ron Hubbard is a revered and god-like figure in The Church of Scientology."], "decomposition": ["What was founded by Ron Hubbard? ", "Would Tom Cruise ever insult #1"], "evidence": [[[["L. Ron Hubbard-86"]], [["Tom Cruise-36"]]], [[["L. Ron Hubbard-1"]], [["Tom Cruise-4"]]], [[["L. Ron Hubbard-1"]], [["Tom Cruise-4"]]]], "golden_sentence": [["Despite objections, on December 18, 1953, Hubbard incorporated the Church of Scientology, Church of American Science and Church of Spiritual Engineering in Camden, New Jersey."], [""]]}, {"qid": "0103720889fc814381f7", "term": "Toyota Supra", "description": "A sports car and grand tourer manufactured by Toyota Motor Corporation", "question": "Would 2020 Toyota Supra lag behind at a Nascar rally?", "answer": true, "facts": ["The 2020 Toyota Supra has a top speed of 155 MPH.", "Nascar stock cars routinely exceed 200 MPH."], "decomposition": ["What speeds do stock cars in a NASCAR race routinely attain?", "What is the top speed of a Toyota Supra?", "Is #2 less than #1?"], "evidence": [[[["Stock car racing-3"]], [["Toyota Supra-61"]], ["operation"]], [[["Stock car racing-2"]], [["Toyota Supra-77"]], [["Stock car racing-2", "Toyota Supra-77"]]], [[["Stock car racing-65"]], [["Toyota Supra-61"]], ["operation"]]], "golden_sentence": [[""], ["The turbo version has a tested top speed of 285\u00a0km/h (177\u00a0mph)[citation needed], but the cars are restricted to just 180\u00a0km/h (112\u00a0mph) in Japan and 250\u00a0km/h (155\u00a0mph) in worldwide markets."]]}, {"qid": "3f656767d45b9474258b", "term": "Reiki", "description": "Pseudoscientific healing technique", "question": "Can Reiki be stored in a bottle?", "answer": false, "facts": ["Reiki practitioners use a technique called palm healing or hands-on healing through which a \"universal energy\" is said to be transferred through the palms of the practitioner to the patient in order to encourage emotional or physical healing.", "Medications are typically stored in pill bottles."], "decomposition": ["What basic property must a thing have to be able to be stored in a bottle?", "By definition, Reiki is a pseudoscientific healing what?", "Do #2's have the property stated in #1?"], "evidence": [[[["Bottle-1"]], [["Reiki-2"]], ["operation"]], [["no_evidence"], [["Reiki-1"]], ["operation"]], [[["Bottle-1"]], [["Reiki-1"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "db33b4830b7e86c0e61c", "term": "D\u00fcsseldorf", "description": "Place in North Rhine-Westphalia, Germany", "question": "Can you see Stonehenge from a window in Dusseldorf?", "answer": false, "facts": ["Dusseldorf is a city in Germany.", "Stonehenge is a prehistoric monument in Wiltshire, England.", "Wiltshire England is around seven hours away from Dusseldorf Germany by car."], "decomposition": ["Where is Stonehenge located?", "Where is D\u00fcsseldorf located?", "Is #1 geographically close to #2?"], "evidence": [[[["Stonehenge-1"]], [["D\u00fcsseldorf-1"]], ["no_evidence"]], [[["Stonehenge-69"], "operation"], [["D\u00fcsseldorf-36"], "operation"], ["no_evidence"]], [[["Stonehenge-1"]], [["D\u00fcsseldorf-1"]], ["operation"]]], "golden_sentence": [["Stonehenge is a prehistoric monument in Wiltshire, England, two miles (3\u00a0km) west of Amesbury."], ["D\u00fcsseldorf (often Dusseldorf in English sources; UK: /\u02c8d\u028as\u0259ld\u0254\u02d0rf/, US: /\u02c8dju\u02d0s-/, German: [\u02c8d\u028fsl\u0329d\u0254\u0281f] (listen); Low Franconian and Ripuarian: D\u00fcsseld\u00f6rp ([\u02c8d\u028fsl\u0329d\u0153\u0250\u032fp]); archaic Dutch: Dusseldorp) is the capital and second-largest city of the most populous German state of North Rhine-Westphalia after Cologne, and the seventh-largest city in Germany, with a population of 617,280."]]}, {"qid": "10f554f2cc2f77d954ba", "term": "Lorem ipsum", "description": "Placeholder text used in publishing and graphic design", "question": "Does Lorem ipsum backwards fail to demonstrate alliteration?", "answer": false, "facts": ["Lorem ipsum backwards is Muspi merol.", "Alliteration is the occurrence of the same letter or sound at the beginning of adjacent or closely connected words.", "Examples of alliteration are phrases like: Mixed messages, and big bang."], "decomposition": ["What is Lorem ipsum spelled backwards?", "What property makes a group of words alliterative?", "Is #2 not present in #1?"], "evidence": [[["operation"], [["Alliteration-1"]], ["operation"]], [["operation"], [["Alliteration-3"]], ["operation"]], [["no_evidence"], [["Alliteration-1"]], ["no_evidence", "operation"]]], "golden_sentence": [["In literature, alliteration is the conspicuous repetition of identical initial consonant sounds in successive or closely associated syllables within a group of words, even those spelled differently."]]}, {"qid": "99c894e0df31535c1e52", "term": "War", "description": "Intense violent conflict between states", "question": "Could casualties from deadliest war rival France's population?", "answer": false, "facts": ["The deadliest war in history was World War II.", "Over 56 million people died during World War II.", "The population of France as of 2019 is 66 million."], "decomposition": ["What is the population of France?", "What was the deadliest war?", "How many people died in #2?", "Is #3 greater than #1?"], "evidence": [[[["France-1"]], [["World War II-1"]], [["World War II-103"]], ["operation"]], [[["France-1"]], [["World War II casualties-1"]], [["World War II casualties-1"]], ["operation"]], [[["France-1"]], [["World War II-1"]], [["World War II-1"]], ["operation"]]], "golden_sentence": [["The country's 18 integral regions (five of which are situated overseas) span a combined area of 643,801 square kilometres (248,573\u00a0sq\u00a0mi) and a total population of 67.08\u00a0million (as of March\u00a02020[update])."], ["World War\u00a0II was the deadliest conflict in human history, marked by 70 to 85 million fatalities, most of whom were civilians in the Soviet Union and China."], ["Most suggest that some 60\u00a0million people died in the war, including about 20 million military personnel and 40\u00a0million civilians."]]}, {"qid": "46b232b91e7a0c67ec35", "term": "Abortion", "description": "Intentionally ending pregnancy", "question": "Do pediatricians perform abortions?", "answer": false, "facts": ["Pediatricians specialize in the treatment of children and adolescents. ", "Training in abortions is not provided to pediatricians in their schooling."], "decomposition": ["What is a Pediatrician's job duties?", "Is abortion in #1?"], "evidence": [[[["Pediatrics-1"]], [["Pediatrics-1"]]], [[["Pediatrics-1"]], ["operation"]], [[["Pediatrics-1"]], [["Abortion-1"], "operation"]]], "golden_sentence": [["neonatology), and as outpatient primary care physicians."], [""]]}, {"qid": "75d5afe87e358ffad6fe", "term": "Olive", "description": "Species of plant", "question": "If you're reducing salt intake, are olives a healthy snack?", "answer": false, "facts": ["The average amount of sodium a healthy person should have is 2,300mg daily.", "A half cup serving of olives has an average of 735mg sodium in it."], "decomposition": ["What is the average amount of sodium a healthy person should have a day?", "How much sodium is in a serving of olives?", "Is #2 a small percentage of #1?"], "evidence": [[[["Health effects of salt-15", "Salt-5"]], [["Olive-83"], "no_evidence"], ["operation"]], [[["Health effects of salt-15"]], [["Olive-83"]], [["Health effects of salt-15", "Olive-83"]]], [[["Sodium in biology-3"]], [["Olive-83"]], ["operation"]]], "golden_sentence": [["", "The World Health Organization recommends that adults should consume less than 2,000\u00a0mg of sodium, equivalent to 5\u00a0grams of salt per day."], ["One hundred grams of cured green olives provide 146 calories, are a rich source of vitamin E (25% of the Daily Value, DV), and contain a large amount of sodium (104% DV); other nutrients are insignificant."]]}, {"qid": "945757573dd015e47b75", "term": "Israelis", "description": "Ethnic group", "question": "Have the Israelis played the Hammerstein Ballroom?", "answer": false, "facts": ["The Israelis are an ethnic group", "The Hammerstein Ballroom is a venue for concerts and musical performances"], "decomposition": ["What kind of groups play in the Hammerstein Ballroom?", "What kind of a group is the Israelis?", "Is #2 included in #1?"], "evidence": [[[["Hammerstein Ballroom-1", "Hammerstein Ballroom-4"]], [["Israelis-1"]], ["operation"]], [[["Hammerstein Ballroom-4"]], [["Israelis-4"]], ["operation"]], [[["Hammerstein Ballroom-1"]], [["Israelis-1"]], ["operation"]]], "golden_sentence": [["", "The Hammerstein Ballroom has seen performances from a wide variety of musical acts and its popularity has varied over the years due mainly to competition within the neighborhood."], ["Israelis (Hebrew: \u05d9\u05e9\u05e8\u05d0\u05dc\u05d9\u05dd Yi\u015bra\u02beelim, Arabic: \u0627\u0644\u0625\u0633\u0631\u0627\u0626\u064a\u0644\u064a\u064a\u0646\u200e al-\u02beIsr\u0101\u02be\u012bliyyin) are the citizens or permanent residents of the State of Israel, a multiethnic state populated by people of different ethnic backgrounds."]]}, {"qid": "9b652cf829ff71f71176", "term": "Attack on Pearl Harbor", "description": "Surprise attack by the Imperial Japanese Navy on the U.S. Pacific Fleet in Pearl Harbor in Hawaii", "question": "Did the Pearl Harbor attack occur during autumn?", "answer": true, "facts": ["Autumn runs from about September 20 to about December 20.", "Pearl Harbor was attacked on December 7, 1941."], "decomposition": ["When did the Pearl Harbor attack happen?", "Where is Pearl Harbor located?", "When is it autumn in #2?", "Does #1 fall within the range of #3?"], "evidence": [[[["Attack on Pearl Harbor-13"]], [["Pearl Harbor-1"]], [["Autumn-1"], "no_evidence"], ["operation"]], [[["Attack on Pearl Harbor-1"]], [["Pearl Harbor-1"]], [["Climate of Hawaii-11"], "no_evidence"], ["no_evidence", "operation"]], [[["Attack on Pearl Harbor-1"]], [["Pearl Harbor-1"]], [["Autumn-3"]], ["operation"]]], "golden_sentence": [[""], ["Pearl Harbor is an American lagoon harbor on the island of Oahu, Hawaii, west of Honolulu."], ["Autumn marks the transition from summer to winter, in September (Northern Hemisphere) or March (Southern Hemisphere), when the duration of daylight becomes noticeably shorter and the temperature cools considerably."]]}, {"qid": "9748b8fde392b41e0df2", "term": "Al-Farabi", "description": "Philosopher in 10th century Central Asia", "question": "Did Al-Farabi ever meet Mohammed?", "answer": false, "facts": ["Al-Farabi was born in 872 AD.", "Mohammed died in 832 AD."], "decomposition": ["How long ago did Mohammed die?", "When was Al-Farabi born?", "Is #1 before #2?"], "evidence": [[[["Muhammad-63"]], [["Al-Farabi-1"]], ["operation"]], [[["Muhammad-1"]], [["Al-Farabi-1"]], ["operation"]], [[["Mohammed ibn Mohammed Alami-1"]], [["Al-Farabi-9"], "no_evidence"], ["operation"]]], "golden_sentence": [["He died on Monday, 8 June 632, in Medina, at the age of 62 or 63, in the house of his wife Aisha."], ["Abu Nasr Al-Farabi (/\u02cc\u00e6lf\u0259\u02c8r\u0251\u02d0bi/; Persian: \u0627\u0628\u0648 \u0646\u0635\u0631 \u0645\u062d\u0645\u062f \u0628\u0646 \u0645\u062d\u0645\u062f \u0641\u0627\u0631\u0627\u0628\u06cc\u200e Ab\u016b Na\u1e63r Mu\u1e25ammad ibn Mu\u1e25ammad al F\u0101r\u0101b\u012b; known in the West as Alpharabius; c. 872 \u2013 between 14 December, 950 and 12 January, 951) was a renowned early Islamic philosopher and jurist who wrote in the fields of political philosophy, metaphysics, ethics and logic."]]}, {"qid": "e3143833e923752573b6", "term": "NASCAR Cup Series", "description": "Top tier auto racing division within NASCAR", "question": "Could William Franklyn-Miller win a 2020 Nascar Cup Series race?", "answer": false, "facts": ["William Franklyn-Miller is an actor known for the TV series Medici: The Magnificent.", "William Franklyn-Miller turned 16 in March of 2020.", "Nascar Cup Series races have a minimum age of 18."], "decomposition": ["Who is William Franklyn-Miller?", "How old is #1?", "What is the minimum age to join the Nascar Cup Series?", "Is #2 larger than #3?"], "evidence": [[["no_evidence"], ["no_evidence"], [["Driver's licenses in the United States-9"], "no_evidence"], ["no_evidence", "operation"]], [[["Medici (TV series)-15"], "no_evidence"], ["no_evidence"], [["Learner's permit-19"], "no_evidence"], ["operation"]], [[["William Franklyn-1"], "operation"], ["no_evidence"], [["NASCAR Cup Series-11"], "no_evidence"], ["no_evidence"]]], "golden_sentence": [["The minimum age to obtain a restricted driver license in the US varies from 14 years, three months in South Dakota to as high as 17 in New Jersey."]]}, {"qid": "72e839af1578ba95271c", "term": "Knight", "description": "An award of an honorary title for past or future service with its roots in chivalry in the Middle Ages", "question": "Are the knights in the Medieval Times show not authentic knights?", "answer": true, "facts": ["The Medieval Times show is popular in the United States.", "The United States does not actually bestow knighthood on its citizens.", "Authentic knights are citizens of certain countries, mainly the United Kingdom, who are given the honorary title by their country."], "decomposition": ["Which country is the Medieval Times show hosted in?", "Does #1 not confer knighthood on its citizen?"], "evidence": [[[["Medieval Times-2"]], ["no_evidence"]], [[["Medieval Times-2"]], [["Order of the British Empire-12", "Order of the British Empire-3"], "operation"]], [[["Medieval Times-1"]], [["Knight-1"], "no_evidence", "operation"]]], "golden_sentence": [["There are ten locations: the nine in the United States are built as replica 11th-century castles; the tenth, in Toronto, Ontario, Canada, is located inside the CNE Government Building."]]}, {"qid": "6358ef77a0d39020ee5d", "term": "Macbeth", "description": "play by William Shakespeare", "question": "Would costumes with robes and pointy hats be helpful for Macbeth?", "answer": true, "facts": ["Macbeth features scenes with three witches throughout the play. ", "Witches are often displayed with pointy hats and long black robes."], "decomposition": ["What characters are in Macbeth?", "What characters wear pointy hats and robes?", "Would any of #1 wear #2?"], "evidence": [[[["Macbeth-2"], "no_evidence"], [["Cloak-10", "Pointed hat-5"]], ["operation"]], [[["Macbeth-5"]], ["no_evidence"], ["operation"]], [[["Macbeth-2"]], [["Witch hat-1"]], ["operation"]]], "golden_sentence": [["The bloodbath and consequent civil war swiftly take Macbeth and Lady Macbeth into the realms of madness and death."], ["", ""]]}, {"qid": "b187a39d67f6d45d2d05", "term": "Lord Voldemort", "description": "Fictional character of Harry Potter series", "question": "Was Lord Voldemort taught by Professor Dumbledore?", "answer": true, "facts": ["Lord Voldemort grew up as the orphan boy Tom Riddle.", "Tom Riddle was brought to Hogwarts by Professor Dumbledore.", "Dumbledore was one of Riddle's teachers during his time as a student."], "decomposition": ["Who did Lord Voldemort grow up as?", "Where was #1 brought to by a professor as a child?", "Was Professor Dumbledore a teacher at #2 when Tom Riddle was there?"], "evidence": [[[["Lord Voldemort-23"]], [["Lord Voldemort-12"]], ["operation"]], [[["Lord Voldemort-5"]], [["Lord Voldemort-12"]], [["Lord Voldemort-12"]]], [[["Lord Voldemort-1"]], [["Lord Voldemort-12"]], ["no_evidence"]]], "golden_sentence": [[""], ["After living in an orphanage, young Riddle met Dumbledore, who told him he was a wizard and arranged for him to attend Hogwarts."]]}, {"qid": "c97d63ee8e5040c6ba2d", "term": "Satanism", "description": "group of ideological and philosophical beliefs based on Satan", "question": "Is Christianity better for global warming than Satanism?", "answer": true, "facts": ["Global warming happens because CO2 is released into the atmosphere and warms it up.", "Humans release two tons of carbon dioxide a year.", "Reducing the population will lead to a reduction of CO2 in the atmosphere.", "The Christian Crusades caused the deaths of nearly three million people.", "In 1980s and 1990s there were allegations of Satanic ritual deaths, though only a few cases were substantiated."], "decomposition": ["How much population reduction is needed for there to be an impact on carbon dioxide levels in the atmosphere?", "How many people have been killed in the name of Christianity?", "How many people have been killed in the name of Satanism?", "Is #2 closer to #1 than #3 is?"], "evidence": [[[["Carbon footprint-18"], "no_evidence"], [["Christianity and violence-25", "Crusades-1", "Persecution of Muslims-33"], "no_evidence"], [["Satanic ritual abuse-32"], "no_evidence"], ["operation"]], [[["Global warming-32", "Global warming-54"], "no_evidence"], [["Crusades-1"], "no_evidence"], [["Satanism-79"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["[citation needed] It is likely that between 3,000 and 5,000 people were executed.", "", ""], ["The Matamoros murders produced the bodies of 12 adults who were ritually sacrificed by a drug gang inspired by the film The Believers, but did not involve children or sexual abuse."]]}, {"qid": "1f724127a8434ef12332", "term": "Purple", "description": "Range of colors with the hues between blue and red", "question": "Were mollusks an ingredient in the color purple?", "answer": true, "facts": ["Mollusks are a group of invertebrate animals including snails and slugs.", "Purple dye was used in the early 15th century BC to color clothing.", "The purple dye used in ancient Tyre was made from a liquid extracted from a gland in snails."], "decomposition": ["Which animal was used in making purple glands in ancient Tyre?", "Is #1 a mollusk?"], "evidence": [[[["Tyrian purple-1"]], [["Sea snail-1"], "operation"]], [[["Tyrian purple-1"]], [["Sea snail-1"]]], [[["Purple-5"]], [["Bolinus brandaris-1"], "operation"]]], "golden_sentence": [["In ancient times, extracting this dye involved tens of thousands of snails and substantial labor, and as a result, the dye was highly valued."], ["Sea snail is a common name for slow moving marine gastropod molluscs usually with visible external shells, such as whelk or abalone."]]}, {"qid": "7f12141e5201093f1587", "term": "Artisan", "description": "skilled craft worker who makes or creates things by hand", "question": "Are twinkies considered artisan made products?", "answer": false, "facts": ["Twinkies are mass produced in multiple commercial bakeries.", "In bakeries, Twinkies are made by machines."], "decomposition": ["How are artisan-made products made?", "How are Twinkies produced?", "Does #2 match with #1?"], "evidence": [[[["Artisanal food-1"]], [["Twinkie-2"]], ["operation"]], [[["Artisanal food-1"]], [["Twinkie-12", "Twinkie-2"], "no_evidence"], ["operation"]], [[["Artisan-2"]], [["Hostess CupCake-4"]], ["operation"]]], "golden_sentence": [["Artisanal food encompasses breads, cheeses, fruit preserves, cured meats, beverages, oils, and vinegars that are made by hand using traditional methods by skilled craftworkers, known as food artisans."], ["Twinkies are produced and distributed by multiple commercial bakeries in China, where Hostess does not own the brand."]]}, {"qid": "e189c16a0245e02edd5a", "term": "Sable", "description": "Species of marten", "question": "Was the sable depicted in Marvel comics anthropomorphic?", "answer": false, "facts": ["Anthropomorphism is the process of giving human characteristics to an animal or non human object.", "A sable is a carnivorous mammal of the weasel family.", "Silver Sable was a female character that first appeared in Marvel Comics in 1985.", "Silver Sable was a human mercenary that wore a silver suit and used hand to hand combat and weaponry to battle war criminals."], "decomposition": ["What characteristics do anthropomorphic characters have? ", "What Marvel character is based on a sable?", "Does #2 fit the criteria for #1?"], "evidence": [[[["Talking animals in fiction-1"]], [["Silver Sable-1"]], ["operation"]], [[["Anthropomorphism-1"]], [["Silver Sable-1"]], ["operation"]], [[["Anthropomorphism-1"]], [["Silver Sable-1"]], ["operation"]]], "golden_sentence": [["Fictional talking animals often are anthropomorphic, possessing human-like qualities but appearing as a creature."], ["Silver Sable (Silver Sablinova) is a fictional character appearing in American comic books published by Marvel Comics."]]}, {"qid": "a06c8f51a3fbde3ca63f", "term": "Mercenary", "description": "Soldier who fights for hire", "question": "Did mercenaries fight for England in the Glorious Revolution?", "answer": false, "facts": ["Mercenaries are soldiers for hire", "There was no military conflict in England in the Glorious Revolution"], "decomposition": ["What happened in England during the Glorious Revolution?", "Did #1 involve combat?"], "evidence": [[[["Glorious Revolution-1"]], [["Wincanton Skirmish-1"]]], [[["Glorious Revolution-1", "Glorious Revolution-38"]], ["operation"]], [[["Glorious Revolution-1"]], [["Glorious Revolution-1"]]]], "golden_sentence": [["The Glorious Revolution, or Revolution of 1688 (Irish: An R\u00e9abhl\u00f3id Ghl\u00f3rmhar, Scottish Gaelic: R\u00e8abhlaid Ghl\u00f2rmhor or Welsh: Chwyldro Gogoneddus), was the deposition and replacement of James II and VII as ruler of England, Scotland and Ireland by his daughter Mary II and his Dutch nephew and Mary's husband, William III of Orange, which took place between November 1688 and May 1689."], ["It was one of the few notable actions fought during the campaign which has sometimes acquired the name the \"bloodless revolution\"."]]}, {"qid": "8e1c0ce9d7c230c45b54", "term": "Groundhog Day", "description": "Traditional method of weather prediction", "question": "Is groundhog day used as a global season indicator? ", "answer": false, "facts": ["Groundhog Day is an American tradition that occurs on February 2nd. ", "Groundhog Day derives from a superstition that if a groundhog sees it's shadow it will mean there are six more weeks of winter.", "People living in the southern hemisphere of the world experience summer while the people in the north experience winter.", "Different global cultures define the dates of seasons differently."], "decomposition": ["Where is Groundhog Day celebrated?", "Is #1 in both the northern and southern hemisphere?"], "evidence": [[[["Groundhog Day-1"]], [["Southern Hemisphere-1"], "operation"]], [[["Groundhog Day-1"]], [["North America-1", "North America-10", "North America-11"]]], [[["Groundhog Day-1"]], [["Winter-1"], "operation"]]], "golden_sentence": [["Groundhog Day (Pennsylvania German: Grund'sau d\u00e5k, Grundsaudaag, Grundsow Dawg, Murmeltiertag; Nova Scotia: Daks Day) is a popular American tradition observed in the United States and Canada on February 2nd."], ["Its surface is 80.9% water, compared with 60.7% water in the case of the Northern Hemisphere, and it contains 32.7% of Earth's land."]]}, {"qid": "f6eaa306769c68019c11", "term": "Yeti", "description": "Folkloric ape-like creature from Asia", "question": "Would a Yeti be likely to have prehensile limbs?", "answer": true, "facts": ["The animals that Yetis are said to look similar to are able to use their hands or toes to grasp items", "The ability to grasp with hands or other limbs is to be prehensile. "], "decomposition": ["What does it mean to be prehensile?", "What animals are Yetis said to look like?", "Would #2 be considered #1?"], "evidence": [[[["Prehensile feet-1"]], [["Yeti-28"]], ["operation"]], [[["Prehensile feet-1"]], [["Yeti-4"]], [["Yeti-4"]]], [[["Prehensility-1"]], [["Yeti-1"]], ["operation"]]], "golden_sentence": [["The term prehensile means \"able to grasp\" (from the Latin prehendere, to take hold of, to grasp)."], [""]]}, {"qid": "5f8764850a304d6ecd6c", "term": "Asteroid", "description": "Minor planet that is not a comet", "question": "Can an asteroid be linked with virginity?", "answer": true, "facts": ["An asteroid discovered in 1807 was named Vesta", "Vesta is the Roman virgin goddess of hearth, home and family"], "decomposition": ["What was the name of the asteroid that was discovered in 1807?", "What did #1 stand for as a Roman goddess?", "Is #2 related to virginity?"], "evidence": [[[["4 Vesta-1"]], [["Vesta (mythology)-1"]], [["Vesta (mythology)-8"]]], [[["4 Vesta-1"]], [["Vesta (mythology)-1"]], ["operation"]], [[["4 Vesta-1"]], [["Vesta (mythology)-1"]], ["operation"]]], "golden_sentence": [["It was discovered by the German astronomer Heinrich Wilhelm Matthias Olbers on 29 March 1807 and is named after Vesta, the virgin goddess of home and hearth from Roman mythology."], ["Vesta (Latin pronunciation:\u00a0[\u02c8w\u025bsta]) is the virgin goddess of the hearth, home, and family in Roman religion."], [""]]}, {"qid": "e9d409e2a5f2040d1fb4", "term": "Sugar Ray Robinson", "description": "American boxer", "question": "Did Sugar Ray Robinson win a fight against Canelo Alvarez?", "answer": false, "facts": ["Sugar Ray Robinson died in 1989", "Canelo Alvarez was born in 1990"], "decomposition": ["In what year did Sugar Ray Robinson die?", "In what year was Canelo Alvarez born?", "Is #2 before #1?"], "evidence": [[[["Sugar Ray Robinson-28"]], [["Canelo \u00c1lvarez-1"]], ["operation"]], [[["Sugar Ray Robinson-1"]], [["Canelo \u00c1lvarez-1"]], ["operation"]], [[["Sugar Ray Robinson-1"]], [["Canelo \u00c1lvarez-1"]], ["operation"]]], "golden_sentence": [["He died in Los Angeles on April 12, 1989 at the age of 67."], ["Santos Sa\u00fal \u00c1lvarez Barrag\u00e1n (American Spanish:\u00a0[sa\u02c8ul \u02c8al\u03b2a\u027ees]; born 18 July 1990), better known as \"Canelo\" \u00c1lvarez, is a Mexican professional boxer who is a three-division world champion."]]}, {"qid": "87e2a3e37113fbfc4efc", "term": "Royal Observatory, Greenwich", "description": "observatory in Greenwich, London, UK", "question": "In geometry terms, is the Royal Observatory in Greenwich similar to a yield sign?", "answer": false, "facts": ["The main building of the Royal Observatory is the Octagon Room.", "A yield sign is shaped like a rounded triangle.", "Two figures are similar if they have the same shape but not necessarily the same size."], "decomposition": ["What is the shape of the Royal Observatory in Greenwich?", "What is the shape of a yield sign?", "Is #1 geometrically similar to #2?"], "evidence": [[[["Royal Observatory, Greenwich-31"]], [["Yield sign-3"]], ["operation"]], [[["Royal Observatory, Greenwich-1"], "no_evidence"], [["Yield sign-3"], "no_evidence"], ["no_evidence", "operation"]], [[["Royal Observatory, Greenwich-7"]], [["Yield sign-5"]], ["operation"]]], "golden_sentence": [[""], ["The triangular yield sign was used as early as 1938 when it was codified in Czechoslovakia in a blue-white variant without words and in 1939 in the Protectorate of Bohemia and Moravia which adopted the current red-white variant."]]}, {"qid": "d1454c6eaacb2f2d2a04", "term": "Monty Python's Flying Circus", "description": "British sketch comedy television series", "question": "Did Monty Python write the Who's on First sketch?", "answer": false, "facts": ["Who's on First debuted in 1945.", "Monty Python's first show was in 1969."], "decomposition": ["When was the Who's on First sketch first performed?", "When was the debut of the Monty Python show?", "Is #2 before #1?"], "evidence": [[[["Who's on First?-2"]], [["Monty Python-1"]], ["operation"]], [[["Abbott and Costello-25", "Who's on First?-3"]], [["Monty Python's Flying Circus-16"]], ["operation"]], [[["Who's on First?-2", "Who's on First?-3"]], [["Monty Python-1"]], ["operation"]]], "golden_sentence": [[""], ["Monty Python (also collectively known as the Pythons) were a British surreal comedy troupe who created the sketch comedy television show Monty Python's Flying Circus, which first aired on the BBC in 1969."]]}, {"qid": "065565731aee70d0249f", "term": "Charlemagne", "description": "King of the Franks, King of Italy, and Holy Roman Emperor", "question": "Did Charlemagne have a bar mitzvah?", "answer": false, "facts": ["Charlemagne was a Roman Catholic", "Bar mitzvah is a coming of age ceremony in Judaism"], "decomposition": ["What was Charlemagne's religion?", "In what religion is a bar mitzvah celebrated?", "Is #1 the same as #2?"], "evidence": [[[["Charlemagne-50"]], [["Bar and bat mitzvah-1"]], ["operation"]], [[["Charlemagne-7"]], [["Bar and bat mitzvah-2"]], ["operation"]], [[["Charlemagne-2"]], [["Bar and bat mitzvah-1"]], ["operation"]]], "golden_sentence": [["In the Saxon Wars, spanning thirty years and eighteen battles, he conquered Saxonia and proceeded to convert it to Christianity."], ["For other uses, see Bar Mitzvah (disambiguation) and Bat Mitzvah (disambiguation) Bar mitzvah (Hebrew: \u05d1\u05b7\u05bc\u05e8 \u05de\u05b4\u05e6\u05b0\u05d5\u05b8\u05d4) is a Jewish coming of age ritual for boys, whereas Bat mitzvah (Hebrew: \u05d1\u05b7\u05bc\u05ea \u05de\u05b4\u05e6\u05b0\u05d5\u05b8\u05d4; Ashkenazi pronunciation: bas mitzveh) is a Jewish coming of age ritual for girls."]]}, {"qid": "3fb5050555122f0522f5", "term": "Common Era", "description": "alternative (and religiously neutral) naming of the traditional calendar era, Anno Domini", "question": "Is entire Common Era minuscule to lifespan of some trees?", "answer": true, "facts": ["The Common Era has lasted for over 2,000 years as of 2020.", "A tree named Methuselah, from California's White Mountains, is almost 5,000 years old."], "decomposition": ["How long has the Common Era lasted?", "How old is the oldest known tree?", "Is #2 greater than #1?"], "evidence": [[[["Common Era-1"]], [["Methuselah (tree)-3"]], [["Methuselah (tree)-3"], "operation"]], [[["21st century-1"]], [["Methuselah (tree)-1"]], ["operation"]], [[["Common Era-1"], "no_evidence"], [["Pinus longaeva-1"]], ["operation"]]], "golden_sentence": [["BCE (Before the Common Era or Before the Current Era) is the era before CE."], ["Methuselah was 4,789 years old when sampled (likely in 1957) by Edmund Schulman and Tom Harlan, with an estimated germination date of 2833\u00a0BC."], [""]]}, {"qid": "e4ecaf6d334281cd48c7", "term": "Kaffir lime", "description": "A citrus fruit native to tropical Southeast Asia and southern China", "question": "Would kaffir lime be good in a White Russian?", "answer": false, "facts": ["A White Russian is a drink containing cream, vodka, and Kahlua.", "Mixing lime and cream results in curdled milk, which is not good to drink."], "decomposition": ["What are the ingredients of a White Russian?", "Do any of #1 curdle when mixed with lime?"], "evidence": [[[["White Russian (cocktail)-1"]], ["no_evidence", "operation"]], [[["White Russian (cocktail)-1"]], [["Citric acid-1", "Curdling-2"], "no_evidence", "operation"]], [[["White Russian (cocktail)-1"]], [["Curdling-3"], "operation"]]], "golden_sentence": [["A White Russian (Russian language: \u0411\u0435\u043b\u044b\u0439 \u0420\u0443\u0441\u0441\u043a\u0438\u0439) is a cocktail made with vodka, coffee liqueur (e.g., Kahl\u00faa or Tia Maria) and cream served with ice in an Old Fashioned glass."]]}, {"qid": "36e8ab9480beecc3dd73", "term": "Chinook salmon", "description": "species of fish", "question": "Could eating Chinook salmon help Ryan Reynolds?", "answer": true, "facts": ["Chinook salmon is high in omega-3 fatty acids.", "Omega-3 fatty acids can aid treatment of depression. ", "Ryan Reynolds has struggled with depression."], "decomposition": ["What mental disorder did Ryan Reynolds suffer from?", "What nutrient may be able to aid in treatment of #1?", "Is chinook salmon high in #2?"], "evidence": [[[["Ryan Reynolds-24"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Ryan Reynolds-24"]], [["Anxiety disorder-29", "Anxiety-58"], "no_evidence"], [["Chinook salmon-2"], "no_evidence"]], [[["Ryan Reynolds-24"]], [["Generalized anxiety disorder-20"]], ["operation"]]], "golden_sentence": [["Reynolds has openly spoken about his lifelong struggle with anxiety, noting in 2018 that he carried out many interviews in the character of Deadpool to alleviate his fears."]]}, {"qid": "e8a429adabaa5da091f5", "term": "Depression (mood)", "description": "state of low mood and fatigue", "question": "In teenagers and young adults with depression, are SSRI medications less safe than they are for adults?", "answer": true, "facts": ["In teens, SSRI medications may increase the risk of suicidal thinking.", "In adults over 25, SSRI medications are regarded as generally safe."], "decomposition": ["What are potential side effects of SSRIs for adults?", "What are the potential side effects of SSRIs for teenagers?", "Are the hazards in #2 worse than the hazards in #1?"], "evidence": [[[["Selective serotonin reuptake inhibitor-37"]], [["Selective serotonin reuptake inhibitor-33"]], ["operation"]], [[["Development and discovery of SSRI drugs-10"], "no_evidence"], [["Development and discovery of SSRI drugs-10"], "no_evidence"], ["no_evidence"]], [[["Selective serotonin reuptake inhibitor-21"], "no_evidence"], [["Selective serotonin reuptake inhibitor-33"]], ["operation"]]], "golden_sentence": [["For adults older than 64, SSRI's seem to reduce the risk of both suicidal behavior."], ["Meta analyses of short duration randomized clinical trials have found that SSRI use is related to a higher risk of suicidal behavior in children and adolescents."]]}, {"qid": "530ebf55c615313ff968", "term": "Anorexia nervosa", "description": "Eating disorder characterized by refusal to maintain a healthy body weight, and fear of gaining weight due to a distorted self image", "question": "Are red legs a sign of failing health in those with Anorexia Nervosa?", "answer": true, "facts": ["Heart failure or disease can lead to the legs becoming red or pink in color.", "Anorexia Nervosa can lead to heart failure and death."], "decomposition": ["What is a complication associated with Anorexia Nervosa that affects the heart?", "What happens to a person's legs when #1 occurs?", "Is #2 a sign of failing health?"], "evidence": [[[["Anorexia nervosa-58"], "no_evidence"], [["Rash-2"], "no_evidence"], ["operation"]], [[["Anorexia nervosa-1"], "no_evidence"], [["Cardiovascular disease-6"], "no_evidence"], ["no_evidence", "operation"]], [[["Anorexia nervosa-53"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Orthostasis in anorexia nervosa indicates worsening cardiac function and may indicate a need for hospitalization."], [""]]}, {"qid": "846abdf483bffce085c9", "term": "Cheshire", "description": "County of England", "question": "Do citizens of Cheshire sing La Marseillaise?", "answer": false, "facts": ["Cheshire is a county located in England in the United Kingdom", "La Marseillaise is the national anthem of France"], "decomposition": ["Which song is referred to as 'La Marseillaise'?", "#1 is usually sung by the citizens of which country?", "Which country is Cheshire located in?", "Is #2 the same as #3?"], "evidence": [[[["La Marseillaise-3"]], [["La Marseillaise-3"]], [["Cheshire-53"]], [["Cheshire-54"], "operation"]], [[["La Marseillaise-1"]], [["La Marseillaise-1"]], [["Cheshire-1"]], ["operation"]], [[["La Marseillaise-1"]], [["La Marseillaise-1"]], [["Cheshire-1"]], ["operation"]]], "golden_sentence": [[""], ["The War of the First Coalition was an effort to stop the revolution, or at least contain it to France."], [""], [""]]}, {"qid": "726fbd328063617f7526", "term": "Ancient Greek", "description": "Version of the Greek language used from roughly the 9th century BCE to the 6th century CE", "question": "Are seasons of Survivor surpassed by number of Ancient Greek letters?", "answer": false, "facts": ["The reality show Survivor has aired 40 seasons as of 2020.", "The Ancient Greek alphabet consisted of 24 letters."], "decomposition": ["How many seasons of Survivor have there been as of 2020?", "How many letters were in the Ancient Greek alphabet?", "Is #2 greater than #1?"], "evidence": [[[["Survivor (American TV series)-4"]], [["Greek alphabet-2"]], ["operation"]], [[["Survivor (American TV series)-4"]], [["Greek alphabet-2"]], ["operation"]], [[["Survivor: Winners at War-1"]], [["Greek alphabet-2"]], ["operation"]]], "golden_sentence": [["The series' 40th season Survivor: Winners at War premiered on February 12, 2020, during the 20th anniversary of the show."], ["In Archaic and early Classical times, the Greek alphabet existed in many different local variants, but, by the end of the fourth century BC, the Euclidean alphabet, with twenty-four letters, ordered from alpha to omega, had become standard and it is this version that is still used to write Greek today."]]}, {"qid": "8b2839ab916d9de05285", "term": "Jumping spider", "description": "family of arachnids", "question": "Would a jumping spider need over half a dozen contact lenses?", "answer": true, "facts": ["Jumping spiders have eight eyes.", "Half a dozen objects is equal to six objects.", "One contact lens is worn per eye."], "decomposition": ["How many eyes do jumping spiders have?", "How much is half a dozen?", "Is #1 more than #2?"], "evidence": [[[["Jumping spider-3"]], [["Dozen-3"]], [["Jumping spider-3"]]], [[["Jumping spider-1"]], [["Dozen-1"], "operation"], ["operation"]], [[["Jumping spider-1"]], [["Dozen-3", "One half-1"]], ["operation"]]], "golden_sentence": [["They have eight eyes, as illustrated."], [""], [""]]}, {"qid": "ba8b1c35482281667ba4", "term": "Harvey Milk", "description": "American politician who became a martyr in the gay community", "question": "Would Harvey Milk have approved of Obama?", "answer": true, "facts": ["Obama awarded Harvey Milk a posthumous Medal of Freedom. ", "Obama was known for supporting marriage equality and LGBT rights. "], "decomposition": ["What was Harvey Milk known for?", "Did Obama support #1?", "Is #1 the same as #2?"], "evidence": [[[["Harvey Milk-1"]], [["Barack Obama-4"]], ["operation"]], [[["Harvey Milk-1"]], [["Barack Obama-4"]], ["operation"]], [[["Harvey Milk-4"]], [["Stuart Milk-4"]], ["operation"]]], "golden_sentence": [["Harvey Bernard Milk (May 22, 1930 \u2013 November 27, 1978) was an American politician and the first openly gay elected official in the history of California, where he was elected to the San Francisco Board of Supervisors."], ["He advocated for gun control in response to the Sandy Hook Elementary School shooting, indicating support for a ban on assault weapons, and issued wide-ranging executive actions concerning global warming and immigration."]]}, {"qid": "cad1c70b6b22b7515868", "term": "Dual-energy X-ray absorptiometry", "description": "diagnostic test for bone mineral density testing", "question": "Would dual-energy X-ray absorptiometry be useful if performed on a crab?", "answer": false, "facts": ["Dual-energy X-ray absorptiometry is typically used to diagnose and follow osteoporosis.", "Osteoporosis is a disease in which bone weakening increases the risk of a broken bone.", "Crabs are invertebrates.", "Invertebrates do not have bones."], "decomposition": ["What condition is diagnosed with dual-energy X-ray absorptiometry?", "What body parts are affected by #1?", "Do crabs have #2?"], "evidence": [[[["Dual-energy X-ray absorptiometry-1"]], [["Dual-energy X-ray absorptiometry-2", "Osteoporosis-1"]], [["Crab-2"], "operation"]], [[["Dual-energy X-ray absorptiometry-2"]], [["Osteoporosis-1"]], [["Crab-1"]]], [[["Dual-energy X-ray absorptiometry-2"]], [["Osteoporosis-1"]], [["Crab-1", "Invertebrate-1"]]]], "golden_sentence": [[""], ["", "Bones that commonly break include the vertebrae in the spine, the bones of the forearm, and the hip."], [""]]}, {"qid": "329fb8c2c6ba2968a7f1", "term": "Aloe vera", "description": "Species of plant", "question": "Would a house full of aloe vera hypothetically be ideal for Unsinkable Sam?", "answer": false, "facts": ["Aloe vera is a plant species that is toxic to cats.", "Unsinkable Sam was a cat that supposedly served during World War II."], "decomposition": ["What kind of animal was Unsinkable Sam?", "What is aloe vera?", "Is #2 safe for #1 to be around?"], "evidence": [[[["Unsinkable Sam-1"]], [["Aloe vera-1"]], ["no_evidence"]], [[["Unsinkable Sam-1"]], [["Aloe vera-1"]], [["Aloe vera-20", "Aloe vera-21"]]], [[["Unsinkable Sam-1"]], [["Aloe vera-1"]], [["Aloe vera-21"], "no_evidence", "operation"]]], "golden_sentence": [["Unsinkable Sam (also known as Oskar or Oscar) is the apocryphal nickname of a ship's cat who purportedly served during World War II with both the Kriegsmarine and the Royal Navy and survived the sinking of three ships."], ["Aloe vera (/\u02c8\u00e6lo\u028ai\u02d0/ or /\u02c8\u00e6lo\u028a/) is a succulent plant species of the genus Aloe."]]}, {"qid": "3f07a6bfcf6a4ef6c615", "term": "Hour", "description": "unit of time", "question": "Can a human eat an entire 12-lb roast turkey in an hour? ", "answer": false, "facts": ["A serving of roast turkey is about 1 pound of uncooked turkey.", "A 12-lb roast turkey would contain about 12 servings of cooked turkey meat.", "One human cannot eat 12 1-lb servings of turkey in one sitting."], "decomposition": ["What is the most food a person has eaten in one hour?", "A 12 pound uncooked turkey provides how much cooked meat?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Hot dog-21", "Nathan's Hot Dog Eating Contest-2", "Nathan's Hot Dog Eating Contest-5", "Pound (mass)-31"]], ["no_evidence"], ["operation"]], [[["Competitive eating-2", "Competitive eating-6"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["An American Institute for Cancer Research (AICR) report found that consuming one daily 50-gram serving of processed meat \u2014 about one hot dog \u2014 increases long-term risk of colorectal cancer by 20 percent.", "The defending men's champion is Joey Chestnut, who ate 71 hot dogs in the 2019 contest.", "The contestant that consumes (and keeps down) the most hot dogs and buns (HDB) in ten minutes is declared the winner.", ""]]}, {"qid": "b543c7bb4eca38a6d4eb", "term": "Kurt Cobain", "description": "American singer, composer, and musician", "question": "Was Kurt Cobain's death indirectly caused by Daniel LeFever?", "answer": true, "facts": ["Kurt Cobain committed suicide with a shotgun.", "Daniel LeFever was the inventor of the American hammerless shotgun."], "decomposition": ["What object caused the death of Kurt Cobain?", "Was #1 invented by Daniel LeFever?"], "evidence": [[[["Suicide of Kurt Cobain-1"]], ["operation"]], [[["Kurt Cobain-3"]], [["Daniel Myron LeFever-1"]]], [[["Kurt Cobain-55"]], [["Shotgun-38"]]]], "golden_sentence": [["The Seattle Police Department incident report states that Cobain \"was found with a shotgun across his body, had a visible head wound and there was a note discovered nearby\"."]]}, {"qid": "aad8af5dc80cbf333287", "term": "Charles Manson", "description": "American criminal, cult leader", "question": "Was Charles Manson's body unwanted?", "answer": false, "facts": ["Charles Manson's body was debated over for so long that he had to stay on ice.", "Charles Manson had four people fighting over his corpse."], "decomposition": ["How many people tried to claim Charles Manson's body?", "Is #1 equal to zero?"], "evidence": [[[["Charles Manson-66"]], ["operation"]], [[["Charles Manson-66"]], ["operation"]], [[["Charles Manson-66"]], ["operation"]]], "golden_sentence": [["Three people stated their intention to claim Manson's estate and body."]]}, {"qid": "4ca669b04f30d15a93f6", "term": "Peach", "description": "species of fruit tree (for the fruit use Q13411121)", "question": "Are peaches best eaten when firm?", "answer": false, "facts": ["Peaches are sweeter and easier to digest when they are soft to the touch.", "People tend to let their peaches sit until they soften. "], "decomposition": ["When a peach is considered best to be eaten, what characteristics does it have?", "When a peach is firm, does it have most of the characteristics listed in #1?"], "evidence": [[[["Peach-38"], "no_evidence"], [["Peach-38"], "no_evidence"]], [[["Peach-38"]], [["Peach (fruit)-5"]]], [[["Peach (fruit)-5"], "no_evidence"], ["operation"]]], "golden_sentence": [["They are highly perishable, and typically consumed or canned within two weeks of harvest."], [""]]}, {"qid": "c92de39697a6f3cd11e5", "term": "Jack Black", "description": "American actor, comedian, musician, music producer and youtuber.", "question": "Is Jack Black's height enough to satisfy Coronavirus distancing?", "answer": false, "facts": ["Jack Black is 5'6\" tall.", "The CDC recommends people stay 6 feet apart."], "decomposition": ["How tall is Jack Black?", "What is the minimum recommended length for social distancing?", "Is #1 at least #2?"], "evidence": [[["no_evidence"], [["Social distancing-9"]], ["operation"]], [["no_evidence"], [["Social distancing-9"]], ["no_evidence", "operation"]], [[["Jack Black-1"], "no_evidence"], [["Social distancing-9"]], ["no_evidence", "operation"]]], "golden_sentence": [["During the COVID-19 pandemic, the CDC revised the definition of social distancing as \"remaining out of congregrate settings, avoiding mass gatherings, and maintaining distance (approximately six feet or two meters) from others when possible\"."]]}, {"qid": "02a890a67ac5a6836f23", "term": "Soldier", "description": "one who serves as part of an organized armed force", "question": "Can children be soldiers in the US Army?", "answer": false, "facts": ["A soldier is a member of the armed forces.", "The US Army is the land warfare branch of the United States.", "The minimum age for enlistment in the US Army is 18 years old. ", "A child is considered is considered to be anyone under the age of 18 years old."], "decomposition": ["What is the minimum age to enlist in the US Army?", "Is a child's age above #1?"], "evidence": [[[["Children in the military-68"]], [["Children in the military-1"], "operation"]], [[["United States Armed Forces-3"]], [["Child-4"], "operation"]], [[["United States Armed Forces-3"], "no_evidence"], [["Child-4"]]]], "golden_sentence": [["In the United States 17-year-olds may join the armed forces with the written agreement of parents."], [""]]}, {"qid": "b6ff7e5872626e418f9c", "term": "Hotel manager", "description": "person managing a hotel", "question": "Could Charlie Bucket be a hotel manager?", "answer": false, "facts": ["Charlie Bucket is a fictional character from \"Charlie and the Chocolate Factory\", portrayed as a child.", "Children cannot be hotel managers."], "decomposition": ["How was Charlie Bucket portrayed in Charlie and the Chocolate Factory?", "Is #1 as an adult?"], "evidence": [[[["Charlie and the Chocolate Factory (film)-6"], "no_evidence"], ["no_evidence"]], [[["Charlie and the Chocolate Factory-4"]], ["operation"]], [[["Charlie and the Chocolate Factory-27"]], [["Boy-1"], "operation"]]], "golden_sentence": [["Since Charlie was the \"least rotten\" of the five, Wonka invites Charlie to come live and work in the factory with him, on the condition that Charlie leave his family behind, just as Wonka did."]]}, {"qid": "142989da691573362145", "term": "White", "description": "color", "question": "Can paresthesia be caused by a white pigment?", "answer": true, "facts": ["Tingling in the hands or feet is a type of paresthesia", "Lead white exposure can lead to lead poisoning", "Symptoms of lead poisoning include tingling in the hands and feet"], "decomposition": ["What kinds of white pigment have adverse health effects?", "What are the symptoms of paresthesia?", "Can any of #1 cause #2?"], "evidence": [[[["Lead paint-7"]], [["Paresthesia-1"]], [["Lead poisoning-1"], "operation"]], [["no_evidence"], [["Paresthesia-1"]], ["no_evidence", "operation"]], [[["Powder-11"], "no_evidence"], [["Paresthesia-1"]], ["operation"]]], "golden_sentence": [["It can cause nervous system damage, stunted growth, kidney damage, and delayed development."], ["Paresthesia is an abnormal sensation of the skin (tingling, pricking, chilling, burning, numbness) with no apparent physical cause."], ["It causes almost 10% of intellectual disability of otherwise unknown cause and can result in behavioral problems."]]}, {"qid": "166a70897d5b57f363d6", "term": "Astrology", "description": "Pseudoscience claiming celestial objects influence human affairs", "question": "Would Elon Musk be more likely to know about astrology than physics?", "answer": false, "facts": ["Elon Musk is a businessman and engineer with a bachelor's degree and unfinished Ph.D. in physics", "Engineering is based on principles of applied physics", "Astrology is not a form of science or applied science"], "decomposition": ["Which field(s) of study did Elon Musk specialize in?", "Is Astrology closely related to (any of) #1?"], "evidence": [[[["Elon Musk-10"]], [["Astrology-34", "Physics-2"], "operation"]], [[["Elon Musk-2"]], [["Astrology-1"]]], [[["Elon Musk-10"]], [["Astrology-1"], "operation"]]], "golden_sentence": [["He left in 1992 to study business and physics at the University of Pennsylvania; he graduated with a Bachelor of Arts degree in economics and a Bachelor of Science degree in physics."], ["Astrology's modern representation in western popular media is usually reduced to sun sign astrology, which considers only the zodiac sign of the Sun at an individual's date of birth, and represents only 1/12 of the total chart.", ""]]}, {"qid": "8b9c8c9bd8386287b483", "term": "Pearl Harbor", "description": "Harbor on the island of Oahu, Hawaii", "question": "Is Pearl Harbor the mythical home of a shark goddess?", "answer": true, "facts": ["The native Hawaiian people believed Pearl Harbor was the home of Ka\u02bbahupahau.", "Ka\u02bbahupahau is a shark goddess in Hawaiian legends. "], "decomposition": ["What did the native Hawaiian people believe Pearl Harbor was home to?", "What was #1?", "IS #2 the same as a shark goddess?"], "evidence": [[[["Pearl Harbor-2"]], [["Pearl Harbor-2"]], [["Pearl Harbor-2"]]], [[["Pearl Harbor-2"]], [["Pearl Harbor-2"]], ["operation"]], [[["Pearl Harbor-2"]], [["Pearl Harbor-2"]], ["operation"]]], "golden_sentence": [[""], [""], [""]]}, {"qid": "ac1919419594482e85ae", "term": "Ocean sunfish", "description": "species of fish", "question": "Would it be impossible to keep an ocean sunfish and a goldfish in the same tank?", "answer": true, "facts": ["Ocean sunfish live in salt water environments.", "Goldfish live in fresh water environments.", "Putting a fish into the wrong water type can cause them to die."], "decomposition": ["What kind of water habitat does the ocean sunfish live in?", "What kind of water habitat do goldfish live in?", "Is #1 interchangeable with #2"], "evidence": [[[["Ocean sunfish-21", "Ocean-2"]], [["Goldfish-1"]], ["operation"]], [[["Ocean sunfish-1"]], [["Goldfish-1"]], ["operation"]], [[["Ocean-11", "Saltwater fish-3"]], [["Freshwater fish-3", "Goldfish-1"]], [["Freshwater fish-3", "Saltwater fish-3"]]]], "golden_sentence": [["Ocean sunfish are native to the temperate and tropical waters of every ocean in the world.", "Saline seawater covers approximately 361,000,000\u00a0km2 (139,000,000\u00a0sq\u00a0mi) and is customarily divided into several principal oceans and smaller seas, with the ocean covering approximately 71% of Earth's surface and 90% of the Earth's biosphere."], ["The goldfish (Carassius auratus) is a freshwater fish in the family Cyprinidae of order Cypriniformes."]]}, {"qid": "cbbf10bda48e6c36fde1", "term": "Maize", "description": "Cereal grain", "question": "Did Demi Lovato's ancestors help turn maize into popcorn?", "answer": true, "facts": ["Demi Lovato's father is Mexican.", "Maize is another word for corn.", "Corn was first domesticated in southern Mexico about 10,000 years ago.", "Popcorn is made from kernels of corn."], "decomposition": ["Where is popcorn from originally?", "Where is Demi Lovato's ancestors from?", "Are #1 and #2 the same?"], "evidence": [[[["Popcorn-5"]], [["Demi Lovato-5"]], ["operation"]], [[["Popcorn-5"]], [["Demi Lovato-5"]], ["operation"]], [[["Popcorn-5"]], [["Demi Lovato-5"]], ["operation"]]], "golden_sentence": [[""], ["Lovato's father was of Mexican descent, with mostly Spanish and Native American ancestors, and came from a family that has been living in New Mexico for generations; he also had distant Portuguese and Jewish ancestry."]]}, {"qid": "ba601929c5ce47ed571b", "term": "Baptism", "description": "Christian rite of admission and adoption, almost invariably with the use of water", "question": "Was Alexander the Great baptized?", "answer": false, "facts": ["Baptism is a symbolic Christian rite using water.", "Christianity started in the first century AD.", "Alexander the Great lived from 356 BC- 323 BC."], "decomposition": ["Baptism is a rite in which religion?", "When did #1 develop?", "When did Alexander the Great die?", "Is #2 before #3?"], "evidence": [[[["Baptism-1"]], [["Christianity in the 1st century-2"]], [["Alexander the Great-62"]], ["operation"]], [[["Baptism-1"]], [["Baptism-8"]], [["Alexander the Great-62"]], [["Alexander the Great-62", "Baptism-8"], "operation"]], [[["Baptism-1"]], [["Christianity-3"]], [["Alexander the Great-1"]], ["operation"]]], "golden_sentence": [["Baptism (from the Greek noun \u03b2\u03ac\u03c0\u03c4\u03b9\u03c3\u03bc\u03b1 baptisma; see below) is a Christian rite of admission and adoption, almost invariably with the use of water, into Christianity."], [""], ["On either 10 or 11 June 323\u00a0BC, Alexander died in the palace of Nebuchadnezzar II, in Babylon, at age 32."]]}, {"qid": "511eb5c2a53c57e4d789", "term": "Scottish people", "description": "ethnic inhabitants of Scotland", "question": "Are Scottish people Albidosi?", "answer": true, "facts": ["The Scottish people emerged from an amalgamation of two Celtic-speaking peoples, the Picts and Gaels, who founded the Kingdom of Scotland.", "What the Picts called themselves is unknown. It has been proposed that they called themselves Albidosi.", "The Kingdom of Scotland is also known as the Kingdom of Alba."], "decomposition": ["Which tribes did the Scottish people emerge from?", "Have any of #1 been referred to as Albidosi?"], "evidence": [[[["Scottish people-1"]], [["Picts-6"]]], [[["Scottish people-1"]], [["Picts-6"]]], [[["Scottish people-1"]], ["no_evidence"]]], "golden_sentence": [["Historically, they emerged from an amalgamation of two Celtic-speaking peoples, the Picts and Gaels, who founded the Kingdom of Scotland (or Alba) in the 9th century."], ["It has been proposed that they called themselves Albidosi, a name found in the Chronicle of the Kings of Alba during the reign of M\u00e1el Coluim mac Domnaill, but this idea has been disputed."]]}, {"qid": "066f186704054cb2367e", "term": "Aldi", "description": "Germany-based supermarket chain", "question": "Would you spend less on your food at Aldi than at Whole Foods?", "answer": true, "facts": ["Whole Foods is known for costing 10-20% more than other stores.", "Aldi is known for having deeply discounted food and home supplies."], "decomposition": ["What is Aldi mainly known for?", "Compared to other stores, how do Whole Foods prices compare?", "Would #1 have goods that cost less than #2?"], "evidence": [[[["Aldi-1"]], [["Whole Foods Market-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Aldi-1"]], [["Whole Foods Market-1", "Whole Foods Market-24"], "no_evidence"], [["Discount store-1"], "operation"]], [[["Aldi-1"]], [["Wild Oats Markets-11"]], ["operation"]]], "golden_sentence": [["Aldi (stylised as ALDI) is the common brand of two German family-owned discount supermarket chains with over 10,000 stores in 20 countries, and an estimated combined turnover of more than \u20ac50 billion."], [""]]}, {"qid": "c20fe3f10e6d94e77eb9", "term": "Los Angeles Memorial Sports Arena", "description": "Former arena in California, United States", "question": "Was Los Angeles Memorial Sports Arena hypothetically inadequate for hosting Coachella?", "answer": true, "facts": ["The Los Angeles Memorial Sports Arena had a capacity of 16,740 people.", "Coachella has had attendance numbers in excess of 99.000 people.", "Coachella relies on an outdoor set up to accommodate the massive crowds."], "decomposition": ["How many people can the Los Angeles Memorial Sports Arena hold?", "How many people usually attend Coachella?", "Is #2 greater than #1?"], "evidence": [[[["Los Angeles Memorial Sports Arena-15"]], [["Coachella Valley Music and Arts Festival-3"]], [["Coachella Valley Music and Arts Festival-3", "Los Angeles Memorial Sports Arena-15"], "operation"]], [[["Los Angeles Memorial Sports Arena-15"]], [["Coachella Valley Music and Arts Festival-3"]], ["operation"]], [[["Los Angeles Memorial Sports Arena-15"]], [["Coachella Valley Music and Arts Festival-20"]], [["Coachella Valley Music and Arts Festival-20"], "operation"]]], "golden_sentence": [["The arena seated up to 16,740 for boxing/wrestling, and 14,546 for hockey."], ["The 2017 festival was attended by 250,000 people and grossed $114.6 million."], ["", ""]]}, {"qid": "0a087ef552c3fcd80fdc", "term": "Viscosity", "description": "Resistance of a fluid to shear deformation", "question": "Does water have viscosity?", "answer": false, "facts": ["Viscosity is resistance of fluid to deformation.", "Water is not resistant to deformation."], "decomposition": ["What is viscosity?", "Is water #1?"], "evidence": [[[["Viscosity-1"]], ["operation"]], [[["Viscosity-1"], "operation"], ["no_evidence"]], [[["Viscosity-1"]], ["operation"]]], "golden_sentence": [["The viscosity of a fluid is a measure of its resistance to deformation at a given rate."]]}, {"qid": "ad608afd91b00cd3b69c", "term": "Pharmacology", "description": "Branch of biology concerning drugs", "question": "Did Julius Caesar read books on Pharmacology?", "answer": false, "facts": ["Pharmacology has its origins in the Middle Ages.", "The Middle Ages took place from 476 AD-1453 AD.", "Julius Caesar lived from 100 BC-44 BC."], "decomposition": ["When did Julius Caesar die?", "When did Pharmacology emerge as a field of study?", "Is #1 after or within #2?"], "evidence": [[[["Assassination of Julius Caesar-1"]], [["Pharmacology-7"]], ["operation"]], [[["Julius Caesar-1"]], [["Pharmacology-4"]], ["operation"]], [[["Julius Caesar-1"]], [["Pharmacology-7"]], ["operation"]]], "golden_sentence": [["Julius Caesar, the Roman dictator, was assassinated by a group of senators on the Ides of March (15 March) of 44 BC during a meeting of the Senate at the Theatre of Pompey in Rome."], ["Pharmacology developed in the 19th century as a biomedical science that applied the principles of scientific experimentation to therapeutic contexts."]]}, {"qid": "9a114ce75c2859505174", "term": "NATO", "description": "Intergovernmental military alliance of Western states", "question": "NATO doesn't recognize double triangle flag countries?", "answer": true, "facts": ["NATO is a members only alliance of several countries.", "Nepal has a double triangle flag.", "Nepal has not been recognized as a member of NATO."], "decomposition": ["What country has a flag with double triangles on it?", "Which countries are part of NATO?", "Is #1 not included in #2?"], "evidence": [[[["Flag of Nepal-5"]], [["Member states of NATO-3"]], ["operation"]], [[["Flag of Nepal-5"]], [["Member states of NATO-2"]], ["operation"]], [[["Flag of Nepal-1"]], [["NATO-30"]], ["operation"]]], "golden_sentence": [["Nepal has simply maintained its ancient tradition, while every other states have adopted a rectangular or square western version."], ["Twelve countries took part in the founding of NATO: Belgium, Canada, Denmark, France, Iceland, Italy, Luxembourg, the Netherlands, Norway, Portugal, the United Kingdom, and the United States."]]}, {"qid": "628607a24bed424c559f", "term": "Chlorine", "description": "Chemical element with atomic number 17", "question": "Could a dichromat probably easily distinguish chlorine gas from neon gas?", "answer": false, "facts": ["A dichromat is someone with color blindness that can have difficulty distinguishing red and green", "Chlorine gas is green or yellow-green", "Neon gas is red"], "decomposition": ["What two colors does a dichromat struggle to distinguish between?", "What color is Chlorine gas?", "What color is Neon Gas?", "Is #1 different from #2 and #3?"], "evidence": [[[["Dichromacy-1"]], [["Chlorine-1"]], [["Neon-3"]], ["operation"]], [[["Gene therapy for color blindness-15"]], [["Chlorine-2"]], [["Neon lamp-25"]], ["operation"]], [[["Dichromacy-1"]], [["Chlorine-11"]], [["Neon-1", "Neon-3"]], ["operation"]]], "golden_sentence": [[""], ["Chlorine is a yellow-green gas at room temperature."], ["Neon gives a distinct reddish-orange glow when used in low-voltage neon glow lamps, high-voltage discharge tubes and neon advertising signs."]]}, {"qid": "1d0d196e91ec97c86de1", "term": "Saltwater crocodile", "description": "species of reptile", "question": "Is the saltwater crocodile less endangered than the European otter?", "answer": true, "facts": ["The saltwater crocodile is listed as \"least concern\" on the International Union for the Conservation of Nature Red List.", "The European otter is listed as \"near threatened\" on the International Union for the Conservation of Nature Red List.", "The International Union for the Conservation of Nature Red List starts with \"least concern\", then \"near threatened\", \"vulnerable\", \"endangered\", \"critically endangered\", \"extinct in the wild\", and \"extinct\"."], "decomposition": ["What is the saltwater crocodile's conservation status on the IUCN red list?", "What is the European otter's conservation status on the IUCN red list?", "Is #1 less severe than #2?"], "evidence": [[[["Saltwater crocodile-1"]], [["Eurasian otter-8"]], [["Least-concern species-1"], "operation"]], [[["Saltwater crocodile-1"]], [["Eurasian otter-8"]], [["IUCN Red List-11"], "no_evidence", "operation"]], [[["Saltwater crocodile-1"]], [["Otter-12"]], ["operation"]]], "golden_sentence": [["It has been listed as Least Concern on the IUCN Red List since 1996."], ["It is listed as Near Threatened by the IUCN Red List."], [""]]}, {"qid": "ec52be6fd6062dc9f2a9", "term": "Muslim world", "description": "Muslim-majority countries, states, districts, or towns", "question": "Is the Muslim world hostile to Israel?", "answer": true, "facts": ["Israel, a small Middle Eastern nation which is considered the Jewish holy land, contains the ancient city of Jerusalem and other ancient holy sites.", "Jerusalem is the third most holy site for Muslims, after Mecca and Medina.", "Prior to English occupation in the 18th-20th centuries, the Muslim-based Ottoman Empire controlled Jerusalem.", "The Muslims want to reclaim Jerusalem and the surrounding holy lands."], "decomposition": ["What is the religious significance of Israel's historic cities to Muslims?", "Are the Muslims presently in control of Israel?", "Considering #1, do the Muslims wish to change the situation of #2?"], "evidence": [[[["Jerusalem-3"], "no_evidence"], [["Israel-68"], "no_evidence"], ["no_evidence", "operation"]], [[["Religious significance of Jerusalem-15"]], [["Islam in Israel-1"]], [["Muslim supporters of Israel-23"]]], [[["Holy Land-1"]], [["Israel-33"], "no_evidence"], [["2006 Lebanon War-1"], "no_evidence", "operation"]]], "golden_sentence": [["As a result, despite having an area of only 0.9 square kilometres (0.35\u00a0sq\u00a0mi), the Old City is home to many sites of seminal religious importance, among them the Temple Mount with its Western Wall, Dome of the Rock and al-Aqsa Mosque, and the Church of the Holy Sepulchre."], [""]]}, {"qid": "2fe64724db0e4ca81f61", "term": "Carnation Revolution", "description": "revolution", "question": "Was the Carnation Revolution the deadliest revolution in Europe?", "answer": false, "facts": ["The Carnation Revolution was initially a 25 April 1974 military coup in Lisbon which overthrew the authoritarian Estado Novo regime.", "Its name arose from the fact that almost no shots were fired, and Celeste Caeiro offered carnations to the soldiers when the population took to the streets to celebrate the end of the dictatorship; other demonstrators followed suit, and carnations were placed in the muzzles of guns and on the soldiers' uniforms.", "Portugal is a country located mostly on the Iberian Peninsula, in southwestern Europe."], "decomposition": ["Why was the Carnation Revolution so named?", "Does #1 imply that no lives were lost?", "Did the Revolution take place in Europe?", "Is #2 or #3 negative?"], "evidence": [[[["Carnation Revolution-2"]], [["Carnation Revolution-7"]], [["Carnation Revolution-1", "Portugal-1"]], ["operation"]], [[["Carnation Revolution-2"]], ["operation"], [["Carnation Revolution-1", "Lisbon-1"]], ["operation"]], [[["Carnation Revolution-2"]], ["operation"], [["Carnation Revolution-1"]], ["operation"]]], "golden_sentence": [["Its name arose from the fact that almost no shots were fired, and Celeste Caeiro offered carnations to the soldiers when the population took to the streets to celebrate the end of the dictatorship; other demonstrators followed suit, and carnations were placed in the muzzles of guns and on the soldiers' uniforms."], [""], ["", "Portugal (Portuguese:\u00a0[pu\u027etu\u02c8\u0263al]), officially the Portuguese Republic (Portuguese: Rep\u00fablica Portuguesa [\u0281\u025b\u02c8pu\u03b2lik\u0250 pu\u027etu\u02c8\u0263ez\u0250]), is a country located mostly on the Iberian Peninsula, in southwestern Europe."]]}, {"qid": "7963b2ce1553ac0269f8", "term": "Surveillance", "description": "monitoring of behavior, activities, or other changing information", "question": "Can you conduct surveillance from a teddy bear?", "answer": true, "facts": ["Surveillance is the act of monitoring or observation", "Nanny cams are used for surveillance of behavior when a family leaves their home and/or children in the care of a third party", "Nanny cams are often placed in common household objects like teddy bears"], "decomposition": ["In what kind of context/environment are nanny cams used for surveillance?", "Would a teddy bear accommodate a nanny can and be commonly found in #1?"], "evidence": [[[["Hidden camera-8"]], [["Hidden camera-8"]]], [[["Hidden camera-1"], "no_evidence"], [["Teddy bear-1"], "no_evidence", "operation"]], [[["Hidden camera-8"]], ["no_evidence"]]], "golden_sentence": [[""], [""]]}, {"qid": "1ca3a40c184bfed91e26", "term": "Osama bin Laden", "description": "Co-founder of al-Qaeda", "question": "Was a Tiny House ceiling out of Osama bin Laden's reach?", "answer": false, "facts": ["Osama bin Laden was 6'5\" tall.", "Tiny Houses have ceilings that are between 7 and 8 feet tall.", "The standing reach of the average 6 foot man is around 8 feet."], "decomposition": ["How tall is Osama bin Laden?", "What is the standing reach of someone who is #1?", "How tall is the ceiling of a typical tiny house?", "Is #2 less than #3?"], "evidence": [[[["Osama bin Laden-14"]], ["operation"], [["Tiny house movement-3"], "no_evidence"], ["no_evidence", "operation"]], [[["Osama bin Laden-14"]], ["no_evidence"], [["Tiny house movement-25"], "no_evidence"], ["no_evidence", "operation"]], [[["Osama bin Laden-14"]], [["NBA Draft Combine-3"], "no_evidence"], [["Tiny house movement-25"], "no_evidence"], ["operation"]]], "golden_sentence": [["The FBI described bin Laden as an adult as tall and thin, between 1.93\u00a0m (6\u00a0ft 4\u00a0in) and 1.98\u00a0m (6\u00a0ft 6\u00a0in) in height and weighing about 73 kilograms (160\u00a0lb), although the author Lawrence Wright, in his Pulitzer Prize-winning book on Al-Qaeda, The Looming Tower, writes that a number of bin Laden's close friends confirmed that reports of his height were greatly exaggerated, and that bin Laden was actually \"just over 6 feet (1.8\u00a0m) tall\"."], [""]]}, {"qid": "bd9eb97d13d56f33bfbd", "term": "Northern fur seal", "description": "The largest fur seal in the northern hemisphere", "question": "Is a northern fur seal needing emergency surgery in July likely a safe anesthesia candidate?", "answer": true, "facts": ["Northern fur seals fast throughout the mating season", "It is recommended that patients, including animals, fast for a time before surgery that requires anesthesia ", "Peak mating season for northern fur seals occurs in June and July"], "decomposition": ["What is recommended for patients needing anesthesia?", "What do northern fur seals do in July?", "Does #2 include #1?"], "evidence": [[[["Anesthesia-12"]], [["Northern fur seal-17"]], ["operation"]], [[["Anesthesia-7"]], [["Northern fur seal-17"]], ["operation"]], [[["Anesthesia-34"], "no_evidence"], [["Northern fur seal-17"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "6731b2b1797c38b7ff3f", "term": "Red Sea", "description": "Arm of the Indian Ocean between Arabia and Africa", "question": "Would it be very difficult for Nuno Gomes to dive to the Red Sea's deepest point?", "answer": true, "facts": ["The Red Sea has a maximum depth of 3,040 m (9,970 ft).", "Nuno Gomes' deepest dive in the Red Sea to date is 318 metres (1,043 ft)."], "decomposition": ["How deep is the Red Sea's maximum depth?", "What is the deepest Nuno Gomes can dive?", "Is #1 greater than #2?"], "evidence": [[[["Red Sea-2"]], [["Nuno Gomes (diver)-2", "Nuno Gomes (diver)-4"]], ["operation"]], [[["Red Sea-2"]], [["Nuno Gomes (diver)-4"]], ["operation"]], [[["Red Sea-2"], "no_evidence"], [["Nuno Gomes (diver)-2"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["It has a maximum depth of 3,040\u00a0m (9,970\u00a0ft) in the central Suakin Trough, and an average depth of 490\u00a0m (1,608\u00a0ft)."], ["Gomes used self-contained underwater breathing apparatus to dive to a depth of 318 metres (1,044\u00a0ft) in the sea.", "citation needed] Gomes is also a renowned cave diver and holds the official current Guinness World Record for the deepest cave dive, done in Boesmansgat cave (South Africa), to a depth of 283\u00a0m (927\u00a0ft), in 1996."]]}, {"qid": "3761a21e99cc3af7f948", "term": "Breast cancer", "description": "cancer that originates in the mammary gland", "question": "Is breast cancer associated with a ribbon?", "answer": true, "facts": ["Breast cancer is one of many diseases associated with a specific color of ribbon.", "Breast cancer's ribbon is pink."], "decomposition": ["Which diseases are associated with a (certain color of) ribbon?", "Is breast cancer included in #1?"], "evidence": [[[["Awareness ribbon-3"]], ["operation"]], [[["Awareness ribbon-3"]], ["operation"]], [[["Awareness ribbon-15"]], [["Awareness ribbon-15"]]]], "golden_sentence": [["Other health and social concerns which have adopted coloured ribbons include Alzheimer's disease, pancreatic cancer (purple), HIV/AIDS (red), bipolar disorder (green), and brain disorder or disability (silver)."]]}, {"qid": "54ccbeecd69ae91abfe2", "term": "Common warthog", "description": "Wild member of the pig family", "question": "Could common warthog be useful for scrimshaw?", "answer": true, "facts": ["Scrimshaw is the process of carving designs or symbols into materials such as ivory, whalebone, and tusks.", "The common warthog has two sets of long tusks.", "The common warthog has large teeth that are harnessed for ivory.", "The common warthog is not an endangered species."], "decomposition": ["What materials can be used in scrimshaw?", "Do warthogs have any of the things in #1?"], "evidence": [[[["Scrimshaw-1"]], [["Phacochoerus-2"], "operation"]], [[["Scrimshaw-1"]], [["Phacochoerus-2"]]], [[["Scrimshaw-1"]], [["Common warthog-3"]]]], "golden_sentence": [["Scrimshaw is scrollwork, engravings, and carvings done in bone or ivory."], ["Although covered in bristly hairs, their bodies and heads appear largely naked from a distance, with only the crest along the back, and the tufts on their cheeks and tails being obviously haired."]]}, {"qid": "189a8c12b9e928d8a9f9", "term": "Spice Girls", "description": "British girl group", "question": "Tata Hexa can accomodate every Spice Girl?", "answer": true, "facts": ["The Spice Girls is a five woman musical group from Britain.", "The Tata Hexa is a car with 6 and 7 seat capacities."], "decomposition": ["How many women are in the Spice Girls group?", "How many people can the Tata Hexa seat?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Spice Girls-1"]], [["Tata Aria-2", "Tata Aria-5"], "no_evidence"], ["operation"]], [[["Spice Girls-1"]], [["Tata Hexa-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Spice Girls-1"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["The group comprises Melanie Brown, also known as Mel B (\"Scary Spice\"), Melanie Chisholm, also known as Mel C (\"Sporty Spice\"), Emma Bunton (\"Baby Spice\"), Geri Halliwell (\"Ginger Spice\"), and Victoria Beckham (\"Posh Spice\")."], ["", ""]]}, {"qid": "a6b239d59dd8df71b49f", "term": "Dosa", "description": "Thin pancakes originating from South India", "question": "Would lumberjacks get full after eating three dosa?", "answer": false, "facts": ["Dosa are thin rice pancakes from South India.", "One dosa is approximately 110 calories.", "The average lumberjack would eat 8000 calories per day."], "decomposition": ["What is a Dosa?", "How many calories are in #1?", "How many calories does a lumberjack need per day?", "Is 3 times #2 a significant amount of #3?"], "evidence": [[[["Dosa-1"]], ["no_evidence"], [["Food energy-14"]], ["operation"]], [[["Dosa-1"]], ["no_evidence"], ["no_evidence"], ["operation"]], [[["Dosa-1"]], ["no_evidence"], [["Food energy-14", "Lumberjack-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["A dosa is a rice pancake, originating from South Asia, made from a fermented batter."], [""]]}, {"qid": "e925a2cf42b2a6f440ce", "term": "Gorilla", "description": "Genus of mammals", "question": "Do gorillas fight with panda bears?", "answer": false, "facts": ["Gorilla distribution is limited to areas of the continent of Africa", "Panda bears are found in the wild only in China"], "decomposition": ["What is the range for Gorillas?", "What is the range for pandas?", "Does #1 overlap #2?"], "evidence": [[[["Gorilla-2"]], [["Giant panda-2"]], ["operation"]], [[["Gorilla-1"]], [["Giant panda-1"]], ["operation"]], [[["Gorilla-2"]], [["Giant panda-2"]], ["operation"]]], "golden_sentence": [["Although their range covers a small percentage of Sub-Saharan Africa, gorillas cover a wide range of elevations."], ["The giant panda lives in a few mountain ranges in central China, mainly in Sichuan, but also in neighbouring Shaanxi and Gansu."]]}, {"qid": "e33a1a7266e6773c5a81", "term": "Foreign and Commonwealth Office", "description": "Ministry of Foreign Affairs of the United Kingdom", "question": "Is the Foreign and Commonwealth Office a European political agency?", "answer": true, "facts": ["The Office is part of the government of the United Kingdom.", "The United Kingdom is in Europe."], "decomposition": ["What country is the Foreign and Commonwealth Office part of?", "Is #1 located in Europe?"], "evidence": [[[["Foreign and Commonwealth Office-1"]], [["Outline of the United Kingdom-1"], "operation"]], [[["Foreign and Commonwealth Office-4"]], [["Turkey\u2013United Kingdom relations-13"]]], [[["Foreign and Commonwealth Office-1"]], [["Outline of the United Kingdom-1"]]]], "golden_sentence": [["The Foreign and Commonwealth Office (FCO), commonly called the Foreign Office (which was the formal name of its predecessor until 1968), or British Foreign Office, is a department of the Government of the United Kingdom."], ["The following outline is provided as an overview of and topical guide to the United Kingdom of Great Britain and Northern Ireland; a sovereign state in Europe, commonly known as the United Kingdom (UK), or Britain."]]}, {"qid": "14777a48904845b75cee", "term": "Knight", "description": "An award of an honorary title for past or future service with its roots in chivalry in the Middle Ages", "question": "Are there any official American knights?", "answer": false, "facts": ["The English monarchy bestows the title of knighthood upon deserving English citizens.", "They only knight English people.", "The American government does not do knightings of its own."], "decomposition": ["Which kind government bestows knighthood on its citizens?", "Would #1 confer knighthood on a citizen of another country?", "Is the American government an example of #1?", "Is #2 or #3 positive?"], "evidence": [[[["Knight-1"]], [["Knight-3"], "no_evidence"], [["Federal government of the United States-1"]], ["operation"]], [[["Knight-1", "Orders, decorations, and medals of the United Kingdom-1"], "no_evidence"], [["Order of the British Empire-6"], "no_evidence"], ["no_evidence", "operation"], ["no_evidence", "operation"]], [[["Order of the British Empire-2"]], [["Order of the British Empire-3"]], [["Federal government of the United States-1"], "operation"], ["operation"]]], "golden_sentence": [[""], [""], ["The federal government of the United States (U.S. federal government) is the national government of the United States, a federal republic in North America, composed of 50 states, a federal district, five major self-governing territories and several island possessions."]]}, {"qid": "34e103f6dd28aa485b2d", "term": "Hypertension", "description": "Long term medical condition", "question": "Are two cans of Campbell's Soup a day good for hypertension?", "answer": false, "facts": ["Hypertension is a medical condition in which the blood pressure is high.", "Salt increases blood pressure.", "Campbell's Soup has an average of 1400 to 1800 mg of sodium (salt).", "The FDA states that sodium intake per day should not exceed 2300 mg."], "decomposition": ["Which substance has generated controversy about Campbell's canned soups regarding health concerns?", "Is excess of #1 good for people with hypertension?"], "evidence": [[[["Chicken soup-45"]], [["Hypertension-24"]]], [[["Soup-11"]], [["Hypertension-16"]]], [[["Campbell Soup Company-29"]], [["Sodium-36"]]]], "golden_sentence": [["Campbell's claims production of a chicken noodle soup that will find broad consumer acceptance\u2014in short, that will sell\u2014is very difficult, so it has to balance healthfulness with sodium content."], [""]]}, {"qid": "64c49be9912e63f5e1da", "term": "Underworld", "description": "The mythic Relm of the Dead, located far underground (aka, Hades; Underworld)", "question": "Can Kit & Kaboodle hypothetically help someone past the Underworld gates?", "answer": false, "facts": ["The Underworld is guarded by a beast known as Cerberus.", "Cerberus is a three-headed dog.", "Cerberus eats the raw flesh of anyone that tries to escape the Underworld.", "Kit & Kaboodle is a brand of cat food. "], "decomposition": ["What guards the gates of the Underworld?", "What kind of creature is #1?", "What kind of animal is Kit & Kaboodle meant for?", "Is #3 the same as #2?"], "evidence": [[[["Cerberus-1"]], [["Cerberus-1"]], ["no_evidence"], ["operation"]], [[["Cerberus-1"]], [["Cerberus-1"]], [["Jaclyn Linetsky-3"], "no_evidence"], ["no_evidence", "operation"]], [[["Cerberus-1"]], [["Cerberus-1"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["In Greek mythology, Cerberus (/\u02c8s\u025c\u02d0rb\u0259r\u0259s/; Greek: \u039a\u03ad\u03c1\u03b2\u03b5\u03c1\u03bf\u03c2 K\u00e9rberos [\u02c8kerberos]), often referred to as the hound of Hades, is a multi-headed dog that guards the gates of the Underworld to prevent the dead from leaving."], [""]]}, {"qid": "1f5ae49933529a734433", "term": "British cuisine", "description": "culinary traditions of the United Kingdom", "question": "Could an American confuse breakfast in British cuisine for dinner?", "answer": true, "facts": ["In British Cuisine, baked beans are served with toast for breakfast.", "In the US, baked beans are served alongside barbecue dinners.", "British 'Full breakfasts' include grilled vegetables like mushrooms and whole cherry tomatoes.", "Grilled mushrooms and tomatoes are used more often in evening dishes in the US."], "decomposition": ["What foods are part of a traditional British breakfast?", "What foods are part of a traditional American dinner?", "Is there overlap between #1 and #2?"], "evidence": [[[["Breakfast-63"]], [["Meal-16"]], ["operation"]], [[["Breakfast-63"]], [["Burger King breakfast sandwiches-6"]], ["operation"]], [[["Full breakfast-1"]], [["Mushroom-28", "Tomato-85"], "no_evidence"], ["operation"]]], "golden_sentence": [["The traditional breakfast most associated with Britain and Ireland remains, however, the full breakfast of eggs (fried, scrambled, or poached) with bacon and sausages, usually with mushrooms, tomatoes, baked beans, fried bread, black pudding or white pudding, and toast."], ["Dinner usually refers to a significant and important meal of the day, which can be the noon or the evening meal."]]}, {"qid": "05f3232699d385d3f7b1", "term": "Reproduction", "description": "Biological process by which new organisms are generated from one or more parent organisms", "question": "Is it true that gay male couples cannot naturally reproduce?", "answer": false, "facts": ["Gay men can have any of the various sex organs that humans have.", "Trans men will sometimes become pregnant with their significant other before transitioning medically. "], "decomposition": ["What defines a male gender?", "What organs are needed to impregnate someone?", "What organs are needed to carry a pregnancy?", "Does #1 exclude persons with either of #2 or #3?"], "evidence": [[[["Male-4"]], [["Sex organ-1"]], [["Uterus-1"]], ["operation"]], [[["Gender-1"]], [["Male reproductive system-2"]], [["Female reproductive system-1"]], ["operation"]], [[["Male-1"]], [["Male reproductive system-2"]], [["Female reproductive system-1", "Pregnancy-1"]], ["operation"]]], "golden_sentence": [[""], ["The testis in the male, and the ovary in the female, are called the primary sex organs."], [""]]}, {"qid": "9496f5445e1d8b749098", "term": "Cheeseburger", "description": "hamburger topped with cheese", "question": "Is a krabby patty similar to a cheeseburger?", "answer": true, "facts": ["A krabby patty is a fictional sandwich featuring a patty on a split bun with toppings like lettuce, onion, and tomato.", "A hamburger is typically served on a bun and offers toppings like lettuce, onion, and tomato."], "decomposition": ["What are the ingredients of a Krabby Patty?", "What are the ingredients of a cheeseburger?", "Is there significant overlap between #1 and #2?"], "evidence": [[[["Krabby Patty-4"]], [["Cheeseburger-10"]], ["operation"]], [[["Krabby Patty-4"]], [["Cheeseburger-1"]], ["operation"]], [[["Krabby Patty-4"]], [["Cheeseburger-1"]], ["operation"]]], "golden_sentence": [["The Krabby Patty is made out of a frozen hamburger with buns, the patty, pickles, lettuce, tomatoes, cheese, ketchup, mustard, and onions and with a Krabby Patty secret formula, though said secret formula has never been revealed in the series."], ["The ingredients used to create cheeseburgers follow similar patterns found in the regional variations of hamburgers, although most start with ground beef."]]}, {"qid": "259b8b83d474a5a8e66b", "term": "Ben & Jerry's", "description": "American ice cream company", "question": "Are both founders of Ben & Jerry's still involved in the company?", "answer": false, "facts": ["Ben & Jerry's was founded by Ben Cohen and Jerry Greenfield.", "The founders sold the company to Unilever in 2000."], "decomposition": ["Who were the founders of Ben & Jerry's ice cream?", "Who owns Ben & Jerry's now?", "Is #2 the same as #1?"], "evidence": [[[["Ben & Jerry's-2"]], [["Ben & Jerry's-1"]], ["operation"]], [[["Ben & Jerry's-2"]], [["Ben & Jerry's-10"]], ["operation"]], [[["Ben & Jerry's-2"]], [["Ben & Jerry's-10"]], ["operation"]]], "golden_sentence": [[""], ["Today it operates globally as a fully owned subsidiary of Unilever."]]}, {"qid": "f2b7ce0c502ad3c2523a", "term": "Apollo 13", "description": "A failed crewed mission to land on the Moon", "question": "Was ship that recovered Apollo 13 named after a World War II battle?", "answer": true, "facts": ["Apollo 13 was recovered by the USS Iwo Jima.", "Iwo Jima was captured from the Imperial Japanese Army during World War II by the US in a conflict called the Battle of Iwo Jima."], "decomposition": ["Which ship recovered Apollo 13 crew?", "What was #1 named for?", "Did #2 occur during World War II?"], "evidence": [[[["USS Iwo Jima (LPH-2)-13"]], [["USS Iwo Jima (LPH-2)-1"]], [["Battle of Iwo Jima-1"]]], [[["Apollo 13-55"]], [["USS Iwo Jima (LPH-2)-1"]], ["operation"]], [[["Apollo 13-55"]], [["Iwo Jima-3"]], [["Iwo Jima-19"]]]], "golden_sentence": [[""], [""], ["The Battle of Iwo Jima (19 February \u2013 26 March 1945) was a major battle in which the United States Marine Corps and Navy landed on and eventually captured the island of Iwo Jima from the Imperial Japanese Army (IJA) during World War II."]]}, {"qid": "9e516be5fc1ecead20b7", "term": "Ringo Starr", "description": "British musician, drummer of the Beatles", "question": "Would Ringo Starr avoid the pot roast at a restaurant?", "answer": true, "facts": ["Ringo Starr is a vegetarian.", "Vegetarianism is the practice of abstaining from the consumption of meat.", "Pot roast is a braised beef dish made by browning a roast-sized piece of beef before slow cooking the meat in a covered dish, sometimes with vegetables, in or over liquid."], "decomposition": ["What dietary system does Ringo Starr follow?", "What type of foods are not allowed to be eaten by someone following #1?", "What is pot roast made of?", "Is #3 part of #2?"], "evidence": [[[["Ringo Starr-71"]], [["Vegetarianism-1"]], [["Pot roast-1"]], ["operation"]], [[["Ringo Starr-71"]], [["Vegetarianism-1"]], [["Pot roast-1"]], [["Beef-1"], "operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence"], ["no_evidence"]]], "golden_sentence": [["Starr is a vegetarian and meditates daily."], [""], ["Pot roast is a braised beef dish made by browning a roast-sized piece of beef before slow cooking the meat in a covered dish, sometimes with vegetables, in or over liquid."]]}, {"qid": "d9a7d0c37f2a44abc5c7", "term": "Hamas", "description": "Palestinian Sunni-Islamist fundamentalist organization", "question": "Is starving Hamas agent eating pig bad?", "answer": false, "facts": ["Hamas is a Sunni-Islam fundamentalist group that strictly prohibits the eating of a pig.", "Sunni Islam has a concept called the Law of Necessity.", "The Law of Necessity states that, \"That which is necessary makes the forbidden permissible.\""], "decomposition": ["What religion do members of Hamas follow?", "What foods are forbidden in #1?", "Are there no exceptions to #2?"], "evidence": [[[["Hamas-2"]], [["Islamic dietary laws-14"]], ["no_evidence", "operation"]], [[["Hamas-1"]], [["Islam-50"]], [["Islamic dietary laws-3"], "operation"]], [[["Hamas-2"]], [["Halal-6"]], [["Halal-6"]]]], "golden_sentence": [[""], [""]]}, {"qid": "c9c2791c6278853c6a5a", "term": "Bern", "description": "Place in Switzerland", "question": "Are Citizens of Bern Switzerland are descendants of Genghis Khan?", "answer": true, "facts": ["Genghis Khan had sixteen children.", "1 in 200 men are direct descendants of Genghis Khan.", "Switzerland has a large Asian immigration population which was around 19,000 in 2018."], "decomposition": ["What ethnic groups contain much of Genghis Khan's descendants?", "Is there a large population of any of #1 in Bern?"], "evidence": [[[["Descent from Genghis Khan-2"], "no_evidence"], [["Bern-39"], "no_evidence"]], [[["Descent from Genghis Khan-2", "Descent from Genghis Khan-22"]], ["no_evidence"]], [[["Genghis Khan-2"], "no_evidence"], [["Bern-39"], "no_evidence", "operation"]]], "golden_sentence": [[""], ["The population was made up of 44,032 Swiss men (35.4% of the population) and 15,092 (12.1%) non-Swiss men."]]}, {"qid": "82d642d27463e4d34d9a", "term": "Lord Voldemort", "description": "Fictional character of Harry Potter series", "question": "Is Lord Voldemort associated with a staff member of Durmstrang?", "answer": true, "facts": ["Igor Karkaroff is the headmaster of Durmstrang school.", "Karkaroff is a former Death Eater.", "The Death Eaters were Voldemort's minions."], "decomposition": ["Who is the headmaster of Durmstrang school?", "What did #1 part of in the past?", "Is #2 related to Lord Voldemort?"], "evidence": [[[["Places in Harry Potter-31"], "no_evidence"], [["Places in Harry Potter-32"]], ["no_evidence"]], [[["Death Eater-30"]], [["Death Eater-31"]], [["Death Eater-1"]]], [[["Places in Harry Potter-31"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "c4d31ae68fd04a9285a9", "term": "Will Ferrell", "description": "American actor, comedian, producer, writer and businessman", "question": "Does Dean Cain have less days to birthday than Will Ferrell every 4th of July?", "answer": false, "facts": ["Will Ferrell was born on July 16th.", "Dean Cain was born on July 31st."], "decomposition": ["What day of the year was Will Ferrell born?", "What day of the year was Dean Cain born?", "How many days away from July fourth is #1?", "How many days away from July fourth is #2?", "Is #4 less than #3?"], "evidence": [[[["Will Ferrell-1"]], [["Dean Cain-1"]], ["operation"], ["operation"], ["operation"]], [[["Will Ferrell-1"]], [["Dean Cain-1"]], ["operation"], ["operation"], ["operation"]], [[["Will Ferrell-1"]], [["Dean Cain-1"]], ["operation"], ["operation"], ["operation"]]], "golden_sentence": [["John William Ferrell (/\u02c8f\u025br\u0259l/; born July 16, 1967) is an American actor, comedian, producer, writer, and businessman."], ["Dean George Cain (n\u00e9 Tanaka; born July 31, 1966) is an American actor, producer, television show host, and former football player, best known for playing the role of Clark Kent/Superman in the TV series Lois & Clark: The New Adventures of Superman."]]}, {"qid": "8be90054240154b79866", "term": "Rahul Dravid", "description": "Indian cricketer", "question": "Does Rahul Dravid belong to the family Gryllidae?", "answer": false, "facts": ["Crickets (also known as \"true crickets\"), of the family Gryllidae, are insects related to bush crickets, and, more distantly, to grasshoppers.", "Cricket is a bat-and-ball game played between two teams of eleven players on a field at the centre of which is a 20-metre (22-yard) pitch with a wicket at each end, each comprising two bails balanced on three stumps.", "Human beings belong to the family Hominidae."], "decomposition": ["What kind of creature is Rahul Dravid?", "Which family does #1 belong to?", "Is #2 the same as Gryllidae?"], "evidence": [[[["Rahul Dravid-1"]], [["Human-6"]], ["operation"]], [[["Homo sapiens-1", "Rahul Dravid-1"]], [["Hominidae-1"]], ["operation"]], [[["Rahul Dravid-1"]], [["Hominidae-20"]], ["operation"]]], "golden_sentence": [["Rahul Sharad Dravid (/\u02ccr\u0259hu\u02d0l dr\u0259v\u026ad/ (listen); born 11 January 1973) is a former Indian cricketer and captain of the Indian national team."], [""]]}, {"qid": "7e2c328c62f5d103c712", "term": "United States presidential election", "description": "type of election in the United States", "question": "Will bumblebees derail the United States presidential election?", "answer": false, "facts": ["The United States presidential election is held in November of each year.", "Bees die off during the winter months except for the queen bee.", "During November, bees go into their hives and hibernate."], "decomposition": ["When is the presidential election?", "What is a necessary characteristic for something to disrupt the election? ", "Are bees #2 in #1?"], "evidence": [[[["United States presidential election-3"]], ["no_evidence"], ["operation"]], [[["Election Day (United States)-1"]], ["no_evidence"], [["Bumblebee-30"], "operation"]], [[["2020 United States presidential election-2"]], [["Electrical disruptions caused by squirrels-9"]], ["operation"]]], "golden_sentence": [["Presidential elections occur quadrennially with registered voters casting their ballots on Election Day, which since 1845 has been the first Tuesday after November 1."]]}, {"qid": "d95d9b6f84dcb0b7ca32", "term": "Ludacris", "description": "American rapper and actor", "question": "Does Ludacris have Greek heritage?", "answer": true, "facts": ["Ludacris's real name is Christopher Brian Bridges", "Christopher is a name derived from Greek origins"], "decomposition": ["What is Ludacris's real name?", "Where is #1 derived from?"], "evidence": [[[["Ludacris-1"]], ["no_evidence"]], [[["Ludacris-1"]], [["Christopher-1"]]], [[["Ludacris-1"]], [["Ludacris-3"], "operation"]]], "golden_sentence": [["Christopher Brian Bridges (born September 11, 1977), better known by his stage name Ludacris (/\u02c8lu\u02d0d\u0259kr\u026as/), is an American rapper and actor."]]}, {"qid": "d709f3014d53be3bddda", "term": "Statue of Freedom", "description": "19th-century statue by Thomas Crawford on top of the US Capitol", "question": "Can you see the Statue of Freedom from the Statue of Liberty?", "answer": false, "facts": ["The Statue of Freedom is in Washington, D.C. on the Capitol Building", "The Statue of Liberty is in New York City"], "decomposition": ["Where is the Statue of Freedom located?", "Where is the Statue of Liberty located?", "Is #1 within reasonable range of visibility from #2?"], "evidence": [[[["Statue of Freedom-2"]], [["Statue of Liberty-1"]], [["Statue of Freedom-2"]]], [[["Statue of Freedom-1"]], [["Statue of Liberty-1"]], ["operation"]], [[["Statue of Freedom-1"]], [["Statue of Liberty-1"]], ["operation"]]], "golden_sentence": [[""], ["The Statue of Liberty (Liberty Enlightening the World; French: La Libert\u00e9 \u00e9clairant le monde) is a colossal neoclassical sculpture on Liberty Island in New York Harbor in New York, in the United States."], [""]]}, {"qid": "fb3de644eb764a4b4857", "term": "Spinach", "description": "species of plant", "question": "Has spinach been a source of power in a comic movie?", "answer": true, "facts": ["The comic character Popeye uses spinach as a source of power.", "A movie was made about Popeye.", "Popeye consumes spinach as a source of power in the movie."], "decomposition": ["Which movie was made for comic character Popeye?", "What was Popeye's source of power in #1", "Is #2 spinach?"], "evidence": [[[["Popeye-53"]], [["Popeye-68"]], [["Popeye-68"], "operation"]], [[["Popeye (film)-1"]], [["Popeye-6"]], ["operation"]], [[["Popeye the Sailor (film)-1"]], [["Popeye the Sailor (film)-4"]], ["operation"]]], "golden_sentence": [["Director Robert Altman used the character in Popeye, a 1980 live-action musical feature film, starring Robin Williams as Popeye (his first starring movie role), Shelley Duvall as Olive Oyl, and Paul L. Smith as Bluto, with songs by Harry Nilsson and Van Dyke Parks."], ["Popeye later attributed his strength to spinach."], [""]]}, {"qid": "ab4c5174eca099db84b1", "term": "Chlorophyll", "description": "group of chemical compounds", "question": "For Hostas to look their best, do they need lots of chlorophyll?", "answer": true, "facts": ["Hostas are characterized by large green striped leaves.", "The green color in plants is attributed to chlorophyll. "], "decomposition": ["What color is a visually appealing hosta?", "Do the get #1 from chlorophyll?"], "evidence": [[[["Hosta-2"]], [["Chlorophyll-2"], "operation"]], [[["Hosta-6"], "no_evidence"], [["Chloroplast-1", "Hosta-2"], "operation"]], [[["Hosta-2"]], [["Chlorophyll-2"], "operation"]]], "golden_sentence": [["Natural mutations of native species are known with yellow-green (\"gold\") colored leaves or with leaf variegation (either white/cream or yellowish edges or centers)."], [""]]}, {"qid": "13147091d12acd2f707b", "term": "Jeremy Irons", "description": "English actor", "question": "Did Jeremy Irons master sweep picking as a child?", "answer": false, "facts": ["Jeremy Irons was the drummer and harmonica player in a four-man school band called the Four Pillars of Wisdom.", "Sweep picking is a guitar playing technique."], "decomposition": ["What kind of musical instrument involves sweet picking?", "What musical instruments did Jeremy Irons play in the school band Four Pillars of Wisdom?", "Is #1 included in #2?"], "evidence": [[[["Sweep picking-1"]], [["Jeremy Irons-5"]], ["operation"]], [[["Sweep picking-1"]], [["Jeremy Irons-5"]], ["operation"]], [[["Guitar picking-14"]], [["Jeremy Irons-5"]], ["operation"]]], "golden_sentence": [[""], ["He was the drummer and harmonica player in a four-man school band called the Four Pillars of Wisdom."]]}, {"qid": "583e621c0ead018de377", "term": "Cookie Monster", "description": "character from the television series Sesame Street", "question": "Would the Cookie Monster decline an offer of free Keebler products?", "answer": false, "facts": ["The Cookie Monster has an enormous appetite and craving for cookies.", "The Keebler Company is an American cookie and cracker manufacturer."], "decomposition": ["What type of food does the Cookie Monster enjoy the most?", "What types of food are produced by the Keebler Company?", "Is #1 not included in #2?"], "evidence": [[[["Cookie-22"]], [["Keebler Company-1"]], ["operation"]], [[["Cookie Monster-1"]], [["Keebler Company-1"]], ["operation"]], [[["Cookie Monster-2"]], [["Keebler Company-9"]], ["operation"]]], "golden_sentence": [["Cookie Monster is a Muppet on the long-running children's television show Sesame Street who is best known for his voracious appetite for cookies and his famous eating phrases, such as \"Me want cookie!"], ["The Keebler Company is an American cookie and cracker manufacturer."]]}, {"qid": "d6a79d3a2465a47ae176", "term": "Winemaking", "description": "the production of wine, starting with the selection of the fruit, its fermentation into alcohol, and the bottling of the finished liquid", "question": "Are grapes essential to winemaking?", "answer": false, "facts": ["Winemaking involves a process known as fermentation where energy is extracted from carbohydrates.", "A cup of cherries has about 19 grams of carbohydrates.", "Cherry wine does not contain any grapes."], "decomposition": ["In winemaking, what is the process in which energy is drawn?", "In #1, where does the energy come from?", "Are grapes the only thing that contains #2?"], "evidence": [[[["Winemaking-19"]], [["Winemaking-19"]], ["no_evidence"]], [[["Ethanol fermentation-1"], "no_evidence"], [["Winemaking-1", "Yeast-27"], "no_evidence"], [["Fruit brandy-1"], "operation"]], [[["Winemaking-32"]], [["Winemaking-7"]], [["Fruit-36"]]]], "golden_sentence": [["In the United States mechanical harvesting is seldom used for premium winemaking because of the indiscriminate picking and increased oxidation of the grape juice."], [""]]}, {"qid": "d7a06bedfdbaa0fd622b", "term": "QWERTY", "description": "keyboard layout where the first line is \"QWERTYUIOP\"", "question": "Can the majority of vowels be typed on the first line of a QWERTY keyboard?", "answer": true, "facts": ["In English the vowels consist of A, E, I, O, U, and sometimes Y.", "The first line of the QWERTY keyboard contains the vowels E, I, O, U, and Y.", "A majority means more than half of the total."], "decomposition": ["What letters are vowels in the English language?", "What are the letters on the first line of a Qwerty keyboard?", "Is more than half of the letters listed in #1 also listed in #2?"], "evidence": [[[["English alphabet-20"]], [["QWERTY-1", "Ray Tomlinson-5"], "no_evidence"], ["operation"]], [[["Vowel-49"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Vowel-49"]], [["QWERTY-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["The letters A, E, I, O, and U are considered vowel letters, since (except when silent) they represent vowels, although I and U represent consonants in words such as \"onion\" and \"quail\" respectively."], ["The name comes from the order of the first six keys on the top left letter row of the keyboard (Q W E R T Y).", ""]]}, {"qid": "01225003f2b1cd4c516e", "term": "Golden eagle", "description": "species of bird", "question": "Is the Golden eagle considered a scavenger bird?", "answer": false, "facts": ["Scavengers are defined as animals that feed on dead carcasses of animals they have not killed themselves.", "Vultures are scavengers that hover in the air and swoop down to feed once they see an animal killed by another animal.", "The Golden eagle has sharp talons to hunt its own prey.", "Golden eagles kill and feed on hares, rabbits, and ground squirrels."], "decomposition": ["Who kills the prey that scavengers feed on?", "Who kills the prey that Golden eagles feed on?", "Is #1 the same as #2?"], "evidence": [[[["Scavenger-1"]], [["Golden eagle-22"]], ["operation"]], [[["Scavenger-1"]], [["Golden eagle-28"]], ["operation"]], [[["Scavenger-1"]], [["Golden eagle-22"]], ["operation"]]], "golden_sentence": [["Decomposers and detritivores complete this process, by consuming the remains left by scavengers."], [""]]}, {"qid": "d1affe92c8e86edc5ae0", "term": "Pyrenees", "description": "Range of mountains in southwest Europe", "question": "Can an elite runner circle the Pyrenees in one day?", "answer": false, "facts": ["The Pyrenees mountains are 305 miles wide.", "An elite runner can cover 100 miles in around 12 hours."], "decomposition": ["How many miles can an elite runner cover in one day?", "How far around in miles are The Pyrenees mountains?", "Is #1 more than #2?"], "evidence": [[[["Usain Bolt-106"]], [["Pyrenees-13"], "no_evidence"], ["operation"]], [[["Ultramarathon-9"], "no_evidence"], [["Pyrenees-1"], "no_evidence"], ["operation"]], [[["How Many Miles to Babylon?-7"], "operation"], ["no_evidence"], ["no_evidence"]]], "golden_sentence": [["Fastest run 150 metres (male) Most medals won at the IAAF Athletics World Championships (male) Most gold medals won at the IAAF Athletics World Championships (male) Most Athletics World Championships Men's 200 m wins Most consecutive Olympic gold medals won in the 100 metres (male) Most consecutive Olympic gold medals won in the 200 metres (male) Most Olympic men's 200 metres Gold medals Fastest run 200 metres (male) Most Men's IAAF World Athlete of Year Trophies First Olympic track sprint triple-double Highest annual earnings for a track athlete Most wins of the 100 m sprint at the Olympic Games First athlete to win the 100 m and 200 m sprints at successive Olympic Games Fastest run 100 metres (male) First man to win the 200 m sprint at successive Olympic Games Most Athletics World Championships Men's 100 m wins Most tickets sold at an IAAF World Championships Most competitive 100 m sprint races completed in sub 10 seconds Fastest relay 4\u00d7100 metres (male) From his record time of 9.58 s for the 100 m sprint, Usain Bolt's average ground speed equates to 37.58\u00a0km/h (23.35\u00a0mph)."], [""]]}, {"qid": "50f6d792667593082a2f", "term": "Hermes", "description": "ancient Greek god of roads, travelers, and thieves", "question": "Is Hermes equivalent to the Roman god Vulcan?", "answer": false, "facts": ["Hermes is the Greek messenger god and god of roads and travelers.", "Mercury is the Roman messenger god of roads and travelers.", "The Roman Vulcan is actually equivalent to the Greek Hephaestus.", "Hermes is equivalent to Mercury."], "decomposition": ["What is Hermes god of?", "Who is the god of #1 in Roman mythology?", "Is #2 the same as Vulcan?"], "evidence": [[[["Hermes-1", "Hermes-8"]], [["Hermes-4"]], ["operation"]], [[["Hermes-1"]], [["Hermes-4"]], [["Vulcan (mythology)-1"], "operation"]], [[["Hermes-1"]], [["Mercury (mythology)-2"]], ["operation"]]], "golden_sentence": [["Hermes is considered the herald of the gods, as well as the protector of human heralds, travellers, thieves, merchants, and orators.", "Later, the epithet supplanted the original name itself and Hermes took over the roles as god of messengers, travelers, and boundaries, which had originally belonged to Pan, while Pan himself continued to be venerated by his original name in his more rustic aspect as the god of the wild in the relatively isolated mountainous region of Arcadia."], ["In Roman tradition, Hermes was known as Mercury, a name derived from the Latin merx, meaning \"merchandise,\" and from where we get the words \"merchant\" and \"commerce.\""]]}, {"qid": "b0f2464c6e2e4770389d", "term": "Jackson Pollock", "description": "American painter", "question": "Is it understandable to compare a blood spatter pattern to a Jackson Pollock piece?", "answer": true, "facts": ["Jackson Pollock is well known for a style of art formed through splashing liquids on canvas.", "Blood spatter patterns are caused by a splash of blood onto a surface or multiple surfaces."], "decomposition": ["What kinds of work pieces is Jackson Pollock well known for?", "How does he form #1", "How is a blood splatter formed?", "Is #2 comparable to #3?"], "evidence": [[[["Jackson Pollock-1"]], [["Jackson Pollock-2"]], [["Bloodstain pattern analysis-5"]], ["operation"]], [[["Jackson Pollock-1"]], [["Jackson Pollock-2"]], [["Bloodstain pattern analysis-4"]], ["operation"]], [[["Jackson Pollock-10"]], [["Jackson Pollock-2"]], [["Blood squirt-1"]], ["operation"]]], "golden_sentence": [[""], [""], [""]]}, {"qid": "1bb68f3777345a3cbf8e", "term": "Johns Hopkins University", "description": "Private research university in Baltimore, Maryland", "question": "Has Johns Hopkins University always treated subjects ethically?", "answer": false, "facts": ["Henrietta Lacks' DNA was used and replicated by Johns Hopkins University without her family's knowledge or approval.", "Henrietta Lacks' family medical history was released by Johns Hopkins University without their knowledge."], "decomposition": ["Did researchers at John Hopkins obtain approval from Henrietta Lacks before using her cancer cells?", "Did John Hopkins obtain approval from Henrietta Lacks or her family before releasing their medical records to the public?", "Are #1 and #2 positive?"], "evidence": [[[["Henrietta Lacks-2"]], [["Henrietta Lacks-19"]], ["operation"]], [[["Henrietta Lacks-2"]], [["Henrietta Lacks-4"]], ["operation"]], [[["Henrietta Lacks-19"]], [["Henrietta Lacks-19"]], [["Henrietta Lacks-19"], "operation"]]], "golden_sentence": [["As was then the practice, no consent was obtained to culture her cells, nor were she or her family compensated for their extraction or use."], ["Neither Henrietta Lacks nor her family gave her physicians permission to harvest her cells."]]}, {"qid": "c4be17c6ea186ca6c312", "term": "Porch", "description": "a room or gallery at the front entrance of a building forming a low front", "question": "In Hey Arnold, did any characters stay on a porch all the time?", "answer": true, "facts": ["Hey Arnold was an animated children's series.", "Hey Arnold featured 'Stoop Kid', a character who never left the front stoop of his home.", "A stoop is the city equivalent of a porch."], "decomposition": ["Where is 'Stoop Kid' in Hey Arnold known to never leave?", "Is #1 in the series equivalent to a porch in real life?"], "evidence": [[[["Hey Arnold!-7"], "no_evidence"], [["Porch-1", "Stoop (architecture)-1"], "no_evidence", "operation"]], [["no_evidence"], [["Stoop (architecture)-2"]]], [[["Hey Arnold!-1"], "no_evidence"], [["Stoop (architecture)-2"]]]], "golden_sentence": [["While its geographic location is never revealed outright, Bartlett described the city as \"an amalgam of large northern cities I have loved, including Seattle (my hometown), Portland (where I went to art school) and Brooklyn (the bridge, the brownstones, the subway)\"."], ["", ""]]}, {"qid": "5a9d931f670665758e23", "term": "Brazilian Navy", "description": "Naval warfare branch of Brazil's military forces", "question": "Are some Brazilian Navy ships built in Britian?", "answer": true, "facts": ["The Brazilian Navy stated in 2018 that they had purchased the helicopter carrier ship HMS Ocean.", "HMS stands for \"His/Her Majesty's Ship\", which is emblazoned on ships of the British Royal Navy. ", "Some of the ships in the Brazilian Navy are guided missile frigates built in Britian."], "decomposition": ["Which helicopter carrier ship did the Brazilian Navy announce that they had acquired in 2018?", "Was #1 built in Britain?"], "evidence": [[[["Brazilian Navy-62"]], [["HMS Ocean (L12)-1"]]], [[["HMS Ocean (L12)-2"]], [["HMS Ocean-1"]]], [[["Aircraft carrier-43"]], ["operation"]]], "golden_sentence": [["The Brazilian Navy stated in 2018 that they had purchased the helicopter carrier ship HMS\u00a0Ocean from their British counterparts."], ["She was constructed in the mid-1990s by Kvaerner Govan on the River Clyde and fitted out by VSEL at Barrow-in-Furness prior to trials and subsequent acceptance in service."]]}, {"qid": "9236488f127d342fc116", "term": "Sable", "description": "Species of marten", "question": "Are sables related to wolverines?", "answer": true, "facts": ["The sable is a species of marten, which make up the genus Martes.", "Wolverines are from the genus Gulo.", "Both the Martes and the Gulo are from the family Mustelidae."], "decomposition": ["What species is a sable?", "What genus is #1 from?", "What genus are wolverines from?", "Are #2 and #3 from the same family?"], "evidence": [[[["Sable-1"]], [["Marten-1"]], [["Gulo-1"]], [["Gulo-1", "Marten-1"]]], [[["Sable-1"]], [["Marten-1"]], [["Gulo-1", "Wolverine-1"]], ["operation"]], [[["Sable-1"]], [["Marten-1"]], [["Gulo-1"]], ["operation"]]], "golden_sentence": [["The sable (Martes zibellina) is a species of marten, a small omnivorous mammal primarily inhabiting the forest environments of Russia, from the Ural Mountains throughout Siberia, and northern Mongolia."], ["The martens constitute the genus Martes within the subfamily Guloninae, in the family Mustelidae."], ["It contains one extant species, the wolverine (G. gulo), as well as several extinct ones."], ["", ""]]}, {"qid": "4e19c2f22dda590d1ccd", "term": "Watchmaker", "description": "artisan who makes and repairs watches", "question": "Is a watchmaker likely to be able to fix an Apple Watch?", "answer": false, "facts": ["A watchmaker makes and repairs watches using tiny instruments to fix coils, springs, gears, and other metal parts..", "The Apple Watch is a computer driven watch that can connect to devices using wireless technology.", "Apple Watches do not have the usual springs and gears of traditional watches."], "decomposition": ["What components of watches do watchmakers repair?", "What are the main components of an Apple Watch?", "Is there an overlap between #1 and #2?"], "evidence": [[[["Watchmaker-1", "Watchmaker-2"], "no_evidence"], [["Apple Watch-2"], "no_evidence"], ["operation"]], [[["Watchmaker-2"]], [["Apple Watch-29"], "no_evidence"], ["operation"]], [[["Watchmaker-1"]], [["Apple Watch-15"]], ["operation"]]], "golden_sentence": [["Modern watchmakers, when required to repair older watches, for which replacement parts may not be available, must have fabrication skills, and can typically manufacture replacements for many of the parts found in a watch.", "These highly skilled workers do not have a watchmaking degree or certificate, but are specifically trained 'in-house' as technicians to service a small number of components of the watch in a true 'assembly-line' fashion, (e.g., one type of worker will dismantle the watch movement from the case, another will polish the case and bracelet, another will install the dial and hands, etc.)."], ["Apple Watch operates primarily in conjunction with the user's iPhone for functions such as configuring the watch, calling and texting, and syncing data with iPhone apps, but can independently connect to a Wi-Fi network for some tasks."]]}, {"qid": "16db93a60337d83f573b", "term": "Noah", "description": "Biblical figure", "question": "Was Noah concerned with buoyancy?", "answer": true, "facts": ["Buoyancy is the property of an object related to its ability to float in liquid", "Noah was tasked with building a boat to house many animals and survive a catastrophic flood", "Boats must be properly buoyant or they will sink"], "decomposition": ["What was Noah famous for building?", "Did #1 have to be buoyant to work?"], "evidence": [[[["Noah-2"]], [["Buoyancy-1", "Buoyancy-2"], "operation"]], [[["Noah's Ark-1"]], [["Ark (river boat)-4"]]], [[["Noah's Ark-1"]], [["Buoyancy-1"], "operation"]]], "golden_sentence": [["According to the Genesis account, Noah labored faithfully to build the Ark at God's command, ultimately saving not only his own family, but mankind itself and all land animals, from extinction during the Flood."], ["", ""]]}, {"qid": "ca07431e90e4ef2e263a", "term": "Fair trade", "description": "form of trade", "question": "Can you buy a fair trade laptop?", "answer": false, "facts": ["Fair trade is a term used with sustainable development focusing on agricultural production", "Laptops are consumer electronics"], "decomposition": ["What type of product is the fair trade label used with? ", "What type of product is a laptop?", "Is #2 the same as #1?"], "evidence": [[[["Fair trade-4"]], [["Laptop-1"]], ["operation"]], [[["Fair trade-1"]], [["Laptop-1"]], ["operation"]], [[["Fair trade-1"]], [["Laptop-1"]], ["operation"]]], "golden_sentence": [[""], ["A laptop (also laptop computer), often called a notebook, is a small, portable personal computer (PC) with a \"clamshell\" form factor, typically having a thin LCD or LED computer screen mounted on the inside of the upper lid of the clamshell and an alphanumeric keyboard on the inside of the lower lid."]]}, {"qid": "3e4741f68735f621de2c", "term": "Giraffe", "description": "Tall African ungulate", "question": "Is it foolish to stand on giraffe's head to see over Eiffel Tower?", "answer": true, "facts": ["The neck of a giraffe can be up to 7 feet in length.", "Including their necks, giraffes can be as tall as 20 feet.", "The Eiffel Tower is 1,063 feet tall."], "decomposition": ["How tall is a giraffe?", "How tall is the Eiffel Tower?", "Is #1 greater than #2?"], "evidence": [[[["Giraffe-16"]], [["Eiffel Tower-3"]], ["operation"]], [[["Giraffe-16"]], [["Eiffel Tower-3"]], ["operation"]], [[["Giraffe-16"]], [["Eiffel Tower-3"]], ["operation"]]], "golden_sentence": [["Fully grown giraffes stand 4.3\u20135.7\u00a0m (14.1\u201318.7\u00a0ft) tall, with males taller than females."], ["The tower is 324 metres (1,063\u00a0ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris."]]}, {"qid": "b282b057141326445e66", "term": "Napoleonic Wars", "description": "Series of early 19th century European wars", "question": "Was a nuclear bomb used in the Napoleonic Wars?", "answer": false, "facts": ["The Napoleonic Wars took place between 1803 and 1815.", "Nuclear bombs have only been used in warfare twice, both times in 1945."], "decomposition": ["When was the Napoleonic Wars?", "What year were nuclear bombs used in war?", "Is #2 in the range of years of #1?"], "evidence": [[[["Napoleonic Wars-1"]], [["Nuclear weapon-2"]], ["operation"]], [[["Napoleonic Wars-1"]], [["Nuclear weapons debate-1"]], ["operation"]], [[["Napoleonic Wars-1"]], [["Nuclear weapon-2"]], ["operation"]]], "golden_sentence": [["The Napoleonic Wars (1803\u20131815) were a series of major conflicts pitting the French Empire and its allies, led by Napoleon I, against a fluctuating array of European powers formed into various coalitions, financed and usually led by the United Kingdom."], [""]]}, {"qid": "354d218a9a40757c6c91", "term": "Nicole Kidman", "description": "Australian-American actress and film producer", "question": "Does Nicole Kidman know any Scientologists?", "answer": true, "facts": ["Nicole Kidman was married to Tom Cruise.", "Tom Cruise is a Scientologist. "], "decomposition": ["Who has Nicole Kidman been married to?", "Have any of #1 practiced Scientology?"], "evidence": [[[["Nicole Kidman-4"]], [["Tom Cruise-36"], "operation"]], [[["Nicole Kidman-32"]], [["Tom Cruise-4"]]], [[["Nicole Kidman-4"]], [["Tom Cruise-4"]]]], "golden_sentence": [["She has been married to singer Keith Urban since 2006, and was earlier married to Tom Cruise."], [""]]}, {"qid": "3ba87020f9c8995b73d8", "term": "Cream", "description": "Dairy product", "question": "Would Kylee Jenner ask for no cream in her coffee?", "answer": true, "facts": ["Kylee Jenner is lactose intolerant.", "Lactose intolerance makes it uncomfortable for people to digest foods containing lactose.", "Cream is a dairy product and is rich in lactose."], "decomposition": ["What dietary condition does Kylee (Kylie) Jenner suffer from?", "What do people who have #1 have to avoid?", "Does cream have #2 in it?"], "evidence": [[["no_evidence"], [["Lactose intolerance-1"]], [["Cream-1"], "operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence"]], [[["Kylie Jenner-1"]], [["Lactose intolerance-1"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "2189ea937acd122abf2e", "term": "JPEG", "description": "Lossy compression method for reducing the size of digital images", "question": "Could the leader of Heaven's Gate save images in JPEG format?", "answer": true, "facts": ["Marshall Applewhite, Jr. was the leader of the Heaven's Gate cult", "Marshall Applewhite, Jr. died in 1997", "JPEG format was released in 1992"], "decomposition": ["Who led the Heaven's Gate cult?", "During what span of years was #1 alive?", "What year was JPEG released?", "Is #2 inclusive of #3?"], "evidence": [[[["Marshall Applewhite-1"]], [["Marshall Applewhite-1"]], [["JPEG-2"]], ["operation"]], [[["Heaven's Gate (religious group)-20"]], [["Marshall Applewhite-1"]], [["JPEG-15"]], ["operation"]], [[["Heaven's Gate (religious group)-1"]], [["Heaven's Gate (religious group)-1"]], [["JPEG-5"]], ["operation"]]], "golden_sentence": [["Marshall Herff Applewhite Jr. (May\u00a017, 1931\u00a0\u2013 March 26, 1997), also known as Do, among other names, was an American cult leader who founded what became known as the Heaven's Gate religious group and organized their mass suicide in 1997, claiming the lives of 39 people."], [""], ["The term \"JPEG\" is an initialism/acronym for the Joint Photographic Experts Group, which created the standard in 1992."]]}, {"qid": "162481eda3e22f7f07f8", "term": "Vitamin C", "description": "nutrient found in citrus fruits and other foods", "question": "Did pirates who had scurvy need more Vitamin C?", "answer": true, "facts": ["Pirates were known for having poor teeth and deteriorated gums.", "Gum deterioration and tooth decay is a symptom of scurvy.", "Scurvy is caused by a lack of dietary vitamin C."], "decomposition": ["What causes scurvy?", "Is #1 the same as insufficient vitamin C intake?"], "evidence": [[[["Scurvy-1"]], ["operation"]], [[["Scurvy-1"]], ["operation"]], [[["Scurvy-1"]], ["operation"]]], "golden_sentence": [["Scurvy is a disease resulting from a lack of vitamin C (ascorbic acid)."]]}, {"qid": "b0442e00463bb7f98554", "term": "Easy Rider", "description": "1969 film by Dennis Hopper", "question": "Did producer of Easy Rider ever star in a movie with Dean Cain's Princeton girlfriend?", "answer": true, "facts": ["Easy Rider was produced by Peter Fonda.", "Dean Cain dated Brooke Shields while at Princeton.", "Brooke Shields and Peter Fonda star in the movie Wanda Nevada."], "decomposition": ["Who produced Easy Rider?", "Who did Dean Cain date while at Princeton?", "What movies did #1 star in?", "What movies did #2 star in?", "Is at least one element of #3 also found in #4?"], "evidence": [[[["Easy Rider-1"]], [["Dean Cain-16"]], [["Peter Fonda-48"], "no_evidence"], [["Brooke Shields-1"], "no_evidence"], [["Wanda Nevada-1"], "no_evidence", "operation"]], [[["Easy Rider-1"]], [["Dean Cain-3"]], [["Wanda Nevada-1"]], [["Wanda Nevada-1"]], ["operation"]], [[["Easy Rider-40"], "operation"], [["Dean Cain-16"], "operation"], ["no_evidence"], ["no_evidence"], ["no_evidence"]]], "golden_sentence": [["Easy Rider is a 1969 American independent road drama film written by Peter Fonda, Dennis Hopper, and Terry Southern, produced by Fonda, and directed by Hopper."], ["While the two were both attending Princeton University, Cain dated actress Brooke Shields for two years."], ["Fonda's later appearances include American Bandits: Frank and Jesse James (2010) for Fred Olen Ray; The Trouble with Bliss (2011); episodes of CSI: NY; Smitty (2012); Harodim (2012); As Cool as I Am (2013); Copperhead (2013); The Ultimate Life (2013); The Harvest (2013); HR (2014); House of Bodies (2014); Jesse James: Lawman (2015); The Runner (2015) with Nicolas Cage; The Ballad of Lefty Brown (2017); The Most Hated Woman in America (2017); Borderland (2017); You Can't Say No (2018); and Boundaries (2018) with Christopher Plummer."], [""], [""]]}, {"qid": "4e6ab8691ff26686be2d", "term": "Laziness", "description": "disinclination to activity or exertion", "question": "Can depression be mistaken for laziness?", "answer": true, "facts": ["Symptoms of depression include low energy, inability to get out of bed, and low motivation.", "Low energy and low motivation can cause someone to seem like they are slacking on responsibility."], "decomposition": ["What are some common symptoms of depression?", "Does any of #1 share characteristics with laziness?"], "evidence": [[[["Depression (mood)-1"]], ["operation"]], [[["Symptom-11"]], [["Laziness-1"]]], [[["Depression (mood)-4"]], [["Laziness-1"], "operation"]]], "golden_sentence": [["People experiencing depression may have feelings of dejection, hopelessness and, sometimes, suicidal thoughts."]]}, {"qid": "03053431e0985719a2ee", "term": "Bulk carrier", "description": "merchant ship specially designed to transport unpackaged bulk cargo", "question": "Is the average bulk carrier ideal for transporting bromine at room temperature?", "answer": false, "facts": ["Bulk carriers are defined as a ship that carries nonliquid cargoes such as grain or ore in bulk.", "Bromine is a liquid at room temperature.", "The average bulk carrier is used for unpackaged bulk cargo, such as grains, coal, ore, steel coils and cement."], "decomposition": ["What are the kinds of cargo that a typical bulk carrier can transport?", "What kind of substance is bromine at room temperature?", "Can any of #1 be classified as #2?"], "evidence": [[[["Bulk carrier-1"]], [["Bromine-1"]], ["operation"]], [[["Bulk carrier-4"], "operation"], [["Bromine-26"]], ["no_evidence"]], [[["Bulk carrier-1"]], [["Bromine-1"]], ["operation"]]], "golden_sentence": [["A bulk carrier, bulker is a merchant ship specially designed to transport unpackaged bulk cargo, such as grains, coal, ore, steel coils and cement, in its cargo holds."], ["It is the third-lightest halogen, and is a fuming red-brown liquid at room temperature that evaporates readily to form a similarly coloured gas."]]}, {"qid": "e8c42cff3ac539190b6c", "term": "Chlorophyll", "description": "group of chemical compounds", "question": "Would human race go extinct without chlorophyll?", "answer": true, "facts": ["Chlorophyll is a pigment in plants responsible for photosynthesis.", "Photosynthesis is the process by which plants release oxygen into the atmosphere.", "Humans need oxygen to live."], "decomposition": ["What is Chlorophyll responsible for in plants?", "What does #1 release into the air?", "Do humans need #2 in order to survive?"], "evidence": [[[["Chlorophyll-1"]], [["Photosynthesis-1"]], ["operation"]], [[["Chlorophyll-6"]], [["Photosynthesis-1"]], [["Breathing-2"]]], [[["Chlorophyll-1"]], [["Photosynthesis-1"]], [["Breathing-2"]]]], "golden_sentence": [["Chlorophyll is essential in photosynthesis, allowing plants to absorb energy from light."], ["In most cases, oxygen is also released as a waste product."]]}, {"qid": "7b9d0cb1e787ee5097fd", "term": "Walt Disney", "description": "American entrepreneur, animator, voice actor and film producer", "question": "Walt Disney dominated his amusement park peers at Academy Awards?", "answer": true, "facts": ["Walt Disney won a total of 26 Academy Awards.", "The founder of Six Flags, Angus G Wynne, had 0 academy awards.", "The founder of Knott's Berry Farm, Walter Knott, had 0 academy awards."], "decomposition": ["At the Academy Awards, how many awards did Walt Disney win?", "At the Academy Awards, how many awards did Angus G Wynne win?", "At the Academy Awards, how many awards did Walter Knott win?", "Is #1 more than #2 and #3?"], "evidence": [[[["Walt Disney-1"]], [["Angus G. Wynne-1"], "no_evidence"], [["Walter Knott-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Walt Disney-1"]], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Walt Disney-1"]], [["Angus G. Wynne-1"]], [["Walter Knott-1"]], ["operation"]]], "golden_sentence": [["As a film producer, Disney holds the record for most Academy Awards earned by an individual, having won 22 Oscars from 59 nominations."], [""], [""]]}, {"qid": "109e859296dceb2ee54e", "term": "Toyota Hilux", "description": "Series of light commercial vehicles produced by the Japanese car-manufacturer Toyota.", "question": "Can a human heart last from NYC to Raleigh NC by Toyota Hiux?", "answer": true, "facts": ["Human hearts can last up to six hours outside the body.", "The distance from NYC to Raleigh, NC is 505 miles.", "The top speed of a Toyota Hilux is 105 MPH."], "decomposition": ["How many hours can a human heart last outside of the human body?", "What is the distance between NYC to Raleigh, NC in miles?", "What is the top speed of a Toyota Hilux in MPH?", "Is #1 times #3 more than #2?"], "evidence": [[[["Organ transplantation-3"]], [["New York City-1", "Raleigh, North Carolina-1"], "operation"], [["Toyota Hilux-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Organ donation-8"], "no_evidence"], ["no_evidence"], [["Toyota Hilux-9"]], ["operation"]], [["no_evidence"], ["no_evidence"], [["Toyota Hilux-2"], "no_evidence"], ["operation"]]], "golden_sentence": [["Tissue may be recovered from donors who die of circulatory death, as well as of brain death \u2013 up to 24 hours past the cessation of heartbeat."], ["", ""], [""]]}, {"qid": "194812ddcddbd130b19f", "term": "History of art", "description": "history of human creation of works for aesthetic, communicative, or expressive purposes", "question": "Can the history of art be learned by an amoeba?", "answer": false, "facts": ["The history of art is the academic study of the development of human artistic expression over time", "Academic study requires human-level intelligence", "An amoeba is a single-celled organism "], "decomposition": ["What intellectual ability is necessary to study the history of art?", "Does an amoeba possess #1?"], "evidence": [[[["Human brain-66", "Human brain-67"]], [["Amoeba-1", "Cell (biology)-1", "Cell (biology)-16"], "operation"]], [[["Learning-1"], "no_evidence"], [["Amoeba-1"], "no_evidence", "operation"]], [[["Art history-6"]], [["Amoeba-25"]]]], "golden_sentence": [["", ""], ["", "", ""]]}, {"qid": "a9020386ad0a9000cda1", "term": "Snow White", "description": "fairy tale", "question": "Do Snow White dwarves best The Hobbit dwarves in battle?", "answer": false, "facts": ["Snow White had seven dwarves.", "There are 13 dwarves in The Hobbit.", "Several of The Hobbit dwarves, including Thorin Oakenshield, were acclaimed warriors."], "decomposition": ["How many dwarves are there in the Snow White story?", "How many dwarves are in The Hobbit?", "Is #1 greater than #2?"], "evidence": [[[["Snow White and the Seven Dwarfs (1937 film)-7"]], [["The Hobbit-7"]], ["operation"]], [[["Snow White-3"]], [["Hobbit-13"], "no_evidence"], ["operation"]], [[["Snow White (Fables)-1"]], [["The Hobbit-26"], "no_evidence"], ["operation"]]], "golden_sentence": [["In reality, the cottage belongs to seven adult dwarfs\u2014named Doc, Grumpy, Happy, Sleepy, Bashful, Sneezy, and Dopey\u2014who work in a nearby mine."], ["Gandalf, an itinerant wizard, introduces Bilbo to a company of thirteen dwarves."]]}, {"qid": "1002d2b8f958e154375b", "term": "Sonnet", "description": "form of poetry with fourteen lines; by the thirteenth century it signified a poem of fourteen lines that follows a strict rhyme scheme and specific structure", "question": "Would Rime of the Ancient Mariner make a good sonnet?", "answer": false, "facts": ["A sonnet is a form of poetry that sticks to a strict 14 line rule.", "The Rime of the Ancient Mariner is a story by Samuel Taylor Coleridge and contains over thirty lines."], "decomposition": ["How many lines are in a Sonnet?", "How many lines is the Rime of the Ancient Mariner?", "Is #2 equal to #1?"], "evidence": [[[["Sonnet-2"]], [["The Rime of the Ancient Mariner-1"]], [["Sonnet-2", "The Rime of the Ancient Mariner-9"]]], [[["Sonnet-2"]], [["The Rime of the Ancient Mariner-1", "The Rime of the Ancient Mariner-13"], "no_evidence"], ["operation"]], [[["Sonnet-2"]], [["The Rime of the Ancient Mariner-13"], "no_evidence"], ["operation"]]], "golden_sentence": [["By the thirteenth century it signified a poem of fourteen lines that follows a strict rhyme scheme and specific structure."], ["The Rime of the Ancient Mariner (originally The Rime of the Ancyent Marinere) is the longest major poem by the English poet Samuel Taylor Coleridge, written in 1797\u201398 and published in 1798 in the first edition of Lyrical Ballads."], ["", ""]]}, {"qid": "79a8c5beb6717b60df78", "term": "Pompey", "description": "1st/2nd-century BC Roman general", "question": "Has type of political association Pompey had with Caesar influenced reality TV?", "answer": true, "facts": ["Pompey, Julius Caesar, and Marcus Licinius Crassus formed a political association called a triumvirate.", "A triumvirate spits rule between three powerful people that get to make decisions.", "Reality show The Challenge: Total Madness appoints three weekly winners to make decisions for the group, known as the Tribunal.", "Reality show American Idol has had three judges making decisions about which contestants advance."], "decomposition": ["Which political association did Pompey form with Julius Caesar and Marcus Licinius Crassus?", "How many people does #1 typically involve?", "How many judges are on reality show American Idol?", "Is #2 equal to #3?"], "evidence": [[[["First Triumvirate-1"]], [["Triumvirate-1"]], [["American Idol-10", "American Idol-9"]], ["operation"]], [[["Pompey-2"]], [["First Triumvirate-1"]], [["American Idol-3"]], ["operation"]], [[["Triumvirate-5"]], [["Triumvirate-3"]], [["American Idol-3"]], ["operation"]]], "golden_sentence": [["The First Triumvirate (60\u201353\u00a0BC) was an informal alliance among three prominent politicians in the late Roman Republic: Julius Caesar, Pompey, and Marcus Licinius Crassus."], ["A triumvirate (Latin: triumvir\u0101tus) is a political regime ruled or dominated by three powerful individuals known as triumvirs (Latin: triumviri)."], ["In the eighth season, Latin Grammy Award-nominated singer-songwriter and record producer Kara DioGuardi was added as a fourth judge.", "The show decided to continue with the three judges format until the eighth season."]]}, {"qid": "dea2785d3f360c3ca83b", "term": "Nissan", "description": "Japanese automobile manufacturer", "question": "Is CEO of Nissan an internationally wanted fugitive?", "answer": true, "facts": ["Carlos Ghosn was CEO of Nissan. ", "With help from an American private-security contractor, Carlos Ghosn fled from Japan to Lebanon on 30 December, breaking his bail conditions", "On 2 January 2020, Interpol issued a red notice to Lebanon seeking Carlos Ghosn's arrest."], "decomposition": ["Which of Nissan's former CEOs have been the subject of corporate unrest?", "Is #1 presently a fugitive on the run?"], "evidence": [[[["Carlos Ghosn-13"]], [["Carlos Ghosn-4"], "operation"]], [[["Carlos Ghosn-29"]], [["Carlos Ghosn-1"], "operation"]], [[["Carlos Ghosn-4"]], [["Carlos Ghosn-4"]]]], "golden_sentence": [[""], [""]]}, {"qid": "bcc51dd8503de859eb30", "term": "Lighthouse of Alexandria", "description": "Ancient lighthouse in Egypt", "question": "Is Statue of Unity hypothetically more level with Statue of Liberty than Lighthouse of Alexandria?", "answer": false, "facts": ["The Statue of Liberty rises 305 feet into the air.", "The Statue of Unity is 597 feet high.", "The Lighthouse of Alexandria was between 338 and 387 feet tall."], "decomposition": ["What is the height of the Statue of Liberty?", "What is the height of the Statue of Unity?", "What is the height of the Lighthouse of Alexandria?", "Is #2 minus #1 less than #3 minus #1?"], "evidence": [[[["Statue of Liberty-31"]], [["Statue of Unity-9"]], [["Lighthouse of Alexandria-1"]], ["operation"]], [[["Statue of Liberty-18"]], [["Statue of Unity-1"]], [["Lighthouse of Alexandria-1"]], ["operation"]], [[["Statue of Liberty-18"]], [["Statue of Unity-1"]], [["Lighthouse of Alexandria-1"]], ["operation"]]], "golden_sentence": [["He proposed a pedestal 114 feet (35\u00a0m) in height; faced with money problems, the committee reduced that to 89 feet (27\u00a0m)."], ["The total height of the structure is 240\u00a0m (790\u00a0ft), with a base of 58\u00a0m (190\u00a0ft) and the statue of 182\u00a0m (597\u00a0ft)."], ["The Lighthouse of Alexandria, sometimes called the Pharos of Alexandria (/\u02c8f\u025b\u0259r\u0252s/; Ancient Greek: \u1f41 \u03a6\u03ac\u03c1\u03bf\u03c2 \u03c4\u1fc6\u03c2 \u1f08\u03bb\u03b5\u03be\u03b1\u03bd\u03b4\u03c1\u03b5\u03af\u03b1\u03c2, contemporary Koine Greek pronunciation:\u00a0[ho p\u02b0\u00e1.ros te\u02d0s a.lek.sandr\u00e9\u02d0a\u02d0s]), was a lighthouse built by the Ptolemaic Kingdom, during the reign of Ptolemy II Philadelphus (280\u2013247 BC), which has been estimated to be at least 100 metres (330\u00a0ft) in overall height."]]}, {"qid": "0f82700f1b60acc45055", "term": "Surfing", "description": "sport that consists of riding a wave", "question": "Is surfing popular in Des Moines, Iowa?", "answer": false, "facts": ["Des Moines is a city in the landlocked state Iowa.", "Surfing involves riding the waves at a beach or ocean.", "There are no beaches in Iowa."], "decomposition": ["What conditions are necessary to be able to surf?", "Does Iowa have (or is close to) the conditions listed in #1?"], "evidence": [[[["Surfing-1"]], [["Des Moines, Iowa-106", "Des Moines, Iowa-108"]]], [[["Surfing-1"]], [["Iowa-16"]]], [[["Surfing-1"]], [["Iowa-1"], "operation"]]], "golden_sentence": [[""], ["", ""]]}, {"qid": "9b61c2b9b9e5c136c66e", "term": "Palaeography", "description": "Study of ancient handwriting", "question": "Paleography hypothetically helps to understand Cthulhu?", "answer": true, "facts": ["Palaeography involves the study of ancient writings.", "Paleography has helped decode Anatolian hieroglyphics from all the way back as the first millenia BC.", "Author H.P. Lovecraft's Cthulhu is an ancient mystical being from eons ago."], "decomposition": ["Palaeography is the study of what?", "Cthulhu is from which age?", "Will a language from #2 be a subject of focus of #1?"], "evidence": [[[["Palaeography-1"]], [["Cthulhu-9"]], ["operation"]], [[["Palaeography-1"]], [["Cthulhu-9"]], ["operation"]], [[["Palaeography-1"]], [["Cthulhu-1", "Cthulhu-9"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Palaeography (UK) or paleography (US; ultimately from Greek: \u03c0\u03b1\u03bb\u03b1\u03b9\u03cc\u03c2, palai\u00f3s, \"old\", and \u03b3\u03c1\u03ac\u03c6\u03b5\u03b9\u03bd, gr\u00e1phein, \"to write\") is the study of ancient and historical handwriting (that is to say, of the forms and processes of writing; not the textual content of documents)."], [""]]}, {"qid": "b8fc9dd958621940a059", "term": "Supreme Court of the United States", "description": "Highest court in the United States", "question": "Do members of the Supreme Court of the United States have longer terms than most senators?", "answer": true, "facts": ["Senators, on average, serve for 10 years.", "Supreme Court Justices serve for their entire life.", "The average term for a Supreme court justice is 16 years."], "decomposition": ["How many years is in a term for a U.S. Senator?", "What is the term for a Supreme Court justice?", "Is #1 a shorter term than #2?"], "evidence": [[[["United States Senate-2"]], [["Supreme Court of the United States-31"]], ["operation"]], [[["United States Senate-16"]], [["Supreme Court of the United States-2"]], ["operation"]], [[["Member of Congress-3"]], [["Supreme Court of the United States-2"]], ["operation"]]], "golden_sentence": [["Each state, regardless of its population size, is equally represented by two senators who serve staggered terms of six years."], [""]]}, {"qid": "5e0172fd690412ed4a06", "term": "1999", "description": "Year", "question": "Were some people afraid of New Years Day coming in 1999?", "answer": true, "facts": ["It was believed that computers might not know how to change from 1999 to 2000 on New Years Day.", "People were concerned that human services and utilities that were computerized might crash due to the Y2K bug.", "People believed that the year 2000 would cause computers to crash due to the 'Y2K' bug."], "decomposition": ["Which New Year's Day followed 1999?", "What concerns were there about computers during the transition from 1999 to #1?", "Was #2 a cause of fear?"], "evidence": [[[["January 1-1"], "no_evidence"], [["Year 2000 problem-1"]], ["operation"]], [[["Year 2000 problem-3"]], [["Year 2000 problem-1"]], ["operation"]], [[["New Year's Day-1", "Year 2000 problem-15"]], [["Year 2000 problem-1"]], [["2000-3", "Year 2000 problem-15"]]]], "golden_sentence": [["January 1 is the first day of the year in the Gregorian Calendar."], ["Problems were anticipated, and arose, because many programs represented four-digit years with only the final two digits \u2013 making the year 2000 indistinguishable from 1900."]]}, {"qid": "135a50c1f88f0ab2a09e", "term": "Chiropractic", "description": "form of alternative medicine", "question": "Are some chiropractic manipulations dangerous?", "answer": true, "facts": ["Manipulations of the neck can lead to complications such as stroke or paralysis.", "Manipulation of the lower back can lead to herniated disks."], "decomposition": ["What body parts do chiropractors manipulate?", "Are any of #1 prone to damage if mishandled?"], "evidence": [[[["Chiropractic-18"]], [["Chiropractic controversy and criticism-34"]]], [[["Chiropractic-1"]], ["operation"]], [[["Chiropractic-1"]], [["Chiropractic-2"], "operation"]]], "golden_sentence": [["Back and neck pain are the specialties of chiropractic but many chiropractors treat ailments other than musculoskeletal issues."], [""]]}, {"qid": "7a41bcd0bfabe954feb9", "term": "Spinal cord", "description": "long, thin, tubular structure made up of nervous tissue", "question": "Can you buy spinal cord at Home Depot?", "answer": false, "facts": ["Home Depot sells home improvement and building supplies", "The spinal cord is an anatomical feature located in the vertebrae"], "decomposition": ["Where are spinal cords found?", "What does Home Depot sell?", "Is #1 listed in #2?"], "evidence": [[[["Spinal cord-1"]], [["The Home Depot-1"]], ["operation"]], [[["Spinal cord-1"]], [["The Home Depot-1"]], ["operation"]], [[["Spinal cord-1"]], [["The Home Depot-1"]], ["operation"]]], "golden_sentence": [["The spinal cord is a long, thin, tubular structure made up of nervous tissue, which extends from the medulla oblongata in the brainstem to the lumbar region of the vertebral column."], ["The Home Depot, Inc. is the largest home improvement retailer in the United States, supplying tools, construction products, and services."]]}, {"qid": "1da255128c7d1b4be517", "term": "Bulk carrier", "description": "merchant ship specially designed to transport unpackaged bulk cargo", "question": "Would eliminating competition in the Japanese bulk carrier market be profitable for a steel company?", "answer": true, "facts": ["62% of bulk carriers are built in Japan", "Bulk carrier hulls are made of steel"], "decomposition": ["Where are most bulk carriers built?", "What materials would #1 use in making bulk carriers?", "Is steel a major component of #2?"], "evidence": [[[["Bulk carrier-2"]], [["Bulk carrier-48"]], [["Bulk carrier-48"]]], [[["Bulk carrier-20", "Bulk carrier-22"], "no_evidence"], [["Shipbuilding-45"]], ["operation"]], [[["Malaysian Bulk Carriers-1"], "no_evidence"], ["no_evidence"], ["operation"]]], "golden_sentence": [["South Korea is the largest single builder of bulk carriers, and 82% of these ships were built in Asia."], ["Bulk carrier hulls are made of steel, usually mild steel."], [""]]}, {"qid": "6495f214a4dcb33b1043", "term": "Isaac Newton", "description": "Influential British physicist and mathematician", "question": "Is Issac Newton often associated with a red fruit?", "answer": true, "facts": ["Issac Newton claimed to have contemplated gravity for the first time after seeing an apple fall.", "In most illustrations of Issac Newton discovering gravity, the apple shown falling is red."], "decomposition": ["Which of Isaac Newton's famous discoveries featured a fruit?", "Is #1 colored red in popular depictions?"], "evidence": [[[["Isaac Newton-84"]], ["no_evidence"]], [[["Isaac Newton-84"]], [["Apple-8"], "operation"]], [[["Isaac Newton-88"]], [["Gala (apple)-1"]]]], "golden_sentence": [["Voltaire then wrote in his Essay on Epic Poetry (1727), \"Sir Isaac Newton walking in his gardens, had the first thought of his system of gravitation, upon seeing an apple falling from a tree.\""]]}, {"qid": "2cb8f9ba2b838e5e0bb5", "term": "Alcatraz Island", "description": "United States historic place", "question": "Would it be possible to fit a football field in Alcatraz Island?", "answer": true, "facts": ["Alcatraz is 511 meters by 180 meters", "A football field is 91 meters by 48 meters"], "decomposition": ["What is the land area of a football field?", "What is the land area of the Alcatraz Island?", "s #1 less than or equal to #2?"], "evidence": [[[["Comparison of American football and rugby union-24"]], [["Alcatraz Island-4"]], [["Alcatraz Island-4", "Comparison of American football and rugby union-24"]]], [[["System of measurement-25"]], [["Alcatraz Island-4"]], ["operation"]], [[["American football field-2"]], [["Alcatraz Island-4"]], ["operation"]]], "golden_sentence": [["Rugby union fields are limited to a maximum length of 144 metres (157\u00a0yd) long (100 metres (110\u00a0yd) between goal lines) and width of 70 metres (77\u00a0yd), while American football fields have a fixed length of 120 yards (110\u00a0m) (100 yards (91\u00a0m) between goal lines) and a width of 160 feet (49\u00a0m)."], ["The total area of the island is reported to be 22 acres (8.9\u00a0ha)."], ["", ""]]}, {"qid": "562363f03dfbf258a284", "term": "Mongoose", "description": "family of mammals", "question": "Did mongoose come from later period than rhinos?", "answer": true, "facts": ["The mongoose originated  in the Neogene geological period.", "Rhinos are from the Paleogene geological period.", "The Paleogene period spans 43 million years from the end of the Cretaceous Period 66 million years ago to the beginning of the Neogene Period."], "decomposition": ["During which period did the mongoose originate?", "Which period did Rhinos originate from?", "Is #2 before #1?"], "evidence": [[[["Mongoose-2"]], [["Rhinoceros-5"]], [["Mongoose-2", "Rhinoceros-5"], "operation"]], [[["Mongoose-1", "Mongoose-2"]], [["Rhinoceros-5"]], ["operation"]], [[["Mongoose-2"]], [["Rhinoceros-5"]], ["operation"]]], "golden_sentence": [["The Herpestidae originated about 21.8\u00a0\u00b1\u00a03.6\u00a0million years ago in the Early Miocene and genetically diverged into two main genetic lineages between 19."], ["The two African species, the white rhinoceros and the black rhinoceros, belong to the tribe Dicerotini, which originated in the middle Miocene, about 14.2 million years ago."], ["", ""]]}, {"qid": "6c4fc12cc0d26807c791", "term": "Penny", "description": "unit of currency in various countries", "question": "Would 1943-S penny be good for making silverware?", "answer": true, "facts": ["Modern pennies are made of zinc and copper.", "The 1943-S penny was made of 99% steel and 1% zinc.", "Modern silverware is made from stainless steel."], "decomposition": ["What are 1943-S pennies made out of?", "What is typically modern silverware made out of?", "Is there any overlap between #1 and #2?"], "evidence": [[[["Penny (United States coin)-5"]], [["Cutlery-5", "Cutlery-6"]], ["operation"]], [[["1943 steel cent-1"]], [["Spoon-17", "Tableware-3"]], ["operation"]], [[["1943 steel cent-1"]], [["Cutlery-1"]], ["operation"]]], "golden_sentence": [["In 1943, at the peak of World War II, zinc-coated steel cents were made for a short time because of war demands for copper."], ["Sterling silver is the traditional material from which good quality cutlery is made.", ""]]}, {"qid": "c11c4b7db6cbe906a923", "term": "Dodgeball", "description": "sport", "question": "Does Felix Potvin have a position on a dodgeball team?", "answer": false, "facts": ["Felix Potvin was an NHL goaltender", "There is no goalie position on a dodgeball team"], "decomposition": ["Which sport and position did F\u00e9lix Potvin play as a professional sportsman?", "Is #1 the same as or required in dodgeball?"], "evidence": [[[["F\u00e9lix Potvin-4"]], [["Dodgeball-10"], "operation"]], [[["F\u00e9lix Potvin-1"]], [["Dodgeball-1"], "operation"]], [[["F\u00e9lix Potvin-1"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "0fe6efc635ea040f492a", "term": "Steven Spielberg", "description": "American film director and screenwriter", "question": "Could Steven Spielberg send emails as a child?", "answer": false, "facts": ["Steven Spielberg was born in 1946.", "Email did not become available to the general public until 1995."], "decomposition": ["When was Stephen Spielberg born?", "When was email invented?", "What is #1 plus 10?", "Is #2 before #3?"], "evidence": [[[["Spielberg (disambiguation)-1"]], [["History of email-12"]], ["operation"], ["operation"]], [[["Steven Spielberg-1"]], [["Email-1"]], ["operation"], ["operation"]], [[["Steven Spielberg-5"]], [["Email-14"]], ["operation"], ["operation"]]], "golden_sentence": [["Steven Spielberg (born December 18, 1946) is an American film director and producer."], ["In 1971 the first ARPANET email was sent, and through RFC 561, RFC 680, RFC 724, and finally 1977's RFC 733, became a standardized working system."]]}, {"qid": "98c95f9b08834aeb8e88", "term": "Red hair", "description": "Hair color", "question": "Does a Disney princess on Broadway have red hair?", "answer": true, "facts": ["Ariel, the princess from Disney's the Little Mermaid, has red hair.", "The Little Mermaid is one of several Disney animated classics that was adapted for the stage and performed on Broadway."], "decomposition": ["What is the name of the princess with red hair?", "What is the name of the animated classic in which #1 is the main star of?", "Has #2 been adapted for Broadway?"], "evidence": [[[["Merida (Brave)-9"]], [["Merida (Brave)-10"]], [["Disney Princess-36"]]], [[["Ariel (The Little Mermaid)-7", "Merida (Brave)-9"]], [["Ariel (The Little Mermaid)-1", "Merida (Brave)-1"]], [["Ariel (The Little Mermaid)-33"]]], [[["Ariel (The Little Mermaid)-7"]], [["Ariel (The Little Mermaid)-49"]], [["The Little Mermaid (musical)-1"], "operation"]]], "golden_sentence": [["Highlighted versions were released later than 2012 Merida has long, wild, curly, red hair, blue eyes, pale skin and a slender body."], ["In Brave, Merida lives in the mystical Scottish kingdom of DunBroch with her mother, Queen Elinor, her father, King Fergus, and her mischievous triplet brothers, Hamish, Hubert, and Harris."], [""]]}, {"qid": "cb900171acc9047115b4", "term": "Bob Marley", "description": "Jamaican singer-songwriter", "question": "Can you find Bob Marley's face in most smoke shops?", "answer": true, "facts": ["Bob Marley's face is on the packaging of a popular brand of rolling papers.", "Bob Marley is a popular graphic to print on t-shirts for sale to smokers."], "decomposition": ["On what items is Bob Marley's face commonly found?", "Are some of #1 sold in most smoke shops?"], "evidence": [[["no_evidence"], [["Tobacconist-1"]]], [["no_evidence"], ["no_evidence", "operation"]], [[["Bob Marley-1"]], [["Head shop-1", "Tobacconist-1"], "operation"]]], "golden_sentence": [["Most retailers of tobacco sell other types of product; today supermarkets, in many countries with a special counter, are usually the main sellers of the common brands of cigarette."]]}, {"qid": "e99819ac9773bdf9a4b7", "term": "Martyr", "description": "person who suffers persecution and death for advocating, refusing to renounce, and/or refusing to advocate a belief or cause, usually a religious one", "question": "Can a martyr saint have been excommunicated?", "answer": true, "facts": ["Joan of Arc was excommunicated by the Catholic Church in 1431.", "Joan of Arc was declared a martyr in 1456 after an investigation ordered by King Charles VII.", "Joan of Arc was canonized a Saint by the Catholic Church on May 16, 1920."], "decomposition": ["Is Joan of Arc considered a matyr?", "Was she initially excommunicated by the Catholic Church?", "Is #1 or #2 negative?"], "evidence": [[[["Canonization of Joan of Arc-1"]], [["Canonization of Joan of Arc-2"]], ["operation"]], [[["Joan of Arc-3"]], [["Heresy-3", "Joan of Arc-37"], "no_evidence"], ["operation"]], [[["Joan of Arc-3"]], [["Canonization of Joan of Arc-2"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "06cd9e88d66df265f371", "term": "Homelessness", "description": "circumstance when people desire a permanent dwelling but do not have one", "question": "Could Toyota stadium house people suffering homelessness in Michigan?", "answer": true, "facts": ["As of 2019 there were an estimated 8,575 people experiencing homelessness in Michigan.", "Toyota stadium has a capacity of 45,000 people."], "decomposition": ["How many people are experiencing homelessness in Michigan?", "What is the capacity of the Toyota stadium?", "Is #1 less than #2?"], "evidence": [[[["Homelessness in the United States by state-127"]], [["Toyota Stadium (Texas)-1", "Toyota Stadium-1"]], ["operation"]], [[["Michigan-1"], "no_evidence"], [["Toyota Stadium-1"]], ["no_evidence", "operation"]], [[["Homelessness-58"], "no_evidence"], [["Toyota Stadium (Texas)-1"], "operation"], ["no_evidence"]]], "golden_sentence": [["Michigan has a high number of homeless individuals on its streets, reaching 97,642 in 2014."], ["Toyota Stadium is a soccer-specific stadium with a 20,500-seat capacity, built and owned by the city of Frisco, Texas, a suburb of Dallas.", "Toyota Stadium (\u8c4a\u7530\u30b9\u30bf\u30b8\u30a2\u30e0, Toyota Sutajiamu) is a 45,000 seat retractable roof stadium in Toyota, Aichi Prefecture, Japan."]]}, {"qid": "39925d60585ec83b9d90", "term": "Chief Justice of the United States", "description": "Presiding judge of the U.S. Supreme Court", "question": "Would it be impossible to seat every Chief Justice of the United States on a Boeing 737?", "answer": false, "facts": ["There have been a total of 17 Chief Justices since the Supreme Court was established.", "The Boeing 737 has evolved through four generations, offering several variants for 85 to 215 passengers."], "decomposition": ["How many Chief Justices has the Supreme Court had?", "What is the least amount of people that a Boeing 737 could hold?", "Is #2 smaller than #1?"], "evidence": [[[["Chief Justice of the United States-5"]], [["Boeing 737-4"]], ["operation"]], [[["Chief Justice of the United States-5"]], [["Boeing 737-5"]], ["operation"]], [[["Supreme Court of Alabama-10"], "no_evidence"], [["Boeing 737-61"], "no_evidence"], ["operation"]]], "golden_sentence": [["Five of the 17 chief justices\u2014John Rutledge, Edward Douglass White, Charles Evans Hughes, Harlan Fiske Stone, and William Rehnquist\u2014served as associate justice prior to becoming chief justice."], ["It evolved through four generations, offering several variants for 85 to 215 passengers."]]}, {"qid": "4f783483940f5a88d06d", "term": "Cookie Monster", "description": "character from the television series Sesame Street", "question": "Is Cookie Monster's diet Paleo friendly?", "answer": false, "facts": ["Cookie Monster is a Sesame Street character that eats copious amounts of chocolate chip cookies.", "The Paleo diet includes foods made from ingredients found during the Paleolithic area.", "Chocolate chip cookies contain soy lecithin and artificial grains.", "Lecithin is used in complex modern industrial processes."], "decomposition": ["What is the major component of the Cookie Monster's diet?", "What does Paleo diet consist of?", "Is #1 one of #2"], "evidence": [[[["Cookie Monster-1"]], [["Paleolithic diet-1", "Paleolithic diet-3"]], ["operation"]], [[["Cookie Monster-1"]], [["Paleolithic diet-3"]], ["operation"]], [[["Cookie Monster-1"]], [["Paleolithic diet-9"]], [["Paleolithic diet-9"]]]], "golden_sentence": [["Despite his voracious appetite for cookies, Cookie Monster shows awareness of healthy eating habits for young children and also enjoys fruits and eggplant."], ["The Paleolithic diet, Paleo diet, caveman diet, or stone-age diet is a modern fad diet requiring the sole or predominant eating of foods presumed to have been available to humans during the Paleolithic era.", "While there is wide variability in the way the paleo diet is interpreted, the diet typically includes vegetables, fruits, nuts, roots, and meat and typically excludes foods such as dairy products, grains, sugar, legumes, processed oils, salt, alcohol, and coffee."]]}, {"qid": "ed05f523a791adab6221", "term": "Bronze Age", "description": "Prehistoric period and age studied in archaeology, part of the Holocene Epoch", "question": "Were all the materials to make a cannon known during the bronze age?", "answer": false, "facts": ["The Bronze Age happened from about 3300 BC to 300 BC.", "Cannons require a fuse, gunpowder, and iron or other material to house the chemicals.", "Gunpowder was discovered around the 9th century AD."], "decomposition": ["What years did the Bronze age encompass?", "What materials are required for a cannon to fire?", "When were all the parts of #2 discovered?", "Are all the dates in #3 before or during #1?"], "evidence": [[[["Bronze Age Europe-21"]], [["Cannon-68"]], [["History of cannon-1"]], ["operation"]], [[["Late Bronze Age collapse-4"], "no_evidence"], [["Cannon-1"]], [["Cannon-5"]], ["operation"]], [[["Prehistory of Anatolia-28"], "no_evidence"], [["Gunpowder-1"], "no_evidence"], [["Military history of the Five Dynasties and Ten Kingdoms-22"], "no_evidence"], ["operation"]]], "golden_sentence": [["The Bronze Age in Ireland commenced in the centuries around 2000 BC when copper was alloyed with tin and used to manufacture Ballybeg type flat axes and associated metalwork."], ["Field artillery cannon in Europe and the Americas were initially made most often of bronze, though later forms were constructed of cast iron and eventually steel."], [""]]}, {"qid": "497d60977e6e7059a71e", "term": "National Hockey League", "description": "North American professional ice hockey league", "question": "Do American teams in National Hockey League outnumber Canadian teams?", "answer": true, "facts": ["The National Hockey League is the premiere North American hockey league.", "The National Hockey League has 7 Canadian teams.", "The National Hockey League has 24 teams from the United States."], "decomposition": ["How many Canadian teams are in the The National Hockey League?", "How many American teams are in the The National Hockey League?", "Is #2 greater than #1?"], "evidence": [[[["National Hockey League-1"]], [["National Hockey League-1"]], ["operation"]], [[["Ice hockey in the United States-5"]], [["Ice hockey in the United States-5"]], ["operation"]], [[["National Hockey League-1"]], [["National Hockey League-1"]], ["operation"]]], "golden_sentence": [["The National Hockey League (NHL; French: Ligue nationale de hockey\u2014LNH) is a professional ice hockey league in North America, currently comprising 31 teams: 24 in the United States and seven in Canada."], ["The National Hockey League (NHL; French: Ligue nationale de hockey\u2014LNH) is a professional ice hockey league in North America, currently comprising 31 teams: 24 in the United States and seven in Canada."]]}, {"qid": "a792b5395697cc902134", "term": "Elon Musk", "description": "American industrialist and investor", "question": "Has Elon Musk's hairline changed?", "answer": true, "facts": ["When Elon Musk was much younger, he was clearly balding.", "Elon Musk does not show any signs of balding as of 2020."], "decomposition": ["What feature of Elon Musk's hair was notable when he was younger?", "Is #1 no longer observable in present times?"], "evidence": [[[["Elon Musk-5"], "no_evidence"], ["no_evidence", "operation"]], [[["Elon Musk-1"], "no_evidence"], [["Hair loss-4"], "no_evidence", "operation"]], [["no_evidence"], ["no_evidence"]]], "golden_sentence": [[""]]}, {"qid": "b2de0404906f54655c38", "term": "Astronomer", "description": "Scientist who studies celestial bodies", "question": "Does Nintendo's link ever see an astronomer?", "answer": true, "facts": ["Link is the main character of the Nintendo franchise 'Zelda\".", "In \"Legend of Zelda: Majora's Mask\" Link meets an astronomer in an observatory."], "decomposition": ["Which game is Link from?", "In #1, did link meet an astronomer?"], "evidence": [[[["The Legend of Zelda-24"]], [["Universe of The Legend of Zelda-60"], "no_evidence", "operation"]], [[["Link (The Legend of Zelda)-1"]], ["no_evidence"]], [[["Link (The Legend of Zelda)-1"]], ["no_evidence", "operation"]]], "golden_sentence": [["The central protagonist of The Legend of Zelda series, Link is the name of various young men who characteristically wear a green tunic and a pointed cap, and are the bearers of the Triforce of Courage."], [""]]}, {"qid": "53679558de62b68d7454", "term": "Swastika", "description": "a geometrical figure and an ancient religious icon in the cultures of Eurasia and 20th-century symbol of Nazism", "question": "Does the swastika have positive uses?", "answer": true, "facts": ["The swastika is used in the Hindu religion to represent the sun.", "People practicing Hindu believe the swastika represents prosperity and good luck."], "decomposition": ["What does the swastika represent in Hinduism?", "What beliefs do Hindu worshippers associate with #1?", "Are #2 positive?"], "evidence": [[[["Swastika-34"]], [["Swastika-34"]], [["Swastika-34"]]], [[["Swastika-1", "Swastika-3"]], [["Swastika-3"]], ["operation"]], [[["Swastika-3"]], [["Swastika-3"]], [["Luck-24", "Prosperity-1"]]]], "golden_sentence": [["Hindu Swastikas The swastika is an important Hindu symbol."], [""], [""]]}, {"qid": "c08c4dcb215fa4dbd301", "term": "Doctorate", "description": "academic or professional degree", "question": "Is a doctorate required to teach at a SUNY School?", "answer": false, "facts": ["At SUNY schools, there are some full time professors with doctorates.", "At SUNY schools, there are adjunct professors who teach with a Masters degree. "], "decomposition": ["Is it the case, that there are no people teaching at SUNY that do not have a doctorate degree?"], "evidence": [[["no_evidence"]], [[["New York (state)-93", "Professor-18", "Professor-5"]]], [[["SUNY Downstate Medical Center-1"], "no_evidence", "operation"]]], "golden_sentence": []}, {"qid": "9830be4c1861e88b46ba", "term": "University of Pittsburgh", "description": "American state-related research university located in Pittsburgh, Pennsylvania", "question": "Did University of Pittsburgh founder have great deal in common with Judith Sheindlin?", "answer": true, "facts": ["Hugh Henry Brackenridge founded University of Pittsburgh in 1787.", "Judith Sheindlin is a judge, lawyer, and author.", "Hugh Henry Brackenridge was a writer, lawyer, judge, and Justice of the Supreme Court of Pennsylvania."], "decomposition": ["Who was the founder of University of Pittsburgh?", "What are the major things #1 is known for?", "What are the major things Judith Sheindlin is known for?", "Is there an overlap between #2 and #3?"], "evidence": [[[["History of the University of Pittsburgh-2"]], [["Hugh Henry Brackenridge-4"]], [["Judy Sheindlin-1"]], [["Judge-5"], "operation"]], [[["University of Pittsburgh-1"]], [["Hugh Henry Brackenridge-1"]], [["Judy Sheindlin-1"]], ["operation"]], [[["History of the University of Pittsburgh-2"]], [["Hugh Henry Brackenridge-1"]], [["Judy Sheindlin-1"]], ["operation"]]], "golden_sentence": [["Founded by Hugh Henry Brackenridge as Pittsburgh Academy in 1787, the University of Pittsburgh is among a select group of universities and colleges established in the 18th century in the United States."], [""], ["Judith Susan Sheindlin (n\u00e9e Blum; born October 21, 1942), known professionally as Judge Judy, is an American prosecution lawyer, former Manhattan family court judge, television personality, and author."], [""]]}, {"qid": "a96e07f94c0489ca507d", "term": "Los Angeles County, California", "description": "County in California, United States", "question": "Is Disney associated with Los Angeles County?", "answer": true, "facts": ["Disney Concert Hall and Disney Studio are located in Los Angeles.", "The city of Los Angeles is located in Los Angeles County."], "decomposition": ["Where are Disney Concert Hall and Disney Studio located?", "Is #1 located in Los Angeles County?"], "evidence": [[[["Walt Disney Animation Studios-1", "Walt Disney Concert Hall-1"]], [["Burbank, California-1", "Central Los Angeles-1", "Downtown Los Angeles-1"], "operation"]], [[["Walt Disney Concert Hall-1", "Walt Disney Studios (division)-1"]], [["Burbank, California-1", "Los Angeles County, California-1"]]], [[["Walt Disney Studios (Burbank)-4"]], [["Burbank, California-1"], "operation"]]], "golden_sentence": [["", "The Walt Disney Concert Hall at 111 South Grand Avenue in downtown Los Angeles, California, is the fourth hall of the Los Angeles Music Center and was designed by Frank Gehry."], ["Burbank is a city in Los Angeles County in the Los Angeles metropolitan area of Southern California, United States, 12 miles (19\u00a0km) northwest of downtown Los Angeles, in the southeastern end of the San Fernando Valley.", "", ""]]}, {"qid": "7db7d2bfe221515eeeb0", "term": "Pea", "description": "species of plant", "question": "Could a bee hummingbird balance a scale with a single pea on it?", "answer": false, "facts": ["The average pea weighs between 0.1 and 0.36 grams.", "Female bee hummingbirds on average weigh 2.6 grams, while on average male bee hummingbirds weigh 1.95 grams."], "decomposition": ["What is the weight range of the average pea?", "What is the weight range of the average bee hummbingbird?", "Is there an overlap between #1 and #2?"], "evidence": [[[["Pea-2"]], [["Bee hummingbird-2"]], ["operation"]], [[["Pea-2"]], [["Bee hummingbird-2"]], ["operation"]], [[["Pea-2"]], [["Bee hummingbird-2"]], ["operation"]]], "golden_sentence": [["The average pea weighs between 0.1 and 0.36\u00a0gram."], ["Females weigh 2.6\u00a0g (0.092\u00a0oz) and are 6.1\u00a0cm (2.4\u00a0in) long, and are slightly larger than males, with an average weight of 1.95\u00a0g (0.069\u00a0oz) and length of 5.5\u00a0cm (2.2\u00a0in)."]]}, {"qid": "3a983b5fc2c864a11325", "term": "Abstract art", "description": "Art with a degree of independence from visual references in the world", "question": "Can photography be considered abstract art?", "answer": false, "facts": ["Abstract art is a form of modern art that does not reflect images of our every day world.", "Abstract art relies on exaggerated colors and shapes.", "Photography is an art that uses cameras to take pictures of events unfolding in the real world."], "decomposition": ["What kind of events/scenarios is depicted in abstract art?", "What kind of imagery does photography capture?", "Is #1 very similar to #2?"], "evidence": [[[["Abstract art-5"]], [["Photography-1"]], ["operation"]], [[["Abstract art-1", "Abstract art-3"]], [["Photography-1", "Photography-2"]], ["operation"]], [[["Abstract art-1"]], [["Photography-68"]], ["operation"]]], "golden_sentence": [[""], ["Photography is the art, application and practice of creating durable images by recording light or other electromagnetic radiation, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as photographic film."]]}, {"qid": "543dbae0f950fdf94038", "term": "Great Depression", "description": "20th-century worldwide economic depression", "question": "Can the Great Depression be treated with Prozac?", "answer": false, "facts": ["Prozac is a pharmaceutical antidepressant for treatment of psychological disorders", "The Great Depression was an economic phenomenon occurring in the early 20th century"], "decomposition": ["What conditions can be treated with Prozac?", "The conditions in #1 are inflicted upon what?", "Does the Great Depression have #2?"], "evidence": [[[["Fluoxetine-1"]], [["Depression (mood)-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Fluoxetine-1"]], [["Major depressive disorder-1"]], [["Great Depression-1"], "operation"]], [[["Fluoxetine-6"]], ["no_evidence"], ["no_evidence"]]], "golden_sentence": [["It is used for the treatment of major depressive disorder, obsessive\u2013compulsive disorder (OCD), bulimia nervosa, panic disorder, and premenstrual dysphoric disorder."], [""]]}, {"qid": "ac6ad458b87d16ea4000", "term": "Edward Snowden", "description": "American whistleblower and former National Security Agency contractor", "question": "Is Edward Snowden in hiding from the United States?", "answer": false, "facts": ["Edward Snowden has an active twitter account and has been on political commentary shows.", "Edward Snowden's country of residence is listed on his Wikipedia."], "decomposition": ["Where does Edward Snowden live?", "Is the location of #1 kept secret?"], "evidence": [[[["Edward Snowden-77"]], [["Edward Snowden-77"], "operation"]], [[["Edward Snowden-78"]], [["Edward Snowden-78"], "no_evidence"]], [[["Edward Snowden-78"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "6b7f978eed51f802f8e4", "term": "Bumblebee", "description": "genus of insects", "question": "Are aggressive bumblebees suicidal?", "answer": false, "facts": ["Bees with barbed stingers lose the barb after attacking a victim and die soon afterwards", "Bumblebees do not have barbed stingers and can sting multiple times without dying"], "decomposition": ["Can bees with non-barbed stingers sting multiple times?", "Do bumblebees have non-barbed stingers?", "By #1 and #2 do bumblebees die after stinging just once?"], "evidence": [[[["Honey bee-61"]], [["Bumblebee-45"]], ["operation"]], [[["Bombus ternarius-23"]], [["Bumblebee-45"]], ["operation"]], [[["Bee sting-6"]], [["Bumblebee-45"]], ["operation"]]], "golden_sentence": [[""], ["Unlike in honeybees, a bumblebee's stinger lacks barbs, so the bee can sting repeatedly without injuring itself; by the same token, the stinger is not left in the wound."]]}, {"qid": "d9a48dd737e3a57da2a8", "term": "Alfred Nobel", "description": "Swedish chemist, innovator, and armaments manufacturer (1833\u20131896)", "question": "Has categories of Nobel prizes remained same since Alfred Nobel established them?", "answer": false, "facts": ["Alfred Nobel established the Nobel prize in his will in 1895.", "Alfred Nobel established 5 Nobel prize categories: Chemistry, Literature, Peace, Physics, and Physiology or Medicine.", "The Nobel prize evolved to include a sixth category of Economic Sciences in 1968."], "decomposition": ["When did Alfred Nobel establish the Nobel prize?", "Has the Nobel Prize remained unchanged in all respects since #1?"], "evidence": [[[["Nobel Prize-1"]], [["Nobel Prize-21"], "operation"]], [[["Nobel Prize-1"]], [["Nobel Prize-2"]]], [[["Alfred Nobel-13"]], [["Alfred Nobel-14", "Alfred Nobel-17"], "operation"]]], "golden_sentence": [["The will of the Swedish chemist, engineer and industrialist Alfred Nobel established the five Nobel prizes in 1895."], [""]]}, {"qid": "7f138d334aa50ce27b4e", "term": "British Airways", "description": "flag carrier airline of the United Kingdom", "question": "Is British Airways the air force of the United Kingdom?", "answer": false, "facts": ["British Airways is a commercial transportation company.", "The military air force of the United Kingdom is named the Royal Air Force. ", "British Airways is headquartered in London. ", "Royal Air Force is part of the British Armed Forces."], "decomposition": ["What is the air force of the United Kingdom known as?", "Is #1 the same as British Airways?"], "evidence": [[[["Royal Air Force-4"]], [["British Airways-1"], "no_evidence"]], [[["Royal Air Force-1"]], [["British Airways-1"]]], [[["Royal Air Force-1"]], [["British Airways-2"], "operation"]]], "golden_sentence": [["While the British were not the first to make use of heavier-than-air military aircraft, the RAF is the world's oldest independent air force: that is, the first air force to become independent of army or navy control."], ["It is the second largest airline in the United Kingdom, based on fleet size and passengers carried, behind easyJet."]]}, {"qid": "7a4a220837a17f87b3c0", "term": "Cerebral palsy", "description": "A group of disorders affecting the development of movement and posture, often accompanied by disturbances of sensation, perception, cognition, and behavior. It results from damage to the fetal or infant brain.", "question": "Is a slime mold safe from cerebral palsy?", "answer": true, "facts": ["Cerebral palsy is a disorder caused by damage to fetal or infant brains.", "Slime molds are simple organisms that are similar to fungi.", "Slime molds do not possess a brain."], "decomposition": ["Damage to what structure can cause cerebral palsy?", "What structures do slime molds have?", "Is #1 listed in #2?"], "evidence": [[[["Cerebral palsy-2"]], [["Plasmodium (life cycle)-3"]], ["operation"]], [[["Cerebral palsy-2"]], [["Slime mold-18"]], ["operation"]], [[["Cerebral palsy-2"]], [["Slime mold-18"], "no_evidence"], ["operation"]]], "golden_sentence": [["Cerebral palsy is caused by abnormal development or damage to the parts of the brain that control movement, balance, and posture."], [""]]}, {"qid": "9dff48bfef4c397be63c", "term": "Harry Potter and the Philosopher's Stone", "description": "1997 fantasy novel by J. K. Rowling", "question": "Was Harry Potter and the Philosopher's Stone popular during the great depression?", "answer": false, "facts": ["The Great Depression started in 1929 and ended in 1933.", "Harry Potter and the Philosopher's Stone was first published in 1997."], "decomposition": ["When did the Great Depression end?", "When was Harry Potter and the Philosopher's Stone first published?", "Is #2 before #1?"], "evidence": [[[["Great Depression-1"]], [["Harry Potter-2"]], [["Harry Potter-2"], "operation"]], [[["Great Depression-1"]], [["Harry Potter and the Philosopher's Stone-2"]], ["operation"]], [[["Great Depression-1"]], [["Harry Potter and the Philosopher's Stone-2"]], ["operation"]]], "golden_sentence": [["The timing of the Great Depression varied across the world; in most countries, it started in 1929 and lasted until the late 1930s."], ["Since the release of the first novel, Harry Potter and the Philosopher's Stone, on 26 June 1997, the books have found immense popularity, critical acclaim and commercial success worldwide."], [""]]}, {"qid": "4c30ec414fdb307b9393", "term": "United States Department of Defense", "description": "United States federal executive department", "question": "Is the CIA part of the Department of Defense?", "answer": false, "facts": ["The Department of Defense covers national defense and the armed forces, led by the Secretary of Defense.", "The CIA is a federal agency within the United States Intelligence Community, which answers to the Director of National Intelligence."], "decomposition": ["Which agencies are under the United States Department of Defense?", "Is the CIA included in #1?"], "evidence": [[[["United States Department of Defense-1"], "no_evidence"], [["Central Intelligence Agency-1"], "no_evidence", "operation"]], [[["United States Department of Defense-2"]], ["operation"]], [[["United States Department of Defense-2"]], ["operation"]]], "golden_sentence": [["The United States Department of Defense (DoD, USDOD or DOD) is an executive branch department of the federal government charged with coordinating and supervising all agencies and functions of the government directly related to national security and the United States Armed Forces."], [""]]}, {"qid": "4bccc861a26979e11451", "term": "Barley", "description": "Species of plant", "question": "Would the owners of the company Peter Griffin works for need barley?", "answer": true, "facts": ["Peter Griffin works for Pawtucket Brewery.", "Pawtucket Brewery produces beer.", "Barley is the preferred grain for making beer."], "decomposition": ["What kind of company is Peter Griffin?", "What does #1 produce?", "Does producing #2 require barley?"], "evidence": [[[["Peter Griffin-2"], "no_evidence"], [["Brewery-1"]], [["Brewery-27"], "operation"]], [[["Peter Griffin-2"]], [["Brewery-1"]], [["Beer-1"], "operation"]], [[["Peter Griffin-2"]], [["Brewery-1"]], [["Barley-1"]]]], "golden_sentence": [[""], [""], [""]]}, {"qid": "28dd04d21d71c90354c0", "term": "Chipmunk", "description": "Tribe of mammals (rodent (marmot))", "question": "Is an Eastern chipmunk likely to die before seeing two leap years?", "answer": true, "facts": ["A leap year happens every four years.", "The Eastern chipmunk has an average lifespan of three years."], "decomposition": ["What is the average lifespan of an Eastern chipmunk?", "How often does a leap year occur?", "Is #2 greater than #1?"], "evidence": [[[["Eastern chipmunk-7"]], [["Leap year-6"]], ["operation"]], [[["Chipmunk-11"]], [["Leap year-2"]], ["operation"]], [[["Chipmunk-11"]], [["Leap year-2"]], ["operation"]]], "golden_sentence": [["On average, eastern chipmunks live three or more years in the wild, but in captivity they may live as long as eight years."], [""]]}, {"qid": "b6e34aaa8ffe169b7c89", "term": "Ahura Mazda", "description": "highest deity of Zoroastrianism", "question": "Does Ahura Mazda have a rivalry with Zeus?", "answer": false, "facts": ["Ahura Mazda is a deity of Zoroastrianism, a contemporary religion", "Zeus is a deity of Greek mythology"], "decomposition": ["What belief system is Ahura Mazda associated with?", "What belief system is Zeus associated with?", "Is #1 the same as #2?"], "evidence": [[[["Ahura Mazda-1"]], [["Zeus-1"]], ["operation"]], [[["Ahura Mazda-1"]], [["Zeus-1"]], ["operation"]], [[["Ahura Mazda-1"]], [["Zeus-1"]], ["operation"]]], "golden_sentence": [["Ahura Mazda (/\u0259\u02cch\u028a\u0259r\u0259 \u02c8m\u00e6zd\u0259/; Avestan: \ud802\udf28\ud802\udf00\ud802\udf30\ud802\udf1b\ud802\udf01 \ud802\udf00\ud802\udf35\ud802\udf0e\ud802\udf2d\ud802\udf00\u200e, romanized:\u00a0Mazd\u0101 Ahura also known as Oromasdes, Ohrmazd, Ahuramazda, Hourmazd, Hormazd, and Hurmuz) is the creator and highest deity of Zoroastrianism."], ["Zeus is the sky and thunder god in ancient Greek religion, who rules as king of the gods of Mount Olympus."]]}, {"qid": "f61269db5c7d0c479469", "term": "Holy Land", "description": "Term used by Jews, Christians, and Muslims to describe the Land of Israel and Palestine", "question": "Did Holy Land belong to Adamu's tribe?", "answer": true, "facts": ["The Holy Land is a place that Jews, Muslims, and Christians revere.", "Adamu was an early king of Assyria.", "The Assyrians were in regions of the Holy Land as far back as 2600 BC.", "The predecessors to the Assyrians were in regions of the Holy Land as far back as 3500 BC."], "decomposition": ["Which place is referred to as the Holy Land?", "Which tribe was Adamu a leader of?", "Did #2 occupy #1?"], "evidence": [[[["Holy Land-1"]], [["Adamu (Assyrian king)-1"]], [["Adamu (Assyrian king)-4", "Assyria-1", "Mesopotamia-1"]]], [[["Holy Land-1"]], [["Adamu (Assyrian king)-1", "Assyrian people-1", "Assyrian people-2", "Assyrian people-51"]], ["no_evidence", "operation"]], [[["Holy place-8"], "operation"], [["Adamu Adamu-2"], "no_evidence"], ["no_evidence"]]], "golden_sentence": [["The Holy Land (Hebrew: \u05d0\u05b6\u05e8\u05b6\u05e5 \u05d4\u05b7\u05e7\u05bc\u05d5\u05b9\u05d3\u05b6\u05e9\u05c1 Eretz HaKodesh, Latin: Terra Sancta; Arabic: \u0627\u0644\u0623\u0631\u0636 \u0627\u0644\u0645\u0642\u062f\u0633\u0629 Al-Ar\u1e0d Al-Muqaddasah or \u0627\u0644\u062f\u064a\u0627\u0631 \u0627\u0644\u0645\u0642\u062f\u0633\u0629 Ad-Diyar Al-Muqaddasah) is an area roughly located between the Jordan River and the Mediterranean Sea that also includes the Eastern Bank of the Jordan River."], [""], ["", "", ""]]}, {"qid": "8f3c8bf968987d6a87c4", "term": "Edward II of England", "description": "14th-century King of England and Duke of Aquitaine", "question": "Was Edward II crucial to England's victory at Battle of Falkirk?", "answer": false, "facts": ["The Battle of Falkirk was a battle between England and the Scots.", "King Edward I led English forces to victory against William Wallace at the Battle of Falkirk.", "The Battle of Falkirk took place in 1298.", "Edward II was born in 1284 and his first campaign with his father against Scotland happened in 1300.", "Edward II was knighted in 1306."], "decomposition": ["When did the Battle of Falkirk occur?", "When did Edward II start appearing at battles with his father?", "Did #2 occur before #1?"], "evidence": [[[["Battle of Falkirk-1"]], [["Edward II of England-9"], "no_evidence"], ["no_evidence", "operation"]], [[["Battle of Falkirk-1"]], [["Edward II of England-1"]], ["operation"]], [[["Battle of Falkirk-1"]], [["Edward II of England-14"]], ["operation"]]], "golden_sentence": [["The Battle of Falkirk (Bl\u00e0r na h-Eaglaise Brice in Gaelic), which took place on 22 July 1298, was one of the major battles in the First War of Scottish Independence."], [""]]}, {"qid": "12556cb8e7a3a3f8dec7", "term": "Model (person)", "description": "person employed to display, advertise and promote products, or to serve as a visual aid", "question": "Would a model be likely to frequently enjoy the menu at Cookout?", "answer": false, "facts": ["Models are known for being very thin on average.", "Cookout serves high calorie American style barbecue food.", "Models often have pressure put on them to maintain a slim figure."], "decomposition": ["What is the typical body shape of a model?", "What kind of food does a cookout typically have?", "Are #2 foods high in calories?", "In order to maintain #1, what kinds of food must a person eat?", "Does #3 match with #4?"], "evidence": [[[["Model (person)-22"]], [["Cook Out (restaurant)-1"]], [["Food energy-4"]], [["Model (person)-24"], "no_evidence"], ["operation"]], [[["The Thin Ideal-21"]], [["Cook Out (restaurant)-1"]], ["operation"], [["Dieting-13"]], ["operation"]], [[["Model (person)-24"]], [["Cook Out (restaurant)-1"]], [["Fast food-6"]], [["Dieting-1"]], ["operation"]]], "golden_sentence": [[""], ["Their menu primarily features grilled hamburgers and cheeseburgers, hot dogs, chicken sandwiches, and North Carolina barbecue."], [""], [""]]}, {"qid": "5968c052e12ee6070155", "term": "Markhor", "description": "species of mammal", "question": "Could a markhor give birth three times in a single year?", "answer": false, "facts": ["The gestation period of a markhor lasts 135\u2013170 days.", "There are 365 days in a year."], "decomposition": ["What is the gestation period of a Markhor?", "How many days are in a year?", "Can #1 be divided into #2 at least 3 times"], "evidence": [[[["Markhor-6"]], [["Calendar year-2"]], [["Calendar year-2", "Markhor-6"], "operation"]], [[["Markhor-6"]], [["Year-3"]], ["operation"]], [[["Markhor-6"]], [["Year-3"]], ["operation"]]], "golden_sentence": [["The gestation period lasts 135\u2013170 days, and usually results in the birth of one or two kids, though rarely three."], ["It has a length of 365 days in an ordinary year, with 8,760 hours, 525,600 minutes, or 31,536,000 seconds; but 366 days in a leap year, with 8,784 hours, 527,040 minutes, or 31,622,400 seconds."], ["Other formula-based calendars can have lengths which are further out of step with the solar cycle: for example, the Julian calendar has an average length of 365.25 days, and the Hebrew calendar has an average length of 365.2468 days.", ""]]}, {"qid": "8a9ce0253034964b99ad", "term": "The Onion", "description": "American news satire organization", "question": "Could a delicious recipe be made with The Onion?", "answer": false, "facts": ["Despite its name, The Onion is not a food, but rather an organization.", "It is not possible to eat a business organization."], "decomposition": ["What is The Onion?", "Is #1 an item you can eat?"], "evidence": [[[["The Onion-1"]], ["operation"]], [[["The Onion-1"]], ["operation"]], [[["The Onion-1"]], [["Company-1", "Food-1"]]]], "golden_sentence": [["The Onion is an American satirical digital media company and newspaper organization that publishes articles on international, national, and local news."]]}, {"qid": "65add255e3bc322f2028", "term": "Fax", "description": "method of transmitting images, often of documents", "question": "Do most college students own a fax machine?", "answer": false, "facts": ["College students typically must submit assignments via email, web portal, or on paper.", "Most colleges have on-campus fax machines available for student use."], "decomposition": ["How do college students typically submit their assignments nowadays?", "Does #1 require a fax machine?"], "evidence": [[[["Student-54"], "no_evidence"], [["Fax-1"], "no_evidence"]], [[["Educational technology-1", "Educational technology-14"], "no_evidence"], [["Fax-1"], "operation"]], [[["Email-1", "Email-45"], "no_evidence"], ["operation"]]], "golden_sentence": [["Accordingly, college students are often called Freshmen, Sophomores, Juniors and Seniors (respectively), unless their undergraduate program calls for more than the traditional 4 years."], ["The original document is scanned with a fax machine (or a telecopier), which processes the contents (text or images) as a single fixed graphic image, converting it into a bitmap, and then transmitting it through the telephone system in the form of audio-frequency tones."]]}, {"qid": "afe20e5d2bd20b20c8b3", "term": "Leafhopper", "description": "family of insects", "question": "Do Leafhoppers compete with Log Cabin syrup producers for resources?", "answer": true, "facts": ["Leafhoppers are insects that feed on sap from trees.", "Log Cabin is a  company that originated in Minnesota and makes several varieties of maple syrup.", "Sap is an ingredient in maple syrup."], "decomposition": ["What does the leafhopper diet consist of?", "What kind of syrup is produced by Log Cabin?", "What are the ingredients in #2?", "Is any substance listed in #1 also found in #3?"], "evidence": [[[["Leafhopper-6"], "no_evidence"], [["Log Cabin syrup-1", "Syrup-2"], "no_evidence"], ["operation"], ["operation"]], [[["Leafhopper-1"]], [["Log Cabin syrup-1"], "no_evidence"], [["Maple syrup-1"]], ["operation"]], [[["Leafhopper-6"]], [["Log Cabin syrup-4"]], [["Maple syrup-6"]], ["operation"]]], "golden_sentence": [["A leafhoppers' diet commonly consists of sap from a wide and diverse range of plants, but some are more host-specific."], ["Log Cabin syrup is an American brand of pre-packaged syrups owned by Pinnacle Foods.", "Syrups can be made by dissolving sugar in water or by reducing naturally sweet juices such as cane juice, sorghum juice, maple sap or agave nectar."]]}, {"qid": "0f1d5115265164de73e0", "term": "Scottish people", "description": "ethnic inhabitants of Scotland", "question": "Does the Pixar film Brave feature Scottish people?", "answer": true, "facts": ["The movie Brave is set in the Scottish highlands.", "Merida, the main character of Brave, is a Princess of Medieval Scotland "], "decomposition": ["Who are the main characters of the Pixar film Brave?", "Are any of #1 from Scotland?"], "evidence": [[[["Brave (2012 film)-4"]], ["operation"]], [[["Brave (2012 film)-4"]], [["Brave (2012 film)-4"], "operation"]], [[["Brave (2012 film)-1", "Brave (2012 film)-4"]], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "f5e048abe42a257e980b", "term": "Chick-fil-A", "description": "American fast food chain", "question": "Would a vegetarian be able to eat something at Chick-fil-A?", "answer": true, "facts": ["Most people who follow a vegetarian diet don't eat meat, fish or poultry. ", "While Chick-fil-A sells chicken, they also offer other items. ", "Items that are meat free include: hash browns, waffle fries, and superfood sides."], "decomposition": ["What foods must a vegetarian avoid?", "What foods are on the menu of Chick-fil-A?", "Are any items in #2 free of #1?"], "evidence": [[[["Vegetarianism-1"]], [["Chick-fil-A-39"]], [["Crinkle-cutting-4", "Potato-1"], "operation"]], [[["Vegetarianism-1"]], [["Chick-fil-A-39"]], ["operation"]], [[["Vegetarianism-1"]], [["Chick-fil-A-39"]], ["operation"]]], "golden_sentence": [["Vegetarianism is the practice of abstaining from the consumption of meat (red meat, poultry, seafood, and the flesh of any other animal), and may also include abstention from by-products of animal slaughter."], ["Dwarf House and Truett's Grill in Griffin, Georgia Chick-fil-a Dwarf House entrance, Griffin, Spalding County, Georgia Truett's Grill, 1455 N Express Way, Griffin, Spalding County, Georgia Chick-fil-a Dwarf House, Griffin, Spalding County, Georgia Based on data from 2018, the most popular (most ordered) item was the waffle fries followed by soft drinks, chicken nuggets, and the original chicken sandwich."], ["", ""]]}, {"qid": "3ac122536d88ed4b52ee", "term": "Eid al-Fitr", "description": "Islamic holiday that marks the end of Ramadan", "question": "Could  jockey win Triple Crown between Eid al-Fitr endpoints?", "answer": false, "facts": ["The Triple Crown is an accomplishment in which a jockey wins three specific races.", "The three Triple Crown races are: Preakness, Kentucky Derby, and Belmont Stakes.", "The three Triple Crown races take place weeks apart.", "Eid al-Fitr is a Muslim holiday that lasts for three consecutive days."], "decomposition": ["How long does Eid al-Fitr last?", "How long is it between the first and last races of the Triple Crown?", "Is #1 longer than #2?"], "evidence": [[[["Eid al-Fitr-4"]], [["Belmont Stakes-1"]], [["Week-1"], "operation"]], [[["Eid al-Fitr-1"]], [["Triple Crown of Thoroughbred Racing (United States)-1"], "no_evidence"], ["operation"]], [[["Eid al-Fitr-1"]], [["Belmont Stakes-1", "Kentucky Derby-1", "Triple Crown of Thoroughbred Racing (United States)-1"]], ["operation"]]], "golden_sentence": [["Eid al-Fitr is celebrated for one to three days, depending on the country."], ["The race, nicknamed The Test of the Champion, and The Run for the Carnations, is the traditional third and final leg of the Triple Crown, third of four majors in horse racing for 3-year old colts and is held five weeks after the Kentucky Derby and three weeks after the Preakness Stakes."], [""]]}, {"qid": "ace3712f426b4c691a70", "term": "Harry Houdini", "description": "American magician, escapologist, and stunt performer", "question": "Did Harry Houdini appear on Chris Angel Mindfreak?", "answer": false, "facts": ["Chris Angel Mindfreak was released in 2005.", "Harry Houdini died in 1926."], "decomposition": ["When did Harry Houdini's career as an entertainer come to an end?", "When was the Criss Angel Mindfreak show first aired?", "Is #2 before #1?"], "evidence": [[[["Harry Houdini-68"]], [["Criss Angel Mindfreak-1"]], ["operation"]], [[["Harry Houdini-39"]], [["Criss Angel Mindfreak-1"]], ["operation"]], [[["Harry Houdini-71"]], [["Criss Angel Mindfreak-1"]], ["operation"]]], "golden_sentence": [["Harry Houdini died of peritonitis, secondary to a ruptured appendix, at 1:26\u00a0p.m. on October 31, 1926, in Room 401 at Detroit's Grace Hospital, aged 52."], ["Criss Angel: Mindfreak is an American reality TV show that aired on A&E from 2005 to 2010."]]}, {"qid": "0c13d6b8a74ac494dc23", "term": "Psychotherapy", "description": "clinically applied psychology for desired behavior modification", "question": "Do some psychotherapy patients have no mental illness?", "answer": true, "facts": ["Psychotherapy is useful for couples navigating relationship issues.", "Grief is a common reason that people seek psychotherapy. "], "decomposition": ["What are some common issues that make people seek psychotherapy?", "Does #1 not always involve mental illness?"], "evidence": [[[["Psychotherapy-1"]], ["operation"]], [[["Psychotherapy-1"]], [["Psychotherapy-4"], "operation"]], [[["Psychotherapy-1"]], [["Psychotherapy-32"], "operation"]]], "golden_sentence": [[""]]}, {"qid": "e5a948bdd4108a36e596", "term": "Lolcat", "description": "image combining a photograph of a cat with text intended to contribute humour", "question": "Is purchasing food for a Lolcat unnecessary?", "answer": true, "facts": ["An image macro is a piece of digital media featuring a picture, or artwork, superimposed with some form of text.", "Food is any substance consumed to provide nutritional support for an organism.", "An organism is any individual entity that embodies the properties of life.", "Digital media does not embody the properties of life."], "decomposition": ["Which kind of entities require food?", "Is a lolcat excluded from #1?"], "evidence": [[[["Eating-1"]], [["Lolcat-2"]]], [[["Food-1", "Organism-1", "Organism-2"]], [["Image macro-1", "Lolcat-1", "Media (communication)-1"]]], [[["Food-1"]], [["Lolcat-2"], "operation"]]], "golden_sentence": [["Animals and other heterotrophs must eat in order to survive \u2014 carnivores eat other animals, herbivores eat plants, omnivores consume a mixture of both plant and animal matter, and detritivores eat detritus."], [""]]}, {"qid": "2402914fa50005cf8bdb", "term": "Common warthog", "description": "Wild member of the pig family", "question": "Would a Common warthog starve in a greenhouse?", "answer": false, "facts": ["A greenhouse is an enclosed building in which plants are grown.", "The Common warthog is an animal that feeds on grasses, roots, berries, and small insects.", "Aphids, fungus gnats, and caterpillars, are common insects found in greenhouses."], "decomposition": ["What kind of things are found in a greenhouse?", "What does the warthog diet consist of?", "Is there significant overlap between #1 and #2?"], "evidence": [[[["Greenhouse-31"]], [["Phacochoerus-2"]], [["Greenhouse-31", "Herbivore-1"]]], [[["Greenhouse-1"]], [["Common warthog-5"]], ["operation"]], [[["Greenhouse-27"]], [["Common warthog-5"]], ["operation"]]], "golden_sentence": [["Many vegetables and flowers can be grown in greenhouses in late winter and early spring, and then transplanted outside as the weather warms."], [""], ["", ""]]}, {"qid": "50230b3edd343195121c", "term": "Saltwater crocodile", "description": "species of reptile", "question": "Would alligator best saltwater crocodile in hypothetical Lake Urmia battle?", "answer": false, "facts": ["Lake Urmia is a salt lake in Iran.", "Saltwater crocodiles have special glands that allow them to survive in salt water.", "Alligators lack glands to stay in salt water for extended periods of time."], "decomposition": ["What kind of water is in Lake Urmia?", "Can alligators survive for long in #1?"], "evidence": [[[["Lake Urmia-1"]], ["no_evidence"]], [[["Lake Urmia-1"]], [["Alligator-8"]]], [[["Lake Urmia-3"]], ["no_evidence"]]], "golden_sentence": [["Lake Urmia (Persian: \u062f\u0631\u06cc\u0627\u0686\u0647 \u0627\u0631\u0648\u0645\u06cc\u0647\u200e, Dary\u00e2che-ye Orumiye) is an endorheic salt lake in Iran."]]}, {"qid": "5add7bc2349d660befff", "term": "Sirius", "description": "Brightest star in the night sky", "question": "Is Sirius part of a constellation of an animal?", "answer": true, "facts": ["Sirius is the brightest star in the constellation Canis Major.", "Canis Major represents a large dog."], "decomposition": ["What constellation is Sirius a part of?", "What does #1 represent?", "Is #2 an animal?"], "evidence": [[[["Canis Major-2"]], [["Canis Major-1"]], ["operation"]], [[["Sirius-4"]], [["Canis Major-1"]], ["operation"]], [[["Sirius-4"]], [["Canis Major-4"]], [["Animal-4"]]]], "golden_sentence": [[""], ["Its name is Latin for \"greater dog\" in contrast to Canis Minor, the \"lesser dog\"; both figures are commonly represented as following the constellation of Orion the hunter through the sky."]]}, {"qid": "bd085ef4bb603f64048b", "term": "The Godfather", "description": "1972 film directed by Francis Ford Coppola", "question": "Is Y2K relevant to the plot of The Godfather?", "answer": false, "facts": ["The story in the Godfather spans from 1945 to 1955.", "Y2K refers to events related to the formatting and storage of calendar data for dates beginning in the year 2000."], "decomposition": ["What era is the story of The Godfather set in?", "What year does Y2K refer to?", "Is #2 included in #1?"], "evidence": [[[["The Godfather-1"]], [["Year 2000 problem-1"]], ["operation"]], [[["The Godfather-1"]], [["Year 2000 problem-10"]], ["operation"]], [[["The Godfather-1"]], [["Year 2000 problem-1"]], ["operation"]]], "golden_sentence": [["The story, spanning from 1945 to 1955, chronicles the Corleone family under patriarch Vito Corleone (Brando), focusing on the transformation of one of his sons, Michael Corleone (Pacino), from reluctant family outsider to ruthless mafia boss."], ["The Year 2000 problem, also known as the Y2K problem, the Millennium bug, Y2K bug, the Y2K glitch, or Y2K, refers to events related to the formatting and storage of calendar data for dates beginning in the year 2000."]]}, {"qid": "26d53544ad52a90eedb0", "term": "Scrabble", "description": "board game with words", "question": "Could a two-year old win a Scrabble tournament?", "answer": false, "facts": ["Scrabble is a word game that requires a large vocabulary in order to play well.", "A two-year old has a very limited vocabulary and lacks the reasoning capability needed to perform well in Scrabble."], "decomposition": ["What size vocabulary do Scrabble champions have?", "What size vocabulary do two-year olds have?", "is #2 greater than #1?"], "evidence": [[["no_evidence"], [["Language development-13"]], ["no_evidence", "operation"]], [[["World Scrabble Championship-4"], "no_evidence"], [["Vocabulary-3"], "no_evidence"], ["no_evidence", "operation"]], [[["Official Scrabble Players Dictionary-9"], "no_evidence"], [["Toddler-5"]], ["operation"]]], "golden_sentence": [["The vocabulary of a 1\u20132 year old should consist of 50 words and can be up to 500."]]}, {"qid": "6acdfc724b0eb2e8cf48", "term": "Chives", "description": "edible species of plant", "question": "Could chives be mistaken for grass?", "answer": true, "facts": ["Chives grow upwards in thin green cylindrical shoots. ", "Grass grows upwards in thin green flat shoots."], "decomposition": ["What is the shape and color of Chives?", "What is the shape and color of grass?", "Is #1 the same as #2?"], "evidence": [[[["Chives-4"]], ["no_evidence"], ["operation"]], [[["Chives-4"], "no_evidence"], [["Poaceae-42"], "no_evidence"], ["operation"]], [[["Chives-6"]], [["Poaceae-15"], "no_evidence"], ["operation"]]], "golden_sentence": [["In culinary use, the green stalks (scapes) and the unopened, immature flower buds are diced and used as an ingredient for omelettes, fish, potatoes, soups, and many other dishes."]]}, {"qid": "da640e42bb08a025625a", "term": "New York Harbor", "description": "harbor in the New York City, U.S.A. metropolitan area", "question": "Does a giant green lady stand in New York Harbor?", "answer": true, "facts": ["New York Harbor is a body of water between south Manhattan and the Atlantic Ocean", "The Statue of Liberty stands in New York Harbor", "The Statue of Liberty is a very large, green statue of a woman"], "decomposition": ["Where is The Statue of Liberty located?", "Is #1 in New York Harbor?"], "evidence": [[[["Statue of Liberty-1"]], [["Statue of Liberty-1"]]], [[["Statue of Liberty-1"]], [["Statue of Liberty-1"]]], [[["Statue of Liberty-1"]], ["operation"]]], "golden_sentence": [["The Statue of Liberty (Liberty Enlightening the World; French: La Libert\u00e9 \u00e9clairant le monde) is a colossal neoclassical sculpture on Liberty Island in New York Harbor in New York, in the United States."], ["The Statue of Liberty (Liberty Enlightening the World; French: La Libert\u00e9 \u00e9clairant le monde) is a colossal neoclassical sculpture on Liberty Island in New York Harbor in New York, in the United States."]]}, {"qid": "5af29b4e6d6dcdd4bb74", "term": "Attack on Pearl Harbor", "description": "Surprise attack by the Imperial Japanese Navy on the U.S. Pacific Fleet in Pearl Harbor in Hawaii", "question": "Was only woman to serve as U.S. Speaker of the House alive during the attack on Pearl Harbor?", "answer": true, "facts": ["Nancy Pelosi is the only woman to ever serve as Speaker of the United States House of Representatives.", "Nancy Pelosi was born on Mar 26, 1940", "The attach on Pearl Harbor occurred on December 7, 1941"], "decomposition": ["Who is the only woman to ever serve as Speaker of the United States House of Representatives?", "When was #1 born?", "When did the attack on Pearl Harbor occur?", "Is #2 before #3?"], "evidence": [[[["Speaker of the United States House of Representatives-3"]], [["Nancy Pelosi-1"]], [["Attack on Pearl Harbor-1"]], ["operation"]], [[["Nancy Pelosi-1"]], [["Nancy Pelosi-1"]], [["Attack on Pearl Harbor-1"]], ["operation"]], [[["Speaker of the United States House of Representatives-3"]], [["Nancy Pelosi-1"]], [["Attack on Pearl Harbor-1"]], ["operation"]]], "golden_sentence": [["The current House speaker, Democrat Nancy Pelosi of California, was elected to the office on January 3, 2019."], ["Nancy Patricia Pelosi (/p\u0259\u02c8lo\u028asi/; n\u00e9e\u00a0D'Alesandro; born March 26, 1940) is an American politician serving as Speaker of the United States House of Representatives since 2019, and previously from 2007 to 2011."], ["The attack on Pearl Harbor was a surprise, preemptive military strike by the Imperial Japanese Navy Air Service upon the United States (a neutral country at the time) against the naval base at Pearl Harbor in Honolulu, Territory of Hawaii, just before 08:00, on Sunday morning, December 7, 1941."]]}, {"qid": "cc12bcafc5dd73f73154", "term": "Honey", "description": "Sweet food made by bees mostly using nectar from flowers", "question": "Is honey associated with queens?", "answer": true, "facts": ["Honey is made by bees.", "Each bee hive is led by a queen bee."], "decomposition": ["What produces honey?", "Do #1 have queens?"], "evidence": [[[["Honey-1"]], [["Honey bee-53"], "operation"]], [[["Honey-1"]], [["Honey bee-53"], "operation"]], [[["Honey-1"]], [["Queen bee-1"]]]], "golden_sentence": [["Bees produce honey from the sugary secretions of plants (floral nectar) or from secretions of other insects (such as honeydew), by regurgitation, enzymatic activity, and water evaporation."], ["Apis queens practice polyandry, with one female mating with multiple males."]]}, {"qid": "09c91aadae2115c71d3b", "term": "Basel", "description": "Place in Basel-Stadt, Switzerland", "question": "Is it dark is Basel during the day in Los Angeles?", "answer": true, "facts": ["Basel is located in the CEST time zone.", "Los Angeles is located in the PDT time zone."], "decomposition": ["What time zone is Basel in?", "What time zone is Los Angeles in?", "What is the time difference in hours between #1 and #2?", "Is #3 at least equal to 8?"], "evidence": [[[["Basel-1", "Central European Summer Time-1", "Central European Summer Time-6"]], [["Pacific Time Zone-1", "Pacific Time Zone-3"]], [["Central European Summer Time-1", "Pacific Time Zone-1"], "operation"], ["operation"]], [[["Basel-1", "UTC+00:30-2"]], [["Pacific Time Zone-9"]], ["no_evidence", "operation"], ["operation"]], [[["Basel-1", "UTC+01:00-1"]], [["Los Angeles-1", "UTC\u221208:00-1"]], ["operation"], ["operation"]]], "golden_sentence": [["", "Central European Summer Time (CEST), sometime referred also as Central European Daylight Time (CEDT), is the standard clock time observed during the period of summer daylight-saving in those European countries which observe Central European Time (UTC+01:00) during the other part of the year.", "Albania, since 1974 Andorra, since 1985 Austria, since 1980 Belgium, since 1980 Bosnia and Herzegovina, since 1983 Croatia, since 1983 Czech Republic, since 1979 Denmark (metropolitan), since 1980 France (metropolitan), since 1976 Germany, since 1980 Gibraltar, since 1982 Hungary, since 1980 Italy, since 1968 Kosovo, since 1983 Liechtenstein, since 1981 Luxembourg, since 1981 Malta, since 1974 Monaco, since 1976 Montenegro, since 1983 Netherlands, since 1977 North Macedonia, since 1983 Norway, since 1980 Poland, since 1977 San Marino, since 1966 Serbia, since 1983 Slovakia, since 1979 Slovenia, since 1983 Spain, since 1974 (except Canary Islands, which instead apply Western European Summer Time) Sweden, since 1980 Switzerland, since 1981 Vatican, since 1966 The following countries have also used Central European Summer Time in the past:"], ["The Pacific Time Zone (PT) is a time zone encompassing parts of western Canada, the western United States, and western Mexico.", "The largest city in the Pacific Time Zone is Los Angeles; the Los Angeles metropolitan area is the largest metropolitan area in the zone."], ["It corresponds to UTC+02:00, which makes it the same as Eastern European Time, Central Africa Time, South African Standard Time and Kaliningrad Time in Russia.", ""]]}, {"qid": "25aeb4a78907cfbf9ab7", "term": "Learning disability", "description": "Range of neurodevelopmental conditions", "question": "Does penicillin cure a learning disability?", "answer": false, "facts": ["Learning disabilities are neurological impairments", "Neurological impairments can result from genetic issues, developmental problems, and accidents like head trauma, malnutrition or exposure to toxins", "Penicillin is an antibiotic that treats bacterial infection"], "decomposition": ["What kind of impairment is a learning disability?", "What are the causes of #1?", "What is Penicillin used to treat?", "Is #3 also listed in #2?"], "evidence": [[[["Learning difficulties-2"]], [["Learning disability-18"]], [["Penicillin-1"]], [["Learning disability-18", "Penicillin-1"]]], [[["Learning disability-1"]], [["Learning disability-17", "Learning disability-18"]], [["Penicillin-1"]], ["operation"]], [[["Learning disability-1"]], [["Learning disability-3"]], [["Side effects of penicillin-1"]], ["operation"]]], "golden_sentence": [["Learning disability, difficulty learning in a typical manner, often divided into: Dyslexia, difficulty in learning to read fluently Dyscalculia, difficulty in learning or comprehending arithmetic Intellectual disability, significantly impaired cognitive functioning and adaptive deficits appearing before adulthood Note that the term learning disability means \"difficulty learning in a typical manner\" in North America but refers specifically to \"intellectual disability\" in the United Kingdom."], ["not present in either parent) which can cause developmental disorders including learning disabilities."], ["Penicillin antibiotics were among the first medications to be effective against many bacterial infections caused by staphylococci and streptococci."], ["", ""]]}, {"qid": "ac5179fabf38fcf165eb", "term": "Mickey Mouse", "description": "Disney cartoon character", "question": "Would Mickey Mouse blend in with the American flag?", "answer": false, "facts": ["The American Flag is colored red, white, and blue.", "Mickey Mouse typically wears red shorts, large yellow shoes, and white gloves.", "The color yellow stands out distinctly from red, white, and blue.", "Things that are colored similarly or identically will blend in with each other."], "decomposition": ["What colors are Mickey Mouse?", "What colors are the American flag?", "Are most of the colors in #1 also found in #2?"], "evidence": [[[["Mickey Mouse-48"]], [["Franco-American Flag-1"]], ["operation"]], [[["Mickey Mouse-1"]], [["Flag of the United States-1"]], ["operation"]], [[["Mickey Mouse-1"]], [["Flag of the United States-1"]], ["operation"]]], "golden_sentence": [[""], ["The colors blue, white, and red symbolise both France and the United States of America."]]}, {"qid": "cc9448f8ffa6c00fc7a3", "term": "Hippie", "description": "diminutive pejorative of hipster: 1960s counterculture participant", "question": "Was hippie culture encouraged by the government in the Soviet Union?", "answer": false, "facts": ["Long hair was associated with the subcultures and youth movements that arose in the Western world during the mid-1960s, such as Hippies.", " M\u00e1ni\u010dky) is a Czech term used for young people with long hair, typically men, in Czechoslovakia through the 1960s and 1970s.", "From the mid-1960s, \"m\u00e1ni\u010dky\" became a target of continuous interest of the state security apparatus."], "decomposition": ["What hairstyle was associated with hippies in the mid 1960's?", "What Czech term was used to describe people with #1?", "Was the government accepting of people who identified as #2?"], "evidence": [[[["History of the hippie movement-8"]], [["M\u00e1ni\u010dka-1"]], [["Counterculture of the 1960s-28"]]], [[["History of the hippie movement-39"]], [["M\u00e1ni\u010dka-4"]], [["Counterculture of the 1960s-29"]]], [[["Red Dog Saloon (Virginia City, Nevada)-4"]], [["M\u00e1ni\u010dka-1"]], [["Counterculture of the 1960s-28"], "operation"]]], "golden_sentence": [["Stylistic differences between beatniks, marked by somber colors, dark shades and goatees, gave way to colorful psychedelic clothing and long hair worn by hippies."], ["M\u00e1ni\u010dka (in plural: M\u00e1ni\u010dky) is a Czech term used for young people with long hair, usually males, in Czechoslovakia through the 1960s and 1970s."], [""]]}, {"qid": "540f20bdd6d89f09c55d", "term": "Giant squid", "description": "Deep-ocean dwelling squid in the family Architeuthidae", "question": "Can you house a giant squid at Soldier Field?", "answer": true, "facts": ["Soldier Field is a football stadium", "Football fields are 120 yards long, or 360 feet", "The maximum length of a giant squid is 43 feet"], "decomposition": ["How long are giant squid?", "What type of field is Soldier Field?", "How long are #2?", "Is #3 equal to or greater than #1?"], "evidence": [[[["Giant squid-1"]], [["Soldier Field-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Giant squid-1"]], [["Soldier Field-1"]], [["American football-36"]], ["operation"]], [[["Giant squid-1"]], [["Soldier Field-1"]], [["Gridiron football-7"]], ["operation"]]], "golden_sentence": [["Giant squid can grow to a tremendous size, offering an example of deep-sea gigantism: recent estimates put the maximum size at 12\u00a0m (39\u00a0ft) or 13\u00a0m (43\u00a0ft) for females and 10\u00a0m (33\u00a0ft) for males from the posterior fins to the tip of the two long tentacles (longer than the colossal squid at an estimated 9\u201310\u00a0m (30\u201333\u00a0ft), but lighter, one of the largest living organisms)."], ["Soldier Field is an American football and soccer stadium located in the Near South Side of Chicago, Illinois, near Downtown Chicago."]]}, {"qid": "1fdbb02ca1a5e0aacba4", "term": "English Channel", "description": "Arm of the Atlantic Ocean that separates southern England from northern France", "question": "Can Iowa be hidden in the English Channel?", "answer": false, "facts": ["The maximum width of the English Channel is 150 miles", "The minimum width of Iowa is 200 miles"], "decomposition": ["What is the maximum width of the English Channel?", "What is the minimum width of Iowa?", "Is #1 greater than or equal to #2?"], "evidence": [[["no_evidence"], ["no_evidence"], ["operation"]], [[["English Channel-2"]], [["Iowa-5"], "no_evidence"], ["no_evidence", "operation"]], [[["English Channel-2"]], [["Geography of Iowa-1"], "no_evidence"], ["operation"]]], "golden_sentence": []}, {"qid": "9a864b47343afdd1e4d7", "term": "Earth's magnetic field", "description": "Magnetic field that extends from the Earth\u2019s inner core to where it meets the solar wind", "question": "Do Flat Earthers doubt the existence of Earth's magnetic field?", "answer": true, "facts": ["Theories about the Earth's magnetic field depend on the globe model of the Earth.", "Flat Earthers are skeptical of most science related to the Earth and space, believing it to be part of a conspiracy coverup."], "decomposition": ["Which theory about the earth do the Flat-Earthers believe?", "Which earth theory supports the existence of the earth's magnetic field?", "Does #1 contradict #2?"], "evidence": [[[["Flat Earth-1"]], [["Earth's magnetic field-1"]], ["operation"]], [[["Modern flat Earth societies-1"]], [["Modern flat Earth societies-8"], "no_evidence"], ["no_evidence", "operation"]], [[["Flat Earth-58"]], [["History of geomagnetism-12"]], ["operation"]]], "golden_sentence": [["The flat Earth model is an archaic conception of Earth's shape as a plane or disk."], [""]]}, {"qid": "45e9c79faa362487e69d", "term": "CNES", "description": "French space agency", "question": "Has CNES planted a French flag on the lunar surface?", "answer": false, "facts": ["The lunar surface is on the moon.", "CNES has not sent a person to the moon."], "decomposition": ["Where is the lunar surface?", "What country is the CNES part of?", "Which countries have sent people or probes to #1?", "Is #2 included in #3?"], "evidence": [[[["Geology of the Moon-1"]], [["CNES-1"]], [["Space Race-2"]], ["operation"]], [[["Moon-3"]], [["CNES-1"]], [["Chinese Lunar Exploration Program-3", "Exploration of the Moon-11"]], ["operation"]], [[["Moon-3"]], [["CNES-1"]], ["no_evidence"], ["operation"]]], "golden_sentence": [[""], ["The National Centre for Space Studies (CNES) (French: Centre national d'\u00e9tudes spatiales) is the French government space agency (administratively, a \"public administration with industrial and commercial purpose\")."], [""]]}, {"qid": "cc81d89c9045ef82a140", "term": "Myth", "description": "Type of traditional narrative", "question": "Was story of Jesus inspired by Egyptian myth?", "answer": true, "facts": ["Jesus was a biblical character that walked on water, was born of a virgin, and was killed beside two thieves.", "Horus was a character in ancient Egyptian myth that walked on water, had a virgin mother, and was executed beside two thieves."], "decomposition": ["What are the main characteristics of the Horus story?", "What are the main characteristics of the Jesus story?", "Is there evidence people believed #1 before #2?", "Is there significant overlap between #1 and #2?", "Are #4 and #3 both \"Yes\"?"], "evidence": [[[["Osiris myth-1"]], [["Jesus-3"]], [["Osiris myth-3"]], [["Jesus-1"]], ["operation"]], [[["Horus-8"], "no_evidence"], [["Jesus-1"], "no_evidence"], [["Ancient Egypt-1"]], ["no_evidence", "operation"], ["operation"]], [[["Horus-11"]], [["Jesus-11"]], [["Jesus-7"], "no_evidence"], [["Horus-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["The remainder of the story focuses on Horus, the product of the union of Isis and Osiris, who is at first a vulnerable child protected by his mother and then becomes Set's rival for the throne."], ["Christian doctrines include the beliefs that Jesus was conceived by the Holy Spirit, was born of a virgin named Mary, performed miracles, founded the Christian Church, died by crucifixion as a sacrifice to achieve atonement for sin, rose from the dead, and ascended into Heaven, from where he will return."], ["Scholars have tried to discern the exact nature of the events that gave rise to the story, but they have reached no definitive conclusions."], [""]]}, {"qid": "8f94b7c7e42495946ba4", "term": "Harbor seal", "description": "Species of mammal", "question": "Can you see live harbor seals in Washington DC?", "answer": true, "facts": ["The Smithsonian National Zoo is in Washington DC.", "There is a harbor seal exhibit at the Smithsonian National Zoo. "], "decomposition": ["Is there a zoo in Washington DC?", "Is there a harbor seal exhibit at #1?"], "evidence": [[[["National Zoological Park (United States)-2"]], [["National Zoological Park (United States)-26"]]], [[["National Zoological Park (United States)-2"]], [["National Zoological Park (United States)-26"]]], [[["National Zoological Park (United States)-2"]], [["National Zoological Park (United States)-26"], "operation"]]], "golden_sentence": [["The National Zoo has two campuses."], ["These include California sea lions, grey seals, harbor seals, North American beavers, North American river otters, bald eagles, common ravens, brown pelicans, and wolves."]]}, {"qid": "150c42dafcaed658c6c5", "term": "Latino", "description": "A group of people in the United States with ties to Latin America", "question": "Is blonde hair green eyed Sara Paxton considered a Latino?", "answer": true, "facts": ["Sara Paxton is an American actress.", "Latino's are people with ancestral ties to Latin America.", "Sara Paxton was born to an Irish/English father and a Mexican/Spanish/Chilean mother.", "Mexico is a country that is part of Latin America."], "decomposition": ["Latinos are people with which nationality?", "Which countries are Sara Paxton's parents from?", "Is any of #2 included in #1?"], "evidence": [[[["Latino (demonym)-1", "Latino (demonym)-2"]], [["Sara Paxton-3"]], ["operation"]], [[["Latin America-12", "Latino (demonym)-1", "Mexico-1"]], [["Sara Paxton-3"]], ["operation"]], [[["Latino (demonym)-18"]], [["Sara Paxton-3"]], ["operation"]]], "golden_sentence": [["The term Latino (/l\u00e6\u02c8ti\u02d0no\u028a, l\u0259-/) is a noun and adjective often used in English, Spanish and Portuguese to refer to people in the United States with cultural ties to Latin America, in particular to those countries which are Spanish- or Portuguese-speaking.", "Various U.S. governmental agencies, especially the Census Bureau, codified their usage, and so have specific definitions which may or may not agree with community usage, and includes a specific list of countries from which American residents stem, which are, or are not, included in the agency's definition of Latino."], ["Paxton's father is of English, Irish, and Scottish descent and he converted to Judaism upon marrying her mother."]]}, {"qid": "1867f2030085e6edc190", "term": "Bee", "description": "Clade of insects", "question": "Does the human stomach destroy a bee if ingested?", "answer": true, "facts": ["Bees have an outer protective exoskeleton made of chitin, a polymer of glucose.", "The human stomach releases harsh acids that break down materials.", "The human stomach breaks down glucose in about 33 hours."], "decomposition": ["What material protects a bee?", "What is #1 made of?", "Can the human stomach digest #2?"], "evidence": [[[["Exoskeleton-3"], "no_evidence"], [["Chitin-1"]], [["Glucose-1"], "operation"]], [[["Bee-40"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Bee-28", "Bee-65"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["These structures are composed of chitin and are approximately six times stronger and twice the stiffness of vertebrate tendons."], ["Chitin (C8H13O5N)n (/\u02c8ka\u026at\u026an/ KY-tin), a long-chain polymer of N-acetylglucosamine, is a derivative of glucose."], [""]]}, {"qid": "f985f6c918b783055527", "term": "Voyages of Christopher Columbus", "description": "1492-1502 voyages to the Americas; beginning of the Columbian exchange", "question": "Could largest ship from Voyages of Christopher Columbus haul Statue of Liberty?", "answer": false, "facts": ["The largest ship Christopher Columbus used was the Santa Maria.", "The Santa Maria has a cargo capacity of 108 tons.", "The Statue of Liberty weighs 225 tons."], "decomposition": ["What was the largest of Columbus' ships?", "What was the cargo capacity of #1?", "How much does the Statue of Liberty weigh?", "Is #2 greater than #3?"], "evidence": [[[["Santa Mar\u00eda (ship)-1"]], [["Builder's Old Measurement-1", "Santa Mar\u00eda (ship)-62"]], [["Statue of Liberty-20"]], [["Ton-2"], "operation"]], [[["Carrack-11"], "no_evidence"], [["Santa Mar\u00eda (ship)-62"]], [["Statue of Liberty-18"], "no_evidence"], ["operation"]], [[["Santa Mar\u00eda (ship)-1"]], [["Santa Mar\u00eda (ship)-63"]], [["Statue of Liberty-20"], "no_evidence"], ["operation"]]], "golden_sentence": [["La Santa Mar\u00eda (The Saint Mary), alternatively La Gallega, was the largest of the three ships used by Christopher Columbus in his first voyage across the Atlantic Ocean in 1492, the others being the Ni\u00f1a and the Pinta."], ["", ""], ["According to Cara Sutherland in her book on the statue for the Museum of the City of New York, 200,000 pounds (91,000\u00a0kg) was needed to build the statue, and the French copper industrialist Eug\u00e8ne Secr\u00e9tan donated 128,000 pounds (58,000\u00a0kg) of copper."], [""]]}, {"qid": "c2f2f7f0e5dd242d0c48", "term": "Yellow pages", "description": "Telephone directory of businesses by category", "question": "Would it be uncommon for a high schooler to use the yellow pages?", "answer": true, "facts": ["High school students are between 14-19 years old. ", "Teenagers now are considered 'digital natives'. ", "'Digital natives' are individuals who have grown up during the computer age, being exposed to technology from early childhood."], "decomposition": ["Which age range do most high school students fall within?", "What is the age range of children who are considered to have grown during the computer age?", "Is #1 similar to #2?"], "evidence": [[[["High school (North America)-1"]], [["Information Age-1"]], ["operation"]], [[["Adolescence-1"]], [["Digital native-1"]], ["operation"]], [["no_evidence"], [["Yellow pages-2"]], ["no_evidence", "operation"]]], "golden_sentence": [["High school is the education students receive from approximately 12 to 17 years old."], [""]]}, {"qid": "39877cfa8d3f4a8546f1", "term": "5", "description": "Natural number", "question": "Is pi in excess of square root of 5?", "answer": true, "facts": ["Pi is a mathematical number approximately equal to 3.14", "The square root of a number are the two numbers multiplied together that equal that number.", "The square root of 5 is around 2.23."], "decomposition": ["What is the square root of 5?", "What is the value of pi?", "Is #2 greater than #1?"], "evidence": [[[["Square root-1"], "no_evidence", "operation"], [["Pi-1"]], ["no_evidence", "operation"]], [[["Square root of 5-1"]], [["Pi-1"]], ["operation"]], [[["Square root of 5-3"]], [["Pi-1"]], ["operation"]]], "golden_sentence": [[""], ["The number \u03c0 (/pa\u026a/) is a mathematical constant."]]}, {"qid": "f41dbb952c3d681302e1", "term": "Iris (mythology)", "description": "Greek goddess of the rainbow", "question": "Would Iris (mythology) and Hermes hypothetically struggle at a UPS job?", "answer": false, "facts": ["UPS is the number one delivery/courier service according to 2019 sales.", "Iris is the goddess of the rainbow and serves as a messenger of the gods in Greek mythology.", "Hermes in Greek mythology was a god that functioned as the emissary and messenger of the gods."], "decomposition": ["What role does the Greek goddess Iris play for gods?", "What role does the Greek deity Hermes play for gods?", "What kind of service does UPS provide?", "Are #1 and #2 much different from #3?"], "evidence": [[[["Iris (mythology)-1"]], [["Hermes-1"]], [["United Parcel Service-1"]], ["operation"]], [[["Iris (mythology)-1"]], [["Hermes-2"]], [["United Parcel Service-1"]], ["operation"]], [[["Iris (mythology)-1"]], [["Hermes-1"]], [["United Parcel Service-1"]], ["operation"]]], "golden_sentence": [["In Greek mythology, Iris (/\u02c8a\u026ar\u026as/; Greek: \u038a\u03c1\u03b9\u03c2 Ancient Greek:\u00a0[\u00ee\u02d0ris]) is the personification and goddess of the rainbow and messenger of the gods."], ["Hermes is considered the herald of the gods, as well as the protector of human heralds, travellers, thieves, merchants, and orators."], ["United Parcel Service (shortened in initials as UPS; stylized in all lowercase) is an American multinational package delivery and supply chain management company."]]}, {"qid": "fcd085ac317980838e87", "term": "Doctor Strange", "description": "Superhero appearing in Marvel Comics publications and related media", "question": "Would Doctor Strange like the Pittsburgh Steelers logo?", "answer": true, "facts": ["The Pittsburgh Steelers logo features patches of red, gold and blue", "Doctor Strange's costume uses the colors red, gold and blue"], "decomposition": ["What colors are in Doctor Strange's costume?", "What are the colors in the logo of the Pittsburgh Steelers?", "Are #1 and #2 the same?"], "evidence": [[[["Doctor Strange-39"]], [["Pittsburgh Steelers-32"]], ["operation"]], [[["Doctor Strange-39"]], [["Logos and uniforms of the Pittsburgh Steelers-5"]], ["operation"]], [[["Doctor Strange-39"], "no_evidence"], [["Pittsburgh Steelers-32"]], ["operation"]]], "golden_sentence": [["There were two distinctly different cloaks worn by Doctor Strange bequeathed to him by his mentor, the Ancient One: a billowing, full-length blue cloak, that had minor abilities and spells woven into it, and the later, red cloak that Strange is usually seen wearing."], ["The rest of the uniform consists of beige pants, yellow with black horizontal stripped socks, and the Steelers regular black helmet."]]}, {"qid": "c2435c8b47500204e0b0", "term": "Torah", "description": "First five books of the Hebrew Bible", "question": "Can you give at least one word from the Torah to all residents of Bunkie Louisiana?", "answer": true, "facts": ["Torah scrolls must be duplicated precisely by a trained scribe.", "The Torah has a total of 8,674 words.", "The population of Bunkie Louisiana is 3,939 people according to a 2018 census."], "decomposition": ["How many words are in the Torah?", "How many residents does Bunkie, Louisiana have?", "Is #1 greater than #2?"], "evidence": [[[["Torah-45"], "no_evidence"], [["Bunkie, Louisiana-1"]], ["no_evidence", "operation"]], [[["Torah-1"], "no_evidence"], [["Bunkie, Louisiana-1"]], ["no_evidence", "operation"]], [[["Sefer Torah-7"], "no_evidence"], [["Bunkie, Louisiana-1"]], ["no_evidence", "operation"]]], "golden_sentence": [["Written entirely in Hebrew, a sefer Torah contains 304,805 letters, all of which must be duplicated precisely by a trained sofer (\"scribe\"), an effort that may take as long as approximately one and a half years."], [""]]}, {"qid": "64b17634ee3e36d706aa", "term": "Leaf", "description": "organ of a vascular plant, composing its foliage", "question": "Does the texture of leaves remain the same independent of their coloring changing?", "answer": false, "facts": ["When leaves turn colors like red, yellow, or brown, they start being cut off from their main supply of nutrients and moisture.", "As leaves change colors, they become dry and brittle.", "Leaves tend to be supple and soft when they are green."], "decomposition": ["When leaves change to red and orange, what does their texture become?", "When leaves are green, what is their texture?", "Is #1 the same as #2?"], "evidence": [[[["Autumn leaf color-1"], "no_evidence"], [["Leaf-1"], "no_evidence"], ["operation"]], [[["Anthocyanin-17"], "no_evidence"], [["Autumn leaf color-4"], "no_evidence"], ["operation"]], [[["Leaf-55"], "no_evidence"], ["no_evidence"], ["operation"]]], "golden_sentence": [["Autumn leaf color is a phenomenon that affects the normal green leaves of many deciduous trees and shrubs by which they take on, during a few weeks in the autumn season, various shades of yellow, orange, red, purple, and brown."], [""]]}, {"qid": "37b72312393f3a383d7d", "term": "Snow White", "description": "fairy tale", "question": "Can all of Snow White's dwarfs play a game of 7 Wonders simultaneously?", "answer": true, "facts": ["The fairy tale character Snow White was friends with seven dwarfs.", "The board game 7 Wonders is for 2 to 7 players."], "decomposition": ["How many players can participate in a game of 7 Wonders?", "How many dwarfs are in the story of Snow White?", "Is #2 less than or equal to #1?"], "evidence": [[[["7 Wonders (board game)-1"], "no_evidence"], [["Snow White and the Seven Dwarfs (1937 film)-1"]], ["no_evidence", "operation"]], [[["7 Wonders (board game)-14"], "no_evidence"], [["Snow White and the Seven Dwarfs (1937 film)-7"]], ["operation"]], [[["7 Wonders (board game)-21"]], [["Snow White-3"]], ["operation"]]], "golden_sentence": [[""], ["Snow White and the Seven Dwarfs is a 1937 American animated musical fantasy film produced by Walt Disney Productions and originally released by RKO Radio Pictures."]]}, {"qid": "befaedcd542fdb70099b", "term": "Dominican Order", "description": "Roman Catholic religious order", "question": "Could the Dominican Order hypothetically defeat Blessed Gerard's order?", "answer": false, "facts": ["The Dominican Order is a Catholic group of friars that several priestly vows.", "Blessed Gerard was the founder of the Order of St John of Jerusalem (Knights Hospitaller).", " The Order of St John of Jerusalem (Knights Hospitaller) were a well trained Catholic military order that fought in the Crusades."], "decomposition": ["To what order did Blessed Gerard belong?", "What is the purpose of members of #1?", "Do members of the Dominican Order have training similar to #2?"], "evidence": [[[["Blessed Gerard-1"]], [["Benedictines-30"], "no_evidence"], [["Dominican Order-1"], "no_evidence", "operation"]], [[["Blessed Gerard-1"]], [["Knights Hospitaller-1"]], [["Dominican Order-2"], "operation"]], [[["Blessed Gerard-1"]], [["Knights Hospitaller-2", "Knights Hospitaller-3"]], [["Dominican Order-2"], "operation"]]], "golden_sentence": [["The Blessed Gerard (c. 1040 \u2013 3 September 1120) was a lay brother in the Benedictine order who was appointed as rector of the hospice in Jerusalem in 1080, and who, in the wake of the success of the First Crusade in 1099, became the founder of the Order of St John of Jerusalem (Knights Hospitaller) (papal recognition in 1113)."], ["to remain in the same community), conversatio morum (an idiomatic Latin phrase suggesting \"conversion of manners\"; see below) and obedience to the community's superior."], [""]]}, {"qid": "3d9c20730d8a77973c29", "term": "Leadership", "description": "ability of an individual or organization to guide other individuals, teams, or entire organizations", "question": "Is Steve Carell's character on The Office portrayed as one with tremendous leadership skills?", "answer": false, "facts": ["Steve Carell plays Michael Scott on The Office.", "Michael Scott is a clueless and naive character that is not meant to be seen as effective in his job as General Manager."], "decomposition": ["Who is Steve Carell's character on The Office?", "What are leadership skills?", "Does #1 possess #2?"], "evidence": [[[["Michael Scott (The Office)-1"]], [["Leadership-1"], "no_evidence"], [["Michael Scott (The Office)-20"], "operation"]], [[["Steve Carell-9"]], [["Skills management-7"]], [["Skills management-7"], "operation"]], [[["Steve Carell-9"]], [["Michael Scott (The Office)-12"]], ["operation"]]], "golden_sentence": [["Michael Gary Scott is a fictional character on NBC's The Office, portrayed by Steve Carell and based on David Brent from the British version of the program."], ["Leadership is both a research area and a practical skill encompassing the ability of an individual or organization to \"lead\" or guide other individuals, teams, or entire organizations."], [""]]}, {"qid": "a8d56055bb3f6262f849", "term": "Christopher Nolan", "description": "British\u2013American film director, screenwriter, and producer", "question": "Could Christopher Nolan's movies finance Cyprus's entire GDP?", "answer": false, "facts": ["The films of Christopher Nolan have grossed around 4.7 billion at the box office.", "The GDP of Cyprus was 24.96 billion in 2018."], "decomposition": ["How much have Christopher Nolan films grossed?", "What is the GDP of Cypress?", "Is #1 greater than #2?"], "evidence": [[[["Christopher Nolan-1"]], [["Cyprus-81", "Cyprus-96"], "no_evidence"], ["no_evidence", "operation"]], [[["Christopher Nolan-1"]], [["Northern Cyprus-47", "Northern Cyprus-50"]], ["operation"]], [[["Christopher Nolan-1"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["His ten films have grossed over US$4.7\u00a0billion worldwide and garnered a total of 34 Oscar nominations and ten wins."], ["According to the 2017 International Monetary Fund estimates, its per capita GDP (adjusted for purchasing power) at $36,442 is below the average of the European Union.", ""]]}, {"qid": "e4e31c24d2fd4ea9c855", "term": "Subway (restaurant)", "description": "American fast food chain", "question": "Did Subway have a sex offender as a spokesperson?", "answer": true, "facts": ["Jared Fogle was a national spokesman for the company in the US starting in January 2000.", "Jared Scott Fogle is a convicted sex offender. "], "decomposition": ["Who was the spokesman for Subway in January 2000?", "Has #1 ever been convicted of a sex crime?"], "evidence": [[[["Jared Fogle-2"]], [["Jared Fogle-19"]]], [[["Jared Fogle-1", "Jared Fogle-8"]], [["Jared Fogle-3"], "operation"]], [[["Jared Fogle-1"]], [["Jared Fogle-1"]]]], "golden_sentence": [[""], [""]]}, {"qid": "f610d2ba5d7fcd7b68ad", "term": "Zorro", "description": "Fictional character", "question": "Did Zorro carve his name into items regularly?", "answer": false, "facts": ["Zorro was known for using his weapon to leave a mark wherever he went.", "The mark Zorro left was the first initial of his name and nothing more."], "decomposition": ["What mark did Zorro leave using his weapon?", "Is his name the answer in #1?"], "evidence": [[[["Zorro-2"]], ["operation"]], [[["Zorro-2"]], ["operation"]], [[["Zorro-18"]], ["operation"]]], "golden_sentence": [["Zorro is an acrobat and an expert in various weapons, but the one he employs most frequently is his rapier, which he uses often to carve the initial \"Z\" on his defeated foes, and other objects."]]}, {"qid": "453393e5849074520928", "term": "Yeti", "description": "Folkloric ape-like creature from Asia", "question": "Is there a Yeti associated with Disney theme parks?", "answer": true, "facts": ["In the 1960s, an attraction called Matterhorn featuring a cartoon version of the Yeti opened in Disneyland.", "Later in 2005, Expedition Everest opened at Animal Kingdom in Disney World, featuring a much scarier version of the Yeti."], "decomposition": ["What 1960s attraction featured a cartoon version of the Yeti?", "Is #1 part of a Disney park?"], "evidence": [[[["Matterhorn Bobsleds-1", "Matterhorn Bobsleds-8"]], ["operation"]], [[["Expedition Everest-12"]], [["Disney's Animal Kingdom-1"]]], [[["Matterhorn Bobsleds-23"]], [["Disney's Animal Kingdom-1"]]]], "golden_sentence": [["", ""]]}, {"qid": "e732fe3659567adf887e", "term": "Football War", "description": "1969 War between Honduras and El Salvador", "question": "Did either side score a touchdown during the Football War?", "answer": false, "facts": ["The Football War was a war in 1969 between Honduras and El Salvador", "The Football War was caused in part by rioting during a FIFA Cup qualifying match", "The FIFA Cup is a soccer tournament", "Touchdowns are scored in American football"], "decomposition": ["Which sport was involved as one of the causes of the Football War?", "Which sport are touchdowns scored in?", "Are #1 and #2 the same?"], "evidence": [[[["Association football-1", "FIFA-1", "Football War-1"]], [["American football-1"]], ["operation"]], [[["Football War-1"]], [["Touchdown-1"]], ["operation"]], [[["Football War-1"]], [["Touchdown-1"]], ["operation"]]], "golden_sentence": [["Association football, more commonly known as football or soccer, is a team sport played with a spherical ball between two teams of 11 players.", "The F\u00e9d\u00e9ration Internationale de Football Association (FIFA /\u02c8fi\u02d0f\u0259/ FEE-f\u0259; French for 'International Federation of Association Football'; Spanish: Federaci\u00f3n Internacional de F\u00fatbol Asociaci\u00f3n; German: Internationaler Verband des Association Football) is a non-profit organization which describes itself as an international governing body of association football, f\u00fatsal, beach soccer, and efootball.", "The Football War (Spanish: La guerra del f\u00fatbol; colloquial: Soccer War or the Hundred Hours' War also known as 100 Hour War) was a brief war fought between El Salvador and Honduras in 1969."], ["American football, referred to as football in the United States and Canada and also known as gridiron, is a team sport played by two teams of eleven players on a rectangular field with goalposts at each end."]]}, {"qid": "b49f91ec03c6a8ccc0b7", "term": "Grief", "description": "reaction to loss of someone or something close or important", "question": "Is grief always obvious when it is being experienced?", "answer": false, "facts": ["Grief has no set external representation. ", "People who are grieving may laugh, cry, or even seem angry."], "decomposition": ["What are the ways a person may express their grief?", "Based on #1, can one always tell when someone is grieving?"], "evidence": [[[["Grief-10"]], [["Grief-10"]]], [[["Grief-1"], "no_evidence"], [["Grief-59"], "no_evidence", "operation"]], [["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["When a loved one dies, it is not unusual for the bereaved to report that they have \"seen\" or \"heard\" the person they have lost."]]}, {"qid": "0c12bda57c281e079200", "term": "Jack Dempsey", "description": "American boxer", "question": "Did Jack Dempsey have most title fight wins in either of his weight classes?", "answer": false, "facts": ["Jack Dempsey competed as a heavyweight and a lightheavyweight.", "Jack Dempsey only had a handful of title defenses as heavyweight champion.", "Wladimir Klitschko had 25 heavyweight title fight wins.", "Jack Dempsey did not hold the lightheavyweight title.", "Dariusz Michalczewski had 23 lightheavyweight title fight wins."], "decomposition": ["What weight class did Jack Dempsey have title fight wins in?", "How many title fight wins did Jack Dempsey have in #1?", "How many title fight wins did Wladimir Klitschko have in #1?", "Is #2 greater than #3?"], "evidence": [[[["Jack Dempsey-1"]], [["Jack Dempsey-12", "Jack Dempsey-22", "Jack Dempsey-23", "Jack Dempsey-26", "Jack Dempsey-27", "Jack Dempsey-28"]], [["Wladimir Klitschko-4"]], ["operation"]], [[["Jack Dempsey-1"]], [["Jack Dempsey-11"], "no_evidence"], [["Heavyweight-6"]], ["operation"]], [[["Jack Dempsey-1"]], [["Jack Dempsey-12", "Jack Dempsey-23", "Jack Dempsey-24", "Jack Dempsey-28"], "no_evidence"], [["Wladimir Klitschko-4"]], ["operation"]]], "golden_sentence": [["William Harrison \"Jack\" Dempsey (June 24, 1895 \u2013 May 31, 1983), nicknamed Kid Blackie, and The Manassa Mauler, was an American professional boxer who competed from 1914 to 1927, and reigned as the world heavyweight champion from 1919 to 1926."], ["Ultimately, Willard was knocked down seven times by Dempsey in the first round.", "", "", "", "", ""], ["Wladimir has defeated 23 opponents for the world heavyweight championship, more than any other heavyweight in history."]]}, {"qid": "050cb409c75d423023be", "term": "Subway (restaurant)", "description": "American fast food chain", "question": "Was Subway involved in a pedophilia scandal?", "answer": true, "facts": ["In 2000, Jared Fogle became a national spokesman for Subway after he lost a lot of weight eating only Subway sandwiches.", "In 2015, Fogle was found guilty of child molestation and possession of child pornography, and Subway terminated its relationship with him."], "decomposition": ["Who was the famous spokesman for Subway?", "What crimes did #1 commit?", "Are #2 directly related to pedophilia?"], "evidence": [[[["Subway (restaurant)-30"]], [["Jared Fogle-3"]], [["Pedophilia-12"], "operation"]], [[["Jared Fogle-1"]], [["Jared Fogle-3"]], [["Child sex tourism-1", "Pedophilia-1"]]], [[["Jared Fogle-6"]], [["Jared Fogle-25"]], [["Jared Fogle-25"]]]], "golden_sentence": [["Jared Fogle was a national spokesman for the company in the US starting in January 2000, giving talks on healthy living and appearing in advertisements."], [""], [""]]}, {"qid": "70ce89eac577e7deb95d", "term": "Earth's magnetic field", "description": "Magnetic field that extends from the Earth\u2019s inner core to where it meets the solar wind", "question": "Are implants from an ORIF surgery affected by the magnetic field of the Earth?", "answer": false, "facts": ["An ORIF surgery is an Open Reduction Internal Fixation, done to fix broken bones.", "Most hardware from ORIF surgeries is made of titanium.", "Titanium is only slightly magnetic and does not affect metal detectors."], "decomposition": ["What kind of materials can be significantly affected by the earth's magnetic field?", "What kind of materials are ORIF surgery implants made of?", "Is #2 included in #1?"], "evidence": [[[["Magnetic mineralogy-4"], "no_evidence"], [["Internal fixation-1"]], ["operation"]], [[["Earth's magnetic field-9"], "no_evidence"], [["Internal fixation-1"]], [["Titanium-7"], "operation"]], [[["Ferromagnetism-1", "Magnetic field-90"], "no_evidence"], [["Internal fixation-1", "Internal fixation-3"]], [["Stainless steel-1"]]]], "golden_sentence": [[""], ["[page\u00a0needed] An internal fixator may be made of stainless steel, titanium alloy, or cobalt-chrome alloy."]]}, {"qid": "52e376f5de53ec038ee0", "term": "Banana", "description": "edible fruit", "question": "Can a banana get a virus?", "answer": true, "facts": ["A virus is a disease that is caused by infectious agents.", "A banana comes from a banana plant.", "Blight is a disease that is caused by infections on plants.", "The Banana bunchy top virus (BBTV) is a plant virus of the genus Babuvirus,that causes diseased streaks."], "decomposition": ["What are the various diseases that affect banana plant?", "Are any of #1 caused by virus?"], "evidence": [[[["Banana bunchy top virus-1"]], [["Nanoviridae-1"], "operation"]], [[["Banana-54", "Banana-55"]], [["Banana bunchy top virus-6"]]], [[["Banana-50"]], ["operation"]]], "golden_sentence": [[""], ["Diseases associated with this family include: stunting."]]}, {"qid": "8a9310722197569475b5", "term": "E.T. the Extra-Terrestrial", "description": "1982 American science fiction film directed by Steven Spielberg", "question": "Would E.T. the Extra-Terrestrial alien hypothetically love Friendly's?", "answer": true, "facts": ["E.T., the main alien from E.T. the Extra-Terrestrial, loved Reese's Pieces candy.", "Friendly's is a restaurant that serves dinner entrees and ice cream dishes.", "Friendly's has several desserts with Reese's candy including the Reese's Peanut Butter Cup Sundae, and Reese's Pieces Sundae."], "decomposition": ["What is E.T. the Extra-Terrestrial's favorite food?", "Does Friendly's serve dishes made with #1?"], "evidence": [[[["E.T. the Extra-Terrestrial-6"]], [["Friendly's-4", "Reese's Pieces-1"]]], [[["E.T. the Extra-Terrestrial-6"]], [["Friendly's-1"], "no_evidence", "operation"]], [[["E.T. the Extra-Terrestrial-6"]], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["", ""]]}, {"qid": "71aff579c3ec7336ab2a", "term": "Sugar Ray Robinson", "description": "American boxer", "question": "Could Sugar Ray Robinson box if he stole in Iran?", "answer": false, "facts": ["Sugar Ray Robinson was an American boxer who relied on his fists to achieve 109 KO victories.", "The penalty for stealing in Iran is having your hand cut off.", "In August 2015 a prisoner in Iran, who was caught stealing, had his right hand and part of his left leg cut off."], "decomposition": ["What body part does Iran cut off if someone steals?", "What body part is necessary for boxing?", "Is #1 different from #2?"], "evidence": [[[["Hudud-26"]], [["Boxing-1"]], ["operation"]], [[["Guardian Council-2", "Sharia-80"], "no_evidence"], [["Boxing-1", "Boxing-4"]], ["operation"]], [[["Hudud-9"], "no_evidence"], [["Boxing-4"], "operation"], ["operation"]]], "golden_sentence": [["Commenting on the verse in the Quran on theft, Yusuf Ali says that most Islamic jurists believe that \"petty thefts are exempt from this punishment\" and that \"only one hand should be cut off for the first theft.\""], ["Boxing is a combat sport in which two people, usually wearing protective gloves, throw punches at each other for a predetermined amount of time in a boxing ring."]]}, {"qid": "d1250ffd37b65dae4f07", "term": "Pizza", "description": "Usually savory dish of flattened bread and toppings", "question": "Would a TMNT coloring book have pizza in it?", "answer": true, "facts": ["TMNT is an abbreviation for 'Teenage Mutant Ninja Turtles'.", "The Teenage Mutant Ninja Turtles canonically only ever ate pizza in the animated series. "], "decomposition": ["What cartoon does TMNT stand for?", "In the animated series, did #1 canonically eat only pizza?"], "evidence": [[[["Teenage Mutant Ninja Turtles-1"]], [["Teenage Mutant Ninja Turtles-9"], "operation"]], [[["Teenage Mutant Ninja Turtles-1"]], [["Teenage Mutant Ninja Turtles-19"], "operation"]], [[["Teenage Mutant Ninja Turtles-1"]], [["Teenage Mutant Ninja Turtles-19"]]]], "golden_sentence": [["Teenage Mutant Ninja Turtles (often shortened to the TMNT or Ninja Turtles) are four fictional teenaged anthropomorphic turtles named after Italian Renaissance artists."], ["Other versions usually follow one of these origins."]]}, {"qid": "744cf33c49c40215ebae", "term": "Crucifixion", "description": "Method of capital punishment in which the victim is tied or nailed to a large wooden beam and left to hang until eventual death", "question": "Does crucifixion violate US eighth amendment?", "answer": true, "facts": ["The eighth amendment prohibits cruel and unusual punishment.", "Crucifixion was particularly barbaric as people do not die instantly and live for several days."], "decomposition": ["What does the Eighth Amendment say about punishment measures?", "What are the features of crucifixion as a method of punishment?", "Is #1 contradicted by #2?"], "evidence": [[[["Eighth Amendment to the United States Constitution-1"]], [["Crucifixion-1"]], ["operation"]], [[["Eighth Amendment to the United States Constitution-1"]], [["Crucifixion-1"]], [["Crucifixion-1", "Eighth Amendment to the United States Constitution-1"], "operation"]], [[["United States constitutional sentencing law-4"]], [["Cruel and unusual punishment-6"]], ["operation"]]], "golden_sentence": [["The Eighth Amendment (Amendment VIII) of the United States Constitution prohibits the federal government from imposing excessive bail, excessive fines, or cruel and unusual punishments."], ["Crucifixion is a method of punishment or capital punishment in which the victim is tied or nailed to a large wooden beam and left to hang, perhaps for several days, until eventual death from exhaustion and asphyxiation.It has been used as a punishment in parts of the world as recently as the twentieth century."]]}, {"qid": "c2d9cbf13d05e5d10959", "term": "Stork", "description": "family of birds", "question": "Should you wrap a gift for a mother of a stillborn in stork wrapping paper?", "answer": false, "facts": ["Storks are used as a symbol of a new baby on the way.", "Stillborn babies are those who are born lifeless."], "decomposition": ["What is the significance of stork to a potential mother?", "Can stillborn babies be regarded as #1?"], "evidence": [[[["White stork-46"]], [["Stillbirth-1"], "operation"]], [[["White stork-46"]], ["operation"]], [[["White stork-47"]], [["Stillbirth-18"], "operation"]]], "golden_sentence": [["According to European folklore, the stork is responsible for bringing babies to new parents."], [""]]}, {"qid": "7a1e2e2c7381a7affea1", "term": "Naruto", "description": "Japanese manga and anime series", "question": "Would the historic Hattori Hanz\u014d admire Naruto?", "answer": false, "facts": ["Naruto is a ninja", "Ninja tactics were considered dishonorable by samurai", "Hattori Hanz\u014d is a famous historical samurai "], "decomposition": ["What was Naruto's profession?", "What was Hattori Hanz\u014d's profession? ", "Did #2 respect the actions of #1?"], "evidence": [[[["Naruto-1"]], [["Hattori Hanz\u014d-1"]], [["Hattori Hanz\u014d-6"], "no_evidence", "operation"]], [[["Naruto-1"]], [["Hattori Hanz\u014d-1"]], [["Ninja-1"], "operation"]], [[["Naruto-7"], "no_evidence"], [["Hattori Hanz\u014d-10"], "operation"], ["no_evidence"]]], "golden_sentence": [["It tells the story of Naruto Uzumaki, a young ninja who seeks to gain recognition from his peers and also dreams of becoming the Hokage, the leader of his village."], ["Hattori Hanz\u014d (\u670d\u90e8 \u534a\u8535, ~1542[better\u00a0source\u00a0needed] \u2014 December 23, 1596), also known as Hattori Masanari or Hattori Masashige (\u670d\u90e8 \u6b63\u6210)[citation needed] and nicknamed Oni no Hanz\u014d (\u9b3c\u306e\u534a\u8535, Demon Hanz\u014d), was a famous Samurai of the Sengoku era, credited with saving the life of Tokugawa Ieyasu and then helping him to become the ruler of united Japan."], [""]]}, {"qid": "71873ef251e5c532a53a", "term": "Uniting Church in Australia", "description": "christian denomination", "question": "Was Muhammed a member of the Uniting Church in Australia?", "answer": false, "facts": ["The Uniting Church in Australia is a combination of Methodist and Presbyterian congregations.", "Methodists and Presbyterians are Christians.", "Muhammed was the Muslim prophet and was not a Christian."], "decomposition": ["Which religion was Muhammed a prophet in?", "What is the religion of the members of the Uniting Church in Australia?", "Is #1 the same as #2?"], "evidence": [[[["Last prophet-2"]], [["Uniting Church in Australia-23"]], ["operation"]], [[["Muhammad-1"]], [["Uniting Church in Australia-1"]], ["operation"]], [[["Muhammad-1"]], [["Uniting Church in Australia-1"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "4362d2e26531a68a6bc8", "term": "Sand cat", "description": "Small wild cat", "question": "Do sand cats avoid eating all of the prey of eels?", "answer": false, "facts": ["Sand cats eat a number of animals including insects, birds, hares, and reptiles.", "Eels prey on fish, worms, frogs, and lizards.", "Lizards are a type of reptile."], "decomposition": ["What does the sand cat's diet consist of?", "What does the eel's diet consist of?", "Are any of the foods in #2 a subtype of a food in #1?", "Is it not the case that #3 is \"yes\"?"], "evidence": [[[["Sand cat-48", "Sand cat-49", "Sand cat-51"]], [["American eel-4"]], [["Invertebrate-1"], "operation"], ["operation"]], [[["Sand cat-48", "Sand cat-49", "Sand cat-50"]], [["Electric eel-18", "Moray eel-8"]], ["operation"], ["operation"]], [[["Sand cat-48"]], [["Eel-1", "European conger-4"], "no_evidence"], ["operation"], ["operation"]]], "golden_sentence": [["Toubou people accounted of sand cats coming to their camps at night and drinking fresh camel milk.", "", "Their faeces and stomachs contained remains of small mammals, birds, small reptiles, and invertebrates."], ["They feed on crustaceans, aquatic insects, small insects, and probably any aquatic organisms that they can find and eat."], ["This includes all animals apart from the subphylum Vertebrata."]]}, {"qid": "9f1a33836ad75a780d20", "term": "Paul the Apostle", "description": "Early Christian apostle and missionary", "question": "Did Paul the Apostle's cause of death violate the tenets of Ahimsa?", "answer": true, "facts": ["Ahimsa is is an ancient Indian principle of nonviolence which applies to all living beings. ", "Ahimsa is a key virtue in Hinduism, Buddhism and Jainism.", "Paul the Apostle was violently beheaded."], "decomposition": ["What is Ahimsa?", "Does #1 believe in non-violence?", "Did Paul the Apostle die due to violence?", "Are #2 and #3 the same answer?"], "evidence": [[[["Ahi\u1e43s\u0101-1"]], [["Ahi\u1e43s\u0101-1"]], [["Decapitation-1", "Paul the Apostle-53", "Violence-1"]], ["operation"]], [[["Ahi\u1e43s\u0101-1"]], ["operation"], [["Paul the Apostle-53"]], ["operation"]], [[["Ahi\u1e43s\u0101-1"]], ["operation"], [["Paul the Apostle-53", "Paul the Apostle-54"]], ["operation"]]], "golden_sentence": [["Ahimsa (Ahinsa) (Sanskrit: \u0905\u0939\u093f\u0902\u0938\u093e IAST: ahi\u1e43s\u0101, P\u0101li: avihi\u1e43s\u0101) (\"compassion\") is an ancient Indian principle of nonviolence which applies to all living beings."], [""], ["", "According to the First Epistle of Clement (95\u201396 AD), Ignatius (110 AD) and Dionysius of Corinth (166\u2013174 AD) Paul was martyred.", ""]]}, {"qid": "1e377ea8d5174c105dd1", "term": "Sophist", "description": "Specific kind of teacher in both Ancient Greece and in the Roman Empire", "question": "Would a sophist use an \u00e9p\u00e9e?", "answer": false, "facts": ["A sophist is a specific kind of teacher in ancient Greece, in the fifth and fourth centuries BC.", "Sophists specialized in using the tools of philosophy and rhetoric, though other sophists taught subjects such as music, athletics and mathematics.", "An \u00e9p\u00e9e is a sword used in fencing.", "The \u00e9p\u00e9e was not developed until the 19th century."], "decomposition": ["How long ago were the sophists around?", "How long ago was the epee developed?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Sophist-1"]], [["\u00c9p\u00e9e-15"]], ["operation"]], [[["Sophist-1"]], [["\u00c9p\u00e9e-1"]], ["operation"]], [[["Sophist-1"]], [["\u00c9p\u00e9e-16"]], ["operation"]]], "golden_sentence": [["A sophist (Greek: \u03c3\u03bf\u03c6\u03b9\u03c3\u03c4\u03ae\u03c2, sophistes) was a specific kind of teacher in ancient Greece, in the fifth and fourth centuries BC."], ["Like the foil (fleuret), the \u00e9p\u00e9e evolved from light civilian weapons such as the smallsword, which, since the late 17th century, had been the most commonly used dueling sword, replacing the rapier."]]}, {"qid": "bb8cc1792dd28ea1bf90", "term": "The Powerpuff Girls", "description": "American animated television series", "question": "In most Mennonite homes, would children know of The Powerpuff Girls?", "answer": false, "facts": ["Mennonites are a religious with similar beliefs to Amish groups.", "Mennonites do not prohibit or view the use of technology as a sin.", "Most Mennonites avoid using television sets at home."], "decomposition": ["On what devices can one watch The Powerpuff Girls?", "What modern items do Mennonites prohibit themselves from using?", "Is #1 different from #2?"], "evidence": [[[["The Powerpuff Girls-1"]], [["Mennonites-59"], "no_evidence"], ["operation"]], [[["The Powerpuff Girls-1"]], [["Mennonites-59"]], ["operation"]], [[["Mennonites-59"]], [["Mennonites-59"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], ["The Holdeman Mennonites do not believe that the use of modern technology is a sin in itself, but they discourage too intensive a use of the Internet and avoid television, cameras and radio."]]}, {"qid": "e8be6603dd5ea0dc9d8d", "term": "Athena", "description": "ancient Greek goddess of wisdom and war", "question": "Is Freya a combination of Athena and Aphrodite?", "answer": true, "facts": ["Athena was the Greek goddess of war.", "Aphrodite was the Greek goddess of love.", "Freya was the Norse goddess of war, love, and fertility."], "decomposition": ["What was Athena the Greek goddess of?", "What was Aphrodite's specialty?", "What was Freya the Norse goddess of?", "Does #3 have part of the same answer as #1 and #2?"], "evidence": [[[["Athena-1"]], [["Aphrodite-1"]], [["Freyja-1"]], ["no_evidence"]], [[["Athena-1"]], [["Aphrodite-1"]], [["Freyja-1"]], ["operation"]], [[["Athena-1"]], [["Aphrodite-1"]], [["Freyja-1"]], ["operation"]]], "golden_sentence": [["Athena or Athene, often given the epithet Pallas, is an ancient Greek goddess associated with wisdom, handicraft, and warfare who was later syncretized with the Roman goddess Minerva."], ["Aphrodite is an ancient Greek goddess associated with love, beauty, pleasure, passion and procreation."], ["Stemming from Old Norse Freyja, modern forms of the name include Freya, Freyia, and Freja."]]}, {"qid": "cd6534cb1f14c4435cdd", "term": "September", "description": "ninth month in the Julian and Gregorian calendars", "question": "Could you brew beer from start to finish in the month of September?", "answer": false, "facts": ["Brewing a batch of beer takes at least 5 weeks.", "There are 30 days, or 4 1/2 weeks, in the month of September."], "decomposition": ["How long does it take to brew a batch of beer?", "How many weeks does September have?", "Is #2 longer than #1?"], "evidence": [[[["Brewing-36"]], [["September-1"]], ["operation"]], [[["Beer-45"], "no_evidence"], [["September-1"]], ["operation"]], [[["Beer-19"]], [["September-3"]], ["operation"]]], "golden_sentence": [["Generally, warm-fermented beers, which are usually termed ale, are ready to drink within three weeks after the beginning of fermentation, although some brewers will condition or mature them for several months."], ["January February March April May June July August September October November December September is the ninth month of the year in the Julian and Gregorian calendars, the third of four months to have a length of 30 days, and the fourth of five months to have a length of less than 31 days."]]}, {"qid": "26c6b860ecdab056ca15", "term": "Foot (unit)", "description": "customary unit of length", "question": "When en route from China to France, must pilots know their altitude in the imperial foot?", "answer": true, "facts": ["Most international airports and aviators use the foot to measure altitude ", "China and North Korea require pilots to use meters for altitude", "Pilots must communicate their altitude with local air traffic control "], "decomposition": ["Which unit of altitude does France require pilots to use?", "Which unit of altitude does China require pilots to use?", "Is #1 or #2 the imperial foot?"], "evidence": [[[["Foot (unit)-2"]], [["Foot (unit)-2"]], ["operation"]], [[["Foot (unit)-26"], "no_evidence"], [["Foot (unit)-3"]], ["operation"]], [[["Foot (unit)-3"], "no_evidence"], [["Foot (unit)-3"], "no_evidence"], ["operation"]]], "golden_sentence": [["Historically the \"foot\" was a part of many local systems of units, including the Greek, Roman, Chinese, French, and English systems."], ["Historically the \"foot\" was a part of many local systems of units, including the Greek, Roman, Chinese, French, and English systems."]]}, {"qid": "749c748962d63411b6e0", "term": "Lieutenant", "description": "junior commissioned officer in many nations' armed forces", "question": "Would Gomer Pyle salute a lieutenant?", "answer": true, "facts": ["Gomer Pyle was a character on a television sitcom", "Pyle was in the US Marine Corp", "Lieutenants are junior commissioned officers in the USMC", "Marine custom dictates that officers are to be saluted by other Marines"], "decomposition": ["Which arm of the US Armed Forces did Gomer Pyle join?", "What was his rank in the #1?", "According to #1 tradition. would a #2 salute a lieutenant?"], "evidence": [[[["Gomer Pyle-4"]], [["Private first class-8"]], [["Lieutenant-1"], "operation"]], [[["Gomer Pyle, U.S.M.C.-2"]], [["Gomer Pyle, U.S.M.C.-12"]], [["Private (rank)-1"]]], [[["Gomer Pyle-7"]], [["Gomer Pyle-8"]], [["Lieutenant-12", "Salute-57"], "no_evidence"]]], "golden_sentence": [["Gomer is a naive (merely due to his pure, uncorrupted, and non-degenerate mind), extremely moral auto mechanic turned United States Marine Corps PFC, from Mayberry, North Carolina."], ["In the United States Marine Corps, the rate of private first class is the second lowest, just under lance corporal and just above Private, equivalent to NATO grade OR-2, being pay grade E-2."], [""]]}, {"qid": "b2dca50e71c00ab636f1", "term": "Prime Minister of the United Kingdom", "description": "Head of UK Government", "question": "Does the Prime Minister of the United Kingdom have poor job security?", "answer": true, "facts": ["The Prime Minister of the United Kingdom is an elected official.", "Elected officials can be recalled with a vote of no confidence in UK parliament. "], "decomposition": ["How does the prime minister of UK get his position?", "Can people who are #1 be easily removed?"], "evidence": [[[["Prime Minister of the United Kingdom-60"]], [["Recall election-1"]]], [["no_evidence"], [["House of Commons of the United Kingdom-8"]]], [[["Prime minister-25"], "no_evidence"], ["no_evidence"]]], "golden_sentence": [[""], ["However, where the facility to recall exists, should any representative come to be perceived as not properly discharging their responsibilities, then they can be called back with the written request of specific number or proportion of voters."]]}, {"qid": "1ec0bb5693255b9fe499", "term": "The Jungle Book", "description": "1894 children's book by Rudyard Kipling", "question": "Does The Jungle Book contain racist subtext?", "answer": true, "facts": ["Baloo, the father character in The Jungle Book, refers to the money characters as \"flat-nosed flaky creeps\" mocking a common black feature feature.", "The antagonist snake character was made to sound like an Indian mag, and was said to hate men.", "King Louie is viewed socially as a racist archetype of a black man."], "decomposition": ["What term did Baloo use to refer to the monkey characters in \"The Jungle Book\"?", "What sentiment did the antagonistic snake express towards men?", "Do #1 and #2 have racist connotation?"], "evidence": [[[["Bandar-log-1"]], [["Kaa-3"]], [["Bandar-log-1"], "no_evidence"]], [[["The Jungle Book-1"], "no_evidence"], [["The Jungle Book (2016 film)-6"], "no_evidence"], [["The Jungle Book (1967 film)-26"], "operation"]], [["no_evidence"], ["no_evidence"], [["The Jungle Book-18"], "no_evidence", "operation"]]], "golden_sentence": [["Bandar-log (Hindi: \u092c\u0928\u094d\u0926\u0930-\u0932\u094b\u0917) is a term used in Rudyard Kipling's The Jungle Book (1894) to describe the monkeys of the Seeonee jungle."], [""], [""]]}, {"qid": "3f7b2461150b534874b0", "term": "Ginger", "description": "Species of plant", "question": "If you're pregnant, might you be recommended ginger?", "answer": true, "facts": ["Pregnancy often causes nausea and stomach upset.", "Small doses of ginger have been shown to help with vomiting and nausea.", "Doctors say that, in small doses, ginger is safe for pregnant women."], "decomposition": ["What symptoms can ginger help alleviate?", "Do pregnant women suffer from any symptoms in #1?"], "evidence": [[[["Ginger-36"]], [["Morning sickness-1"]]], [[["Ginger-36"]], [["Pregnancy-1"], "operation"]], [[["Jamaica ginger-2"]], [["Signs and symptoms of pregnancy-15"], "operation"]]], "golden_sentence": [["Evidence that ginger helps alleviate nausea and vomiting resulting from chemotherapy or pregnancy is inconsistent."], ["About 10% of women still have symptoms after the 20th week of pregnancy."]]}, {"qid": "7c5f6aae580960a50e3b", "term": "History of the world", "description": "Recorded history of humanity", "question": "Are the events of Star Trek: The Next Generation in the history of the world?", "answer": false, "facts": ["The history of the world includes factual events.", "Star Trek: TNG is a fictional television show. "], "decomposition": ["Which universe is Star Trek: The Next Generation set in?", "Is #1 the same as the real world?"], "evidence": [[[["Star Trek: The Next Generation-1"]], ["no_evidence"]], [[["Star Trek: The Next Generation-1"]], [["Science fiction-1"], "operation"]], [[["Star Trek: The Next Generation-70"]], ["operation"]]], "golden_sentence": [["Set in the 24th century, when Earth is part of the United Federation of Planets, it follows the adventures of a Starfleet starship, the USS Enterprise-D, in its exploration of the Milky Way galaxy."]]}, {"qid": "43d1fa1f7ba03b5398af", "term": "Cuisine of Hawaii", "description": "Cuisine of Hawaii", "question": "Is pig meat considered inedible within the cuisine of Hawaii?", "answer": false, "facts": ["SPAM is a pork and ham product that is very popular around the world.", "In the cuisine of Hawaii, SPAM is a cherished and widely used ingredient."], "decomposition": ["What are some popular dishes in Hawaiian cuisine?", "Is pork (pig meat) excluded from #1?"], "evidence": [[[["Cuisine of Hawaii-11"], "no_evidence"], ["operation"]], [[["Cuisine of Hawaii-2"]], [["Spam (food)-1", "Spam musubi-1"]]], [[["K\u0101lua-1"]], [["K\u0101lua-3"], "operation"]]], "golden_sentence": [[""]]}, {"qid": "636257b2e312c70abd0b", "term": "Hanging", "description": "execution or suicide method involving suspension of a person by a ligature", "question": "Is hanging a viable execution method on a ship at sea?", "answer": true, "facts": ["Hanging is typically set up using rope.", "Ships have plenty of rope on board because their operation relies heavily on rope."], "decomposition": ["What materials are necessary for hanging?", "Would #1 be available on a ship?"], "evidence": [[[["Gallows-1"]], ["operation"]], [[["Hanging-4"]], [["Rigging-1"]]], [[["Hanging-15"], "operation"], ["operation"]]], "golden_sentence": [["A gallows (or scaffold) is a frame, typically wooden, from which objects can be hung or \u201cweighed\u201d."]]}, {"qid": "e9786fb07ec36be17af1", "term": "Southern United States", "description": "Cultural region of the United States", "question": "Can you hunt Iberian wolves in the Southern United States?", "answer": false, "facts": ["The Iberian wolf inhabits northern Portugal and northwestern Spain.", "Portugal and Spain are not located in the Southern United States."], "decomposition": ["What is the range of the Iberian wolf?", "Is #1 located in the Southern United States?"], "evidence": [[[["Iberian wolf-1"]], ["operation"]], [[["Iberian wolf-1"]], [["United States-1", "Western Europe-1"]]], [[["Iberian wolf-1"]], [["Iberian Peninsula-64", "United States-1"]]]], "golden_sentence": [["The Iberian wolf (Canis lupus signatus) also known as Spanish wolf, is a proposed subspecies of grey wolf that inhabits the northwest of the Iberian Peninsula, which includes northern Portugal and northwestern Spain."]]}, {"qid": "b77c4ef940f20e6245c7", "term": "1980 United States presidential election", "description": "49th quadrennial presidential election in the United States", "question": "Were there greater landslides than 1980 United States presidential election?", "answer": true, "facts": ["A landslide refers to a competitor beating their opponent by a wide margin.", "Ronald Reagan defeated Jimmy carter in the 1980 United States presidential election by around 8 million votes.", "Franklin D. Roosevelt won the 1936 United States presidential election over Alf Landon by more than 11 million votes.", "In 1804 Thomas Jefferson received 162 (92%) of the electoral votes while Charles Cotesworth Pinckney received only 14 (8%)."], "decomposition": ["By what votes margin did Ronald Reagan defeat Jimmy Carter in the 1980 US Presidential election?", "By how many votes was Franklin D. Roosevelt leading Alf Landon in the 1936 US Presidential election?", "How many more votes did Thomas Jefferson receive than Charles Cotesworth Pinckney in the 1804 United States presidential election?", "Are #2 and #3 greater individually than #1?"], "evidence": [[[["Ronald Reagan-50"]], [["Franklin D. Roosevelt-52"]], [["Thomas Jefferson-73"], "no_evidence"], ["operation"]], [[["1980 United States presidential election-50"]], [["1936 United States presidential election-4"]], [["1804 United States presidential election-3", "Thomas Jefferson-73"]], ["operation"]], [[["1980 United States presidential election-4"]], [["1936 United States presidential election-4"]], [["1804 United States presidential election-3"]], ["operation"]]], "golden_sentence": [["On November 4, Reagan won a decisive victory over Carter, carrying 44 states and receiving 489 electoral votes to Carter's 49 in six states plus D.C."], ["In the election against Landon and a third-party candidate, Roosevelt won 60.8% of the vote and carried every state except Maine and Vermont."], [""]]}, {"qid": "605f6d6853b8909362d7", "term": "Swastika", "description": "a geometrical figure and an ancient religious icon in the cultures of Eurasia and 20th-century symbol of Nazism", "question": "Does the word swastika have meaning in sanskrit?", "answer": true, "facts": ["In Sanskrit, the word swastika is a combination of \u2018su\u2019 (meaning \u2018good\u2019) and \u2018asti\u2019 (meaning \u2018to exist\u2019)", "This meaning of swastika  from Sanskrit this gets translated as \u2018all is well.\u2019 "], "decomposition": ["Was the word swastika derived from a Sanskrit word?"], "evidence": [[[["Swastika-9"]]], [[["Swastika-3"], "operation"]], [[["Swastika-3"], "operation"]]], "golden_sentence": [["The earliest known use of the word swastika is in Panini's Ashtadhyayi which uses it to explain one of the Sanskrit grammar rules, in the context of a type of identifying mark on a cow's ear."]]}, {"qid": "cda58156dc2c43645f9b", "term": "Gothenburg", "description": "City in V\u00e4sterg\u00f6tland and Bohusl\u00e4n, Sweden", "question": "Could the Toyota Stadium sit a tenth of the population of Gotheburg?", "answer": false, "facts": ["The Toyota Stadium seats 45,000 people", "Gothenburg has a population of over five hundred thousand"], "decomposition": ["How many people can the Toyota Stadium sit?", "What is the population of Gothenburg?", "Is #2 less than #1?"], "evidence": [[[["Toyota Stadium (Texas)-1", "Toyota Stadium-1"]], [["Gothenburg-1"]], ["operation"]], [[["Toyota Stadium-1"]], [["Gothenburg-17"]], ["operation"]], [[["Toyota Stadium-1"]], [["Gothenburg-1"]], ["operation"]]], "golden_sentence": [["Toyota Stadium is a soccer-specific stadium with a 20,500-seat capacity, built and owned by the city of Frisco, Texas, a suburb of Dallas.", "Toyota Stadium (\u8c4a\u7530\u30b9\u30bf\u30b8\u30a2\u30e0, Toyota Sutajiamu) is a 45,000 seat retractable roof stadium in Toyota, Aichi Prefecture, Japan."], ["It is situated by Kattegat, on the west coast of Sweden, and has a population of approximately 570,000 in the city proper and about 1 million inhabitants in the metropolitan area."]]}, {"qid": "d2a9ec826307384a06ee", "term": "Portuguese Colonial War", "description": "1961\u20131974 armed conflicts in Africa between Portugal and independence movements", "question": "Do all of the African regions that participated in the Portugese Colonial War share an official language?", "answer": true, "facts": [" The current African nations of Angola, Guinea-Bissau and Mozambique participated in the the Portugese Colonial War.", "The Portugese Colonial War was a decisive struggle in Lusophone Africa.", "Lusaphone countries are those that include Portugese as an official language."], "decomposition": ["Which African nations participated in the Portuguese Colonial War?", "Which African region did all of #1 belong to?", "Do all nations in #2 share official language?"], "evidence": [[[["Portuguese Colonial War-1"], "no_evidence"], [["Portuguese Colonial War-1"]], [["Lusophone-1"]]], [[["Portuguese Colonial War-24"]], [["Southern Africa-2"]], [["Swahili language-1"]]], [[["Portuguese Colonial War-2"]], [["Sub-Saharan Africa-1"], "no_evidence"], [["Portuguese Angola-36", "Portuguese Guinea-4", "Portuguese Mozambique-55"], "no_evidence", "operation"]]], "golden_sentence": [["The war was a decisive ideological struggle in Lusophone Africa, surrounding nations, and mainland Portugal."], ["The war was a decisive ideological struggle in Lusophone Africa, surrounding nations, and mainland Portugal."], ["The Lusofonia, also known as the Lusophone World (Mundo Lus\u00f3fono), is the corresponding community of Lusophone nations, which exist in Europe, the Americas, Africa, Asia, and Oceania."]]}, {"qid": "823870257265d46bb543", "term": "Mount Emei", "description": "mountain", "question": "Can a Liebherr LTM 11200-9.1 hypothetically lift Mount Emei?", "answer": true, "facts": ["Mount Emei is a 70 ton mountain located in China.", "The Liebherr LTM 11200-9.1 is the world's strongest crane that can lift 1200 tons."], "decomposition": ["How much does Mount Emei weigh?", "How much can a Liebherr LTM 11200-9.1 lift?", "Is #2 greater than #1?"], "evidence": [[[["Mount Emei-2"], "no_evidence"], [["Crane (machine)-43"], "no_evidence"], ["operation"]], [[["Mount Emei-1"], "no_evidence"], [["Liebherr Group-2"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence"]]], "golden_sentence": [[""], [""]]}, {"qid": "90ee9826590212f8971a", "term": "Solo (music)", "description": "musical piece or part of musical piece performed by a single musician", "question": "Can the Department of Defense perform a solo?", "answer": false, "facts": ["A solo is the part of a musical piece performed by a single musician", "The Department of Defense is a US government agency composed of many individuals and unrelated to music"], "decomposition": ["How many are part of a solo performance? ", "What is the Department of Defense?", "How many people are part of #2?", "Is #1 the same as #3?"], "evidence": [[[["First solo flight-1"]], [["United States Department of Defense-1"]], [["United States Department of Defense-1"]], ["operation"]], [[["Solo (music)-1"]], [["United States Department of Defense-1"]], [["United States Department of Defense-1"]], ["operation"]], [[["Solo (music)-1"]], [["United States Department of Defense-2"]], [["United States Department of Defense-1"]], ["operation"]]], "golden_sentence": [[""], ["The United States Department of Defense (DoD, USDOD or DOD) is an executive branch department of the federal government charged with coordinating and supervising all agencies and functions of the government directly related to national security and the United States Armed Forces."], ["More employees include over 826,000 National Guard and Reservists from the armed forces, and over 732,000 civilians bringing the total to over 2.8\u00a0million employees."]]}, {"qid": "0d78c0254bd42e14f369", "term": "Bumblebee", "description": "genus of insects", "question": "Does a bumblebee have to worry about spider veins?", "answer": false, "facts": ["Spider veins is a condition in which the veins become inflamed.", "Bumblebees have a free flowing blood system and do not have veins or arteries."], "decomposition": ["What anatomical features are necessary for a being to have spider veins?", "Do bumblebees have #1?"], "evidence": [[[["Telangiectasia-1"]], [["Drone (bee)-7"], "no_evidence", "operation"]], [[["Telangiectasia-1"]], [["Blood vessel-1", "Insect physiology-12"], "operation"]], [[["Skin-1", "Telangiectasia-1"]], [["Arthropods in culture-1", "Invertebrate-1"], "operation"]]], "golden_sentence": [["Telangiectasias, also known as spider veins, are small dilated blood vessels that can occur near the surface of the skin or mucous membranes, measuring between 0.5 and 1 millimeter in diameter."], [""]]}, {"qid": "410585fdf1662246b920", "term": "Bee", "description": "Clade of insects", "question": "Are queen bees unnecessary for growing apples?", "answer": true, "facts": ["Mason bees are solitary (they live alone)", "Mason bees are efficient pollinators for orchards", "Apple trees are grown in orchards"], "decomposition": ["What is the social structure of Mason bees?", "Where are Mason bees recognized as efficient pollinators?", "Is #1 needless of a queen, and are apples grown in #2?"], "evidence": [[[["Bee-27"]], [["Mason bee-2", "Orchard-7"], "no_evidence"], ["operation"]], [[["Mason bee-5"]], [["Osmia lignaria-8"]], [["Apple-38", "Mason bee-5"]]], [[["Mason bee-1"]], [["Mason bee-11"], "no_evidence"], [["Apple-1"], "no_evidence", "operation"]]], "golden_sentence": [["Most other bees, including familiar insects such as carpenter bees, leafcutter bees and mason bees are solitary in the sense that every female is fertile, and typically inhabits a nest she constructs herself."], ["", "In eastern North America, many orchards are along the shores of Lake Michigan (such as the Fruit Ridge Region), Lake Erie, and Lake Ontario."]]}, {"qid": "93a807ab8302db2bace0", "term": "Law & Order", "description": "original television series (1990-2010)", "question": "Can you taste Law & Order?", "answer": false, "facts": ["Law & Order is a television show.", "Television shows cannot be tasted, only viewed. "], "decomposition": ["What is Law & Order?", "Is the answer to #1 something that can be tasted?"], "evidence": [[[["Law & Order-2"]], [["Taste-1"]]], [[["Law & Order (franchise)-1"]], ["operation"]], [[["Law & Order-1"]], [["Taste-1", "Television show-30"]]]], "golden_sentence": [[""], [""]]}, {"qid": "d60b52766a54fc0a5f4e", "term": "Osama bin Laden", "description": "Co-founder of al-Qaeda", "question": "Does Osama bin Laden put a wafer on his tongue every Sunday?", "answer": false, "facts": ["Osama bin Laden was an Islamic fundamentalist", "The practice of putting a wafer on your tongue is called Communion", "Communion is a Christian religious practice", "Christians commonly attend religious services on Sunday"], "decomposition": ["What is the practice of putting a wafer on your tongue called?", "What religion practices #1 on Sundays?", "Does Osama bin Laden practice #2?"], "evidence": [[[["Eucharist-1"]], [["Christianity-37"]], [["Osama bin Laden-1"]]], [[["Eucharist-65"]], [["Eucharist-1"]], [["Osama bin Laden-1"], "operation"]], [[["Eucharist-95"]], [["Eucharist-1"]], [["Osama bin Laden-10"], "operation"]]], "golden_sentence": [[""], ["Thus, as Justin described, Christians assemble for communal worship on Sunday, the day of the resurrection, though other liturgical practices often occur outside this setting."], ["Osama bin Mohammed bin Awad bin Laden /o\u028a\u02c8s\u0251\u02d0m\u0259 b\u026an \u02c8l\u0251\u02d0d\u0259n/ (Arabic: \u0623\u0633\u0627\u0645\u0629 \u0628\u0646 \u0645\u062d\u0645\u062f \u0628\u0646 \u0639\u0648\u0636 \u0628\u0646 \u0644\u0627\u062f\u0646\u200e, Us\u0101mah bin Mu\u1e25ammad bin \u02bfAwa\u1e0d bin L\u0101din; March 10, 1957 \u2013 May 2, 2011), also rendered Usama bin Ladin, was a founder of the pan-Islamic militant organization al-Qaeda."]]}, {"qid": "6d5007b7b9b9a1a97ebc", "term": "Easy Rider", "description": "1969 film by Dennis Hopper", "question": "Will the producer of Easy Rider become an octogenarian in 2021?", "answer": false, "facts": ["The producer of Easy Rider was Peter Fonda.", "Peter Fonda died in 2019 at the age of 79.", "An octogenarian is someone who is between 80 and 89 years old and is still alive."], "decomposition": ["Who produced Easy Rider?", "What characteristics does someone need to be considered an octogenarian?", "What characteristics does #1 have?", "Are all the characteristics in #2 also in #3?"], "evidence": [[[["Easy Rider-1"]], [["Illustrations of the rule against perpetuities-2"]], [["Peter Fonda-1"]], ["operation"]], [[["Easy Rider-1"]], ["no_evidence"], [["Peter Fonda-1"]], ["operation"]], [[["Easy Rider-1"]], [["Ageing-46"], "no_evidence"], [["Peter Fonda-58"]], ["operation"]]], "golden_sentence": [["Easy Rider is a 1969 American independent road drama film written by Peter Fonda, Dennis Hopper, and Terry Southern, produced by Fonda, and directed by Hopper."], ["The rule presumes that anyone, even an octogenarian (i.e., someone between 80 and 90 years of age) can parent a child, regardless of sex or health."], [""]]}, {"qid": "c8584dcd0f933a074d48", "term": "Blueberry", "description": "section of plants", "question": "Was the Treaty of Versailles settled over blueberry scones?", "answer": false, "facts": ["Blueberries are native to North America.", "Blueberries did not come to Europe until the 1930's.", "The treaty of Versailles was made effective in 1920. "], "decomposition": ["Where was the The Treaty of Versailles settled?", "When did blueberries first go over to #1?", "When was The Treaty of Versailles settled?", "Did #2 occur before #3?"], "evidence": [[[["Paris Peace Conference (1919\u20131920)-1"]], [["Blueberry-33"], "no_evidence"], [["Treaty of Versailles-1"]], ["operation"]], [[["Paris-1", "Treaty of Versailles-1", "Versailles, Yvelines-1"]], [["Blueberry-33"]], [["Treaty of Versailles-1"]], ["operation"]], [[["Treaty of Versailles-7"]], ["no_evidence"], [["Treaty of Versailles-7"]], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["Highbush blueberries were first introduced to Germany, Sweden and the Netherlands in the 1930s, and have since been spread to Lithuania, Latvia, Estonia, Romania, Poland, Italy, Hungary and other countries of Europe."], ["It was signed on 28 June 1919 in Versailles, exactly five years after the assassination of Archduke Franz Ferdinand, which had directly led to the war."]]}, {"qid": "f6fd4ab1e9192239c1c7", "term": "Colitis", "description": "inflammation of the colon or the large intestine", "question": "Is it best to avoid kola nuts with colitis?", "answer": true, "facts": ["Colitis is a disease in which the colon becomes inflamed.", "Many things can trigger colitis, including dairy, alcohol, and caffeine.", "The kola nut is the fruit of the tropical cola tree that contains caffeine inside."], "decomposition": ["What triggers colitis? ", "Are any of the triggers in #1 present in the kola nut?"], "evidence": [[[["Colitis-17"], "no_evidence"], [["Kola nut-2"], "operation"]], [[["Colitis-10"], "no_evidence"], [["Kola nut-1"], "no_evidence"]], [[["Colitis-3"], "no_evidence"], [["Kola nut-1"], "no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "bbad8333e5c1187d77a2", "term": "Donald Duck", "description": "Disney cartoon character", "question": "Would Donald Duck be allowed into most grocery stores?", "answer": false, "facts": ["Donald Duck is known for not wearing any pants or shoes.", "Most grocery stores have a \"No shoes, No Shirt, No Service\" policy."], "decomposition": ["What is Donald Duck known for not wearing?", "Would most grocery stores serve people without #1?"], "evidence": [[[["Donald Duck-1"]], ["no_evidence"]], [[["Donald Duck-1"]], [["Indecent exposure-1"], "no_evidence", "operation"]], [[["Donald Duck-1"], "no_evidence"], [["Indecent exposure-1"]]]], "golden_sentence": [["He typically wears a sailor shirt and cap with a bow tie."]]}, {"qid": "8f2f87efee7060ab6b72", "term": "Indian Ocean", "description": "The ocean between Africa, Asia, Australia and Antarctica (or the Southern Ocean)", "question": "Does the United States of America touch the Indian Ocean?", "answer": false, "facts": ["The United States of America is bordered by the Atlantic and Pacific Oceans.", "Even the westernmost point of the USA, the Hawaiian Islands, is too far east in the Pacific to be anywhere near that ocean's border with the Indian Ocean."], "decomposition": ["What oceans does the US of America border?", "Is the Indian Ocean part of #1?"], "evidence": [[[["East Coast of the United States-1", "West Coast of the United States-1"]], ["operation"]], [[["Arctic Ocean-2", "Arctic Ocean-3", "Atlantic Ocean-2", "Pacific Ocean-1"]], ["operation"]], [[["Borders of the United States-2"]], ["operation"]]], "golden_sentence": [["Regionally, the term refers to the coastal states and area east of the Appalachian Mountains that have shoreline on the Atlantic Ocean, from north to south, Maine, New Hampshire, Massachusetts, Rhode Island, Connecticut, New York, New Jersey, Delaware, Maryland, Virginia, North Carolina, South Carolina, Georgia, and Florida.", "More specifically, it refers to an area defined on the east by the Alaska Range, Cascade Range, Sierra Nevada, and Mojave Desert, and on the west by the Pacific Ocean."]]}, {"qid": "84d5830f0a1fe2231ebe", "term": "QWERTY", "description": "keyboard layout where the first line is \"QWERTYUIOP\"", "question": "Can second row of QWERTY keyboard spell Abdastartus's kingdom?", "answer": true, "facts": ["QWERTY keyboards have one row of numbers followed by three rows of letters.", "The second row of the QWERTY keyboard has the letters: QWERTYUIOP.", "Abdastartus was king of Tyre from 929 \u2013 921 BC."], "decomposition": ["What letters are on the second row of a QWERTY keyboard?", "What was Abdastartus' kingdoms name?", "Are all the letters in #2 also found in #1?"], "evidence": [[[["QWERTY-1"], "no_evidence"], [["Abdastartus-1"]], ["operation"]], [[["QWERTY-1"], "no_evidence"], [["Abdastartus-3"]], ["operation"]], [[["QWERTY-9"]], [["Abdastartus-1"]], ["operation"]]], "golden_sentence": [["The name comes from the order of the first six keys on the top left letter row of the keyboard (Q W E R T Y)."], ["Abdastartus (\u2018Abd-\u2018Ashtart) was a king of Tyre, son of Baal-Eser I (Beleazarus) and grandson of Hiram I."]]}, {"qid": "6add1a22df575a73564d", "term": "Pottery", "description": "Craft of making objects from clay", "question": "Are all types of pottery safe to cook in?", "answer": false, "facts": ["Some types of pottery glaze are unsafe for contact with food meant for human consumption. ", "Antique pottery pieces may have hazardous levels of lead in them."], "decomposition": ["Are all antique or glazed pottery safe to cook in?"], "evidence": [[[["Pottery-35"], "no_evidence"]], [[["Pottery-4"], "no_evidence", "operation"]], [[["Pottery-62"], "no_evidence"]]], "golden_sentence": [[""]]}, {"qid": "0010f6920845dd38d867", "term": "New Year's Eve", "description": "holiday celebrated on 31 December", "question": "Should you ask a neighbor for candy on New Year's Eve?", "answer": false, "facts": ["Halloween is a holiday where children knock on doors of houses in their neighborhood asking for treats", "Halloween falls on October 31st", "New Year's Eve is a celebration of the end of the year held on December 31st"], "decomposition": ["On which holiday do children go trick-or-treating?", "What is the date of #1?", "When is New Year's Eve celebration?", "Are #2 and #3 the same?"], "evidence": [[[["Halloween-3"]], [["Halloween-1"]], [["New Year's Eve-1"]], ["operation"]], [[["Trick-or-treating-1"]], [["Halloween-11"]], [["New Year's Eve-36"]], ["operation"]], [[["Trick-or-treating-1"]], [["Halloween-1"]], [["New Year's Eve-1"]], ["operation"]]], "golden_sentence": [["Halloween activities include trick-or-treating (or the related guising and souling), attending Halloween costume parties, carving pumpkins into jack-o'-lanterns, lighting bonfires, apple bobbing, divination games, playing pranks, visiting haunted attractions, telling scary stories, as well as watching horror films."], ["Halloween or Hallowe'en (a contraction of Hallows' Even or Hallows' Evening), also known as Allhalloween, All Hallows' Eve, or All Saints' Eve, is a celebration observed in many countries on 31 October, the eve of the Western Christian feast of All Hallows' Day."], ["In many countries, New Year's Eve is celebrated at evening parties, where many people dance, eat, drink, and watch or light fireworks."]]}, {"qid": "7ddc87ad83364c23b3b5", "term": "Eye surgery", "description": "medical specialty", "question": "Would Eye surgery on a fly be in vain?", "answer": true, "facts": ["Researchers at BYU have been developing smaller surgical instruments to improve medical procedures.", "BYU researchers  created robotically-controlled forceps that can pass through a hole about 3 millimeters in size.", "The eye of a fly is considerably small and estimates range from .5mm to 2mm."], "decomposition": ["What levels of precision can be reached by robot-assisted surgery?", "What is the size range of the eye of a fly?", "Is #1 considerably larger than the range of #2?"], "evidence": [[[["Robot-assisted surgery-21"], "no_evidence"], [["Fly-9"], "no_evidence"], ["operation"]], [[["Robot-assisted surgery-28"], "no_evidence"], [["Fly-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Robot-assisted surgery-2"], "no_evidence"], [["Fly-14"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "6ede62088aac32b88169", "term": "Sonnet", "description": "form of poetry with fourteen lines; by the thirteenth century it signified a poem of fourteen lines that follows a strict rhyme scheme and specific structure", "question": "Can Jabberwocky be considered a sonnet?", "answer": false, "facts": ["A sonnet is a fourteen line poem that follows certain rhyme schemes.", "Jabberwocky is an 1871 poem by Lewis Carroll.", "Jabberwocky is a 28 line poem that uses nonsense words."], "decomposition": ["How many lines does a sonnet have?", "How many lines did the poem Jabberwocky have?", "Is #1 the same as #2?"], "evidence": [[[["Sonnet-2"]], [["Jabberwocky-21"], "no_evidence"], ["operation"]], [[["Sonnet-2"]], [["Jabberwocky-1", "Jabberwocky-21"]], ["operation"]], [[["Sonnet-2"]], [["Jabberwocky-21"], "no_evidence"], ["operation"]]], "golden_sentence": [["By the thirteenth century it signified a poem of fourteen lines that follows a strict rhyme scheme and specific structure."], ["Parsons suggests that this is mirrored in the prosody of the poem: in the tussle between the tetrameter in the first three lines of each stanza and trimeter in the last lines, such that one undercuts the other and we are left off balance, like the poem's hero."]]}, {"qid": "0071170f6430e556011a", "term": "White", "description": "color", "question": "Is white light the absence of color?", "answer": false, "facts": ["White light is formed by the combination of red, green, and blue light.", "Instead, black is considered to be the absence of color."], "decomposition": ["How many colors of light are mixed to create white light?", "Is #1 equal to zero?"], "evidence": [[[["Color mixing-3"]], ["operation"]], [[["White-41"]], ["operation"]], [[["White-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["If all three primary colors of light are mixed in equal proportions, the result is neutral (gray or white)."]]}, {"qid": "832bb2e11b070fa9befb", "term": "Christopher Walken", "description": "American actor", "question": "Could Christopher Walken enlist in the United States Marine Corps?", "answer": false, "facts": ["Christopher Walken is 77 years old.", "The maximum age for first-time enlistment in the United States Marine Corps is 28."], "decomposition": ["How old is Christopher Walken?", "What is the age limit for enlistment in the United States Marine Corps?", "Is #1 less than or equal to #2?"], "evidence": [[[["Christopher Walken-1"]], [["United States Armed Forces-3"], "no_evidence"], ["operation"]], [[["Christopher Walken-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Christopher Walken-1"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Christopher Walken (born Ronald Walken, March 31, 1943) is an American actor, singer, comedian, director, producer, screenwriter, and dancer, who has appeared in more than 100 films and television programs, including Annie Hall (1977), The Deer Hunter (1978), The Dogs of War (1980), The Dead Zone (1983), A View to a Kill (1985), Batman Returns (1992), True Romance (1993), Pulp Fiction (1994), Antz (1998), Vendetta (1999), Sleepy Hollow (1999), Joe Dirt (2001), Catch Me If You Can (2002), Hairspray (2007), Seven Psychopaths (2012), the first three Prophecy films, The Jungle Book (2016), and Irreplaceable You (2018)."], ["Although conscription has been used in the past, it has not been used since 1973, but the Selective Service System retains the power to conscript males, and requires that all male citizens and residents residing in the U.S. between the ages of 18\u201325 register with the service."]]}, {"qid": "71675f82163c4d478121", "term": "Milky Way", "description": "Spiral galaxy containing our Solar System", "question": "Is number of stars in Milky Way at least ten times earth's population?", "answer": true, "facts": ["The number of stars in the Milky Way galaxy is between 100 and 400 billion.", "Earth's population in 2018 was 7.5 billion people."], "decomposition": ["How many stars are in the Milky Way galaxy?", "What is the number of the human population on earth?", "Is #1 greater than or equal to ten times #2?"], "evidence": [[[["Milky Way-2"]], [["World population-1"]], ["operation"]], [[["Milky Way-2"]], [["World population-1"]], ["operation"]], [[["Milky Way-2"]], [["World population-1"]], ["operation"]]], "golden_sentence": [["It is estimated to contain 100\u2013400 billion stars and more than 100 billion planets."], ["In demographics, the world population is the total number of humans currently living, and was estimated to have reached 7.8 billion people as of March 2020[update]."]]}, {"qid": "deb506c10295c2d44006", "term": "The Hobbit", "description": "Fantasy novel by J. R. R. Tolkien", "question": "Can The Hobbit be read in its entirety in four minutes?", "answer": true, "facts": ["The Hobbit is a 95,356 word book by J.R.R. Tolkien.", "Speed reader Howard Stephen Berg could read at the speed of 25,000 words per minute.", "Speed reader Maria Teresa Calderon from the Philippines claimed to be able to read 80,000 words per minute with 100% comprehension."], "decomposition": ["How many words are in the Hobbit?", "How many words per minute could Maria Teresa Calderon read?", "What is #2 multiplied by 4?", "Is #3 greater than or equal to #1?"], "evidence": [[[["The Hobbit-3"], "no_evidence"], [["Speed reading-19"], "no_evidence"], ["no_evidence", "operation"], ["no_evidence", "operation"]], [["no_evidence"], [["Speed reading-19"]], ["operation"], ["operation"]], [[["The Hobbit-1"], "no_evidence"], [["Speed reading-19"]], ["operation"], ["operation"]]], "golden_sentence": [[""], ["Howard Stephen Berg from the United States has claimed to be the Guinness World Record holder for fast reading with a speed of 25,000 words per minute, and Maria Teresa Calderon from the Philippines claims to have earned the Guinness World Record for World's Fastest Reader at 80,000 words per minute reading speed and 100% comprehension."]]}, {"qid": "adbc3577cb5f63a18fdd", "term": "Peach", "description": "species of fruit tree (for the fruit use Q13411121)", "question": "Will you see peach blossoms and Andromeda at the same time?", "answer": false, "facts": ["Peach trees bloom in the spring.", "Andromeda is visible in the fall."], "decomposition": ["When do peach trees bloom?", "When can you see Andromeda?", "Is #1 the same as #2?"], "evidence": [[[["Peach-18"]], [["Andromeda Galaxy-56"]], [["Andromeda Galaxy-56", "Peach-18"]]], [[["Peach-5"]], [["Andromeda (constellation)-3"], "no_evidence"], ["operation"]], [[["Peach-18"]], [["Andromeda (constellation)-1", "Andromeda (constellation)-3"], "no_evidence"], ["operation"]]], "golden_sentence": [["The trees flower fairly early (in March in western Europe) and the blossom is damaged or killed if temperatures drop below about \u22124\u00a0\u00b0C (25\u00a0\u00b0F)."], ["Andromeda is best seen during autumn nights in the Northern Hemisphere when it passes high overheard, reaching its highest point around midnight in October, and two hours later each successive month."], ["", ""]]}, {"qid": "89092070cd7e78f68069", "term": "Arnold Schwarzenegger", "description": "Austrian-American actor, businessman, bodybuilder and politician", "question": "Could Arnold Schwarzenegger hypothetically defeat Haf\u00fe\u00f3r Bj\u00f6rnsson in a powerlifting competition if both are at their peak strength?", "answer": false, "facts": ["A powerlifting competition is won by the person who lifts the most combined weight across the squat, deadlift, and bench press.", "Arnold Schwarzenegger's powerlifting personal records are a 545 lb squat, 520 lb bench press, and a 710 lb deadlift.", "Haf\u00fe\u00f3r Bj\u00f6rnsson's powerlifting personal records in competition are a 970 lb squat, 551 lb bench press, and a 904 lb deadlift."], "decomposition": ["What lifts are contested in powerlifting?", "What are Arnold Schwarzenegger's personal records in #1?", "What are Haf\u00fe\u00f3r Bj\u00f6rnsson's personal records in #1?", "Is the sum of #2 greater than the sum of #3?"], "evidence": [[[["Powerlifting-1"]], [["Arnold Schwarzenegger-24"]], [["Haf\u00fe\u00f3r J\u00fal\u00edus Bj\u00f6rnsson-4"]], ["operation"]], [[["Powerlifting-1"]], [["Arnold Schwarzenegger-24"]], [["Haf\u00fe\u00f3r J\u00fal\u00edus Bj\u00f6rnsson-15"], "no_evidence"], ["operation"]], [[["Powerlifting-1"]], [["Arnold Schwarzenegger-24"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Powerlifting is a strength sport that consists of three attempts at maximal weight on three lifts: squat, bench press, and deadlift."], [""], [""]]}, {"qid": "79f0de2f2b9baeebb10f", "term": "Mesopotamia", "description": "Historical region within the Tigris\u2013Euphrates river system", "question": "Was Mesopotamia part of what is now China?", "answer": false, "facts": ["Mesopotamia occupies an area of land called the Fertile Crescent which is part of what is currently known as the Middle East.", "China is much further East, beyond the Stans and India."], "decomposition": ["What land did Mesopotamia occupy?", "What is #1 currently known as?", "Is #2 located in China?"], "evidence": [[[["Mesopotamia-1"]], [["Fertile Crescent-1"]], [["China-1"], "operation"]], [[["Mesopotamia-1"]], [["Mesopotamia-1"]], [["Central Asia-1", "Western Asia-1"]]], [[["Mesopotamia-6"]], [["Mesopotamia-6"]], [["Mesopotamia-6"]]]], "golden_sentence": [["Mesopotamia (Greek: \u039c\u03b5\u03c3\u03bf\u03c0\u03bf\u03c4\u03b1\u03bc\u03af\u03b1) is a historical region of Western Asia situated within the Tigris\u2013Euphrates river system, in the northern part of the Fertile Crescent, in modern days roughly corresponding to most of Iraq, Kuwait, the eastern parts of Syria, Southeastern Turkey, and regions along the Turkish\u2013Syrian and Iran\u2013Iraq borders."], [""], [""]]}, {"qid": "3d4837d8ed659c6c578f", "term": "Doctor Who", "description": "British science fiction TV series", "question": "In Doctor Who, did the war doctor get more screen time than his successor?", "answer": false, "facts": ["The War Doctor was succeeded by the \"9th Doctor\". ", "The War Doctor was featured in two episodes of Doctor Who.", "The 9th Doctor was featured in 13 episodes of Doctor Who."], "decomposition": ["Who was the successor of the War Doctor?", "How many episodes was #1 in?", "How many episodes was the War Doctor in?", "Is #3 greater than #2?"], "evidence": [[[["War Doctor-1"]], [["Ninth Doctor-4", "Ninth Doctor-5", "Ninth Doctor-6"]], [["War Doctor-10", "War Doctor-8"]], ["operation"]], [[["War Doctor-1"]], [["Ninth Doctor-4", "Ninth Doctor-5"]], [["Doctor Who-47"]], ["operation"]], [[["War Doctor-1"]], [["Ninth Doctor-1"], "no_evidence"], [["War Doctor-10", "War Doctor-7", "War Doctor-8", "War Doctor-9"]], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["", "", ""], ["", ""]]}, {"qid": "851189b269b3b32727c2", "term": "Pancake", "description": "Thin, round cake made of eggs, milk and flour", "question": "Are some types of pancakes named after coins?", "answer": true, "facts": ["Silver dollar pancakes are a variety that is smaller than traditional pancakes.", "Silver dollars are a type of American coin."], "decomposition": ["What are some common types of pancakes?", "Is any of #1 named after a coin?"], "evidence": [[[["Pancake-12"], "no_evidence"], ["operation"]], [[["Pancake-12"]], [["Silver dollar-1"]]], [[["Pancake-67", "Pancake-69", "Pancake-70"]], [["Pancake-69"], "operation"]]], "golden_sentence": [[""]]}, {"qid": "13fea84ea5ba187f4c61", "term": "Fantasy", "description": "Genre of literature, film, television and other artforms", "question": "Would J.K Rowling's top sellers be on a fantasy shelf?", "answer": true, "facts": ["J.K Rowling's top sellers are her Harry Potter series.", "Harry Potter is a series about a boy who goes to a magical school to learn wizardry."], "decomposition": ["What is J. K. Rowling's top selling book?", "Is #1 fantasy?"], "evidence": [[[["J. K. Rowling-1"]], [["Harry Potter-1"], "operation"]], [[["J. K. Rowling-1"]], [["J. K. Rowling-1"]]], [[["J. K. Rowling-22"]], ["operation"]]], "golden_sentence": [["She is best known for writing the Harry Potter fantasy series, which has won multiple awards and sold more than 500\u00a0million copies, becoming the best-selling book series in history."], ["Harry Potter is a series of fantasy novels written by British author J. K. Rowling."]]}, {"qid": "a2acf87cbea61645c69e", "term": "P. G. Wodehouse", "description": "English author", "question": "Was P. G. Wodehouse's favorite book The Hunger Games?", "answer": false, "facts": ["P. G. Wodehouse died in 1975.", "The Hunger Games was published in 2008."], "decomposition": ["When did P. G. Wodehouse die?", "When was the Hunger Games first published?", "Did #2 happen before #1?"], "evidence": [[[["P. G. Wodehouse-1"]], [["The Hunger Games (novel)-1"]], ["operation"]], [[["P. G. Wodehouse-51"]], [["The Hunger Games (novel)-1"]], ["operation"]], [[["P. G. Wodehouse-51"]], [["The Hunger Games (novel)-1"]], ["operation"]]], "golden_sentence": [["Sir Pelham Grenville Wodehouse KBE (/\u02c8w\u028adha\u028as/, WOOD-howss; 15 October 1881\u00a0\u2013 14 February 1975) was an English author and one of the most widely read humorists of the 20th century."], ["The Hunger Games is a 2008 dystopian novel by the American writer Suzanne Collins."]]}, {"qid": "6035608c1826c1cccce4", "term": "Limbic system", "description": "structures of the brain", "question": "Will The Exorcist stimulate limbic system?", "answer": true, "facts": ["The limbic system of the brain contains regions that detect fear, control bodily functions and perceive sensory information.", "The Exorcist has been called one of the scariest movies of all time.", "The Exorcist ranked number 3 on the American Film Institute's 100 Years/100 Thrills list."], "decomposition": ["Based on its functions, what kinds of stimuli is the limbic system likely to respond to?", "What is the general opinion of the movie The Exorcist?", "Does #2 suggest that the movie will generate #1?"], "evidence": [[[["Limbic system-17"]], [["The Exorcist-6"]], [["Limbic system-17", "The Exorcist (film)-1"]]], [[["Limbic system-17"]], [["The Exorcist (film series)-16", "The Exorcist (film)-78"]], ["operation"]], [[["Limbic system-9"]], [["The Exorcist (film)-1"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], ["The adaptation is faithful to the book, which itself has been commercially successful (hitting the New York Times bestseller list)."], ["", ""]]}, {"qid": "585d693a06d0d161422c", "term": "Judo", "description": "modern martial art, combat and Olympic sport", "question": "Do silicone suits make judo difficult?", "answer": true, "facts": ["Judo is a martial art that requires combatants to grip their opponents and throw them in various ways.", "Judo practitioners traditionally wear an outfit called a gi, which opponents use to grip and throw.", "Silicone is one of the slipperiest substances on the planet."], "decomposition": ["What maneuvers are required to do Judo?", "What characteristics does an article of clothing need to have in order to do #1 effectively?", "What characteristics does a silicone suit have? ", "Is #3 excluded from #2?"], "evidence": [[[["Judo-1"]], [["Keikogi-1"], "no_evidence"], [["Silicone rubber-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Leopold's maneuvers-6"], "no_evidence"], ["no_evidence"], [["Silicone-47"]], ["operation"]], [[["Judo-1"]], [["Judo-48"]], [["Silicone-1"]], ["operation"]]], "golden_sentence": [["Strikes and thrusts by hands and feet as well as weapons defences are a part of judo, but only in pre-arranged forms (kata, \u5f62) and are not allowed in judo competition or free practice (randori, \u4e71\u53d6\u308a)."], [""], ["Silicone rubber is generally non-reactive, stable, and resistant to extreme environments and temperatures from \u221255 to 300\u00a0\u00b0C (\u221267 to 572\u00a0\u00b0F) while still maintaining its useful properties."]]}, {"qid": "6425d5f276ae1dfd0dcd", "term": "JAG (TV series)", "description": "American legal drama television series (1996-2005)", "question": "Could you watch all of JAG in six months?", "answer": true, "facts": ["JAG has 227 episodes in the entire series.", "Each episode of JAG is between 42-47 minutes long.", "There are over 200,000 minutes in a month. ", "The entire series of JAG is under 12,000 minutes. "], "decomposition": ["How many episodes of JAG are there?", "How long is an episode of JAG?", "What is #1 multiplied by #2?", "How many minutes are there in six months?", "Is #3 less than or equal to #4?"], "evidence": [[["no_evidence"], ["no_evidence"], ["operation"], [["Year-57"]], ["operation"]], [["no_evidence"], ["no_evidence"], ["operation"], ["no_evidence", "operation"], ["operation"]], [["no_evidence"], [["Television pilot-19"], "no_evidence"], ["operation"], ["no_evidence", "operation"], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "754ece310418e8e1942b", "term": "Solubility", "description": "Capacity of a designated solvent to hold a designated solute in homogeneous solution under specified conditions", "question": "In isopropyl alcohol, is the solubility of salt low?", "answer": true, "facts": ["Isopropyl alcohol is unique in the sense that salt remains visible.", "When salt has high solubility, it becomes invisible."], "decomposition": ["How does high solubility affect the visibility of salt?", "Does salt in isopropyl alcohol fail to exhibit #1?"], "evidence": [[[["Salt-15"]], [["Isopropyl alcohol-4"]]], [[["Isopropyl alcohol-4"]], ["operation"]], [[["Solubility-1"]], [["Isopropyl alcohol-4"], "no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "c18b952937e394c60daf", "term": "Noble gas", "description": "group of chemical elements tend to be chemically inert and thus form odorless, colorless, monatomic gases with low reactivity; consists of helium, neon, argon, krypton, xenon, radon, and possibly oganesson", "question": "Was the Japanese street aesthetic once illuminated by noble gasses?", "answer": true, "facts": ["Japan was known for their use of brightly lit businesses and signage.", "The signs in Japan were lit with neon for a long time."], "decomposition": ["What were the signs in Japan known for in the past?", "Were noble gasses the reason behind #1?"], "evidence": [[[["Neon lighting-3"], "no_evidence"], [["Neon lighting-1"]]], [["no_evidence"], [["Noble gas-1"], "operation"]], [[["Neon sign-1"], "no_evidence"], [["Neon-1", "Neon-3"], "operation"]]], "golden_sentence": [[""], ["Neon lights were named for neon, a noble gas which gives off a popular orange light, but other gases and chemicals are used to produce other colors, such as hydrogen (red), helium (yellow), carbon dioxide (white), and mercury (blue)."]]}, {"qid": "1524d4e66aead7697278", "term": "Rick and Morty", "description": "Animated sitcom", "question": "Could Rich and Morty be triggered for children of alcoholics?", "answer": true, "facts": ["Rick, one of the titular characters of Rick and Morty, is often seen drunk and speaking abusively to Morty.", "Morty's mother Beth is depicted multiple times neglecting her children while getting drunk on wine. ", "Trauma triggers can occur when someone is exposed to something that reminds them of a traumatic situation. "], "decomposition": ["What depictions are common triggers for children of alcoholics?", "Do any of the characters from Rick and Morty exhibit the characteristics in #1?"], "evidence": [[[["Alcoholism-13"], "no_evidence"], [["Rick and Morty-5"], "operation"]], [[["Alcoholism-13"], "no_evidence"], [["Rick and Morty-5"], "no_evidence", "operation"]], [[["Adult Children of Alcoholics-4"]], [["Adult Children of Alcoholics-4", "Rick and Morty-4", "Rick and Morty-5"]]]], "golden_sentence": [["Alcoholism can also lead to child neglect, with subsequent lasting damage to the emotional development of the alcoholic's children."], ["Rick is an eccentric and alcoholic mad scientist, who eschews many ordinary conventions such as school, marriage, love, and family."]]}, {"qid": "c371e80e7763d99af680", "term": "Gorillaz", "description": "British virtual band", "question": "Does it seem like the Gorillaz is composed of more members than they have?", "answer": true, "facts": ["In music videos for Gorillaz songs, there are four animated bandmates playing.", "Gorillaz is a collaboration of 3 band members."], "decomposition": ["How many band members are in Gorillaz?", "How many animated band members are in Gorillaz videos?", "Is #2 more than #1?"], "evidence": [[[["Gorillaz-1"]], [["Gorillaz-1"]], ["operation"]], [[["Gorillaz-1"]], [["Gorillaz-1"]], ["operation"]], [[["Gorillaz-1"]], [["Gorillaz-1"]], ["operation"]]], "golden_sentence": [["The band primarily consists of four animated members: Stuart \"2-D\" Pot, Murdoc Niccals, Noodle, and Russel Hobbs."], ["The band primarily consists of four animated members: Stuart \"2-D\" Pot, Murdoc Niccals, Noodle, and Russel Hobbs."]]}, {"qid": "35ae18ff0d7a3cc26286", "term": "Oceanography", "description": "The study of the physical and biological aspects of the ocean", "question": "Does a person suffering from Thalassophobia enjoy oceanography?", "answer": false, "facts": ["Thalassophobia is a deep and persistent fear of the sea.", "Oceanography is the study of bodies of water.", "Oceanographers frequently observe and interact with bodies of water such as lakes, seas, and oceans."], "decomposition": ["What do people that have thalassophobia fear?", "Oceanography is the study of what?", "Is #1 excluded from #2?"], "evidence": [[[["Thalassophobia-1"]], [["Oceanography-1"]], ["operation"]], [[["Thalassophobia-1"]], [["Oceanography-1"]], ["operation"]], [[["Thalassophobia-1"]], [["Oceanography-1"]], ["operation"]]], "golden_sentence": [["Thalassophobia can include fear of being in deep bodies of water, fear of the vast emptiness of the sea, of sea waves, sea creatures, and fear of distance from land."], ["Oceanography (compound of the Greek words \u1f60\u03ba\u03b5\u03b1\u03bd\u03cc\u03c2 meaning \"ocean\" and \u03b3\u03c1\u03ac\u03c6\u03c9 meaning \"write\"), also known as oceanology, is the study of the physical and biological aspects of the ocean."]]}, {"qid": "3457bc4f9b526efc87a9", "term": "United Airlines", "description": "Airline in the United States", "question": "Is Glycol something United Airlines would buy?", "answer": true, "facts": ["Glycol is a commonly used de-icing fluid for commercial planes.", "American Airlines flies all year round, including throughout the winter."], "decomposition": ["What is Glycol commonly used for?", "What cold season does American Airlines fly its planes?", "Would #1 be helpful in #2?"], "evidence": [[[["Diol-2", "Diol-4"]], [["American Airlines-1"], "no_evidence"], ["operation"]], [[["Ethylene glycol-1"]], [["Winter-1"]], [["Ethylene glycol-16"]]], [[["Diol-2", "Ethylene glycol-1"]], [["Winter-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["", "Another example is propane-1,2-diol, or alpha propylene glycol, HO\u2212CH2\u2212CH(OH)\u2212CH3, used in the food and medicine industry, as well as a relatively non-poisonous antifreeze product."], [""]]}, {"qid": "c5bf1208714f7912b9d0", "term": "Poseidon", "description": "Ancient Greek god of the sea, earthquakes and horses", "question": "Is Poseidon similar to the god Vulcan?", "answer": false, "facts": ["Poseidon is the Greek god of the sea and water, and is the brother of Zeus.", "Neptune is the Roman god of the sea.", "Hephaestus is the Greek god of fire.", "Hephaestus's ROman equivalent is Vulcan, the Roman god of fire."], "decomposition": ["What are the major characteristics of Poseidon?", "What are the major characteristics of Vulcan?", "Is there a significant overlap between #1 and #2?"], "evidence": [[[["Poseidon-43"]], [["Vulcan (mythology)-44"]], ["operation"]], [[["Poseidon-1"]], [["Vulcan (mythology)-1"]], ["operation"]], [[["Greek sea gods-6"]], [["Vulcan (mythology)-1"]], ["operation"]]], "golden_sentence": [["He was associated with dolphins and three-pronged fish spears (tridents)."], ["Vulcan had a happy childhood with dolphins as his playmates and pearls as his toys."]]}, {"qid": "987996513acfde1806ef", "term": "Green Party of England and Wales", "description": "Political party in England and Wales", "question": "Can members of the Green Party of England and Wales vote in the USA?", "answer": false, "facts": ["Green Party of England Wales isn't registered in the USA.", "People who live in England can't vote in the USA."], "decomposition": ["Members of the Green Party of England and Wales are from which country?", "Can people living in #1 vote in the US?"], "evidence": [[[["Green Party of England and Wales-1"]], ["operation"]], [[["Green Party of England and Wales-1"]], [["Voting rights in the United States-101"]]], [[["Green Party of England and Wales-1"]], [["Voting rights in the United States-101"]]]], "golden_sentence": [[""]]}, {"qid": "f08c287b49890b6036cb", "term": "Golden Gate Bridge", "description": "suspension bridge on the San Francisco Bay", "question": "Do depressed people travel to the Golden Gate Bridge often?", "answer": true, "facts": ["The Golden Gate Bridge is one of the most popular suicide spots in the USA.", "Suicide is often caused by severe depression."], "decomposition": ["What is the ultimate end that severe depression can lead to?", "Is the Golden Gate Bridge a place where #1 is known to often happen?"], "evidence": [[[["Major depressive disorder-22"]], [["Suicides at the Golden Gate Bridge-4"], "operation"]], [[["Suicide-7"]], [["Golden Gate Bridge-50"]]], [[["Suicide-1"]], [["Suicides at the Golden Gate Bridge-4"]]]], "golden_sentence": [[""], ["The Golden Gate Bridge, referred to by Krista Tippett as a \"suicide magnet\", is the second-most used suicide site/suicide bridge in the world, after the Nanjing Yangtze River Bridge (see List of suicide sites)."]]}, {"qid": "6627a8d9112a0bbccdb2", "term": "Olive oil", "description": "liquid fat extracted by pressing olives", "question": "Do some people soak in olive oil and water?", "answer": true, "facts": ["Adding olive oil to bath water is a common practice for dry skin.", "In baths, people tend to soak for a period of time. "], "decomposition": ["During which activity do people soak in water for some time?", "Is it common to add olive oil water for dry skin during #1?"], "evidence": [[[["Bathing-1"]], [["Bathing-44"], "no_evidence"]], [[["Bathing-1"]], [["Bathing-44"], "no_evidence"]], [[["Bathing-1"]], [["Olive oil-28"], "no_evidence", "operation"]], [[["Bathing-1"]], ["no_evidence"]]], "golden_sentence": [["By analogy, especially as a recreational activity, the term is also applied to sun bathing and sea bathing."], [""]]}, {"qid": "d3809b674117e8a14703", "term": "Durian", "description": "genus of plants", "question": "Are Durian fruits an olfactory delight?", "answer": false, "facts": ["Durian is a plant type that produces several kinds of fruit.", "Olfactory refers to the human sense of smell.", "Pleasant smells according to polls include flowers and sweet foods.", "Durian fruits have been banned in Singapore due to its overwhelming smell."], "decomposition": ["What kind of smell is the durian known for?", "Is #1 pleasant?"], "evidence": [[[["Durian-3"]], ["operation"]], [[["Durian-3"]], ["operation"]], [[["Durian-3"]], [["Durian-3"]]]], "golden_sentence": [["Some people regard the durian as having a pleasantly sweet fragrance, whereas others find the aroma overpowering with an unpleasant odour."]]}, {"qid": "e031734cb7e938d2d367", "term": "Aerosmith", "description": "American rock band", "question": "Can Aerosmith legally drive in the carpool lane?", "answer": true, "facts": ["Aerosmith is a rock band with five members", "Carpool lanes require at least two occupants in each vehicle"], "decomposition": ["How many people does the band Aerosmith have?", "What is the minimum number of occupants required to use the carpool lane?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Aerosmith-1"]], [["High-occupancy vehicle lane-1"]], ["operation"]], [[["Aerosmith-1"]], [["High-occupancy vehicle lane-1"]], ["operation"]], [[["Aerosmith-1"]], [["High-occupancy vehicle lane-1"]], ["operation"]]], "golden_sentence": [[""], ["The normal minimum occupancy level is 2 or 3 occupants."]]}, {"qid": "ae53eaea1654e7ed6362", "term": "Petroleum", "description": "Naturally occurring hydrocarbon liquid found underground", "question": "Can petroleum jelly be used as fuel in a car?", "answer": false, "facts": ["Petroleum is a highly reactive liquid used to power cars.", "Petroleum jelly is a solid substance used as an ointment on cuts and scrapes to promote healing.", "Petroleum jelly does not oxidize on exposure to the air and is not readily acted on by chemical reagents."], "decomposition": ["What is petroleum jelly used for?", "Does #1 include fueling cars?"], "evidence": [[[["Petroleum jelly-8"]], [["Petroleum jelly-8"]]], [[["Petroleum jelly-2"]], [["Gasoline-1"], "operation"]], [[["Petroleum jelly-15", "Petroleum jelly-23", "Petroleum jelly-24", "Petroleum jelly-26", "Petroleum jelly-8"]], ["operation"]]], "golden_sentence": [["Most uses of petroleum jelly exploit its lubricating and coating properties, including use on dry lips and dry skin."], [""]]}, {"qid": "d319de6df716b83b5e58", "term": "Spider wasp", "description": "family of insects", "question": "Do spider wasps have eight legs?", "answer": false, "facts": ["A spider wasp is a kind of wasp, which is an insect.", "Insects all have six legs."], "decomposition": ["What kind of animal is a spider wasp?", "Do #1's have eight legs?"], "evidence": [[[["Spider wasp-1"]], [["Spider wasp-5"]]], [[["Spider wasp-1", "Wasp-1"]], [["Insect-1"], "operation"]], [[["Spider wasp-1"]], ["no_evidence"]]], "golden_sentence": [[""], [""]]}, {"qid": "af81ff8803c7edc0f521", "term": "Newt Gingrich", "description": "50th Speaker of the United States House of Representatives", "question": "Is Newt Gingrich's nickname a type of Reptile?", "answer": false, "facts": ["Newt Gingrich was born Newton Leroy Gingrich and goes by the nickname Newt.", "A newt is a type of Amphibian with smooth, sticky skin..", "Reptiles have dry and hard skin."], "decomposition": ["What was Newt Gingrich's nickname?", "Is #1 a reptile?"], "evidence": [[[["Newt Gingrich-1"]], [["Newt-32"], "operation"]], [[["Newt Gingrich-1"], "no_evidence"], [["Newt-1"], "operation"]], [[["Newt Gingrich-1"]], [["Newt-32"]]]], "golden_sentence": [[""], ["Newts, as with salamanders in general and other amphibians, serve as bioindicators because of their thin, sensitive skin and evidence of their presence (or absence) can serve as an indicator of the health of the environment."]]}, {"qid": "e6e13b833e90a52e90ba", "term": "Tomato", "description": "Edible berry of the tomato plant, originating in South America", "question": "Do you need both hot and cold water to peel a tomato?", "answer": true, "facts": ["The first step in removing the skin from at tomato is to quickly submerge it in boiling water.", "The second step in removing the skin from a tomato is to take the tomatoes out of the boiling water and put them into ice water."], "decomposition": ["What are the various steps involved in peeling tomatoes?", "Does any of #1 use hot water?", "Does any of #1 use cold water?", "Are #2 and #3 positive?"], "evidence": [[[["Blanching (cooking)-1", "Peel (fruit)-1"]], [["Blanching (cooking)-7"], "operation"], [["Blanching (cooking)-9"], "operation"], ["operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Tomato-89"], "no_evidence"], ["no_evidence", "operation"], ["no_evidence", "operation"], ["no_evidence", "operation"]]], "golden_sentence": [["The process has three stages: preheating, blanching, and cooling.", ""], ["For hot water blanching, vegetables are immersed under pre-warmed water (70 to 100\u00a0\u00b0C) for varying amounts of time, depending on type and quantity."], ["Directly following the heat treatment, vegetables/fruits are quickly chilled by cold water."]]}, {"qid": "bec1d1994edb803367d9", "term": "Friday", "description": "day of the week", "question": "Does 2015 have more unlucky Friday's than usual?", "answer": true, "facts": ["Friday the 13th is known as an unlucky Friday because of the number 13.", "A year can have as many as three Friday the 13ths.", "One Friday the 13th is the average per year.", "There were 3 Friday the 13ths in 2015."], "decomposition": ["How many Friday the 13ths were in 2015?", "What is the usual number of Friday the 13ths per year?", "Is #1 more than #2?"], "evidence": [[[["Friday the 13th-1"]], [["Friday the 13th-1"]], ["operation"]], [[["Friday the 13th-1"]], [["Friday the 13th-1"]], ["operation"]], [[["Friday the 13th-25"]], [["Friday the 13th-1"]], ["operation"]]], "golden_sentence": [["It occurs when the 13th day of the month in the Gregorian calendar falls on a Friday, which happens at least once every year but can occur up to three times in the same year\u2014for example, in 2015, Friday the 13th occurred in February, March, and November."], ["2017 through 2020 will all have two Friday the 13ths each, and the years 2021 and 2022 will both have just one occurrence each."]]}, {"qid": "326e1cad9b5dfd211246", "term": "Estonian language", "description": "Finno-Ugric language spoken in Estonia", "question": "Did Jesus know anyone who spoke Estonian?", "answer": false, "facts": ["Estonian is the language of Estonia, which is located in Northern Europe near Finland.", "Jesus is recorded to have lived and traveled almost exclusively within the borders of Galilee.", "Galilee is a territory within the borders of northern Israel.", "Israel is located on the Mediterranean sea in the Middle East.", "Israel is almost 2,000 miles from Estonia. "], "decomposition": ["Where is Estonian spoken?", "Where did Jesus live and travel?", "Where is #2 located?", "Is #1 close to #3?"], "evidence": [[[["Estonian language-1"]], [["Jesus-114", "Jesus-22"], "no_evidence"], [["Bethlehem-1", "Jerusalem-1", "Lower Galilee-1"], "no_evidence"], ["operation"]], [[["Estonia-1"]], [["Nazareth-14"]], [["Nazareth-1"]], [["Nazareth-54"], "operation"]], [[["Estonian language-1"]], [["Jesus-5"]], [["Nazareth-1"]], ["operation"]]], "golden_sentence": [["It is the official language of Estonia, spoken natively by about 1.1 million people; 922,000 people in Estonia and 160,000 outside Estonia."], ["Around AD 30, Jesus and his followers traveled from Galilee to Jerusalem to observe Passover.", ""], ["Bethlehem (/\u02c8b\u025b\u03b8l\u026ah\u025bm/; Arabic: \u0628\u064a\u062a \u0644\u062d\u0645\u200e Bayt La\u1e25m, \"House of Meat\"; Hebrew: \u05d1\u05b5\u05bc\u05d9\u05ea \u05dc\u05b6\u05d7\u05b6\u05dd Bet Le\u1e25em, Hebrew pronunciation:\u00a0[bet \u02c8le\u03c7em], \"House of Bread\"; Ancient Greek: \u0392\u03b7\u03b8\u03bb\u03b5\u03ad\u03bc Greek pronunciation:\u00a0[b\u025b\u02d0t\u02b0le.\u00e9m]; Latin: Bethleem; initially named after Canaanite fertility god Lehem) is a city located in the central West Bank, Palestine, about 10\u00a0km (6.2 miles) south of Jerusalem.", "Both Israel and the Palestinian Authority claim Jerusalem as their capital, as Israel maintains its primary governmental institutions there and the State of Palestine ultimately foresees it as its seat of power; however, neither claim is widely recognized internationally.", "The Lower Galilee consists of three different regions which differ in their geological structure:"]]}, {"qid": "b9f716bd86764f57564c", "term": "The Dark Knight (film)", "description": "2008 film directed by Christopher Nolan", "question": "Would The Dark Knight be appropriate for a preschool class?", "answer": false, "facts": ["Preschoolers are between 3 and 5 years old.", "The Dark Knight is rated PG-13.", "PG-13 is a rating that means parents are strongly cautioned that the content of a film may not be appropriate for children under 13."], "decomposition": ["What is the average age of preschoolers?", "What is the Dark Knight rated?", "What is the minimum age to watch something rated #2?", "Is age #1 above #3?"], "evidence": [[[["Preschool-4"]], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Preschool-4"]], ["no_evidence"], ["no_evidence"], ["operation"]], [[["Preschool-4"]], ["no_evidence"], [["PG-13 (disambiguation)-1"]], ["operation"]]], "golden_sentence": [["Preschool (US and UK) from 2 to 5 years old- held in a Nursery School; readiness has to do with whether the child is developmentally appropriate, potty training is a big factor, so a child can start as early as 2 years old."]]}, {"qid": "9d0ccf79c106ae7d6b4d", "term": "Jujutsu", "description": "Japanese martial art", "question": "Are all limbs required for jujutsu?", "answer": false, "facts": ["Jujutsu is a Japanese martial art that uses unarmed combat to subdue opponents.", "Nick Newell, a congenital amputee, got his black belt after two straight submission wins.", "Fighter Aaron LaPointe has succeeded in martial arts with a fully paralyzed arm."], "decomposition": ["What kind of sport is jujutsu?", "Which sport did Nick Newell get a black belt in?", "Did Nick Newell have all limbs intact or is #2 not a form of #1?"], "evidence": [[[["Jujutsu-1"]], [["Nick Newell-1", "Nick Newell-2"], "no_evidence"], ["operation"]], [[["Jujutsu-1"]], [["Nick Newell-1"], "no_evidence"], [["Nick Newell-2"], "no_evidence", "operation"]], [[["Jujutsu-1"]], ["no_evidence"], [["Nick Newell-2"]]]], "golden_sentence": [["Jujutsu (English: /d\u0292u\u02d0\u02c8d\u0292\u028atsu\u02d0/ joo-JOOT-soo; Japanese: \u67d4\u8853 j\u016bjutsu listen\u00a0(help\u00b7info)), also known as Japanese Jujutsu, JJJ, Jujitsu or Japanese Ju-Jitsu, is a family of Japanese martial arts and a method of close combat for defeating an opponent in which one uses either a short weapon or bare hands, and selected subset of techniques from certain styles of Japanese Jujutsu were used to develop modern martial arts and combat sports, such as Judo, Sambo, ARB, Brazilian jiu-jitsu and mixed martial arts."], ["Nick Newell (born March 17, 1986 in Milford, Connecticut) is an American mixed martial artist, currently signed to Bellator MMA, competing in the lightweight division.", "On joining his high school wrestling team, he thought about quitting after his very first session because it was 'the hardest thing I had done in my life, but his mother did not allow him to, and instead he worked hard from then on."]]}, {"qid": "e5443e5f3129b92acbd9", "term": "Underworld", "description": "The mythic Relm of the Dead, located far underground (aka, Hades; Underworld)", "question": "Can you get a ride on Amtrak to the Underworld?", "answer": false, "facts": ["Amtrak is a passenger railroad service operating in North America", "The Underworld is a fictional location from mythology and religion"], "decomposition": ["Which major regions are covered by the passenger railroad service 'Amtrak'?", "Is the Underworld part of #1?"], "evidence": [[[["Amtrak-1"]], [["Underworld-1"], "operation"]], [[["Amtrak-1"]], [["Underworld-1"]]], [[["Amtrak-1"]], [["Underworld-1"], "operation"]]], "golden_sentence": [["The National Railroad Passenger Corporation, doing business as Amtrak (reporting marks AMTK, AMTZ), is a passenger railroad service that provides medium and long-distance intercity service in the contiguous United States and to nine Canadian cities."], ["The underworld is the supernatural world of the dead in various religious traditions, located below the world of the living."]]}, {"qid": "687e090ecef13d3ce117", "term": "The Jackson 5", "description": "American pop music family group", "question": "Did Jackson 5 members exceed number in The Osmonds?", "answer": false, "facts": ["The Jackson 5 was composed of: Jackie, Tito, Jermaine, Marlon and Michael.", "The Osmonds consisted of:  Alan, Wayne, Merrill, Jay and Donny."], "decomposition": ["How many members did The Jackson 5 have?", "How many members did The Osmonds have?", "Is #1 greater than #2?"], "evidence": [[[["The Jackson 5-1"]], [["The Osmonds-1"]], ["operation"]], [[["The Jackson 5-1"]], [["The Osmonds-1"]], [["The Jackson 5-1", "The Osmonds-1"], "operation"]], [[["The Jackson 5-1"]], [["Quintet-1", "The Osmonds-1"]], ["operation"]]], "golden_sentence": [[""], ["Currently consisting of a duo of original members Merrill Osmond and Jay Osmond, the group's best known configurations have been as a quartet (billed as the Osmond Brothers) and as a quintet (as the Osmonds)."]]}, {"qid": "649408b7e3690c2d014a", "term": "Mickey Mouse", "description": "Disney cartoon character", "question": "Did Mickey Mouse appear in a cartoon with Bugs Bunny in 1930?", "answer": false, "facts": ["Bugs Bunny was created in the late 1930s.", "Mickey Mouse was created in 1928.", "Mickey Mouse appears in Disney cartoons.", "Bugs Bunny appears in Warner Bros. cartoons."], "decomposition": ["When was Bugs Bunny created?", "Is #1 before 1930?"], "evidence": [[[["Bugs Bunny-1"]], ["operation"]], [[["Bugs Bunny-1"]], ["operation"]], [[["Bugs Bunny-1"]], ["operation"]]], "golden_sentence": [["Bugs Bunny is an animated cartoon character, created in the late 1930s by Leon Schlesinger Productions (later Warner Bros. Cartoons) and voiced originally by Mel Blanc."]]}, {"qid": "4db767cbc68141bd5745", "term": "Isaac Newton", "description": "Influential British physicist and mathematician", "question": "Is Isaac Newton buried at the same church as the author of Great Expectations?", "answer": true, "facts": ["Isaac Newton is buried at Westminster Abbey.", "Charles Dickens's book Great Expectations was published  in 1861.", "Charles Dickens is buried at the Poets' Corner of Westminster Abbey.", "Westminster Abbey is a large church in the City of Westminster, London, England."], "decomposition": ["Who is the author of 'Great Expectations'?", "Where is #1 resting place?", "Where was Isaac Newton buried?", "Are #2 and #3 the same?"], "evidence": [[[["Great Expectations-1"]], [["Charles Dickens-53"]], [["Isaac Newton-46"]], ["operation"]], [[["Great Expectations-1"]], [["Charles Dickens-53"]], [["Isaac Newton-46"]], ["operation"]], [[["Great Expectations-2"]], [["Charles Dickens-53"]], [["Isaac Newton-46"]], ["operation"]]], "golden_sentence": [["Great Expectations is the thirteenth novel by Charles Dickens and his penultimate completed novel, which depicts the education of an orphan nicknamed Pip (the book is a bildungsroman, a coming-of-age story)."], ["He never regained consciousness, and the next day, he died at Gads Hill Place."], ["His body was buried in Westminster Abbey."]]}, {"qid": "938ac30f09fb0f01def3", "term": "Butler", "description": "male domestic worker in charge of all the male household staff", "question": "Do most middle class families have butlers?", "answer": false, "facts": ["Butlers make about $60,000 per year on average for their work.", "Middle class income is between $48,000 and $145,000."], "decomposition": ["What is a butler?", "How much does #1 make per year on average?", "How much is the average middle class income?", "Would #3 be enough to pay #2?"], "evidence": [[[["Butler-1"]], ["no_evidence"], [["Middle class-34"]], ["operation"]], [[["Butler-1"]], [["Butler-15"], "no_evidence"], [["Middle class-24"], "no_evidence"], ["operation"]], [[["Butler-1"]], [["Butler-20"], "no_evidence"], [["Middle class-37"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["A butler is usually male, and in charge of male servants, while a housekeeper is usually a woman, and in charge of female servants."], ["Middle class is defined here for the US as those adults with a net wealth of between US$50,000 and US$500,000 in mid 2015."]]}, {"qid": "c630eede2f80423c67ff", "term": "C", "description": "Letter of the Latin alphabet", "question": "Is letter C crucial to spelling the two most common words in English language?", "answer": false, "facts": ["The most common word in the English language is \"the\".", "The second most common word in the English language is \"be\"."], "decomposition": ["What is the most common word in the English language?", "What is the second most common word in the English language?", "What letters make up #1?", "What letters make up #2?", "Is the letter \"c\" found in both #3 and #4?"], "evidence": [[["no_evidence"], ["no_evidence"], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Most common words in English-5"], "no_evidence"], [["Most common words in English-5"], "no_evidence"], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Most common words in English-5"], "no_evidence"], [["Most common words in English-5"], "no_evidence"], ["operation"], ["operation"], ["operation"]]], "golden_sentence": []}, {"qid": "94e29532febe6a7edcac", "term": "Jury", "description": "sworn body of people convened to render a verdict officially submitted to them by a court, or to set a penalty or judgment", "question": "Is a felony jury enough people for a Bunco game?", "answer": true, "facts": ["Felonies and other serious crimes have a jury of 12 people.", "Bunco is a parlour game requiring 12 or more players."], "decomposition": ["How many people are on a felony criminal jury?", "How many players are needed for a game of Bunco?", "Is #2 the same or less than #1?"], "evidence": [[[["Jury-4"]], [["Bunco-1"]], ["operation"]], [[["Jury-2"]], [["Bunco-1"]], ["operation"]], [[["Jury-4"]], [["Bunco-1"]], ["operation"]]], "golden_sentence": [["The size of the jury varies; in criminal cases involving serious felonies there are usually 12 jurors."], ["Bunco (also Bunko or Bonko) is a parlour game generally played with twelve or more players, divided into groups of four, trying to score points while taking turns rolling three dice."]]}, {"qid": "910a05ba2dcbd3b22b68", "term": "Odyssey", "description": "Epic poem attributed to Homer", "question": "In baseball, is a \"Homer\" named after the poet Homer who wrote the Odyssey?", "answer": false, "facts": ["Homer is a famous poet who wrote the epic poem the Odyssey.", "The Odyssey is about a character Odysseus on an epic journey home after the fall of Troy.", "In baseball a trip around all the bases is called a Home Run.", "\"Homer\" is a shortened name for Home Run."], "decomposition": ["What does the baseball term homer mean?", "Is #1 the same thing as the poet Homer?"], "evidence": [[[["Home run-1", "Home run-15"]], [["Homer-1"], "operation"]], [[["Home run-1", "Home run-35"]], [["Homer-1"], "operation"]], [[["Home run-1", "Home run-14", "Home run-2"]], [["Homer-1"], "operation"]]], "golden_sentence": [["", "The term \"four-run homer\" is seldom used; instead, it is nearly always called a \"grand slam\"."], ["Homer (/\u02c8ho\u028am\u0259r/; Ancient Greek: \u1f4d\u03bc\u03b7\u03c1\u03bf\u03c2 Greek pronunciation:\u00a0[h\u00f3m\u025b\u02d0ros], H\u00f3m\u0113ros) is the semi-legendary author of the Iliad and the Odyssey, two epic poems that are the central works of ancient Greek literature."]]}, {"qid": "5941b118c03bcf02163e", "term": "Tax collector", "description": "person who collects taxes", "question": "Does Kenny G hold the qualifications to be a tax collector?", "answer": true, "facts": ["The qualifications to be a tax collector in the US inlude a bachelor's degree in accounting.", "Kenny G studied accounting at the University of Washington and graduated magna cum laude."], "decomposition": ["What are the qualifications to be a tax collector?", "Does Kenny G possess #1?"], "evidence": [[["no_evidence"], ["no_evidence"]], [[["Certified Public Accountant-16"], "no_evidence"], [["Kenny G-5"], "operation"]], [[["Audit-1", "Tax collector-1"], "no_evidence"], [["Kenny G-5"], "operation"]]], "golden_sentence": []}, {"qid": "4a1e8300c5dc71fc1ba8", "term": "Guitarist", "description": "person who plays the guitar", "question": "Do guitarists need both hands to play?", "answer": true, "facts": ["The left hand typically positions the chords on the fretboard.", "The right hand plays the strings, either strumming a whole chord or finger-picking individual strings.", "The position of the left hand on the fretboard changes the tones of the strings played by the right hand, so both hands are necessary."], "decomposition": ["Which musical instrument do guitarists play?", "How many hands are typically used to play #1?", "Is #2 equal to two?"], "evidence": [[[["Guitarist-1"]], [["Guitarist-2", "Guitarist-3"]], ["no_evidence", "operation"]], [[["Guitarist-1"]], [["Guitar-1"]], ["operation"]], [[["Guitarist-1"]], [["Guitar-1"]], ["operation"]]], "golden_sentence": [["A guitarist (or a guitar player) is a person who plays the guitar."], ["", ""]]}, {"qid": "34c95fb710fb156c6aee", "term": "Karachi", "description": "Megacity in Sindh, Pakistan", "question": "Karachi was a part of Alexander the Great's success?", "answer": true, "facts": ["Karachi is a city in modern day Pakistan.", "Krokola was an ancient port located in what is now Karachi.", "Alexander the Great stationed his fleet in Krokola on his way to Babylon.", "Alexander the Great defeated Darius and conquered Babylon before expanding his empire."], "decomposition": ["What is Karachi?", "What was the name of the ancient port that was once located in #1?", "Before expanding his empire, what city did Alexander the Great conquer?", "Did Alexander the Great station his fleet at #2 prior to #3?"], "evidence": [[[["Karachi-1"]], [["Karachi-8"]], [["Achaemenid Assyria-41"]], ["operation"]], [[["Karachi-1"]], [["Karachi-8"]], ["no_evidence"], ["operation"]], [[["Karachi-1"]], [["Port of Karachi-2"]], [["Alexander the Great-51"], "no_evidence"], [["Karachi-8"], "no_evidence", "operation"]]], "golden_sentence": [["Ranked as a beta-global city, the city is Pakistan's premier industrial and financial centre, with an estimated GDP of $114\u00a0billion (PPP) as of 2014.. Karachi is Pakistan's most cosmopolitan city, its most linguistically, ethnically, and religiously diverse city, as well as one of Pakistan's most secular and socially liberal cities."], ["The expansive Karachi region is believed to have been known to the Ancient Greeks, and may have been the site of Barbarikon, an ancient seaport which was located at the nearby mouth of the Indus River."], ["The Babylonian Chronicles now show the vitality of Greek culture in ancient cities like Babylon."]]}, {"qid": "7139a0b08191886f65b2", "term": "Maize", "description": "Cereal grain", "question": "Would a bodybuilder choose maize over chicken breast for dinner?", "answer": false, "facts": ["Bodybuilders aim to eat high amounts of protein in order to stimulate muscle growth.", "Maize contains 9.4 grams of protein per 100 grams.", "Baked chicken breast contains 31 grams of protein per 100 grams."], "decomposition": ["What nutrient is critical for bodybuilding?", "How much #1 is in maize?", "How much #1 is in chicken breast?", "Is #2 greater than #3?"], "evidence": [[[["Bodybuilding-39"]], [["Maize-76"]], ["no_evidence"], ["operation"]], [[["Bodybuilding-41"]], [["Maize-77"], "no_evidence"], [["Chicken as food-11"]], ["no_evidence", "operation"]], [[["Bodybuilding-41"]], [["Maize-76"]], [["Chicken as food-11"]], ["operation"]]], "golden_sentence": [["In preparation of a contest, a sub-maintenance level of food energy is combined with cardiovascular exercise to lose body fat."], ["Raw, yellow, sweet maize kernels are composed of 76% water, 19% carbohydrates, 3% protein, and 1% fat (table)."]]}, {"qid": "22aaaff79d20ebbecda1", "term": "King Kong (2005 film)", "description": "2005 film directed by Peter Jackson", "question": "Was King Kong (2005 film) the lead actress's biggest box office role?", "answer": true, "facts": ["Naomi Watts starred in King Kong (2005 film).", "Naomi Watts has starred in several movies such as Divergent and the RIng.", "Divergent: Insurgent made 295 million at the box office worldwide .", "The Ring made 248 million at the box office worldwide .", "King Kong (2005 film) made 550 million at the box office worldwide."], "decomposition": ["Who was the lead actress in King Kong (2005)?", "What other films has #1 starred in?", "How much did King Kong (2005) make at the box office?", "How much did each of #2 make at the box office?", "Is #3 greater than any of #4?"], "evidence": [[[["King Kong (2005 film)-1"]], [["Naomi Watts-1"]], [["King Kong (2005 film)-2"]], [["Flirting (film)-9", "For Love Alone-8"]], ["operation"]], [[["King Kong (2005 film)-1"]], [["Naomi Watts filmography-2"], "no_evidence"], [["King Kong (2005 film)-2"]], [["The Divergent Series: Insurgent-3"], "no_evidence"], ["no_evidence", "operation"]], [[["King Kong (2005 film)-1"]], [["Naomi Watts filmography-2"], "no_evidence"], [["King Kong (2005 film)-28"]], [["Naomi Watts-19"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], ["After moving to the United States, Watts struggled as an actress for years, but managed to obtain parts in the films Tank Girl (1995), Children of the Corn IV: The Gathering (1996), and Dangerous Beauty (1998), and the television series Sleepwalkers (1997\u20131998)."], ["It was released on December 14, 2005 in Germany and the United States, and made an opening of $50.1 million."], ["Flirting grossed $1,655,044 at the box office in Australia and $2,415,396 in the USA.", ""]]}, {"qid": "70cdf8c41a420bbee8dd", "term": "Do it yourself", "description": "building, modifying, or repairing something without the aid of experts or professionals", "question": "Are some Do It Yourself projects potentially lethal?", "answer": true, "facts": ["Deep fried turkey can be made at home, but a small mistake can cause the entire setup to erupt into a grease fire.", "Home roofing repair can be a DIY project but without proper safety gear a fall can be deadly."], "decomposition": ["How could DIY deep fried turkey go wrong in case of a mistake?", "What accidents could DIY home roofing cause if something went wrong?", "Are #1 and #2 deadly?"], "evidence": [[[["Turkey fryer-5"]], [["Falling (accident)-1", "Roofer-1"], "no_evidence"], ["operation"]], [[["Turkey fryer-5"]], [["Falling (accident)-17"], "no_evidence"], [["Burn-4", "Falling (accident)-17"], "operation"]], [[["Turkey fryer-5"]], [["Home repair-8"]], ["no_evidence"]]], "golden_sentence": [["Deep-frying a turkey uses oil over an open flame, thus it presents some hazards."], ["", ""]]}, {"qid": "44b0d7e96eb459b9ab36", "term": "Andes", "description": "Mountain range in South America", "question": "Has mummification in the Andes been prevented by rainfall?", "answer": false, "facts": ["The Andes includes high, dry zones without precipitation.", "Dry climates do not impede mummification.", "Many mummies have been found in the Andes."], "decomposition": ["What type of climate is present in the Andes?", "Does #1 cause rainfall?"], "evidence": [[[["Andes-18"]], ["operation"]], [[["Andes-18"]], ["no_evidence", "operation"]], [[["Andes-19"]], [["Desert-1"], "operation"]]], "golden_sentence": [[""]]}, {"qid": "d82e5684bf102a1e182c", "term": "Lettuce", "description": "Species of annual plant of the daisy family, most often grown as a leaf vegetable", "question": "Can lettuce result in spontaneous abortion?", "answer": true, "facts": ["Food-borne pathogens that can survive on lettuce include Listeria monocytogenes, ", "Listeria monocytogenes is the causative agent of listeriosis.", "The manifestations of listeriosis include intrauterine or cervical infections in pregnant women, which may result in spontaneous abortion."], "decomposition": ["What diseases can be caused by contaminated lettuce?", "Can any of #1 cause intrauterine or cervical infections?"], "evidence": [[[["Lettuce-4"]], [["Cervix-30", "Escherichia coli-1", "Salmonella-1"], "no_evidence", "operation"]], [[["Lettuce-4"]], [["Salmonella-22"], "no_evidence", "operation"]], [[["Lettuce-4"]], ["no_evidence"]]], "golden_sentence": [["Contaminated lettuce is often a source of bacterial, viral, and parasitic outbreaks in humans, including E. coli and Salmonella."], ["This inflammation may be of the endocervix or ectocervix.", "", ""]]}, {"qid": "67d0f066e50df02f006b", "term": "1800", "description": "Year", "question": "Is number of different US President's in 1800s a lucky number in Hong Kong?", "answer": false, "facts": ["There were 24 different US President's in the 1800s.", "4 is an unlucky number in Chinese numerology.", "Where East Asian and Western cultures blend, such as in Hong Kong, it is possible in some buildings that the thirteenth floor along with all the floors with 4s to be omitted. "], "decomposition": ["How many U.S. Presidents served during the 1800's?", "What number is unlucky in Chinese numerology?", "Does #1 end with a number other than #2?"], "evidence": [[[["John Adams-1", "William McKinley-1"]], [["Chinese numerology-7"]], ["operation"]], [[["John Adams-1", "William McKinley-1"]], [["Chinese numerology-1"]], ["operation"]], [[["USS President (1800)-31"], "no_evidence"], [["Chinese numerology-7"], "operation"], ["no_evidence"]]], "golden_sentence": [["John Adams (October 30, 1735 \u2013 July 4, 1826) was an American statesman, attorney, diplomat, writer, and Founding Father who served as the second president of the United States, from 1797 to 1801.", "William McKinley (born William McKinley Jr., January 29, 1843 \u2013 September 14, 1901) was the 25th president of the United States from 1897, until his assassination in 1901."], ["The number 4 (\u56db, pinyin: s\u00ec; Cantonese Yale: sei) is considered an unlucky number in Chinese because it is nearly homophonous to the word \"death\" (\u6b7b pinyin: s\u01d0; Cantonese Yale: s\u00e9i)."]]}, {"qid": "57cae08abb315b023dcf", "term": "Slot machine", "description": "Casino gambling machine", "question": "Do any video games about the end of civilization have slot machines?", "answer": true, "facts": ["Fallout New Vegas is a game that takes place after the apocalypse has ocurred. ", "In Fallout New Vegas, players can go to casinos and play on slot machines."], "decomposition": ["What video games take place in a post-apocalyptic world?", "Which video games have slot machines?", "Is at least one game in #1 found in #2?"], "evidence": [[[["Fallout (series)-2", "Fallout: New Vegas-1"]], [["Fallout: New Vegas-4"]], ["operation"]], [[["Fallout: New Vegas-1", "The Last of Us-1"], "no_evidence"], [["Fallout: New Vegas-4"], "no_evidence"], ["operation"]], [[["Fallout (series)-1"], "no_evidence"], [["Fallout: New Vegas-2", "Fallout: New Vegas-4"], "no_evidence"], ["operation"]]], "golden_sentence": [["", "A spin-off of the Fallout series, the game is set in a post-apocalyptic open world environment that encompasses a region consisting of Arizona, California, and Nevada."], [""]]}, {"qid": "6ffed6324812d6103c70", "term": "Eminem", "description": "American rapper and actor", "question": "Would Terence Tao outperform Eminem in a math competition?", "answer": true, "facts": ["Eminem disliked math and social studies, and dropped out of high school at age 17.", "Terence Tao was a child prodigy in mathematics, attending university-level mathematics courses at the age of 9.", "From 1992 to 1996, Tao was a graduate student at Princeton University under the direction of Elias Stein, receiving his Ph.D. at the age of 21."], "decomposition": ["What achievements did Terence Tao make as a student of mathematics?", "What was Eminem's disposition to mathematics as a high-schooler?", "Does #1 indicate a higher mathematical ability than #2?"], "evidence": [[[["Terence Tao-2"]], [["Eminem-8"]], [["Fields Medal-2"]]], [[["Terence Tao-1"]], [["Eminem-8"]], ["operation"]], [[["Terence Tao-11", "Terence Tao-8"]], [["Eminem-8"]], ["operation"]]], "golden_sentence": [[""], ["Although he was interested in English, he never explored literature (preferring comic books) and disliked math and social studies."], [""]]}, {"qid": "8497718847564957358b", "term": "Tonsure", "description": "hairstyle related to religious devotion", "question": "Does a person using tonsure have hair at the top of their scalp?", "answer": false, "facts": ["Tonsure involves shaving some or all of the hair from the head.", "Tonsure styles include a large bald spot at the top of the scalp."], "decomposition": ["What parts of the head are shaved for the tonsure hairstyle?", "Is the top of the scalp excluded from #1?"], "evidence": [[[["Tonsure-1"]], ["operation"]], [[["Tonsure-1"]], ["operation"]], [[["Tonsure-1"]], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "2b31b8da8da683e0f2ae", "term": "Nordic countries", "description": "Geographical and cultural region in Northern Europe and the North Atlantic", "question": "Does someone from Japan need a passport to go to a Nordic country?", "answer": true, "facts": ["The Nordic Passport Union allows citizens of the Nordic countries: Denmark (Faroe Islands included since 1 January 1966, Greenland not included), Sweden, Norway (Svalbard, Bouvet Island and Queen Maud Land not included), Finland and Iceland (since 24 September 1965) to cross approved border districts without carrying and having their passport checked.", "Japan is not one of the approved countries."], "decomposition": ["What countries recognize the Nordic Passport Union?", "Is Japan included in #1?"], "evidence": [[[["Nordic Passport Union-1"]], ["operation"]], [[["Nordic Passport Union-1"]], ["operation"]], [[["Nordic Passport Union-1"]], ["operation"]]], "golden_sentence": [["The Nordic Passport Union allows citizens of the Nordic countries\u00a0\u2013 Iceland, Denmark, Norway, Sweden, and Finland\u00a0\u2013 to travel and reside in another Nordic country (and Svalbard) without any travel documentation (e.g."]]}, {"qid": "09eaaf2d0f2a18d4b785", "term": "Tibia", "description": "larger of the two bones of the leg below the knee for vertebrates", "question": "Would you find a tibia beside parsley on a holiday plate?", "answer": true, "facts": ["The tibia of a goat is eaten during Passover, a Jewish holiday", "Parsley is served on a Passover seder plate beside the goat shank "], "decomposition": ["How is Passover celebrated?", "What part of a goat is eaten during #1?", "Is parsley typically served on the same plate as #2?"], "evidence": [[[["Passover-13", "Passover-7"]], [["Tibia-1", "Zeroa-1"]], [["Passover Seder plate-4"]]], [[["Passover-22"]], [["Passover-23"]], [["Parsley-15"], "no_evidence"]], [[["Passover-13"]], [["Passover-87"], "no_evidence"], [["Parsley-16"], "operation"]]], "golden_sentence": [["", ""], ["The tibia is connected to the fibula by the interosseous membrane of the leg, forming a type of fibrous joint called a syndesmosis with very little movement.", "(Hebrew: \u05d6\u05e8\u05d5\u05b9\u05e2) is a lamb shank bone or roast chicken wing or neck used on Passover and placed on the Seder plate."], [""]]}, {"qid": "047076176152d4a0f35d", "term": "Hanuman", "description": "The divine monkey companion of Rama in Hindu mythology", "question": "Did Hanuman ever experience an orgasm?", "answer": false, "facts": ["Hanuman was a life long celibate.", "Celibates refrain from all sexual activity.", "Orgasms are only experienced during sexual activity."], "decomposition": ["What does one have to do to experience an orgasm?", "Which of Hanuman's characteristics concerned his #1 aspect?", "Do people who identify as #2 engage in #1?"], "evidence": [[[["Sexual intercourse-1"]], [["Hanuman-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Orgasm-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Orgasm-1"]], [["Hanuman-1"]], ["operation"]]], "golden_sentence": [[""], ["He is viewed as the ideal combination of \"strength, heroic initiative and assertive excellence\" and \"loving, emotional devotion to his personal god Rama\", as Shakti and Bhakti."]]}, {"qid": "a05639b4a48b32464e15", "term": "U2", "description": "Four-member Irish rock band, from Dublin", "question": "Could someone listen to the entire U2 debut studio album during an episode of Peppa Pig?", "answer": false, "facts": ["U2's debut studio album was titled Boy.", "The album, Boy, is 42 minutes and 52 seconds long.", "An episode of Peppa Pig has a running time of approximately 5 minutes."], "decomposition": ["What is the title of U2's debut studio album?", "How long is #1?", "How long is Peppa Pig episodes?", "Is #3 longer than #2?"], "evidence": [[[["Boy (album)-1"]], ["no_evidence"], [["Peppa Pig-4"]], ["no_evidence", "operation"]], [[["Boy (album)-1"]], ["no_evidence"], [["Peppa Pig-6"]], ["operation"]], [[["Disco Boy-5"], "no_evidence"], ["operation"], [["Peppa Pig-8"], "no_evidence"], ["operation"]]], "golden_sentence": [["Boy is the debut studio album by Irish rock band U2."], ["Each episode is approximately five minutes long."]]}, {"qid": "57640869b6fa19a5cdc9", "term": "Soul music", "description": "Genre of music", "question": "Would Brian Warner be a good singer for a soul music band?", "answer": false, "facts": ["Soul music is a music genre that originated in the United States African American community in the 1950s.", "Soul music combines elements of African-American gospel music, rhythm and blues and jazz.", "Brian Warner is the lead singer of the band Marilyn Manson.", "The band Marilyn Manson plays industrial heavy metal music."], "decomposition": ["What kind of music does Brian Warner play?", "Is soul music listed in #1?"], "evidence": [[[["Marilyn Manson-2"]], ["operation"]], [[["Marilyn Manson (band)-46"]], ["operation"]], [[["Marilyn Manson-1", "Marilyn Manson-21", "Marilyn Manson-8"]], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "80938e730dc5d1aa7ecc", "term": "Comma", "description": "Punctuation mark", "question": "Would three commas be sufficient for displaying US 2018 GDP?", "answer": false, "facts": ["The 2018 GDP of US was 20.54 trillion dollars.", "There are three commas in a billion.", "There are four commas in a trillion."], "decomposition": ["How much was US GDP in 2018?", "When written in figures, how many commas would #1 contain?", "Is #2 less than or equal to three?"], "evidence": [[[["United States-142"], "no_evidence"], [["Trillion-2"], "operation"], ["operation"]], [[["United States-142"]], [["Trillion-2"]], ["operation"]], [[["Economy of the United States-21"]], [["Trillion-2"]], [["Trillion-2"], "operation"]]], "golden_sentence": [[""], ["1,000,000,000,000,000,000, i.e."]]}, {"qid": "cc703e3ede09f42a9d39", "term": "Kanji", "description": "adopted logographic Chinese characters used in the modern Japanese writing system", "question": "Can a person who knows only English read Kanji?", "answer": false, "facts": ["Kanji is a Japanese language.", "People who only know English can't read Japanese."], "decomposition": ["Is knowledge of Kanji included in English language?"], "evidence": [[[["Kanji-1"]]], [[["Kanji-1"]]], [[["Kanji-1"]]]], "golden_sentence": [[""]]}, {"qid": "64407ff48ab5701a36b2", "term": "Walt Disney", "description": "American entrepreneur, animator, voice actor and film producer", "question": "Was Walt Disney able to email his illustrations to people living far away?", "answer": false, "facts": ["Walt Disney died in 1966", "Modern email came into existence in 1971"], "decomposition": ["When was email first used?", "When did Walt Disney die?", "Is #1 before #2?"], "evidence": [[[["Email-9"]], [["Walt Disney-1"]], ["operation"]], [[["Email-1"]], [["Walt Disney-1"]], ["operation"]], [[["Email-1"]], [["Walt Disney-1"]], ["operation"]]], "golden_sentence": [["Computer-based mail and messaging became possible with the advent of time-sharing computers in the early 1960s, and informal methods of using shared files to pass messages were soon expanded into the first mail systems."], ["Walter Elias Disney (/\u02c8d\u026azni/; December 5, 1901\u00a0\u2013\u00a0December 15, 1966) was an American entrepreneur, animator, writer, voice actor and film producer."]]}, {"qid": "d46496aa4b4ecbeac5a3", "term": "AirTrain JFK", "description": "People mover system at JFK Airport in New York City", "question": "Could Katharine Hepburn have ridden the AirTrain JFK?", "answer": false, "facts": ["The AirTrain JFK was built in December 17, 2003.", "Katharine Hepburn died on June 29, 2003."], "decomposition": ["When was the AirTrain JFK built?", "When did Katharine Hepburn die?", "Is #1 before #2?"], "evidence": [[[["AirTrain JFK-20"]], [["Katharine Hepburn-1"]], ["operation"]], [[["AirTrain JFK-2"]], [["Katharine Hepburn-1"]], ["operation"]], [[["AirTrain JFK-15"]], [["Katharine Hepburn-63"]], ["operation"]]], "golden_sentence": [[""], ["Katharine Houghton Hepburn (May 12, 1907\u00a0\u2013 June 29, 2003) was an American actress who was a leading lady in Hollywood for more than 60 years."]]}, {"qid": "756a40598bfc7d0650a4", "term": "Great Recession", "description": "Early 21st-century global economic decline", "question": "Was Great Recession the period of severest unemployment?", "answer": false, "facts": ["The Great Recession had an unemployment peak of 10%.", "The Great Depression saw global GDP decline by almost 30% and unemployment approach 25%.", "US unemployment numbers approached 15% in May 2020 due to the Coronavirus."], "decomposition": ["What was the unemployment rate during the Great Recession?", "What was the US unemployment rate in May 2020?", "Is #2 less than #1?"], "evidence": [[[["Great Recession-43"]], [["Unemployment in the United States-21"]], ["operation"]], [[["Effects of the Great Recession-11"]], [["Unemployment in the United States-21"]], [["Effects of the Great Recession-11", "Unemployment in the United States-21"], "operation"]], [[["Effects of the Great Recession-11"]], [["Unemployment-153"], "no_evidence"], ["operation"]]], "golden_sentence": [["The unemployment rate peaked at 10.0% in October 2009 and did not return to its pre-recession level of 4.7% until May 2016."], ["On May 8, 2020, the Bureau of Labor Statistics reported that 20.5 million nonfarm jobs were lost and the unemployment rate rose to 14.7 percent in April."]]}, {"qid": "a66d463ac4de80be2d0a", "term": "Groundhog Day", "description": "Traditional method of weather prediction", "question": "At Christmastime, do some films remind us that groundhog day is approaching?", "answer": true, "facts": ["Jack Frost is a 1979 stop motion Christmas film.", "In Jack Frost, the groundhog is a character and gets his own song reminding people of his own holiday."], "decomposition": ["What is the name of a stop motion Christmas film that was released in 1979?", "In #1, what does the groundhog get?", "Does #2 remind people of Groundhog Day?"], "evidence": [[[["Jack Frost (TV special)-1"]], [["Jack Frost (TV special)-6"]], ["operation"]], [[["Jack Frost (TV special)-1"]], [["Jack Frost (TV special)-2", "Jack Frost (TV special)-6"]], ["operation"]], [[["Jack Frost (TV special)-1"]], [["Jack Frost (TV special)-2"]], ["operation"]]], "golden_sentence": [["Jack Frost is a 1979 Christmas stop motion animated television special produced by Rankin/Bass Productions."], [""]]}, {"qid": "e4ca0c7c07f117c61b41", "term": "Pearl hunting", "description": "Collecting pearls from wild mollusks", "question": "Would Michael Phelps be good at pearl hunting?", "answer": true, "facts": ["Pearl hunters swim underwater to collect pearls from oysters.", "Michael Phelps is the most decorated Olympic swimmer of all time."], "decomposition": ["What do pearl hunters do?", "What is Michael Phelps famous for?", "Does #2 help with accomplishing #1?"], "evidence": [[[["Pearl hunting-1"]], [["Michael Phelps-1"]], [["Pearl hunting-2"]]], [[["Pearl hunting-1"]], [["Michael Phelps-1"]], ["operation"]], [[["Pearl hunting-2"]], [["Michael Phelps-1"]], ["operation"]]], "golden_sentence": [[""], ["Michael Fred Phelps II (born June 30, 1985) is an American former competitive swimmer and the most successful and most decorated Olympian of all time, with a total of 28 medals."], [""]]}, {"qid": "4b4e74d3a11ce41c7c0a", "term": "Hornet", "description": "Genus of eusocial wasp", "question": "Do hornets provide meaningful data for oceanographers?", "answer": false, "facts": ["Hornets live on land", "Oceanographers study oceans"], "decomposition": ["Where do hornets live?", "What do oceanographers study?", "Is #1 the same as #2?"], "evidence": [[[["Hornet-2", "Hornet-8"]], [["Oceanography-1"]], ["operation"]], [[["Hornet-2"]], [["Oceanography-1"]], ["operation"]], [[["Hornet-2"]], [["Oceanography-1"]], ["operation"]]], "golden_sentence": [["", "Hornets are found mainly in the Northern Hemisphere."], ["An oceanographer is a person who studies many matters concerned with oceans including marine geology, physics, chemistry and biology ."]]}, {"qid": "a3f7664b67bc41f666c0", "term": "USB", "description": "Industry standard", "question": "Is 500GB USB device enough to save 10 hours of Netflix shows a day?", "answer": false, "facts": ["5 hours of Netflix programming uses up approximately 1 TB of data.", "1 TB is equal to 1000 GB of data."], "decomposition": ["How many terabytes of data does 5 hours of Netflix use up?", "What is #1 multiplied by 2?", "How many GB are in a TB?", "What is #3 multiplied by #2?", "Is #4 less than 500?"], "evidence": [[["no_evidence"], ["no_evidence", "operation"], [["Terabyte-2"]], ["no_evidence", "operation"], ["no_evidence", "operation"]], [[["TiVo-46"], "no_evidence"], ["operation"], [["Terabyte-2"]], ["operation"], ["operation"]], [[["Streaming media-39"], "no_evidence", "operation"], ["no_evidence", "operation"], ["no_evidence", "operation"], ["no_evidence", "operation"], ["no_evidence", "operation"]]], "golden_sentence": [["1 TB = 1000000000000bytes = 1012bytes = 1000gigabytes."]]}, {"qid": "aa04b8bb6bd0e53691fa", "term": "Maritime pilot", "description": "mariner who manoeuvres ships through dangerous or congested waters", "question": "Can COVID-19 spread to maritime pilots?", "answer": true, "facts": ["Maritime pilots are human beings.", "COVID-19 can spread among human population. "], "decomposition": ["Which organisms are susceptible to COVID-19?", "Are maritime pilots one of #1?"], "evidence": [[[["Coronavirus disease 2019-1"], "no_evidence"], ["operation"]], [[["Coronavirus disease 2019-1"]], [["Maritime pilot-1"], "operation"]], [[["Coronavirus disease 2019-83"], "no_evidence"], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "34e37ff7d701fb56bce7", "term": "B", "description": "letter in the Latin alphabet", "question": "Could B be mistaken for an Arabic numeral?", "answer": true, "facts": ["The letter 'B' resembles a figure-8 with a flattened left side.", "The Arabic numeral '8' is drawn as one large circle and a smaller circle immediately on top, intersecting each other. ", "A 'figure-8' is a shape consisting of two intersecting circles, the larger on the bottom."], "decomposition": ["Which figure is the letter B similar in appearance to?", "Is #1 an Arabic numeral?"], "evidence": [[[["8-1", "B-1"]], [["Arabic numerals-1"], "operation"]], [[["B-1", "Beta-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Bet (letter)-3"], "no_evidence"], ["no_evidence"]]], "golden_sentence": [["", "It represents the voiced bilabial stop in many languages, including English."], ["Arabic numerals are the ten digits: 0,\u00a01,\u00a02,\u00a03,\u00a04,\u00a05,\u00a06,\u00a07,\u00a08 and\u00a09."]]}, {"qid": "a94b41cb63fd5a2fa51c", "term": "Grizzly bear", "description": "Subspecies of mammal", "question": "Did occupants of Vellore Fort need to defend themselves from Grizzly Bears?", "answer": false, "facts": ["The Vellore Fort was a 16th century stronghold in India.", "Grizzly Bears are native to the North American continent."], "decomposition": ["Where is the Vellore Fort located?", "Where can grizzly bears be found?", "Is #1 included in #2?"], "evidence": [[[["Vellore Fort-1"]], [["Brown bear-21"], "no_evidence"], ["operation"]], [[["Vellore Fort-1"]], [["Grizzly bear-1"]], ["operation"]], [[["Vellore Fort-1"]], [["Grizzly bear-1"]], [["India-1", "North America-1"], "operation"]]], "golden_sentence": [["Vellore Fort is a large 16th-century fort situated in heart of the Vellore city, in the state of Tamil Nadu, India built by Vijayanagara kings."], ["They can also be found on the Japanese island of Hokkaid\u014d, which holds the largest number of non-Russian brown bears in eastern Asia with about 2,000\u20133,000 animals."]]}, {"qid": "d82f9289acc4ff849e0d", "term": "Gladiator", "description": "combatant who entertained audiences in the Roman Republic and Roman Empire", "question": "Did a gladiator kill his opponent with a shotgun?", "answer": false, "facts": ["The gladiator games lasted for nearly a thousand years, reaching their peak between the 1st century BC and the 2nd century AD.", "The gladiator games finally declined during the early 5th century.", "The shotgun was not invented until approximately the 18th century."], "decomposition": ["When did the gladiator games take place?", "When was the shotgun invented?", "Is #2 within the range of #1?"], "evidence": [[[["Gladiator-4"]], [["Shotgun-38"]], [["Gladiator-4", "Shotgun-38"], "operation"]], [[["Gladiator-4"]], [["Shotgun-31"]], ["operation"]], [[["Gladiator-4"]], [["Shotgun-32"]], ["operation"]]], "golden_sentence": [["The gladiator games lasted for nearly a thousand years, reaching their peak between the 1st century BC and the 2nd century AD."], ["Working for Barber & LeFever in Syracuse, N.Y. he introduced his first hammerless shotgun in 1878."], ["", ""]]}, {"qid": "792fb4889f570f0b495e", "term": "Immanuel Kant", "description": "Prussian philosopher", "question": "Would Immanuel Kant be disgusted by the Black Lives Matter movement?", "answer": true, "facts": ["Immanuel Kant believed that Africans occupied the second lowest position on his racial hierarchy, below Whites and Asians.", "The Black Lives Matter movement advocates for racial equality and anti-racism."], "decomposition": ["What were Immanuel Kant's views on race?", "What are the main beliefs of the Black Lives Matter movement?", "Is #1 significantly different from #2?"], "evidence": [[["no_evidence"], [["Black Lives Matter-15"]], ["operation"]], [[["Scientific racism-41"]], [["Black Lives Matter-14", "Black Lives Matter-9"]], ["operation"]], [[["Scientific racism-1", "Scientific racism-41"]], [["Black Lives Matter-15"]], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "7131b8bf60b4ecda252c", "term": "Pregnancy", "description": "time when children develop inside the mother's body before birth", "question": "Will 2020 elephant pregnancy last past next year with 4 solar eclipses?", "answer": false, "facts": ["The gestation period of elephants are around 95 weeks.", "The year 2029 is the next year with 4 solar eclipses."], "decomposition": ["What is the duration of an elephant's gestation period?", "How many years from 2020 will there be a year with four solar eclipses?", "Is #1 greater than #2?"], "evidence": [[[["Elephant-48"]], [["Solar eclipse-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Elephant-48"]], ["no_evidence"], ["operation"]], [[["Birth-5"], "no_evidence"], [["Solar eclipse-20"]], ["operation"]]], "golden_sentence": [["Gestation in elephants typically lasts around two years with interbirth intervals usually lasting four to five years."], [""]]}, {"qid": "ae30f4c736c9fa7343e6", "term": "Fear", "description": "Basic emotion induced by a perceived threat", "question": "Is an espresso likely to assuage fear?", "answer": false, "facts": ["Fear raises heart rate", "Caffeine raises heart rate", "Coffee may also increase symptoms such as jitteriness and nausea "], "decomposition": ["What does fear typically do to a person's heart rate?", "What does espresso typically do to a person's heart rate?", "Is #1 the opposite of #2?"], "evidence": [[[["Fear-4"]], [["Caffeine-32", "Espresso-2"]], ["operation"]], [[["Heart rate-15"]], [["Caffeine-3"]], ["operation"]], [[["Fear-20"]], [["Caffeine-44", "Espresso-2"]], ["operation"]]], "golden_sentence": [["An innate response for coping with danger, it works by accelerating the breathing rate (hyperventilation), heart rate, vasoconstriction of the peripheral blood vessels leading to blushing and sanskadania of the central vessels (pooling), increasing muscle tension including the muscles attached to each hair follicle to contract and causing \"goosebumps\", or more clinically, piloerection (making a cold person warmer or a frightened animal look more impressive), sweating, increased blood glucose (hyperglycemia), increased serum calcium, increase in white blood cells called neutrophilic leukocytes, alertness leading to sleep disturbance and \"butterflies in the stomach\" (dyspepsia)."], ["According to the DSM-5, caffeine intoxication may be diagnosed if five (or more) of the following symptoms develop after recent consumption of caffeine: restlessness, nervousness, excitement, insomnia, flushed face, diuresis (increased production of urine), gastrointestinal disturbance, muscle twitching, rambling flow of thought and speech, tachycardia (increased heart rate) or cardiac arrythmia, periods of inexhaustibility, and psychomotor agitation.", ""]]}, {"qid": "391822031ce8b617ece7", "term": "Marco Polo", "description": "Italian explorer and merchant noted for travel to central and eastern Asia", "question": "Do you often hear Marco Polo's name shouted near water?", "answer": true, "facts": ["\"Marco Polo\" is a popular game among children and adults played while swimming.", "To play \"Marco Polo\", one person shouts \"Marco\" and the other shouts \"Polo\" while avoiding being tagged."], "decomposition": ["What is the game Marco Polo?", "When is #1 typically played?", "Does #2 occur near or in water?"], "evidence": [[[["Marco Polo (game)-1"]], [["Marco Polo (game)-1"]], [["Swimming pool-1"]]], [[["Marco Polo (game)-1", "Marco Polo (game)-2"]], ["no_evidence"], ["operation"]], [[["Marco Polo (game)-2"]], [["Marco Polo (game)-2"]], [["Marco Polo (game)-2"]]]], "golden_sentence": [["Marco Polo /\u02c8m\u0251\u02d0rko\u028a \u02c8po\u028alo\u028a/ (listen) is an American form of tag played in a swimming pool."], [""], [""]]}, {"qid": "d36a36fb39d99a716e86", "term": "Donatello", "description": "Italian painter and sculptor", "question": "Did Donatello use a smartphone?", "answer": false, "facts": ["Donatello died on December 13, 1466.", "The first smartphone did not come out until 1992."], "decomposition": ["What years was Donatello alive?", "When was the first smartphone released?", "Did #2 occur during #1?"], "evidence": [[[["Donatello-1"]], [["Smartphone-6"]], ["operation"]], [[["Donatello-1"]], [["Smartphone-16"]], ["operation"]], [[["Donatello-1"]], [["Smartphone-6"]], ["operation"]]], "golden_sentence": [[""], ["The first commercially available device that could be properly referred to as a \"smartphone\" began as a prototype called \"Angler\" developed by Canova in 1992 while at IBM and demonstrated in November of that year at the COMDEX computer industry trade show."]]}, {"qid": "5a7ed8e6de88d57dea93", "term": "The Dark Knight (film)", "description": "2008 film directed by Christopher Nolan", "question": "Was the death of Heath Ledger caused by his work on The Dark Knight?", "answer": false, "facts": ["Heath Ledger accidentally overdosed on prescription medication.", "Heath Ledger's overdose led to his death. "], "decomposition": ["What was the cause of Heath Ledger's death?", "Is #1 related to his work on the The Dark Knight?"], "evidence": [[[["Heath Ledger-3"]], [["Heath Ledger-21", "Heath Ledger-22", "Heath Ledger-30"]]], [[["Heath Ledger-29"]], [["Heath Ledger-29"], "no_evidence"]], [[["Heath Ledger-3"]], [["Heath Ledger-3"]]]], "golden_sentence": [["Ledger died on the afternoon of 22 January 2008 due to an accidental overdose of prescription medications."], ["", "", ""]]}, {"qid": "b984e221298c48b62f92", "term": "Parody", "description": "Imitative work created to mock, comment on or trivialise an original work", "question": "Is \"A Tale of Two Cities\" a parody of the Bible?", "answer": false, "facts": ["\"A Tale of Two Cities\" is an original work by Charles Dickens.", "The Bible is a religious text written down in the early centuries AD.", "A parody is a deriative work intended to make fun of another piece of media. "], "decomposition": ["Was the story of \"A Tale of Two Cities\" written as an imitation of the Bible?"], "evidence": [[[["A Tale of Two Cities-1", "A Tale of Two Cities-48"], "operation"]], [[["A Tale of Two Cities-1", "Bible-1"]]], [[["A Tale of Two Cities-1"]]]], "golden_sentence": [["", ""]]}, {"qid": "334208b0a3657d6d00ad", "term": "D", "description": "letter in the Latin alphabet", "question": "Is the letter D influenced by the shape of ancient doors?", "answer": true, "facts": ["D is the fourth letter of the Latin alphabet", "D is a descendent of the ancient Phoenician Dalet", "Dalet was represented by a glyph of a door"], "decomposition": ["Which ancient language did the letter 'D' descend from?", "What was used to represent 'D' in #1?", "Was #2 a symbol of a door?"], "evidence": [[[["D-2"]], [["Dalet-2"]], [["Dalet-2"]]], [[["D-2"]], [["D-2"]], ["operation"]], [[["D-2"]], [["D-2"]], [["D-2", "Logogram-1"]]]], "golden_sentence": [["The Semitic letter D\u0101leth may have developed from the logogram for a fish or a door."], ["plosive ([d])."], ["The letter is based on a glyph of the Middle Bronze Age alphabets, probably called dalt \"door\" (door in Modern Hebrew is delet), ultimately based on a hieroglyph depicting a"]]}, {"qid": "926cf73f4577c691f9bc", "term": "Acetylene", "description": "chemical compound", "question": "Did Julio Gonzalez like acetylene?", "answer": true, "facts": ["Julio Gonzalez was an artist who welded metal to create sculptures", "Welding is achieved by using a blowtorch on metal", "Blowtorches use acetylene as fuel"], "decomposition": ["What technique did Julio Gonzalez use to create his scultures?", "What is the main tool used for #1?", "What is a common fuel for #2?"], "evidence": [[[["Julio Gonz\u00e1lez (sculptor)-5"], "no_evidence"], [["Welding-10"]], [["Acetylene-14"], "operation"]], [[["Julio Gonz\u00e1lez (sculptor)-4"]], [["Oxy-fuel welding and cutting-3"]], [["Oxy-fuel welding and cutting-30"]]], [[["Julio Gonz\u00e1lez (sculptor)-1", "Julio Gonz\u00e1lez (sculptor)-4"]], [["Welding-10"]], [["Acetylene-14"]]]], "golden_sentence": [[""], [""], ["Combustion of acetylene with oxygen produces a flame of over 3,600\u00a0K (3,330\u00a0\u00b0C; 6,020\u00a0\u00b0F), releasing 11.8\u00a0kJ/g."]]}, {"qid": "cfb0279a48c69e5d98ee", "term": "Supreme Court of Canada", "description": "highest court of Canada", "question": "Is clerk of Supreme Court of Canada safe profession for someone with seismophobia?", "answer": true, "facts": ["Seismophobia is the extreme fear of earthquakes.", "The Supreme Court of Canada is located in Ottawa.", "The Ottawa-Gattineau region is located far from active tectonic plates."], "decomposition": ["What is seismophobia a fear of?", "Movement of what causes #1?", "Where is the Supreme Court of Canada located?", "Is #3 located near active #2's?"], "evidence": [[[["2019\u201320 Puerto Rico earthquakes-23"]], [["Earthquake-3"]], ["no_evidence"], [["Earthquake-25"], "operation"]], [[["Earthquake-1"], "no_evidence"], [["Earthquake-3"]], [["Supreme Court of Canada-19"]], [["Ottawa-16"], "operation"]], [[["2019\u201320 Puerto Rico earthquakes-23"], "no_evidence"], [["Seismology-5"]], [["Supreme Court of Canada-19"]], [["Ottawa-16"], "operation"]]], "golden_sentence": [[""], ["Earthquakes are caused mostly by rupture of geological faults but also by other events such as volcanic activity, landslides, mine blasts, and nuclear tests."], [""]]}, {"qid": "d553251e73a13d5dd59d", "term": "Homelessness", "description": "circumstance when people desire a permanent dwelling but do not have one", "question": "Does Antarctica have a lot of problems relating to homelessness?", "answer": false, "facts": ["Antarctica has no permanent residents.", "Exposure to the elements would be deadly for homeless people during certain times of year."], "decomposition": ["What do homeless people lack?", "Does the weather in Antarctica support life without #1?"], "evidence": [[[["Homelessness-1"]], [["Antarctica-2"], "operation"]], [[["Homelessness-1"]], [["Antarctica-47"]]], [[["Homelessness-6"]], [["Antarctica-43", "Climate of Antarctica-10", "Homelessness-6"]]]], "golden_sentence": [["Homelessness is defined as living in housing that is below the minimum standard or lacks secure tenure."], [""]]}, {"qid": "ee10a753bb488d24929e", "term": "Cooper (profession)", "description": "Maker of staved vessels such as barrels", "question": "Are coopers required in the beverage industry?", "answer": true, "facts": ["Coopers make barrels.", "Barrels are used to store certain alcoholic beverages during production."], "decomposition": ["What liquids are barrels made for?", "Are any of #1 part of the beverage industry?"], "evidence": [[[["Barrel-2"]], [["Drink-1"], "operation"]], [[["Barrel-4"]], [["Sake-1"], "operation"]], [[["Barrel-1", "Barrel-2", "Barrel-4"]], ["operation"]]], "golden_sentence": [["Modern wooden barrels for wine-making are made of French common oak (Quercus robur), white oak (Quercus petraea), or American white oak (Quercus alba) and typically have standard sizes: \"Bordeaux type\" 225 litres (59\u00a0US\u00a0gal; 49\u00a0imp\u00a0gal), \"Burgundy type\" 228 litres (60\u00a0US\u00a0gal; 50\u00a0imp\u00a0gal) and \"Cognac type\" 300 litres (79\u00a0US\u00a0gal; 66\u00a0imp\u00a0gal)."], [""]]}, {"qid": "d4f4d22b298fcae3097c", "term": "Gandalf", "description": "Fictional character created by J. R. R. Tolkien", "question": "Gandalf hypothetically defeats Rincewind in a wizard battle?", "answer": true, "facts": ["Gandalf is a 2000 plus year old wizard that has fought orcs and spirits in Middle Earth.", "Rincewind is the protagonist of the Discworld series.", "Rincewind is a failed student at the Unseen University for wizards in Ankh-Morpork.", "Rincewind is described by other wizards as the magical equivalent to the number zero."], "decomposition": ["How powerful is Gandalf as portrayed in LOTR?", "How powerful is Rincewind as portrayed at the Unseen University for wizards?", "Does #1 include far more experience and accomplishments than #2?"], "evidence": [[[["Gandalf-2"]], [["Rincewind-1"]], ["no_evidence"]], [[["Gandalf-2"], "no_evidence"], [["Rincewind-1"]], ["operation"]], [[["Gandalf-2"]], [["Rincewind-1"]], ["operation"]]], "golden_sentence": [["As a wizard and the bearer of a Ring of Power, Gandalf has great power, but works mostly by encouraging and persuading."], ["He is a failed student at the Unseen University for wizards in Ankh-Morpork, and is often described by scholars as \"the magical equivalent to the number zero\"."]]}, {"qid": "e88a9d2174f77dba7899", "term": "Alfa Romeo", "description": "Italian automotive manufacturer", "question": "Would an Alfa Romeo vehicle fit inside a barn?", "answer": true, "facts": ["Alfa Romeo makes cars.", "Barns are large enough to hold a car."], "decomposition": ["What is the average length of an Alfa Romeo?", "What is the average size of a barn?", "Is #1 smaller than #2?"], "evidence": [[[["Alfa Romeo-46"], "no_evidence"], ["no_evidence"], ["operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Alfa Romeo-1"], "no_evidence"], [["Barn-1"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""]]}, {"qid": "38ceb74a261d7f6369e6", "term": "Tonsillitis", "description": "Inflammation of the tonsils", "question": "Is strep throat harmless to singer Rita Ora after her 2020 tonsilitis surgery?", "answer": false, "facts": ["Tonsilitis is an inflammation of the tonsils.", "Singer Rita Ora had her tonsils removed in February of 2020 due to tonsilitis.", "Strep throat can still grow in the throat of people without tonsils."], "decomposition": ["What causes strep throat?", "Does #1 only flourish when tonsils are present?"], "evidence": [[[["Streptococcal pharyngitis-1"]], ["no_evidence"]], [[["Throat irritation-7"]], [["Streptococcal pharyngitis-1"]]], [[["Streptococcal pharyngitis-2"]], [["Streptococcal pharyngitis-1"], "no_evidence", "operation"]]], "golden_sentence": [["Streptococcal pharyngitis, also known as strep throat, is an infection of the back of the throat including the tonsils caused by group A streptococcus (GAS)."]]}, {"qid": "8e02182ff8c6a3c1aca1", "term": "United States Military Academy", "description": "U.S. Army's federal service academy in West Point, New York", "question": "Would the United States Military Academy reject an applicant with multiple sclerosis?", "answer": true, "facts": ["Multiple Sclerosis is a progressive condition affecting the brain and spinal chord.", "The US Military Academy does not give waivers for serious progressive conditions."], "decomposition": ["What kind of condition is Multiple Sclerosis?", "Would the US Military Academy have to reject someone with #1?"], "evidence": [[[["Multiple sclerosis-5"]], [["United States Military Academy-36"], "no_evidence"]], [[["Multiple sclerosis-1"]], [["United States Naval Academy-99"], "operation"]], [[["Multiple sclerosis-59"], "operation"], ["no_evidence"]]], "golden_sentence": [[""], [""]]}, {"qid": "9601160420a5dcd452a2", "term": "Metropolitan Museum of Art", "description": "Art museum in New York City, New York", "question": "Could someone in Tokyo take a taxi to the The Metropolitan Museum of Art?", "answer": false, "facts": ["Tokyo is located in Japan.", "Japan and the United States are separated by the Pacific Ocean.", "A taxi is not capable of travelling over water."], "decomposition": ["Where is Tokyo?", "Where is the Metropolitan Museum of Art?", "What separates #1 and #2?", "Can a taxi drive on #3?"], "evidence": [[[["Tokyo-1"]], [["Metropolitan Museum of Art-1"]], [["Pacific Ocean-1"]], [["Taxicab-44", "Water taxi-1"]]], [[["Tokyo City-5"]], [["Metropolitan Museum of Art-3"]], [["Ocean-3"], "operation"], [["Ocean-3"]]], [[["Tokyo-1"]], [["Metropolitan Museum of Art-58"]], [["Pacific Ocean-1"]], [["Taxicab-1"], "no_evidence", "operation"]]], "golden_sentence": [["Tokyo (/\u02c8to\u028aki\u02cco\u028a, -\u02cckjo\u028a/; Japanese: \u6771\u4eac, T\u014dky\u014d [to\u02d0k\u02b2o\u02d0] (listen)), officially Tokyo Metropolis (\u6771\u4eac\u90fd, T\u014dky\u014d-to), is the capital and most populous prefecture of Japan."], ["The Metropolitan Museum of Art of New York City, colloquially \"the Met\", is the largest art museum in the United States."], [""], ["", ""]]}, {"qid": "5dda1fa1daee6051d5da", "term": "Game engine", "description": "Software-development environment designed for building video games", "question": "Does Adobe Suite have video game engine coding?", "answer": true, "facts": ["Adobe applications runs on the C++ framework.", "Many video games are run on Unity game engine.", "The Unity game engine is a C++ coded engine."], "decomposition": ["What framework does Adobe Suite run on?", "What game engine do most video games run on?", "What type of engine is #2?", "Is the framework for #1 the same as the engine for #3?"], "evidence": [[[["Adobe Creative Suite-1", "Starling Framework-2"]], [["Starling Framework-1"], "no_evidence"], [["Starling Framework-3"], "no_evidence"], ["operation"]], [[["Adobe Creative Suite-1"], "no_evidence"], [["Game engine-27"], "no_evidence"], [["Game engine-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Adobe Creative Suite-1", "C++-2"], "no_evidence"], [["Unreal Engine-1"]], [["Unreal Engine-1"]], ["operation"]]], "golden_sentence": [["", ""], [""], [""]]}, {"qid": "bcfe22f69733d002bbec", "term": "Europa (moon)", "description": "The smallest of the four Galilean moons of Jupiter", "question": "Is Europa (moon) name origin related to Amunet?", "answer": false, "facts": ["Europa (moon) gets its name from the Greek Classical Mythology story.", "Europa was a woman that Zeus was in love with, and he changed into a bull to deceive her.", "Amunet is a primordial goddess in Ancient Egyptian religion."], "decomposition": ["What was the moon 'Europa' named after?", "Which myth is #1 a part of?", "Is Amunet a part of #2?"], "evidence": [[[["Europa (moon)-6"]], [["Europa (moon)-6"]], [["Amunet-1"]]], [[["Europa (moon)-1"]], [["Europa (consort of Zeus)-1"]], [["Amunet-1"], "operation"]], [[["Europa (moon)-1"]], [["Europa (consort of Zeus)-2"]], [["Amunet-1"], "operation"]]], "golden_sentence": [["Like all the Galilean satellites, Europa is named after a lover of Zeus, the Greek counterpart of Jupiter."], ["Europa is named after Europa, daughter of the king of Tyre, a Phoenician noblewoman in Greek mythology."], [""]]}, {"qid": "68e43e6f331cb4c9f3b5", "term": "Hunger", "description": "Sustained inability to eat sufficient food", "question": "Was Jean Valjean imprisoned due to hunger?", "answer": true, "facts": ["Jean Valjean was sentenced to imprisonment due to theft of property.", "The item Jean Valjean stole was a loaf of bread for his family."], "decomposition": ["What crime was Jean Valjean convicted of?", "What did Jean Valjean gain from #1?", "Who did he give #2 to?", "Is hunger experienced by #3 the main reason for wanting #2?"], "evidence": [[[["Jean Valjean-1"]], [["Jean Valjean-1"]], [["Jean Valjean-1"]], ["operation"]], [[["Jean Valjean-1"]], [["Jean Valjean-1"]], [["Jean Valjean-1"]], ["operation"]], [[["Jean Valjean-6"]], [["Jean Valjean-7"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["The story depicts the character's 19-year-long struggle to lead a normal life after serving a prison sentence for stealing bread to feed his sister's children during a time of economic depression and various attempts to escape from prison."], [""], [""]]}, {"qid": "f8a8780a0b9173f43e5c", "term": "Hypothermia", "description": "A human body core temperature below 35.0\u00b0C", "question": "Would someone on Venus be unlikely to experience hypothermia?", "answer": true, "facts": ["Hypothermia typically occurs from exposure to extreme cold.", "The average surface temperature on Venus is 863\u00b0F.", "A warmer surface temperature on the planet will result in a higher body temperature for people on that planet."], "decomposition": ["What is the average surface temperature on Venus?", "In order for the human body to experience hypothermia, it would have to be exposed to temperature that are what in relation to body temp?", "What is human body temperature?", "Does #1 meet the condition of #2 relative to #3?"], "evidence": [[[["Venus-23"]], [["Hypothermia-1"]], [["Human body temperature-4"]], ["operation"]], [[["Venus-2"]], [["Hypothermia-2"]], [["Human body temperature-7"]], ["operation"]], [[["Venus-19"]], [["Hypothermia-5"]], [["Human body temperature-4"]], ["operation"]]], "golden_sentence": [[""], ["Hypothermia is defined as a body core temperature below 35.0\u00a0\u00b0C (95.0\u00a0\u00b0F) in humans."], ["In humans, the average internal temperature is 37.0\u00a0\u00b0C (98.6\u00a0\u00b0F), though it varies among individuals."]]}, {"qid": "7bde4b4e29452e46c504", "term": "Jack Black", "description": "American actor, comedian, musician, music producer and youtuber.", "question": "Is Jack Black unlikely to compete with Bear McCreary for an award?", "answer": true, "facts": ["Jack Black is a musician but not a composer", "Bear McCreary is a composer", "Their interests are similar but their skills not overlap in awards categories"], "decomposition": ["What music-related occupation does Bear McCreary have?", "What types of awards are won by notable figures who work as #1? ", "What music-related occupation does Jack Black have?", "What types of awards have been won by notable figures who work as #3?", "Are #2 and #4 separate categories of awards?"], "evidence": [[[["Bear McCreary-1"]], [["American Society of Composers, Authors and Publishers-18"]], [["The Pick of Destiny-8"]], [["Jack Black-24"]], [["American Society of Composers, Authors and Publishers-18", "Jack Black-24"]]], [[["Bear McCreary-1"]], [["American Society of Composers, Authors and Publishers-18", "International Film Music Critics Association-1"], "no_evidence"], [["Jack Black-1"]], [["Grammy Award-1", "Grammy Award-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Bear McCreary-1"]], [["Bear McCreary-2"]], [["Jack Black-16"]], [["Grammy Award for Best Hard Rock Performance-1"]], ["operation"]]], "golden_sentence": [["Bear McCreary (born February 17, 1979) is an American musician and composer of film, television, and video games scores based in Los Angeles, California."], ["ASCAP honors its top members in a series of annual awards shows in seven different music categories: pop, rhythm and soul, film and television, Latin, country, Christian, and concert music."], ["Bonus tracks Tenacious D Jack Black \u2013 lead vocals, acoustic guitar Kyle Gass \u2013 backing vocals, acoustic and electric guitar, recorder on \"Papagenu (He's My Sassafrass)\" Additional Musicians Dave Grohl \u2013 drums, demon vocals on \"Beelzeboss (The Final Showdown)\" John Spiker \u2013 bass guitar, backing vocals on \"Car Chase City\", clavinet on \"Papagenu (He's My Sassafrass)\" John Konesky \u2013 electric guitar Liam Lynch \u2013 additional guitar on \"Beelzeboss (The Final Showdown)\" and \"Break In-City (Storm the Gate!)\""], [""], ["", ""]]}, {"qid": "d471dc95de45cd5f2f07", "term": "Insomnia", "description": "The inability to fall or stay sleep", "question": "Would Cuba Libre consumption help with insomnia?", "answer": false, "facts": ["Cuba Libre is a highball cocktail consisting of cola, rum, and in many recipes lime juice on ice.", "Traditionally, the cola ingredient is Coca-Cola (\"Coke\"), and the alcohol is a light rum such as Bacardi. ", "Coca-Cola typically contains caffeine.", "Caffeine consumption often promotes insomnia."], "decomposition": ["What is the traditional source of cola in Cuba Libre?", "Does intake of #1 help treat insomnia?"], "evidence": [[[["Rum and Coke-1"]], [["Caffeine-42", "Coca-Cola-38"], "operation"]], [[["Coca-Cola-1", "Rum and Coke-1"]], ["operation"]], [[["Rum and Coke-1"]], [["Caffeine-18", "Coca-Cola formula-9"]]]], "golden_sentence": [["Traditionally, the cola ingredient is Coca-Cola (\"Coke\"), and the alcohol is a light rum such as Bacardi."], ["As a result, caffeine temporarily prevents or relieves drowsiness, and thus maintains or restores alertness.", ""]]}, {"qid": "c54cb93209ae537549ba", "term": "Solubility", "description": "Capacity of a designated solvent to hold a designated solute in homogeneous solution under specified conditions", "question": "Does Nigella Lawson care about solubility?", "answer": true, "facts": ["Nigella Lawson is a chef", "Chefs are concerned with cooking processes and nutrition", "Solubility plays a role in both the chemistry of cooking processes as well as the body's interaction with substances that it ingests"], "decomposition": ["What is Nigella Lawson's major occupation?", "What kind of substances and processes is the concept of solubility applicable to?", "What kind of substances and processes are of importance to #1?", "Are any of #2 included in #3?"], "evidence": [[[["Nigella Lawson-1"]], [["Solubility-1"], "no_evidence"], [["Cooking-34"], "no_evidence"], ["operation"]], [[["Nigella Lawson-1"]], [["Solubility-1"]], [["Cooking-17"]], ["operation"]], [[["Nigella Lawson-1"]], [["Solubility-1"], "no_evidence"], [["Cooking-14"], "no_evidence"], ["operation"]]], "golden_sentence": [["She is the daughter of Nigel Lawson, a former Conservative Chancellor of the Exchequer, and Vanessa (n\u00e9e Salmon) Lawson, whose family owned the J. Lyons and Co. food and catering business."], [""], [""]]}, {"qid": "5326c0aa78fbbd9152c8", "term": "Daytona 500", "description": "Auto race held in Daytona, Florida, United States", "question": "Can E6000 cure before a hoverboard finishes the Daytona 500? ", "answer": true, "facts": ["The Daytona 500 is 500 miles", "A hoverboard can move at six to eight miles per hour", "E6000 fully cures in 24 to 72 hours"], "decomposition": ["How long is the Daytona 500?", "How fast can a hoverboard move in hours?", "What is #1 divided by #2?", "How many hours does it take for a E6000 to cure?", "Is #4 more less than #3?"], "evidence": [[[["Daytona 500-1"]], [["Franky Zapata-12"], "no_evidence"], ["operation"], [["Conroe (microprocessor)-8"], "no_evidence"], ["operation"]], [[["Daytona 500-1"]], [["Self-balancing scooter-9"]], ["operation"], [["Adhesive-28"], "no_evidence"], ["operation"]], [[["Daytona 500-1"]], [["Hoverboard-18"]], ["operation"], [["Adhesive-42"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["The Daytona 500 is a 500-mile-long (805\u00a0km) NASCAR Cup Series motor race held annually at Daytona International Speedway in Daytona Beach, Florida."], [""], [""]]}, {"qid": "850d4bf8f05ca174a7ef", "term": "Pope", "description": "Leader of the Catholic Church", "question": "Could the Pope be on an episode of Pimp My Ride?", "answer": true, "facts": ["Pimp My Ride is a show where people's cars are upgraded and improved", "The Pope has a personal vehicle called the Popemobile"], "decomposition": ["In the show Pimp My Ride, what type of vehicles are upgraded and improved?", "Does the Pope have #1?"], "evidence": [[[["Pimp My Ride-5"]], [["Landaulet (car)-7"]]], [[["Pimp My Ride-1"]], [["Popemobile-1"]]], [[["Pimp My Ride-1"]], [["Popemobile-1"], "operation"]]], "golden_sentence": [["Most changes are only cosmetic, and mechanical work is generally only done to enable the car to run; the show has replaced entire engines with new engines."], [""]]}, {"qid": "b40834811679a3b97efd", "term": "Penny", "description": "unit of currency in various countries", "question": "Are pennies commonly used in Canada?", "answer": false, "facts": ["Canada used pennies historically as one cent coins.", "Canada stopped minting pennies in 2012. "], "decomposition": ["What coins are used in Canada?", "Are pennies among #1?"], "evidence": [[[["Canadian dollar-16"]], ["operation"]], [[["Coins of the Canadian dollar-2"]], [["Coins of the Canadian dollar-2"]]], [[["Canadian dollar-16"]], ["operation"]]], "golden_sentence": [["Coins are produced by the Royal Canadian Mint's facilities in Winnipeg, Manitoba, and Ottawa, Ontario, in denominations of 5\u00a2 (nickel), 10\u00a2 (dime), 25\u00a2 (quarter), 50\u00a2 (50\u00a2 piece) (though the 50\u00a2 piece is no longer distributed to banks and is only available directly from the mint, therefore seeing very little circulation), $1 (loonie), and $2 (toonie)."]]}, {"qid": "3c9bd7b39141d6dbb37b", "term": "Drag king", "description": "female performance artists who dress and behave in masculine way for performance", "question": "Do drag kings take testosterone to look masculine?", "answer": false, "facts": ["Drag Kings will use contouring and makeup to make their facial features appear more masculine and chiseled. ", "Testosterone is prescribed for transgender men to help with transitioning and dysphoria.", "Drag kings often identify as women, but dress as men for show."], "decomposition": ["Which features of themselves do drag kings modify to look masculine?", "Would #1 require testosterone intake?"], "evidence": [[[["Passing (gender)-31"], "no_evidence"], [["Transgender hormone therapy (male-to-female)-42"], "operation"]], [[["Drag king-1"]], ["operation"]], [[["Drag king-1"]], [["Testosterone-1"], "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "10762b4c603b1baeef0b", "term": "Halloween", "description": "Holiday celebrated October 31", "question": "Will Chick Fil A be open on Halloween 2021?", "answer": false, "facts": ["Chick Fil A restaurants close on Sundays.", "Halloween 2021 falls on a Sunday."], "decomposition": ["What day of the week does Halloween fall on in 2021?", "What days of the week is Chick Fil A closed?", "Is #1 included in #2?"], "evidence": [[["no_evidence"], [["Chick-fil-A-18"]], ["no_evidence", "operation"]], [["no_evidence"], [["Chick-fil-A-2"]], ["operation"]], [["no_evidence"], [["Chick-fil-A-2"]], ["operation"]]], "golden_sentence": [["The founder's beliefs are responsible for the chain's most well-known and distinctive feature: all Chick-fil-A locations (both corporate owned and franchised) are closed on Sundays, as well as on Thanksgiving and Christmas."]]}, {"qid": "81e7ef1ce0396a42a7e3", "term": "Alfa Romeo", "description": "Italian automotive manufacturer", "question": "Can you order an Alfa Romeo at Starbucks?", "answer": false, "facts": ["Alfa Romeo is a brand of automobile", "Starbucks sells coffee, tea, food, and some drink products like thermoses"], "decomposition": ["What kind of product is an Alfa Romeo?", "What kind of goods does Starbucks sell?", "Is #1 found in #2?"], "evidence": [[[["Alfa Romeo Giulietta (940)-1"]], [["Starbucks-1"]], ["operation"]], [[["Alfa Romeo-1"]], [["Starbucks-1"]], ["operation"]], [[["Alfa Romeo-1"]], [["Starbucks-1"]], ["operation"]]], "golden_sentence": [["The Alfa Romeo Giulietta (Type 940) is a small family car (C-segment) produced by the Italian automaker Alfa Romeo."], ["Starbucks locations serve hot and cold drinks, whole-bean coffee, microground instant coffee known as VIA, espresso, caffe latte, full- and loose-leaf teas including Teavana tea products, Evolution Fresh juices, Frappuccino beverages, La Boulange pastries, and snacks including items such as chips and crackers; some offerings (including their annual fall launch of the Pumpkin Spice Latte) are seasonal or specific to the locality of the store."]]}, {"qid": "7fd3efc1fb2853ecec03", "term": "United States Marine Corps", "description": "Amphibious warfare branch of the United States Armed Forces", "question": "Would a recruit for the United States Marine Corps be turned away for self harm?", "answer": true, "facts": ["Self harm is when someone intentionally causes injury or pain to themselves.", "Self harm is almost always related to a mental health condition.", "Those experiencing mental wellness related illnesses cannot join the Marines."], "decomposition": ["What medical condition is self harm usually related to?", "Are people who suffer from #1 turned away from the US Marine Corps?"], "evidence": [[[["Self-harm-3"]], [["United States Marine Corps-2"], "no_evidence", "operation"]], [[["Self-harm-22"]], ["no_evidence"]], [[["Self-harm-16", "Self-harm-17"]], ["no_evidence"]]], "golden_sentence": [["Self-harm is often associated with a history of trauma, including emotional and sexual abuse."], [""]]}, {"qid": "f2f3378339c688f743a0", "term": "Frost", "description": "coating or deposit of ice that may form in humid air in cold conditions, usually overnight", "question": "Would it be unusual to see frost in September in Texas?", "answer": true, "facts": ["Texas is a Southern state of the United States, known for high heat.", "On average, Texas is between 68 and 89 degrees during the month of September.", "Frost forms at 32 degrees or lower."], "decomposition": ["What are the average temperatures in Texas during the month of September?", "What temperature does frost form at?", "Is #1 warmer than #2?"], "evidence": [[[["Climate of Dallas-3"], "no_evidence"], [["Frost (temperature)-1", "Frost-1"]], ["operation"]], [[["Texas-29", "Texas-30", "Texas-31"]], [["Dew point-1", "Frost-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Climate of Texas-3"], "no_evidence"], [["Dew point-1", "Frost-20", "Frost-5"]], ["operation"]]], "golden_sentence": [[""], ["Frost or freezing occurs when the temperature of air falls below the freezing point of water (0\u00a0\u00b0C, 32\u00a0\u00b0F, 273.15 K).", "Frost is a thin layer of ice on a solid surface, which forms from water vapor in an above freezing atmosphere coming in contact with a solid surface whose temperature is below freezing, and resulting in a phase change from water vapor (a gas) to ice (a solid) as the water vapor reaches the freezing point."]]}, {"qid": "d7f54c172a7dbbc9c220", "term": "Breakdancing", "description": "Style of street dance", "question": "Did breakdancing grow in popularity during WW2?", "answer": false, "facts": ["Breakdancing was created by the African American youth in the early 1970s.", "World War II was a global war that lasted from 1939 to 1945."], "decomposition": ["When did break-dancing experience a growth in popularity?", "Through which period did World War II take place?", "Is #1 within #2?"], "evidence": [[[["Breakdancing-2"]], [["World War II-1"]], ["operation"]], [[["Breakdancing-2"]], [["World War II-1"]], ["operation"]], [[["Breakdancing-2"]], [["World War II-1"]], ["operation"]]], "golden_sentence": [["By the late seventies, the dance had begun to spread to other communities and was gaining wider popularity; at the same time, the dance had peaked in popularity among African Americans and Puerto Ricans."], ["World War\u00a0II (often abbreviated as WWII or WW2), also known as the Second World War, was a global war that lasted from 1939 to 1945."]]}, {"qid": "65dbf48e522ebc41283c", "term": "Vitamin C", "description": "nutrient found in citrus fruits and other foods", "question": "Can vitamin C rich fruits be bad for health?", "answer": true, "facts": ["Oranges are fruits that are rich in vitamin C.", "Oranges are very acidic fruits that can wear down tooth enamel.", "Too much Vitamin C can cause nausea and diarrhea."], "decomposition": ["Which vitamin are oranges rich in?", "Is #1 the same as vitamin C?", "Can excess of #1 be harmful to a person's health?", "Can the acidity of oranges have adverse effects on human consumers?", "Are #2, #3 and #4 positive?"], "evidence": [[[["Orange (fruit)-20"]], ["operation"], [["Vitamin C megadosage-7"]], [["Citric acid-39", "Orange (fruit)-41"]], ["operation"]], [[["Orange (fruit)-20"]], ["operation"], ["no_evidence", "operation"], [["Citric acid-39"]], ["no_evidence", "operation"]], [[["Vitamin C-69"]], [["Vitamin C-69"]], [["Vitamin C-22"]], ["no_evidence"], ["no_evidence"]]], "golden_sentence": [["As oranges are rich in vitamin C and do not spoil easily, during the Age of Discovery, Portuguese, Spanish, and Dutch sailors planted citrus trees along trade routes to prevent scurvy."], [""], ["Although a weak acid, exposure to pure citric acid can cause adverse effects.", ""]]}, {"qid": "8ef0e2b9b85c8e5aff92", "term": "Monarch butterfly", "description": "milkweed butterfly in the family Nymphalidae", "question": "Could a monarch butterfly rule a kingdom?", "answer": false, "facts": ["A monarch butterfly would be easily killed by a human due to its small size.", "A monarch butterfly does not have the intellectual capacity to rule over a kingdom of humans."], "decomposition": ["Does a monarch butterfly have the physical capacity to rule over humans?", "Does a monarch butterfly have the intellectual ability to rule over humans?", "Is #1 or #2 positive?"], "evidence": [[[["Monarch butterfly-1"]], [["Monarch butterfly-1"]], [["Monarch butterfly-1"]]], [[["Monarch butterfly-1"]], [["Butterfly-15"], "no_evidence"], ["operation"]], [[["Monarch butterfly-1"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""], [""]]}, {"qid": "bae29ccc964a0f2599e9", "term": "Crucifixion", "description": "Method of capital punishment in which the victim is tied or nailed to a large wooden beam and left to hang until eventual death", "question": "If it socially acceptable to wear an icon depicting crucifixion? ", "answer": true, "facts": ["The crucifixion of Jesus is a common sign used by Catholics and Christian denominations. ", "Many jewelry stores offer necklaces with the Crucifixion of Jesus Christ."], "decomposition": ["Which common symbol is used by Catholics to depict crucifixion?", "Is #1 commonly found in jewelry stores?"], "evidence": [[[["Christian symbolism-6"], "no_evidence"], [["Christian cross variants-3"], "operation"]], [[["Crucifix-2"]], [["Crucifix-12"], "no_evidence", "operation"]], [[["Crucifixion-2"]], ["no_evidence"]]], "golden_sentence": [["The cross (crucifix, Greek stauros) in this period was represented by the letter T. Clement of Alexandria in the early 3rd century calls it \u03c4\u1f78 \u03ba\u03c5\u03c1\u03b9\u03b1\u03ba\u1f78\u03bd \u03c3\u03b7\u03bc\u03b5\u1fd6\u03bf\u03bd (\"the Lord's sign\") he repeats the idea, current as early as the Epistle of Barnabas, that the number 318 (in Greek numerals, \u03a4\u0399\u0397) in Genesis 14:14 was a foreshadowing (a \"type\") of the cross (T, an upright with crossbar, standing for 300) and of Jesus (\u0399\u0397, the first two letters of his name \u0399\u0397\u03a3\u039f\u03a5\u03a3, standing for 18)."], [""]]}, {"qid": "098898b07acfc2ef53be", "term": "Brake", "description": "mechanical device that inhibits motion", "question": "Can people die from brake failure?", "answer": true, "facts": ["Brake failure is the inability of brakes to function.", "When vehicles experience brake failure, they cannot be stopped safely, which results in a crash.", "People die in vehicular crashes."], "decomposition": ["What is a brake failure?", "What can #1 lead to in a car?", "Have people died from #2?"], "evidence": [[[["Disc brake-63"]], [["Traffic collision-1", "Traffic collision-50"]], [["Falco (musician)-22"], "operation"]], [[["Brake-1"], "no_evidence"], [["Traffic collision-1", "Traffic collision-24"], "no_evidence"], [["Traffic collision-3"], "operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence"]]], "golden_sentence": [["Brake failure can result from failure of the piston to retract, which is usually a consequence of not operating the vehicle during prolonged storage outdoors in adverse conditions."], ["", ""], [""]]}, {"qid": "e33022f1d5349cf443eb", "term": "Humour", "description": "tendency of experiences to provoke laughter and provide amusement", "question": "Was the man who played the male lead in Mrs. Doubtfire known for his humour?", "answer": true, "facts": ["Robin Williams played the male lead in Mrs. Doubtfire.", "Robin Williams had a prolific standup and film comedy career."], "decomposition": ["Who played the male  lead in MRs. Doubtfire?", "What was the career of #1?", "Is #2 a humorous job?"], "evidence": [[[["Mrs. Doubtfire-1"]], [["Robin Williams-1"]], ["operation"]], [[["Mrs. Doubtfire-3"]], [["Robin Williams-14"]], ["operation"]], [[["Mrs. Doubtfire-1"]], [["Robin Williams-1"]], [["Comedian-1"], "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "b109a28b32d88289e5d4", "term": "Stone Cold Steve Austin", "description": "American professional wrestler", "question": "Coud every wife of Stone Cold Steve Austin fit in Audi TT?", "answer": true, "facts": ["Stone Cold Steve Austin has been married to 4 different women.", "The Audi TT is a sports car with 4 seats."], "decomposition": ["How many wives has Stone Cold Steve Austin had?", "How many people can sit in an Audi TT", "Is #2 at least #1?"], "evidence": [[[["Stone Cold Steve Austin-67"]], [["Audi TT-2"]], ["operation"]], [[["Stone Cold Steve Austin-67"]], [["2+2 (car body style)-1", "Audi TT-2"]], ["operation"]], [[["Stone Cold Steve Austin-67"]], [["Audi TT-1"]], ["operation"]]], "golden_sentence": [["In late 2009, Austin married his fourth wife, Kristin, with whom he splits his time between his home in Marina Del Rey, California and his property in Nevada that he has named the Broken Skull Ranch 2.0."], ["For each of its three generations, the TT has been available as a 2+2 coup\u00e9 and as a two-seater roadster employing consecutive generations of the Volkswagen Group A platform, starting with the A4\u00a0(PQ34)."]]}, {"qid": "6fb8dabcfc346a40c461", "term": "Hamlet", "description": "tragedy by William Shakespeare", "question": "Is Hamlet more common on IMDB than Comedy of Errors?", "answer": true, "facts": ["IMDB, The Internet Movie Database, catalogs movies.", "IMDB lists 6 Hamlet movie adaptations.", "IMDB lists 5 Comedy of Errors movie adaptations."], "decomposition": ["How many listings of Hamlet are there on IMDB?", "How many listing of Comedy of Errors is there on IMDB?", "Is #1 greater than #2?"], "evidence": [[[["Hamlet-3"], "no_evidence"], [["The Comedy of Errors-24", "The Comedy of Errors-25"], "no_evidence"], ["no_evidence", "operation"]], [[["Hamlet-92"], "no_evidence"], [["The Comedy of Errors-24"]], ["operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["", ""]]}, {"qid": "086ca3f17514b92679ae", "term": "Freemasonry", "description": "group of fraternal organizations", "question": "Has Freemasonry been represented on the Moon?", "answer": true, "facts": ["Freemasonry is a group of fraternal organizations rooted in fraternities of stonemasons of the fourteenth century.", "Buzz Aldrin was initiated into the Freemason fraternity in 1955", "Buzz Aldrin and Neil Armstrong were the first men to land on the moon in 1969."], "decomposition": ["What occupation goes into space?", "Have any #1 been Free Masons?", "Have any people listed in #2 been to the moon?"], "evidence": [[[["Astronaut-1"]], [["James Irwin-1", "James Irwin-23"]], [["James Irwin-1"]]], [[["Astronaut-1"]], [["Buzz Aldrin-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Astronaut-1"]], [["John Glenn-62"]], [["John Glenn-3"], "operation"]]], "golden_sentence": [["Although generally reserved for professional space travelers, the terms are sometimes applied to anyone who travels into space, including scientists, politicians, journalists and tourists."], ["", ""], ["He was the eighth person to walk on the Moon and the first, and youngest, of those astronauts to die."]]}, {"qid": "54642dc5698eee627f7f", "term": "Legend", "description": "Traditional story of heroic humans.", "question": "Do urban legends always have to occur in cities?", "answer": false, "facts": ["An urban legend is commonly accepted folk lore. ", "Urban means related to a city.", "Many urban legends occur in rural towns that lack dense population.", "Most Urban legends are unverified due to lack of witnesses. "], "decomposition": ["In what settings do urban legends occur?", "Does #1 only consist of urban environments?"], "evidence": [[[["Chupacabra-1", "Chupacabra-5", "Urban legend-1"], "no_evidence"], [["Urban area-1"], "operation"]], [[["Urban legend-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Legend-15"]], [["Legend-15"]]]], "golden_sentence": [["", "", "An urban legend, urban myth, urban tale, or contemporary legend is a genre of folklore comprising stories circulated as true, especially as having happened to a friend or family member, often with horrifying or humorous elements."], ["In urbanism, the term contrasts to rural areas such as villages and hamlets; in urban sociology or urban anthropology it contrasts with natural environment."]]}, {"qid": "2d04751e4eba89f1b18d", "term": "Copper", "description": "Chemical element with atomic number 29", "question": "Would a fungal life-form be threatened by a pigment from copper?", "answer": true, "facts": ["Verdigris is a pigment made from copper", "Verdigris is also used as a fungicide "], "decomposition": ["Which element is the pigment verdigris derived from?", "Is #1 copper and verdigris also used as a fungicide?"], "evidence": [[[["Verdigris-1"]], [["Verdigris-6"]]], [[["Verdigris-1"]], [["Copper-5", "Verdigris-6"], "operation"]], [[["Verdigris-1"]], [["Copper-5", "Verdigris-6"], "operation"]]], "golden_sentence": [["Verdigris is the common name for a green pigment obtained through the application of acetic acid to copper plates or the natural patina formed when copper, brass or bronze is weathered and exposed to air or seawater over time."], ["It is also used industrially as a fungicide, a catalyst for organic reactions, and in dyeing (The Merck Index , Ninth Ed., 1976)."]]}, {"qid": "9d583c21fc259514105c", "term": "Goldstone Deep Space Communications Complex", "description": "United States historic place", "question": "Do the telescopes at Goldstone Deep Space Communications Complex work the night shift?", "answer": true, "facts": ["The night shift is considered to be the hours of 11pm - 7am.", "The telescopes at Goldstone Deep Space Communications Complex are running 24 hours a day."], "decomposition": ["What hours are typically considered the night shift?", "What hours do the telescopes at Goldstone Deep Space Communications Complex run?", "Is there any overlap between #1 and #2?"], "evidence": [[[["Shift work-11"]], [["Goldstone Deep Space Communications Complex-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Shift work-11"], "no_evidence"], ["no_evidence"], ["operation"]], [["no_evidence"], [["Astronomy-2", "Goldstone Deep Space Communications Complex-1"], "no_evidence"], ["no_evidence"]]], "golden_sentence": [["One study suggests that, for those working a night shift (such as 23:00 to 07:00), it may be advantageous to sleep in the evening (14:00 to 22:00) rather than the morning (08:00 to 16:00)."], [""]]}, {"qid": "13a0a29aeea4238d96b0", "term": "Jackson Pollock", "description": "American painter", "question": "Was Jackson Pollock straight edge?", "answer": false, "facts": ["Jackson Pollock was a famous painter.", "Straight Edge is a punk inspired lifestyle who's adherents abstain from alcohol and drugs.", "Jackson Pollock was an alcoholic.", "Jackson Pollock died in a car crash while driving under the influence of alcohol."], "decomposition": ["What substances do people avoid if they are straight edge?", "Did Jackson Pollock always avoid #1?"], "evidence": [[[["Straight edge-5"]], [["Jackson Pollock-17"], "operation"]], [[["Straight edge-1"]], [["Jackson Pollock-3"]]], [[["Straight edge-1"]], [["Jackson Pollock-3"], "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "79b64cc67a173b231643", "term": "Mercedes-Benz", "description": "automobile brand of Daimler AG", "question": "Is it legal for a licensed child driving Mercedes-Benz to be employed in US?", "answer": true, "facts": ["The minimum age for driving in the US is 16.", "Child labor laws in the US require a child to be 14 years of age or older to work."], "decomposition": ["What is the minimum driving age in the US?", "What is the minimum age for someone to be employed in the US?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Graduated driver licensing-35"], "no_evidence"], [["Child labour law-10"]], ["operation"]], [[["Driver's licenses in the United States-9"]], [["Child labor laws in the United States-2"]], ["operation"]], [[["Driver's licenses in the United States-9"]], [["Child labour-66", "Legal working age-1"]], ["operation"]]], "golden_sentence": [["At the age of 16 a driver with a Instruction Permit may apply for a Provisional License, to obtain it the driver must not having received a traffic conviction within the last 6 months, and if under the age of 18, the driver must have had the Instructional Permit for 6 months prior to obtaining the Provisional License and have a parent, legal guardian, or employer certify the driver has had 40 hours of experience and 10 of those hours must be during \"progressively challenging circumstances\"."], ["Person's under the age of 18 are not permitted to work underground such as in a mine."]]}, {"qid": "5297737156179200e8d1", "term": "Sofer", "description": "profession", "question": "Would a sofer be a bad job for a vegan?", "answer": true, "facts": ["A sofer is a transcriber of religious texts, and has its origins in Judaism.", "Sofers transcribe texts on a material known as vellum.", "Vellum is made of materials derived from calfskin.", "Vegans do not use any animal products."], "decomposition": ["What materials do sofers use?", "What products do vegans refuse to use?", "Is #1 included in #2?"], "evidence": [[[["Marshmallow sofa-5"]], [["Veganism-9"]], [["Leather-4"]]], [[["Sofer-1"]], [["Veganism-1"]], ["operation"]], [[["Parchment-1"]], [["Veganism-1"]], ["operation"]]], "golden_sentence": [["The cushions were covered in either fabric, vinyl, or leather in bright colors."], ["An article in the Society's magazine, the Vegetarian Messenger, in 1851 discussed alternatives to shoe leather, which suggests the presence of vegans within the membership who rejected animal use entirely, not only in diet."], [""]]}, {"qid": "3f780a27d24d75f85a1a", "term": "Year", "description": "Orbital period of the Earth around the Sun", "question": "Can an African Elephant get pregnant twice in a year?", "answer": false, "facts": ["There are 365 days in one year.", "It takes around 645 days for an African Elephant to give birth to one baby elephant."], "decomposition": ["What is the gestation period of an African elephant?", "Is #1 less than a year?"], "evidence": [[[["African elephant-26"]], ["operation"]], [[["African elephant-28"]], ["operation"]], [[["African elephant-26"]], ["operation"]]], "golden_sentence": [["Calves are born after a gestation period of up to nearly two years."]]}, {"qid": "a84b7a50031e8c1d5f72", "term": "JAG (TV series)", "description": "American legal drama television series (1996-2005)", "question": "Did Joan Crawford guest star on  JAG (TV series)?", "answer": false, "facts": ["JAG began airing in 1995.", "Joan Crawford died in 1977."], "decomposition": ["When did Joan Crawford's career as a television actress come to an end?", "When was the TV series JAG launched?", "Is #2 before #1?"], "evidence": [[[["Joan Crawford-64"]], [["NCIS (TV series)-19"], "no_evidence"], ["operation"]], [[["Joan Crawford-61", "Joan Crawford-64"]], [["JAG (season 1)-1"]], ["operation"]], [[["Joan Crawford-37"]], [["JAG (season 1)-1"]], ["operation"]]], "golden_sentence": [["Crawford died on May 10, 1977 at her New York apartment of a myocardial infarction."], [""]]}, {"qid": "6f3bc15a9a29b2a9a7a0", "term": "Elijah Cummings", "description": "U.S. Representative from Maryland", "question": "Will Elijah Cummings vote for Joe Biden in the next presidential elections?", "answer": false, "facts": ["The next presidential elections will take place in November of 2020", "Elijah Cummings passed away in October of 2019"], "decomposition": ["When will the next presidential election be held?", "When did Elijah Cummings pass away?", "Is #2 after #1?"], "evidence": [[[["2020 United States presidential election-1"]], [["Elijah Cummings-1"]], ["operation"]], [[["2020 United States presidential election-1"]], [["Elijah Cummings-1"]], ["operation"]], [[["2020 United States presidential election-1"]], [["Elijah Cummings-1"]], ["operation"]]], "golden_sentence": [["The 2020 United States presidential election is scheduled for Tuesday, November 3, 2020."], ["Elijah Eugene Cummings (January 18, 1951 \u2013 October 17, 2019) was an American politician and civil rights advocate who served in the United States House of Representatives for Maryland's 7th congressional district from 1996 until his death in October of 2019."]]}, {"qid": "0f0fb3e9039920eb780a", "term": "Coen brothers", "description": "American filmmakers", "question": "Do people watching Coen brothers films in Guinea Bissau need subtitles?", "answer": true, "facts": ["The Coen brothers direct films primarily using English", "The primary languages used in Guinea Bissau are Creole, native African languages, and Portuguese"], "decomposition": ["In what country do the Coen Brothers make films?", "What is the primary language spoken in #1?", "What is the primary language spoken in Guinea Bissau?", "Is #3 different than #2?"], "evidence": [[[["Coen brothers-1"]], [["Languages of the United States-1"]], [["Guinea-Bissau-3"]], ["operation"]], [[["Coen brothers-1"]], [["American English-2"]], [["Guinea-Bissau-3"]], ["operation"]], [[["Coen brothers-13"], "no_evidence"], [["United States-80"]], [["Guinea-Bissau-3"]], ["operation"]]], "golden_sentence": [[""], ["The most commonly used language in the United States is English (specifically, American English), which is the de facto national language."], ["However, Creole is the national language and also considered the language of unity."]]}, {"qid": "5db49dd6d438eabf8e4c", "term": "Great Pyramid of Giza", "description": "Largest pyramid in the Giza Necropolis, Egypt", "question": "Is Great Pyramid of Giza the last wonder of its kind?", "answer": true, "facts": ["The Great Pyramid of Giza is classified as one of the Seven Wonders of the Ancient World.", "Five of the ancient wonders were destroyed, and a sixth (the Hanging Gardens of Babylon) may not have existed.", "The Great Pyramid of Giza is largely intact as of 2020."], "decomposition": ["What are the wonders of the ancient world that are either destroyed or non-existent?", "What is the wonder of the ancient world that is still intact?", "Has #2 survived a much longer time than #1?"], "evidence": [[[["Seven Wonders of the Ancient World-1"]], [["Great Pyramid of Giza-1"]], ["operation"]], [[["Seven Wonders of the Ancient World-1"]], [["Great Pyramid of Giza-1"]], ["operation"]], [[["Seven Wonders of the Ancient World-1"]], [["Great Pyramid of Giza-1"]], ["no_evidence", "operation"]]], "golden_sentence": [["The Colossus of Rhodes, the Lighthouse of Alexandria, the Mausoleum at Halicarnassus, the Temple of Artemis and the Statue of Zeus were all destroyed."], ["The Great Pyramid of Giza (also known as the Pyramid of Khufu or the Pyramid of Cheops) is the oldest and largest of the three pyramids in the Giza pyramid complex bordering present-day Giza in Greater Cairo, Egypt."]]}, {"qid": "896c4452a7c3ca99ff56", "term": "Hydrogen", "description": "Chemical element with atomic number 1", "question": "Hydrogen's atomic number squared exceeds number of Spice Girls?", "answer": false, "facts": ["Hydrogen is the first element and has an atomic number of one.", "To square a number, you multiply it by itself.", "The Spice Girls has five members."], "decomposition": ["What is the atomic number of hydrogen?", "How many people are in the Spice Girls band?", "Is the square of #1 greater than #2?"], "evidence": [[[["Hydrogen-1"]], [["Spice Girls-25"]], ["operation"]], [[["Hydrogen-1"]], [["Spice Girls-1"]], ["operation"]], [[["Hydrogen-1"]], [["Spice Girls-1"]], ["operation"]]], "golden_sentence": [["Hydrogen is the chemical element with the symbol H and atomic number\u00a01."], ["On 8 July 2016, Brown, Bunton, and Halliwell released a video celebrating the 20th anniversary of their first single \"Wannabe\", alongside a website under the name \"The Spice Girls - GEM\" and teased news from them as a three-piece."]]}, {"qid": "ebd793e8642207b63767", "term": "Fibonacci number", "description": "integer in the infinite Fibonacci sequence", "question": "If you have a serious injury in Bangladesh, would you probably dial a Fibonacci number?", "answer": false, "facts": ["The number for emergency services in Bangladesh is 999", "999 is not a Fibonacci sequence integer "], "decomposition": ["What number would you dial for help with an emergency in Bangladesh?", "Is #1 a Fibonacci number?"], "evidence": [[[["999 (emergency telephone number)-2"]], ["no_evidence"]], [[["Emergency telephone number-27"], "no_evidence"], [["Fibonacci-12"], "operation"]], [[["999 (emergency telephone number)-2"]], [["Fibonacci-12"], "operation"]]], "golden_sentence": [["Countries and territories using 999 include Bahrain, Bangladesh, Botswana, Eswatini, Ghana, Hong Kong, Ireland, Kenya, Macau, Malaysia, Mauritius, Poland, Qatar, Sudan, Saudi Arabia, Singapore, Trinidad and Tobago, the Seychelles, Uganda, the United Arab Emirates, the United Kingdom, and Zimbabwe."]]}, {"qid": "7676bdc3b59765df5ec2", "term": "Donkey", "description": "El burrito de sheck", "question": "Are Donkeys part of Christmas celebrations?", "answer": true, "facts": ["\"Dominic The Donkey\" is a popular Christmas song.", "\"Nestor The Ling Eared Christmas Donkey\" is a popular Christmas Movie."], "decomposition": ["Which animals have been popularly recognized as part of the Christmas culture?", "Are donkeys one of #1?"], "evidence": [[[["Nestor, the Long-Eared Christmas Donkey-2"]], ["operation"]], [[["Christmas-1"], "no_evidence"], [["Nativity of Jesus in art-12"], "no_evidence", "operation"]], [[["Nativity scene-20"]], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "e800338edbe1d78ce911", "term": "Medicine", "description": "The science and practice of the diagnosis, treatment, and prevention of physical and mental illnesses", "question": "Did polio medicine save the life of polio vaccine creator?", "answer": false, "facts": ["Jonas Salk developed the first polio vaccine.", "Jonas Salk died of a heart attack in 1995.", "Heart attacks are commonly treated with beta blockers."], "decomposition": ["Who created polio vaccine?", "Did #1 have his life saved by the use of polio vaccine?"], "evidence": [[[["Polio vaccine-3"]], [["Jonas Salk-43"]]], [[["Polio vaccine-30"]], [["Jonas Salk-43"], "no_evidence", "operation"]], [[["Polio vaccine-3"]], ["no_evidence"]]], "golden_sentence": [["An inactivated polio vaccine, developed a few years later by Jonas Salk, came into use in 1955."], [""]]}, {"qid": "d4125729eae6e69c340d", "term": "Latitude", "description": "The angle between zenith at a point and the plane of the equator", "question": "Is latitude required to determine the coordinates of an area?", "answer": true, "facts": ["Longitude is one of the required data points needed for determining coordinates.", "Latitude is the other angle required to determine coordinates of an area. "], "decomposition": ["What are the two sets of data points that determine coordinates of a location?", "Is latitude one of the answers to #1?"], "evidence": [[[["Geographic coordinate system-15"]], ["operation"]], [[["Geographic coordinate system-4"]], ["operation"]], [[["Geographic coordinate system-15"]], ["operation"]]], "golden_sentence": [["The grid formed by lines of latitude and longitude is known as a \"graticule\"."]]}, {"qid": "0ec653c4dbc405d2531b", "term": "Asiana Airlines", "description": "airline in South Korea", "question": "Can Harry Potter book a flight on Asiana Airlines?", "answer": false, "facts": ["Asiana Airlines is the second largest airline in South Korea", "Harry Potter is a fictional character"], "decomposition": ["Which universe does Harry Potter exist in?", "Does Asiana Airlines exist in #1?"], "evidence": [[[["Fiction-1", "Fictional universe of Harry Potter-1"]], [["Asiana Airlines-1", "Universe-8"], "operation"]], [[["Fictional universe of Harry Potter-1"]], [["Asiana Airlines-1"]]], [[["Harry Potter-1"]], [["Asiana Airlines-1"], "no_evidence", "operation"]]], "golden_sentence": [["", "The fictional universe of British author J. K. Rowling's Harry Potter series of fantasy novels comprises two distinct societies: the Wizarding World and the Muggle world."], ["Asiana Airlines Inc. (Korean:\u00a0\uc544\uc2dc\uc544\ub098\ud56d\uacf5; Hanja:\u00a0\uc544\uc2dc\uc544\ub098\u822a\u7a7a; RR:\u00a0Asiana Hanggong KRX: 020560; formerly Seoul Airlines) is South Korea's second-largest airline, behind Korean Air.", ""]]}, {"qid": "ae194dc50ac5f00d2103", "term": "Monkey", "description": "Animal of the \"higher primates\" (the simians), but excluding the apes", "question": "Would a monkey outlive a human being on average?", "answer": false, "facts": ["The average human lifespan is 79 years.", "The longest-lived monkey species have a lifespan about 45-50 years in captivity."], "decomposition": ["How long does the average human live?", "What is the longest lifespan of a monkey?", "Is #2 larger than #1?"], "evidence": [[[["Life expectancy-2"]], [["Monkey-20"], "no_evidence"], ["operation"]], [[["Old age-99"]], [["Night monkey-9"], "no_evidence"], ["operation"]], [[["Life expectancy-2"]], [["Little Mama-2"]], ["operation"]]], "golden_sentence": [["In societies with life expectancies of 30, for instance, a 40 year remaining timespan at age 5 may not be uncommon, but a 60 year one was."], ["Helper monkeys are usually trained in schools by private organizations, taking seven years to train, and are able to serve 25\u201330 years (two to three times longer than a guide dog)."]]}, {"qid": "36fc877d664515422f52", "term": "Olive oil", "description": "liquid fat extracted by pressing olives", "question": "Would Carmine's kitchen staff be panicked if they had no olive oil?", "answer": true, "facts": ["Carmine's is an Italian restaurant.", "Olive oil is a large component of a lot of Italian cooking."], "decomposition": ["What kind of food does Carmine's serve?", "What are essential ingredients in #1?", "Is olive oil listed in #2?"], "evidence": [[["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], [["Pizza in the United States-3"]], ["operation"]], [[["Carmine Romano-2"], "no_evidence"], [["Italian cuisine-16"]], ["operation"]]], "golden_sentence": []}, {"qid": "108d01cc9929cd10b481", "term": "Snow leopard", "description": "species of mammal", "question": "Can a snow leopard eat twice its own body weight?", "answer": true, "facts": ["The average snow leopard weighs 72 pounds.", "The favorite food of snow leopards is an ibex.", "The average weight of an ibex is 150 pounds."], "decomposition": ["How much do snow leopards weigh on average?", "What is a snow leopard's favorite food?", "How much does #2 weigh?", "Is #3 at least twice as much as #1?"], "evidence": [[[["Snow leopard-17"]], [["Snow leopard-31"]], [["Bharal-3"]], [["Bharal-3", "Snow leopard-17"]]], [[["Snow leopard-17"]], [["Snow leopard-31"]], [["Snow leopard-31"]], ["no_evidence", "operation"]], [[["Snow leopard-17"]], [["Snow leopard-31"]], [["Snow leopard-31"]], ["operation"]]], "golden_sentence": [["It weighs between 22 and 55\u00a0kg (49 and 121\u00a0lb), with an occasional large male reaching 75\u00a0kg (165\u00a0lb) and small female of under 25\u00a0kg (55\u00a0lb)."], ["It prefers prey ranging in weight from 36 to 76\u00a0kg (79 to 168\u00a0lb), but also hunts smaller mammals such as marmot, pika and vole species."], ["Body mass can range from 35 to 75\u00a0kg (77 to 165\u00a0lb)."], ["", ""]]}, {"qid": "6254c356eba1826a7cc0", "term": "Fibonacci number", "description": "integer in the infinite Fibonacci sequence", "question": "Are there five different single-digit Fibonacci numbers?", "answer": true, "facts": ["The first six numbers in the Fibonacci sequence are 1,1,2,3,5,8.", "Since 1 is doubled, there are only five different single digit numbers."], "decomposition": ["What are the single-digit numbers in the Fibonacci sequence?", "How many unique numbers are in #1?", "Does #2 equal 5?"], "evidence": [[[["Fibonacci-12"]], ["operation"], ["operation"]], [[["Random Fibonacci sequence-4"]], ["operation"], ["operation"]], [[["Fibonacci-12"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Fibonacci omitted the \"0\" included today and began the sequence with 1, 1, 2, ... ."]]}, {"qid": "f66862ae6168fdf27827", "term": "Sahara", "description": "desert in Africa", "question": "Can Spartina Patens thrive in the Sahara Desert?", "answer": false, "facts": ["Spartina Patens is a type of cordgrass that grows in salt marshes.", "Spartina Patens requires a marsh-like environment to thrive.", "The Sahara Desert is known for being dry and very hot."], "decomposition": ["What soil conditions are suitable for the growth of Spartina Patens?", "Is #1 likely to be present in the Sahara desert?"], "evidence": [[[["Spartina patens-2"]], [["Sahara-2"], "operation"]], [[["Spartina patens-2"]], ["operation"]], [[["Spartina patens-1"]], [["Sahara-1"]]]], "golden_sentence": [[""], [""]]}, {"qid": "e258ef3e92f88d653e01", "term": "Cucumber", "description": "species of plant", "question": "Is growing seedless cucumber good for a gardener with entomophobia?", "answer": true, "facts": ["Seedless cucumber fruit does not require pollination", "Cucumber plants need insects to pollinate them", "Entomophobia is a fear of insects"], "decomposition": ["What are people with Entomophobia fearful of?", "How do #1's usually help in the process of gardening?", "Do seedless cucumbers not require #2?"], "evidence": [[[["Entomophobia-1"]], [["Pollination-4"]], [["Cucumber-3"], "operation"]], [[["Entomophobia-1"]], [["Pollination-1"], "no_evidence"], [["Cucumber-3"], "operation"]], [[["Entomophobia-1"]], [["Cucumber beetle-1"]], [["Cucumber-4"], "operation"]]], "golden_sentence": [["More specific cases included apiphobia (fear of bees), myrmecophobia (fear of ants), and lepidopterophobia (fear of moths and butterflies)."], [""], ["A tendril emerges from cucumber vines to facilitate climbing A string lattice supports vine growth A bulb shaped cucumber hanging on the vine Organic Gardener Holding a Fresh Salad Cucumber A few cultivars of cucumber are parthenocarpic, the blossoms creating seedless fruit without pollination."]]}, {"qid": "fd913f0fdeedd4a23556", "term": "PayPal", "description": "Online financial services company based in San Jose, California", "question": "Would it be unusual to use paypal for drug deals?", "answer": true, "facts": ["Paypal prohibits the use of their platform for drugs or drug paraphernalia. ", "Using paypal leaves a digital footprint of any drug purchase."], "decomposition": ["Which kind of payments are prohibited on Paypal?", "Does #1 include payment for drug deals?"], "evidence": [[["no_evidence"], ["operation"]], [[["PayPal-55"]], ["operation"]], [[["Reception of WikiLeaks-37"], "no_evidence"], ["operation"]]], "golden_sentence": []}, {"qid": "67f7030eefb3bbddc425", "term": "Casio", "description": "Japanese electronics company", "question": "Is Casio's founding year a composite number?", "answer": true, "facts": ["Electronics company Casio was founded in 1946.", "A composite number is a number that can be divided by numbers other than 1 and itself.", "1946 can be divided by 278 and 7."], "decomposition": ["What condition(s) makes a number composite?", "When was Casio founded?", "Does #2 satisfy #1?"], "evidence": [[[["Composite number-1"]], [["Casio-2"]], ["operation"]], [[["Composite number-2"]], [["Casio-2"]], ["operation"]], [[["Condition number-4"], "no_evidence"], [["Casio-2"], "operation"], ["no_evidence"]]], "golden_sentence": [["A composite number is a positive integer that can be formed by multiplying two smaller positive integers."], ["Casio was established as Kashio Seisakujo in April 1946 by Tadao Kashio (\u6a2b\u5c3e\u5fe0\u96c4 1917\u20131993), an engineer specializing in fabrication technology."]]}, {"qid": "00ac6b32e25a17ebde6f", "term": "Al-Farabi", "description": "Philosopher in 10th century Central Asia", "question": "Would ISIS agree with Al-Farabi's religious sect?", "answer": false, "facts": ["The philosopher Al-Farabi was believed to be a Shia Muslim.", "ISIS is an extremist Sunni Muslim group.", "The Sunni and Shia are constantly at war\u2014Sunni often use car bombs, while Shia favor death squads."], "decomposition": ["What religious sect did Al-Farabi belong to?", "What religious sect does ISIS belong to?", "Do #1 and #2 avoid conflict with each other?"], "evidence": [[[["Al-Farabi-11"]], [["Islamic State of Iraq and the Levant-1"]], ["no_evidence", "operation"]], [[["Al-Farabi-11"]], [["Islamic State of Iraq and the Levant-1"]], [["Shia\u2013Sunni relations-4"]]], [[["Al-Farabi-11"], "no_evidence"], [["Islamic State of Iraq and the Levant-64"]], [["Shia\u2013Sunni relations-1"], "operation"]]], "golden_sentence": [["Henry Corbin writes that the evidence supports the opinion common in Iran that al-Farabi was a Shia Muslim."], ["The Islamic State of Iraq and the Levant (ISIL; /\u02c8a\u026as\u0259l, \u02c8a\u026as\u026al/), also known as the Islamic State of Iraq and Syria (ISIS; /\u02c8a\u026as\u026as/), officially known as the Islamic State (IS) and also known by its Arabic-language acronym Daesh (Arabic: \u062f\u0627\u0639\u0634\u200e, romanized:\u00a0D\u0101\u02bfish, IPA:\u00a0[\u02c8da\u02d0\u0295\u026a\u0283]), is a militant group and a former unrecognised proto-state that follows a fundamentalist, Salafi jihadist doctrine of Sunni Islam, however it is widely considered an anti-Islamic organisation."]]}, {"qid": "466a076c58dbe5791a6d", "term": "Lord Voldemort", "description": "Fictional character of Harry Potter series", "question": "Would half muggle wizards fear Lord Voldemort?", "answer": true, "facts": ["Lord Voldemort created a slur to describe half muggle wizards, calling them mudbloods.", "Lord Voldemort sought to purge the wizarding world of half muggle wizards through death."], "decomposition": ["What did Lord Voldemort seek to do to half muggle wizards?", "Is #1 enough to instill fear in them?"], "evidence": [[[["Lord Voldemort-2"]], ["no_evidence"]], [[["Lord Voldemort-2"]], ["operation"]], [[["Lord Voldemort-4", "Lord Voldemort-6"], "no_evidence"], [["Fear-1"], "operation"]]], "golden_sentence": [[""]]}, {"qid": "d5c0d47d48382da3b04e", "term": "Construction worker", "description": "tradesman, labourer, or professional employed in the physical construction of the built environment", "question": "Is a construction worker required to build a portfolio?", "answer": false, "facts": ["Construction workers build physical constructs, usually buildings or structures", "A portfolio is a collection of items of a similar type, including art, writing, or financial investments"], "decomposition": ["What is a portfolio?", "Who are people that builds #1?", "Is a construction worker among #2?"], "evidence": [[[["Career portfolio-1"]], [["Career portfolio-8"]], ["operation"]], [[["Artist's portfolio-1"]], [["Artist-1"]], [["Construction worker-1"]]], [[["Artist's portfolio-1", "Portfolio (finance)-1"]], [["Artist's portfolio-1", "Portfolio (finance)-2"]], ["operation"]]], "golden_sentence": [["Career portfolios serve as a proof of one's skills, abilities, and potential in the future."], ["A typical type of a portfolio is one used by artists."]]}, {"qid": "eba320a185feb91d2ccc", "term": "Lactobacillus", "description": "genus of bacteria", "question": "Are vinegar pickled cucumbers rich in lactobacillus?", "answer": false, "facts": ["Pickles made with vinegar are not probiotic and are simply preserved.", "Pickles made through a soak in a salt brine solution begin to ferment because of lactobacillus. "], "decomposition": ["What natural process is lactobacillus associated with?", "Do pickles made with vinegar undergo #1?"], "evidence": [[[["Lactobacillus-11"]], [["Pickling-2"]]], [[["Lactobacillus-11"]], [["Pickling-3"], "no_evidence", "operation"]], [[["Lactobacillus-11"]], [["Lactobacillus brevis-8"]]]], "golden_sentence": [[""], [""]]}, {"qid": "8d63eec9581e95857700", "term": "Ammonia", "description": "Chemical compound of nitrogen and hydrogen", "question": "Would a dog easily notice ammonia?", "answer": true, "facts": ["Ammonia has a characteristic pungent smell.", "Dogs have an extremely strong sense of smell, almost 40 times as sensitive as humans."], "decomposition": ["What common chemical has a characteristic pungent smell?", "What common pet has a sense of smell ten thousand to a hundred thousand times better than humans?", "Can #2 sense #1?"], "evidence": [[[["Ammonia-1"]], [["Dog anatomy-117"], "no_evidence"], ["operation"]], [[["Ammonia-1"], "no_evidence"], [["Dog anatomy-117"], "no_evidence"], ["operation"]], [[["Ammonia-1"]], [["Tracking (dog)-3"]], ["no_evidence"]]], "golden_sentence": [["A stable binary hydride, and the simplest pnictogen hydride, ammonia is a colourless gas with a characteristic pungent smell."], ["Dogs have roughly forty times more smell-sensitive receptors than humans, ranging from about 125\u00a0million to nearly 300\u00a0million in some dog breeds, such as bloodhounds."]]}, {"qid": "9b3a36c29b7506be7e8a", "term": "Curling", "description": "Team sport played on ice", "question": "Are the brooms from curling good for using on house floors?", "answer": false, "facts": ["Curling brooms are designed for use within the sport specifically. ", "Curling brooms do not have traditional bristle heads, and the heads are costly to replace. "], "decomposition": ["What are the characteristics of brooms used in curling?", "What are the characteristics of brooms used for house cleaning?", "Does #1 completely match #2?"], "evidence": [[[["Curling-31"]], [["Broom-2"]], ["operation"]], [[["Curling-31"], "no_evidence"], [["Broom-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Curling-31"]], [["Broom-1"]], ["operation"]]], "golden_sentence": [["Curling brushes may have fabric, hog hair, or horsehair heads."], ["The majority of brooms are somewhere in between, suitable for sweeping the floors of homes and businesses, soft enough to be flexible and to move even light dust, but stiff enough to achieve a firm sweeping action."]]}, {"qid": "a695c4c7e3e999447150", "term": "Christopher Walken", "description": "American actor", "question": "Is Christopher Walken close to achieving EGOT status?", "answer": false, "facts": ["EGOT refers to people that have won an Emmy, a Grammy, an Oscar, and a Tony Award.", "Christopher Walken won the Oscar in 1979 for Best Actor in a Supporting Role.", "Christopher Walken was nominated for two Tony Awards but has never won.", "Christopher Walken was nominated for an Emmy Award but has never won.", "Christopher Walken has never been nominated for a Grammy."], "decomposition": ["What awards are included in EGOT?", "What entertainment awards has Christopher Walken won?", "Do the awards listed in #2 belong to at least 3 different awards listed in #1?"], "evidence": [[[["EGOT (disambiguation)-1"]], [["Christopher Walken-1"]], [["Christopher Walken-1"]]], [[["Emmy Award-2"]], [["Christopher Walken-2", "Christopher Walken-43"]], ["operation"]], [[["EGOT (disambiguation)-1"]], [["Christopher Walken-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["EGOT is an acronym for \"Emmy, Grammy, Oscar, Tony\" in reference to persons who have won all four awards."], ["He was nominated for the same award and won BAFTA and Screen Actors Guild Awards for Catch Me If You Can."], ["Christopher Walken (born Ronald Walken, March 31, 1943) is an American actor, singer, comedian, director, producer, screenwriter, and dancer, who has appeared in more than 100 films and television programs, including Annie Hall (1977), The Deer Hunter (1978), The Dogs of War (1980), The Dead Zone (1983), A View to a Kill (1985), Batman Returns (1992), True Romance (1993), Pulp Fiction (1994), Antz (1998), Vendetta (1999), Sleepy Hollow (1999), Joe Dirt (2001), Catch Me If You Can (2002), Hairspray (2007), Seven Psychopaths (2012), the first three Prophecy films, The Jungle Book (2016), and Irreplaceable You (2018)."]]}, {"qid": "d46984965c52206429b4", "term": "Black Sea", "description": "Marginal sea of the Atlantic Ocean between Europe and Asia", "question": "Can sunlight travel to the deepest part of the Black Sea?", "answer": false, "facts": ["The Black Sea has a maximum depth of 2,212 meters", "Sunlight does not penetrate water below 1000 meters"], "decomposition": ["What is the maximum depth of the Black Sea?", "How deep can sunlight penetrate a sea?", "Is #1 less than #2?"], "evidence": [[[["Black Sea-2"]], [["Deep sea-1"]], ["operation"]], [[["Black Sea-2"]], [["Photic zone-5"]], ["operation"]], [[["Black Sea-2"]], [["Photic zone-5"]], ["operation"]]], "golden_sentence": [["The Black Sea has an area of 436,400\u00a0km2 (168,500\u00a0sq\u00a0mi) (not including the Sea of Azov), a maximum depth of 2,212\u00a0m (7,257\u00a0ft), and a volume of 547,000\u00a0km3 (131,000\u00a0cu\u00a0mi)."], ["The deep sea or deep layer is the lowest layer in the ocean, existing below the thermocline and above the seabed, at a depth of 1000 fathoms (1800 m) or more."]]}, {"qid": "4d100c0be72ad40b6829", "term": "Justin Bieber", "description": "Canadian singer-songwriter and actor", "question": "Did U.S. soldiers listen to Justin Bieber's Believe album during the Battle of Baghdad?", "answer": false, "facts": ["The Battle of Baghdad was the U.S. invasion of Baghdad in the year 2003.", "Justin Bieber's album Believe was released in 2012."], "decomposition": ["When did the Battle of Baghdad take place?", "When was the Justin Bieber album Believe released?", "Is #2 before #1?"], "evidence": [[[["Battle of Baghdad (2003)-1"]], [["Believe (Justin Bieber album)-1"]], ["operation"]], [[["Battle of Baghdad (2003)-1"]], [["Believe (Justin Bieber album)-1"]], ["operation"]], [[["Battle of Baghdad (2003)-1"]], [["Believe (Justin Bieber album)-1"]], ["operation"]]], "golden_sentence": [["The Battle of Baghdad, also known as the Fall of Baghdad, was a military invasion of Baghdad that took place in early April 2003, as part of the invasion of Iraq."], ["Believe is the third studio album by Canadian singer Justin Bieber, released on June 15, 2012, by Island Records."]]}, {"qid": "8191e0a247e0de6561af", "term": "Honey bee", "description": "Eusocial flying insect of genus Apis, producing surplus honey", "question": "Can a honey bee sting a human more than once?", "answer": false, "facts": ["Human skin is tough, and the bee's stinger gets lodged in the skin.", "The stinger becomes separated from the bee which dies soon after."], "decomposition": ["What happens to a bee's stinger when it stings a human?", "Are bees able to survive if #1 happens?"], "evidence": [[[["Bee sting-6"], "no_evidence"], ["no_evidence", "operation"]], [[["Stinger-7"]], [["Stinger-7"]]], [[["Honey bee-61"]], ["operation"]]], "golden_sentence": [["Although it is widely believed that a worker honey bee can sting only once, this is a partial misconception: although the stinger is in fact barbed so that it lodges in the victim's skin, tearing loose from the bee's abdomen and leading to its death in minutes, this only happens if the skin of the victim is sufficiently thick, such as a mammal's."]]}, {"qid": "70eadb8b367a183d6115", "term": "Tailor", "description": "person who makes, repairs, or alters clothing professionally, typically men's clothing", "question": "Would an expensive tailor use adhesive to create a shorter hem on slacks?", "answer": false, "facts": ["Adhesive hems are usually performed at home with a DIY kit.", "Professionally hemmed pants are created using a needle and thread."], "decomposition": ["Is a tailor professional when creating hems?", "Is using adhesive a professional way to create hems?", "Are #1 and #2 the same?"], "evidence": [[[["Tailor-1"]], [["Adhesive-14"], "no_evidence"], ["operation"]], [[["Hem-2"]], [["Hem-3"], "operation"], ["operation"]], [[["Tailor-1"]], [["Adhesive-2"]], ["operation"]]], "golden_sentence": [["A tailor is a person who makes, repairs, or alters clothing professionally, especially suits and men's clothing."], [""]]}, {"qid": "501a04cc323394ca0bd8", "term": "Clark Gable", "description": "American actor", "question": "Did Clark Gable marry more women once than Richard Burton?", "answer": true, "facts": ["Richard Burton was married to Elizabeth Taylor twice. ", "Richard Burton married Sally Burton, Suzy Hunt, and Sybil Williams once.", "Clark Gable was married to five different women, one time each."], "decomposition": ["How many different women was Richard Burton married to?", "How many different wives did Clark Gable have?", "Is #2 more than #1?"], "evidence": [[[["Richard Burton-57"]], [["Clark Gable-78"]], ["operation"]], [[["Richard Burton-57"]], [["Clark Gable-14", "Clark Gable-34", "Clark Gable-39"]], ["operation"]], [[["Richard Burton-57"]], ["no_evidence"], ["no_evidence"]]], "golden_sentence": [["Burton was married five times, twice consecutively to Taylor."], ["Gable was married five times."]]}, {"qid": "8ea7b38bc4cc270f2db7", "term": "Lolcat", "description": "image combining a photograph of a cat with text intended to contribute humour", "question": "Did the 40th president of the United States forward lolcats to his friends?", "answer": false, "facts": ["The 40th president of the United States was Ronald Reagan", "Ronald Reagan died in 2004", "The first recorded use of the term lolcat occurred in 2006"], "decomposition": ["Who was the 40th president of the United States?", "In what year did #1 die?", "In what year did the first lolcat appear?", "Is #3 before or the same as #2?"], "evidence": [[[["Ronald Reagan-1"]], [["Ronald Reagan-120"]], [["Lolcat-4"]], ["operation"]], [[["Ronald Reagan-1"]], [["Ronald Reagan-1"]], [["Lolcat-4"]], ["operation"]], [[["Ronald Reagan-1"]], [["Ronald Reagan-1"]], [["Lolcat-4"]], ["operation"]]], "golden_sentence": [["Ronald Wilson Reagan (/\u02c8re\u026a\u0261\u0259n/; February 6, 1911 \u2013 June 5, 2004) was an American politician who served as the 40th president of the United States from 1981 to 1989 and became a highly influential voice of modern conservatism."], ["Reagan died of pneumonia, complicated by Alzheimer's disease, at his home in the Bel Air district of Los Angeles, California, on the afternoon of June 5, 2004."], ["Lev Grossman of Time wrote that the oldest known example \"probably dates to 2006\", but later corrected himself in a blog post where he recanted his statement based on the anecdotal evidence readers had sent him, placing the origin of \"Caturday\" and many of the images now known by a few as \"lolcats\" in early 2005."]]}, {"qid": "9a4418693d689ddef54a", "term": "Daily Mirror", "description": "British daily tabloid newspaper owned by Reach plc.", "question": "Did William Shaespeare read the Daily Mirror?", "answer": false, "facts": ["The Daily Mirror is a British tabloid founded in 1903.", "William Shakespeare died in 1616."], "decomposition": ["When did William Shakespeare die?", "When was the Daily Mirror founded?", "Is #2 before #1?"], "evidence": [[[["William Shakespeare-17"]], [["Daily Mirror-1"]], ["operation"]], [[["William Shakespeare-1"]], [["Daily Mirror-1"]], ["operation"]], [[["William Shakespeare-1"]], [["Daily Mirror-1"]], ["operation"]]], "golden_sentence": [["Shakespeare died on 23 April 1616, at the age of 52."], ["The Daily Mirror is a British national daily tabloid newspaper founded in 1903."]]}, {"qid": "f3d5ea62f49db4b318eb", "term": "Comic book", "description": "Publication of comics art", "question": "Does Disney own a major comic book publisher?", "answer": true, "facts": ["The three biggest comic book publishers are DC, Marvel, and IDW.", "Disney has owned Marvel since 2007."], "decomposition": ["What are the top three biggest comic book publishers?", "What comic book publishers does Disney own?", "Is there any overlap between #1 and #2?"], "evidence": [[[["Big Two Comics-1"], "no_evidence"], [["Marvel Comics-1"]], ["operation"]], [[["American comic book-39", "Marvel Comics-1"], "no_evidence"], [["Marvel Comics-1"]], ["operation"]], [[["Comic book-2"]], [["Marvel Entertainment-1"]], ["operation"]]], "golden_sentence": [["Big Two Comics in the American comic book industry refers to the two largest publishers: DC Comics, a subsidiary of Warner Bros., known as the publisher of books featuring Superman, Batman, Wonder Woman, The Flash, Green Lantern and Green Arrow; and Marvel Comics, a subsidiary of The Walt Disney Company, known as the publisher of books featuring Spider-Man, the X-Men, the Avengers, the Hulk, and the Fantastic Four."], [""]]}, {"qid": "d800f521133136c9c45f", "term": "Jackfruit", "description": "species of plant", "question": "Can jackfruit be used as a weapon?", "answer": true, "facts": ["Jackfruit is the fruit of a species of plant called the Jacktree.", "Jackfruit can weigh up to one hundred and twenty pounds.", "Jackfruit is covered in little spikes.", "Jackfruit can be thrown or flung at an enemy.", "A weapon is a thing that is used to cause bodily harm."], "decomposition": ["What are the prominent physical features of a jackfruit?", "Does #1 make it a suitable weapon?"], "evidence": [[[["Jackfruit-2"]], ["operation"]], [[["Jackfruit-12"]], [["Tubercle-1"], "operation"]], [[["Jackfruit-12"]], [["Jackfruit-12", "Weapon-2"], "no_evidence"]]], "golden_sentence": [["The jackfruit is a multiple fruit composed of hundreds to thousands of individual flowers, and the fleshy petals of the unripe fruit are eaten."]]}, {"qid": "f453fe513b607e6976a0", "term": "James Bond", "description": "Media franchise about a British spy", "question": "Do the James Bond and Doctor Who series have a similarity in format?", "answer": true, "facts": ["The character of James Bond has been played by numerous actors. ", "The character of The Doctor from Doctor Who has been played by many actors."], "decomposition": ["Who has played James Bond?", "Who has played the Doctor? ", "Are multiple actors listed for #1 and #2?"], "evidence": [[[["Portrayal of James Bond in film-4"]], [["The Doctor (Doctor Who)-111"]], ["operation"]], [[["Portrayal of James Bond in film-4"]], [["The Doctor (Doctor Who)-1"]], ["operation"]], [[["James Bond-2"], "no_evidence"], [["The Doctor (Doctor Who)-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["Daniel Craig is the incumbent Bond in the long-running Eon series, and will play the part for a fifth time in the latest film, No Time to Die, to be released in November 2020."], ["Patrick Troughton with Colin Baker in The Two Doctors."]]}, {"qid": "bb9b502ef47832f20ddc", "term": "Shiva", "description": "One of the principal deities of Hinduism.", "question": "Does Sam Harris worship Shiva?", "answer": false, "facts": ["Sam Harris is an atheist.", "Atheism is, in the broadest sense, an absence of belief in the existence of deities."], "decomposition": ["What is Sam Harris' religious affiliation?", "Does a #1 worship any gods?"], "evidence": [[[["Sam Harris-1"]], [["Atheism-1"]]], [[["Sam Harris-1"]], [["Atheism-1"]]], [[["Sam Harris-12"]], ["operation"]]], "golden_sentence": [[""], ["In an even narrower sense, atheism is specifically the position that there are no deities."]]}, {"qid": "4844082807382960069d", "term": "Lord Voldemort", "description": "Fictional character of Harry Potter series", "question": "Would Lord Voldemort hypothetically be an effective fighter after Final Fantasy silence is cast?", "answer": false, "facts": ["Lord Voldemort is a powerful wizard from the Harry Potter Series.", "Lord Voldemort casts magical curses and charms on his enemies.", "Silence spell in Final Fantasy mutes the enemies spells.", "Mute makes it impossible for characters to cast any spells."], "decomposition": ["What does Lord Voldemort use in combat against enemies?", "What would Lord Voldemort have to do in order to cast #1?", "Which ability does the silence spell in Final Fantasy affect?", "Can all of #2 still be done when #3 is gone?"], "evidence": [[[["Lord Voldemort-15"], "no_evidence"], [["Incantation-1"], "no_evidence"], [["Speech-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Lord Voldemort-28"], "no_evidence"], [["Lord Voldemort-29"], "no_evidence"], ["no_evidence"], ["operation"]], [[["Lord Voldemort-2", "Lord Voldemort-30"]], ["no_evidence"], ["no_evidence"], ["operation"]]], "golden_sentence": [["When Harry willingly walks into Voldemort's camp in the Forbidden Forest, Voldemort strikes him down with the Elder Wand."], [""], [""]]}, {"qid": "738616d861c2fedc7ce7", "term": "Astrology", "description": "Pseudoscience claiming celestial objects influence human affairs", "question": "Does Capricorn astrology symbol have all of the parts of a chimera?", "answer": false, "facts": ["The Capricorn astrology symbol is a sea goat which consists of: a goat, and a fish.", "A chimera is a legendary beast that is made up of: a lion, a goat, and a snake."], "decomposition": ["What are the parts of the capricorn symbol?", "What is the chimera made up of?", "Does #1 include all of #2?"], "evidence": [[[["Capricorn (astrology)-1"]], [["Chimera (mythology)-1"]], ["operation"]], [[["Capricorn (astrology)-2"]], [["Chimera (mythology)-1"]], ["operation"]], [[["Capricorn (astrology)-2"]], [["Chimera (mythology)-3"]], ["operation"]]], "golden_sentence": [["In astrology, Capricorn is considered an earth sign, negative sign, and one of the four cardinal signs."], ["The Chimera (/k\u026a\u02c8m\u026a\u0259r\u0259/ or /ka\u026a\u02c8m\u026a\u0259r\u0259/, also Chimaera (Chim\u00e6ra); Greek: \u03a7\u03af\u03bc\u03b1\u03b9\u03c1\u03b1, Ch\u00edmaira \"she-goat\") according to Greek mythology, was a monstrous fire-breathing hybrid creature of Lycia in Asia Minor, composed of the parts of more than one animal."]]}, {"qid": "99b66bd3bdae8cd165c5", "term": "Capsaicin", "description": "chemical compound", "question": "Is Black Lives Matter connected with capsaicin?", "answer": true, "facts": ["Black Lives Matter has held numerous protests", "Protesters at Black Lives Matter events have had pepper spray used against them by police", "Capsaicin is the main ingredient of pepper spray"], "decomposition": ["What are the common practical applications of the capsaicin compund?", "What kind of activities does the Black Lives Matter movement engage in?", "Is any of #1 relevant to any of #2?"], "evidence": [[[["Capsaicin-11", "Capsaicin-5", "Capsaicin-7"]], [["Black Lives Matter-20"]], [["Pepper spray-1", "Riot control-1"], "operation"]], [[["Capsaicin-2"]], [["Black Lives Matter-30"]], ["operation"]], [[["Capsaicin-11"]], [["Movement for Black Lives-6"]], [["Movement for Black Lives-7"], "operation"]]], "golden_sentence": [["When the spray comes in contact with skin, especially eyes or mucous membranes, it produces pain and breathing difficulty, discouraging protestors and assailants.", "Because of the burning sensation caused by capsaicin when it comes in contact with mucous membranes, it is commonly used in food products to provide added spice or \"heat\" (piquancy), usually in the form of spices such as chili powder and paprika.", "It may be applied in cream form for the temporary relief of minor aches and pains of muscles and joints associated with arthritis, backache, strains and sprains, often in compounds with other rubefacients."], ["BLM generally engages in direct action tactics that make people uncomfortable enough that they must address the issue."], ["", ""]]}, {"qid": "9086bd81f6b7a4e3bb75", "term": "Breast", "description": "Region of the torso of a primate containing the mammary gland", "question": "Do people in middle school usually get breast exams?", "answer": false, "facts": ["Women should begin giving themselves breast exams at the age of 20.", "Middle school students are usually preteens or young teens."], "decomposition": ["What age do people usually get breast exams?", "How old are the students in Middle school in the US?", "Is #1 the same as #2?"], "evidence": [[[["Mammography-1", "Mammography-3"]], [["Secondary education in the United States-1"]], ["operation"]], [[["Mammography-1", "Mammography-3"]], [["Middle school-88"]], ["operation"]], [[["Breast cancer screening-3"]], [["Secondary education in the United States-1"]], ["operation"]]], "golden_sentence": [["", "For the average woman, the U.S. Preventive Services Task Force recommends (2016) mammography every two years between the ages of 50 and 74, concluding that \"the benefit of screening mammography outweighs the harms by at least a moderate amount from age 50 to 74 years and is greatest for women in their 60s\"."], ["The first is the ISCED lower secondary phase, a junior high school or middle school for students grade 6 (age 11\u201312) through grade 8 (age 13\u201314)."]]}, {"qid": "f1c39561105f6b9188ca", "term": "Astrophotography", "description": "specialized type of photography for recording images of astronomical objects and large areas of the night sky", "question": "Is it difficult to conduct astrophotography in the summer in Sweden?", "answer": true, "facts": ["Astrophotography is used to photograph the night sky.", "Swedish summers have short nights."], "decomposition": ["What does Astrophotography take photos of?", "Are #1's short in the summers of Sweden?"], "evidence": [[[["Astrophotography-1"]], [["Sweden-59"], "operation"]], [[["Astrophotography-1"]], [["Sweden-56"], "operation"]], [[["Astrophotography-1"]], [["Tourism in Sweden-6"]]]], "golden_sentence": [["Astrophotography is photography of astronomical objects, celestial events, and areas of the night sky."], ["When hot continental air hits the country, the long days and short nights frequently bring temperatures up to 30\u00a0\u00b0C (86\u00a0\u00b0F) or above even in coastal areas."]]}, {"qid": "f6cf71a30a67d7e8b8d6", "term": "Tokyo Tower", "description": "observation tower", "question": "Did Tokyo Tower designers appreciate Stephen Sauvestre?", "answer": true, "facts": ["Tokyo Tower is a communications tower in Japan, built in 1958, that was inspired by the Eiffel Tower.", "Stephen Sauvestre was the architect of the the Eiffel Tower which was competed in 1889."], "decomposition": ["Which architectural design is Stephen Sauvestre famous for?", "Was #1 influential in the design of the Tokyo Tower?"], "evidence": [[[["Stephen Sauvestre-1"]], [["Tokyo Tower-1"]]], [[["Stephen Sauvestre-3"]], ["operation"]], [[["Stephen Sauvestre-1"]], [["Tokyo Tower-1"]]]], "golden_sentence": [["He is notable for being one of the architects contributing to the design of the world-famous Eiffel Tower, built for the 1889 Universal Exposition in Paris, France."], [""]]}, {"qid": "2b49a76700775fd2c326", "term": "Augustus", "description": "First emperor of the Roman Empire", "question": "Would a hippie hypothetically be bummed out by Augustus's Pax Romana?", "answer": false, "facts": ["A hippie was a member of the counterculture movement of the 1960s.", "One of the most prevalent hippie statements is peace and love.", "The Pax Romana was a near 200 year era of peace in the Roman Empire that began during the reign of Augustus.", "Augustus had several loves, including three wives."], "decomposition": ["What ideals did hippies promote?", "What was the defining attribute of Pax Romana?", "Is #2 not a subset of #1?"], "evidence": [[[["Hippie-13"]], [["Pax Romana-1"]], ["operation"]], [[["17 Hippies-5"], "no_evidence"], [["Pax Romana-4"], "operation"], ["no_evidence"]], [[["Hippie-13"]], [["Pax Romana-1"]], ["operation"]]], "golden_sentence": [["Hippies rejected established institutions, criticized middle class values, opposed nuclear weapons and the Vietnam War, embraced aspects of Eastern philosophy, championed sexual liberation, were often vegetarian and eco-friendly, promoted the use of psychedelic drugs which they believed expanded one's consciousness, and created intentional communities or communes."], ["The Pax Romana (Latin for \"Roman Peace\") is a roughly 200-year-long period in Roman history which is identified with increased and sustained inner hegemonial peace and stability (though not meaning without wars, expansion and revolts)."]]}, {"qid": "9df260c72825e53adf67", "term": "Oyster", "description": "salt-water bivalve mollusc", "question": "Can oysters be preserved without refrigeration? ", "answer": true, "facts": ["In some types of Korean kimchi, oysters are placed between the leaves of nappa cabbage. ", "Many grocery stores carry canned oysters in the shelf stable section. "], "decomposition": ["How are oysters preserved in various types of Korean kimchi?", "What are the common methods of preserving oysters in grocery stores?", "Do any of #1 or #2 not require refrigeration?"], "evidence": [[[["Kimchi-28"], "no_evidence"], [["Oyster-57", "Oyster-61"]], ["operation"]], [[["Korean cuisine-25"], "no_evidence"], ["no_evidence"], ["operation"]], [[["Oyster-48"], "no_evidence"], [["Oyster-35"]], ["operation"]]], "golden_sentence": [[""], ["", ""]]}, {"qid": "d4f876eb1bfc97cc98f4", "term": "The Atlantic", "description": "Magazine and multi-platform publisher based in Washington, D.C.", "question": "Could the Atlantic readers fill 500 battalions?", "answer": false, "facts": ["A battalion is a military unit of measurement that includes 1000 soldiers.", "As of 2018 The Atlantic has a circulation of 478,534."], "decomposition": ["What is the number of readers (copies in circulation) of The Atlantic magazines?", "What is the average number of soldiers in a battalion in the US?", "Is #1 at least equal to 500 times #2?"], "evidence": [[["no_evidence"], [["Battalion-1"]], ["operation"]], [[["The Atlantic-24"]], [["Battalion-21"], "no_evidence"], ["operation"]], [[["The Atlantic-24"]], [["Battalion-21"]], ["operation"]]], "golden_sentence": [["Typically a battalion consists of 300 to 800 soldiers and is divided into a number of companies."]]}, {"qid": "96fa36925ad415aad587", "term": "Hollywood", "description": "District in Los Angeles, California, United States", "question": "Is it normally unnecessary to wear a coat in Hollywood in July?", "answer": true, "facts": ["The average high temperature in Hollywood in July is 77.2\u00b0F.", "The average low temperature in Hollywood in July is 61.5\u00b0F.", "A coat is a garment worn on the upper body for warmth."], "decomposition": ["What is the average high temperature in Hollywood in July?", "What is the average low temperature in Hollywood in July?", "What temperature does one usually wear a coat?", "Is #3 outside of #1 to #2?"], "evidence": [[[["Hollywood-1", "Los Angeles-36"]], [["Los Angeles-34"]], [["Overcoat-1", "Winter-5"]], ["operation"]], [[["Climate of Los Angeles-5", "Climate of Los Angeles-7"], "no_evidence"], [["Climate of Los Angeles-7"], "no_evidence"], [["Winter clothing-1"]], ["operation"]], [[["Los Angeles-35"], "no_evidence"], [["Los Angeles-35"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["", "For example, the average July maximum temperature at the Santa Monica Pier is 75\u00a0\u00b0F (24\u00a0\u00b0C) whereas it is 95\u00a0\u00b0F (35\u00a0\u00b0C) in Canoga Park, 15 miles (24\u00a0km) away."], [""], ["", "A similar but less extreme effect is found in Europe: in spite of their northerly latitude, the British Isles have not a single non-mountain weather station with a below-freezing mean January temperature."]]}, {"qid": "764c453c8d4ae8385763", "term": "Toyota Hilux", "description": "Series of light commercial vehicles produced by the Japanese car-manufacturer Toyota.", "question": "Can the Toyota Hilux tip the scales against Mr. Ed?", "answer": true, "facts": ["The current generation of Toyota Hilux weighs at least 4,310 lbs", "Mr. Ed was portrayed by an adult horse", "The average adult horse weighs up to 2,000 lbs"], "decomposition": ["What does a Toyota Hilux weigh?", "What does an adult horse weigh?", "Is #1 greater than #2?"], "evidence": [[["no_evidence"], [["Horse-13"]], ["no_evidence", "operation"]], [["no_evidence"], [["Horse-13"]], ["operation"]], [[["Toyota Hilux-1"], "no_evidence"], [["Horse-12"]], ["operation"]]], "golden_sentence": [["They can weigh from about 700 to 1,000 kilograms (1,540 to 2,200\u00a0lb)."]]}, {"qid": "f01ab9fe251cba818f97", "term": "Cerebral palsy", "description": "A group of disorders affecting the development of movement and posture, often accompanied by disturbances of sensation, perception, cognition, and behavior. It results from damage to the fetal or infant brain.", "question": "Could a young Wizard of Oz Scarecrow have gotten Cerebral palsy?", "answer": false, "facts": ["Cerebral palsy is a disease that results from damage to a young person's brain.", "The Scarecrow in the Wizard of Oz did not have a brain and was on a quest to get one."], "decomposition": ["Which organ of the body can cerebral palsy be traced back to?", "Did the Scarecrow in Wizard of Oz initially have #1 ?"], "evidence": [[[["Cerebral palsy-5"]], [["Scarecrow (Oz)-3"], "operation"]], [[["Cerebral palsy-2"]], [["Scarecrow (Oz)-1"], "operation"]], [[["Cerebral palsy-2"]], [["The Wizard of Oz (1939 film)-6"]]]], "golden_sentence": [["Cerebral palsy is defined as \"a group of permanent disorders of the development of movement and posture, causing activity limitation, that are attributed to non-progressive disturbances that occurred in the developing fetal or infant brain.\""], ["Indeed, both believe they have neither."]]}, {"qid": "b57d410e98900008dfbd", "term": "Glenn Beck", "description": "American talk radio and television host", "question": "Would Glen Beck and Stephen Colbert be likely to tour together?", "answer": false, "facts": ["Glenn Beck is a right wing commentator known for strong opinions and serious tone.", "Stephen Colbert is a liberal political commentator who takes a comedic approach to his work."], "decomposition": ["What political party does Glen Beck support?", "What political party does Stephen Colbert support?", "Is #1 the same as #2?"], "evidence": [[[["Glenn Beck-49"]], [["Stephen Colbert-62"]], [["Stephen Colbert-62"], "operation"]], [[["Glenn Beck-46"]], [["Stephen Colbert-62"]], ["operation"]], [[["Glenn Beck-46"]], [["Stephen Colbert-3"]], ["operation"]]], "golden_sentence": [["On March 18, 2015, Beck officially announced that he had left the Republican Party, saying that the GOP had failed to effectively stand against the president on Obamacare and immigration reform, and because of the GOP establishment's opposition to insurgent lawmakers such as Mike Lee and Ted Cruz."], ["In an interview at the Kennedy School of Government at Harvard Institute of Politics, he said he has \"no problems with Republicans, just Republican policies\"."], [""]]}, {"qid": "a620ff6594c038f9ff2d", "term": "Maroon 5", "description": "American pop punk band", "question": "Could Maroon 5 have hypothetically held a concert at Roman Colosseum?", "answer": true, "facts": ["The Roman Colosseum had a capacity of 87,000 people. ", "Maroon 5 has held concerts at Brazil's Allianz Parque, which has a capacity of close to 44,000.", "Almost 30,000 people attended Maroon 5's 2015 Madison Square Garden concert over two days."], "decomposition": ["How many spectators could the Roman Colosseum hold?", "How many people were in attendance at Maroon 5's largest concert?", "Is #1 greater than #2?"], "evidence": [[[["Colosseum-1"]], ["no_evidence"], ["operation"]], [[["Colosseum-1"]], [["Super Bowl LIII halftime show-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Colosseum-2"]], [["Maroon V Tour-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["Built of travertine limestone, tuff (volcanic rock), and brick-faced concrete, it was the largest amphitheatre ever built at the time and held 50,000 to 80,000 spectators."]]}, {"qid": "b34a1da22d946a417c13", "term": "Junk (ship)", "description": "Type of boat", "question": "Does Carmen Electra own a junk?", "answer": false, "facts": ["A junk is a boat.", "Boats are sailed on open water.", "Carmen Electra has a fear of open water."], "decomposition": ["What is another name for a junk?", "Where does one use #1?", "Does Carmen Electra like being in #2?"], "evidence": [[[["Junk (ship)-1"]], [["Junk (ship)-6"]], ["no_evidence", "operation"]], [[["Junk (ship)-1"]], [["Junk (ship)-1"]], [["Carmen Electra-1"], "no_evidence", "operation"]], [[["Junk (ship)-1"]], [["Sailing ship-1"]], ["no_evidence"]]], "golden_sentence": [["A junk is a type of Chinese sailing ship."], [""]]}, {"qid": "91e556c0664b80bb7048", "term": "Ashland, Oregon", "description": "City in Oregon, United States", "question": "Is 2018 Ashland, Oregon population inadequate to be a hypothetical military division?", "answer": false, "facts": ["The 2018 population of Ashland Oregon was 21,263 people.", "The number of soldiers in a military division is between 10,000 and 25,000 people."], "decomposition": ["What was the population of Ashland, Oregon in 2018?", "How many soldiers are in a military division?", "Is #1 less than the minimum in #2?"], "evidence": [[[["Ashland, Oregon-1"], "no_evidence"], [["Division (military)-16"], "no_evidence"], ["no_evidence"]], [[["Ashland, Oregon-1"]], [["Division (military)-1"]], ["operation"]], [[["Ashland, Oregon-1"]], [["Division (military)-1"]], ["operation"]]], "golden_sentence": [["The city's population was 20,078 at the 2010 census and was estimated to be 21,263 as of 2018."], [""]]}, {"qid": "ebd6f9b3225d079b7cf9", "term": "Audiobook", "description": "recording of a text being read", "question": "Do Youtube viewers get unsolicited audiobook advice often?", "answer": true, "facts": ["Audible is one of the most common sponsors for Youtubers to have.", "Audible is an audiobook subscription service. ", "Audible ads typically involve discussing a book that the speaker has recently listened to."], "decomposition": ["What company is one of the most common sponsors for Youtubers to have?", "What do the ads for #1 typically involve?", "Does #2 involve someone giving audiobook advice?"], "evidence": [[[["Audible (store)-1"], "no_evidence"], [["Audible (store)-11"], "no_evidence"], ["operation"]], [[["YouTube-3"], "no_evidence"], [["YouTube-3"]], ["no_evidence"]], [[["Audible (store)-16"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Through its production arm, Audible Studios, Audible has also become the world's largest producer of downloadable audiobooks."], [""]]}, {"qid": "3b99c6f24412cada9d18", "term": "Osama bin Laden", "description": "Co-founder of al-Qaeda", "question": "Did Osama bin Laden likely abstain from alcohol?", "answer": true, "facts": ["Osama bin Laden belonged to the religion of Islam.", "Islam prohibits the consumption of alcohol."], "decomposition": ["What religion was Osama bin Laden?", "Does #1 prohibit consumption of alcohol?"], "evidence": [[[["Osama bin Laden-10"]], [["Alcohol law-14"], "operation"]], [[["Osama bin Laden-10"]], [["Islamic culture-45"]]], [[["Osama bin Laden-16"]], [["Islamic dietary laws-9"], "operation"]]], "golden_sentence": [["Bin Laden was raised as a devout Sunni Muslim."], [""]]}, {"qid": "5dc3a6b846cab70d9aed", "term": "Garlic", "description": "species of plant", "question": "Are fresh garlic cloves as easy to eat as roasted garlic cloves?", "answer": false, "facts": ["Allicin is the component of garlic that makes it 'spicy' feeling in the mouth.", "When garlic is cooked, the Allicin in it is removed."], "decomposition": ["What makes garlic uncomfortable to eat?", "Does #1 remain after cooking?"], "evidence": [[[["Garlic-31", "Garlic-32"]], [["Garlic-34", "Garlic-35"]]], [[["Garlic-22"]], [["Garlic-22"], "operation"]], [[["Garlic-22"]], [["Garlic-22"], "operation"]]], "golden_sentence": [["", ""], ["", ""]]}, {"qid": "48beff0eb1f864f7ac70", "term": "JPEG", "description": "Lossy compression method for reducing the size of digital images", "question": "Would JPEG be a good format for saving an image of Da Vinci's Vitruvian Man?", "answer": false, "facts": ["JPEG is not well suited for line drawings and other textual or iconic graphics, where the sharp contrasts between adjacent pixels can cause noticeable artifacts. ", "Da Vinci's Vitruvian Man is a line drawing done in pen and ink."], "decomposition": ["What kind of details are portrayed in Da Vinci's Vitruvian Man?", "Are JPEGs an ideal format for saving pictures containing #1?"], "evidence": [[[["Vitruvian Man-3"]], [["JPEG-29"]]], [[["Vitruvian Man-3"]], [["JPEG-1", "JPEG-110"]]], [[["Vitruvian Man-2"], "no_evidence"], [["JPEG-1"], "no_evidence", "operation"]]], "golden_sentence": [["This image demonstrates the blend of mathematics and art during the Renaissance and demonstrates Leonardo's deep understanding of proportion."], [""]]}, {"qid": "b0d2b446867d5ab5799b", "term": "Lionel Richie", "description": "American singer-songwriter, musician, record producer and actor", "question": "Did Lionel Richie ever have dinner with Abraham Lincoln?", "answer": false, "facts": ["Abraham Lincoln died in 1865.", "Lionel Richie was born in 1949."], "decomposition": ["When did Abraham Lincoln die?", "When was Lionel Richie born?", "Is #2 before #1?"], "evidence": [[[["Abraham Lincoln-1"]], [["Lionel Richie-1"]], ["operation"]], [[["Outline of Abraham Lincoln-2"]], [["Lionel Richie-1"]], ["operation"]], [[["Abraham Lincoln-1"]], [["Lionel Richie-1"]], ["operation"]]], "golden_sentence": [["Abraham Lincoln (/\u02c8l\u026a\u014bk\u0259n/; February 12, 1809 \u2013 April 15, 1865) was an American statesman and lawyer who served as the 16th president of the United States (1861\u20131865)."], ["Lionel Brockman Richie Jr. (born June 20, 1949) is an American singer, songwriter, composer, multi-instrumentalist, record producer and actor."]]}, {"qid": "e4c43167391cf62bb1e6", "term": "Hair", "description": "protein filament that grows from follicles found in the dermis, or skin", "question": "Is it safe to eat hair?", "answer": true, "facts": ["Hair is made of keratin.", "Food manufacturers use L-cysteine as a food additive.", "L-cysteine is made from keratin."], "decomposition": ["What is hair made of?", "What else is made from #1?", "Are any of #2 used in food production?"], "evidence": [[[["Hair-2"]], [["Alpha-keratin-1"]], ["no_evidence", "operation"]], [[["Hair-6"]], [["Hair-6"]], [["Food-1"]]], [[["Hair-1"]], [["Beef-1"]], [["Hamburger-1"], "operation"]]], "golden_sentence": [["Most common interest in hair is focused on hair growth, hair types, and hair care, but hair is also an important biomaterial primarily composed of protein, notably alpha-keratin."], ["Due to its tightly wound structure, it can function as one of the strongest biological materials and has various uses in mammals, from predatory claws to hair for warmth."]]}, {"qid": "ad1f8a41208a942f75a9", "term": "Washington Monument", "description": "Obelisk in Washington, D.C.", "question": "Did Sojourner Truth use the elevator at the Washington Monument?", "answer": false, "facts": ["The Washington Monument was opened to the public in October 1888.", "Sojourner Truth died November 26, 1883. "], "decomposition": ["When did Sojourner Truth pass away?", "When was the Washington Monument opened to the public?", "Is #2 before #1?"], "evidence": [[[["Sojourner Truth-1"]], [["Washington Monument-26"]], [["Washington Monument-26"], "operation"]], [[["Sojourner Truth-1"]], [["Washington Monument-2"]], ["operation"]], [[["Sojourner Truth-1"]], [["Washington Monument-2"]], ["operation"]]], "golden_sentence": [["Sojourner Truth (/so\u028a\u02c8d\u0292\u025c\u02d0rn\u0259r \u02c8tru\u02d0\u03b8/; born Isabella [Belle] Baumfree; c.\u20091797 \u2013 November 26, 1883) was an American abolitionist and women's rights activist."], ["The monument opened to the public on October 9, 1888."], [""]]}, {"qid": "22d2c05fdc5858633429", "term": "Florence", "description": "Capital and most populous city of the Italian region of Tuscany", "question": "Was Florence a Theocracy during Italian Renaissance?", "answer": true, "facts": ["The Italian Renaissance was a period of history from the 13th century to 1600.", "A theocracy is a type of rule in which religious leaders have power.", "Friar Girolamo Savonarola was the ruler of Florence, after driving out the Medici family, from November 1494 \u2013 23 May 1498."], "decomposition": ["When was the Italian Renaissance?", "When did Friar Girolamo Savonarola rule Florence?", "Is #2 within the span of #1?", "Did Friar Girolamo Savonarola belong to a religious order during #3?"], "evidence": [[[["Italian Renaissance-1"]], [["Girolamo Savonarola-1"], "no_evidence"], ["operation"], [["Girolamo Savonarola-1"]]], [[["Italian Renaissance-1"]], [["Republic of Florence-38", "Republic of Florence-39", "Republic of Florence-40"]], ["operation"], [["Girolamo Savonarola-1"], "operation"]], [[["Italian Renaissance-1"]], [["Girolamo Savonarola-2", "Girolamo Savonarola-3"]], ["operation"], [["Dominican Order-1", "Girolamo Savonarola-1"], "operation"]]], "golden_sentence": [["The Italian Renaissance (Italian: Rinascimento [rina\u0283\u0283i\u02c8mento]) was a period in Italian history that covered the 15th (Quattrocento) and 16th (Cinquecento) centuries, spreading across Europe and marking the transition from the Middle Ages to Modernity."], ["Girolamo Savonarola (UK: /\u02ccs\u00e6v\u0252n\u0259\u02c8ro\u028al\u0259/, US: /\u02ccs\u00e6v\u0259n-, s\u0259\u02ccv\u0252n-/, Italian:\u00a0[d\u0292i\u02c8r\u0254\u02d0lamo savona\u02c8r\u0254\u02d0la]; 21 September 1452 \u2013 23 May 1498) was an Italian Dominican friar from Ferrara and preacher active in Renaissance Florence."], ["Girolamo Savonarola (UK: /\u02ccs\u00e6v\u0252n\u0259\u02c8ro\u028al\u0259/, US: /\u02ccs\u00e6v\u0259n-, s\u0259\u02ccv\u0252n-/, Italian:\u00a0[d\u0292i\u02c8r\u0254\u02d0lamo savona\u02c8r\u0254\u02d0la]; 21 September 1452 \u2013 23 May 1498) was an Italian Dominican friar from Ferrara and preacher active in Renaissance Florence."]]}, {"qid": "99e05e5266bad40f519b", "term": "Beauty and the Beast", "description": "traditional fairy tale", "question": "Were Beauty and the Beast adaptations devoid of Kurt Sutter collaborators?", "answer": false, "facts": ["Beauty and the Beast is a fairy tale adapted into several movie and TV shows.", "Kurt Sutter created the TV series Sons of Anarchy and The Shield.", "Charlie Hunnam and Ron Perlman starred in Sons of Anarchy.", "Ron Perlman starred in the TV series Beauty and the Beast which aired from 1987-1990."], "decomposition": ["Which characters were featured in Kurt Sutter's Sons of Anarchy and The Shield?", "Which characters were featured in TV series Beauty and the Beast?", "Is there no character common to #1 and #2?"], "evidence": [[[["Clay Morrow-1", "The Shield-1"], "no_evidence"], [["Beauty and the Beast (1987 TV series)-13"]], [["Ron Perlman-1"], "operation"]], [[["Clay Morrow-1"], "no_evidence"], [["Ron Perlman-5"], "no_evidence"], ["operation"]], [[["Ron Perlman-1", "The Shield-1"]], [["Beauty and the Beast (1987 TV series)-1"]], ["operation"]]], "golden_sentence": [["", ""], [""], [""]]}, {"qid": "a7d4323b2654bba262e1", "term": "Swan", "description": "large water bird", "question": "Would a Nike shoebox be too small to fit a swan in?", "answer": true, "facts": ["Nike Shoeboxes are usually 14\" x 10\" x 5\".", "An average swan is 4-5.6 ft in length."], "decomposition": ["What is the average size of a Nike Shoebox?", "What is the average length of a swan?", "Is #2 smaller than #1?"], "evidence": [[[["Shoe size-13", "Sneakers-16"], "no_evidence"], [["Swan-3"], "no_evidence"], ["operation"]], [[["Shoe-1"], "no_evidence"], [["Swan-3"]], ["operation"]], [["no_evidence"], [["Swan-3"]], ["no_evidence", "operation"]]], "golden_sentence": [["", ""], ["The largest species, including the mute swan, trumpeter swan, and whooper swan, can reach a length of over 1.5\u00a0m (59\u00a0in) and weigh over 15\u00a0kg (33\u00a0lb)."]]}, {"qid": "dd18067d2bc3703bc9a4", "term": "Johann Sebastian Bach", "description": "German composer", "question": "Did Johann Sebastian Bach ever win a Grammy Award?", "answer": false, "facts": ["Johann Sebastian Bach died in 1750.", "The first Grammy Awards ceremony was held on May 4, 1959."], "decomposition": ["In what year did Johann Sebastian Bach die?", "When was the first Grammy Award given?", "Is #2 before #1?"], "evidence": [[[["Johann Sebastian Bach-1"]], [["Grammy Award-3"]], ["operation"]], [[["Johann Sebastian Bach-34"]], [["Grammy Award-6"]], ["operation"]], [[["Johann Sebastian Bach-5"]], [["Grammy Award-6"]], ["operation"]]], "golden_sentence": [["21 March]\u00a01685\u00a0\u2013 28 July 1750) was a German composer and musician of the Baroque period."], ["The first Grammy Awards ceremony was held on May 4, 1959, to honor and respect the musical accomplishments by performers for the year 1958."]]}, {"qid": "68c7660071b240af3394", "term": "Chinchilla", "description": "Rodent genus", "question": "Are chinchillas cold-blooded?", "answer": false, "facts": ["Chinchillas are rodents.", "Rodents are mammals.", "All mammals are warm-blooded."], "decomposition": ["What type of animal are Chinchillas?", "What animal class is #1?", "Are #2s cold blooded?"], "evidence": [[[["Chinchilla-1"]], [["Rodent-1"]], [["Mammal-53"], "operation"]], [[["Chinchilla-2"]], [["Chinchilla-2"]], [["Mammal-53"], "operation"]], [[["Chinchilla-3"]], [["Rodent-1"]], [["Mammal-53"]]]], "golden_sentence": [["Chinchillas are either of two species (Chinchilla chinchilla and Chinchilla lanigera) of crepuscular rodents of the parvorder Caviomorpha."], ["Rodents (from Latin Rodere, \"to gnaw\") are mammals of the order Rodentia, which are characterized by a single pair of continuously growing incisors in each of the upper and lower jaws."], ["Like birds, mammals can forage or hunt in weather and climates too cold for ectothermic (\"cold-blooded\") reptiles and insects."]]}, {"qid": "fda03424c16a25d26597", "term": "Samsung", "description": "South Korean multinational conglomerate", "question": "Is Samsung accountable to shareholders?", "answer": true, "facts": ["Samsung is a publicly traded company.", "Publicly traded companies are ultimately accountable to shareholders. "], "decomposition": ["What kind of company is Samsung?", "Are #1's accountable to shareholders?"], "evidence": [[[["Samsung Electronics-1"], "no_evidence"], ["operation"]], [[["Samsung-1"]], [["Conglomerate (company)-21"], "operation"]], [[["Samsung-14"], "operation"], ["operation"]]], "golden_sentence": [["Samsung Electronics Co., Ltd. (Korean:\u00a0\uc0bc\uc131\uc804\uc790; Hanja:\u00a0\u4e09\u661f\u96fb\u5b50; RR:\u00a0Samsung Jeonja; literally 'tristar electronics') is a South Korean multinational electronics company headquartered in Suwon, South Korea."]]}, {"qid": "653bfcc879c8f9985466", "term": "Alan Alda", "description": "American actor, director, and writer", "question": "Is Alan Alda old enough to have fought in the Vietnam War?", "answer": true, "facts": ["Alan Alda was born in 1936.", "The Vietnam War was from 1955 to 1975, with American involvement from 1965 to 1973.", "American soldiers must be at least 18 years old.", "Alan Alda was 29 in 1965."], "decomposition": ["When were US forces first involved in the Vietnam war?", "When was Alan Alda born?", "What is the minimum age required to join the US Army?", "What is #1 minus #2?", "Is #4 greater than or equal to #3?"], "evidence": [[[["Vietnam War-58"]], [["Alan Alda-2"]], [["United States Army Recruiting Command-13"]], ["operation"], ["operation"]], [[["Vietnam War-2"]], [["Alan Alda-2"]], ["no_evidence"], ["operation"], ["no_evidence", "operation"]], [[["Vietnam War-1"]], [["Alan Alda-2"]], [["United States Armed Forces-3"]], ["operation"], ["operation"]]], "golden_sentence": [["The first deployment of 3,500 in March 1965 was increased to nearly 200,000 by December."], ["Alda was born Alphonso Joseph D'Abruzzo on January 28, 1936 in the Bronx, New York City."], ["With the Cold War looming, the Congress authorized the Selective Service Act of 1948 to enable President Truman to provide for 21 months of active Federal service, with all men from ages 18 to 26 required to register."]]}, {"qid": "79d0c7b74f7741428cda", "term": "Prophet", "description": "person claiming to speak for divine beings", "question": "Did Disney's second film rip off a prophet story?", "answer": true, "facts": ["Disney's second film, Pinocchio, was released in 1940.", "The biblical prophet Jonah was swallowed by a whale.", "In Pinocchio, Gepetto is swallowed by a giant whale while searching for Pinocchio."], "decomposition": ["What is Disney's second film?", "In #1, what happens to Gepetto while searching for Pinocchio?", "In a biblical prophet, what happens to Jonah?", "Is #2 the same as #3?"], "evidence": [[[["Pinocchio (1940 film)-1"]], [["Pinocchio (1940 film)-9"]], [["Jonah-1"]], ["operation"]], [[["Pinocchio (1940 film)-1"]], [["Pinocchio (1940 film)-9"]], [["Jonah-4"]], ["operation"]], [[["Pinocchio (1940 film)-1"]], [["Pinocchio (1940 film)-9"]], [["Jonah-1"]], ["operation"]]], "golden_sentence": [["It was the second animated feature film produced by Disney, made after the first animated success Snow White and the Seven Dwarfs (1937)."], ["Geppetto, Figaro, Cleo, and Jiminy are washed up safely on a beach, but Pinocchio is shown apparently lifeless in a rockpool."], ["Caught in a storm, he orders the ship's crew to cast him overboard, whereupon he is swallowed by a giant fish."]]}, {"qid": "abbd6cc38a4ea34f1b69", "term": "Uranium", "description": "Chemical element with atomic number 92", "question": "Is eating a Dicopomorpha echmepterygis size Uranium pellet fatal?", "answer": false, "facts": ["Dicopomorpha echmepterygis is a wingless insect that is .13mm large.", "Uranium is a radioactive element that is dangerous if ingested in large doses.", "25mg of Uranium would cause kidney damage, while 50mg would cause complete kidney failure in humans."], "decomposition": ["How much does a Dicopomorpha echmepterygis weigh?", "How much ingested Uranium is fatal for a human?", "Is #1 greater than #2?"], "evidence": [[[["Dicopomorpha echmepterygis-1", "Micrometre-1"], "no_evidence"], [["Iron tris(dimethyldithiocarbamate)-7"], "no_evidence"], ["operation"]], [[["Dicopomorpha echmepterygis-1"], "no_evidence"], [["Uranium-40"], "no_evidence"], ["no_evidence", "operation"]], [[["Dicopomorpha echmepterygis-1"], "no_evidence"], [["Self-harm-12"], "no_evidence"], ["operation"]]], "golden_sentence": [["With a body length averaging 186 \u03bcm (for 8 specimens measured, which ranged from 139 to 240 \u03bcm), males of D. echmepterygis have the shortest body length of all known insects (smaller than certain species of Paramecium and amoeba, which are single-celled organisms).", ""], [""]]}, {"qid": "bcdb7cc97f9e866993e9", "term": "Knight", "description": "An award of an honorary title for past or future service with its roots in chivalry in the Middle Ages", "question": "Can musicians become knights?", "answer": true, "facts": ["Elton John was knighted by the Queen of England.", "Tom Jones was knighted by the Queen of England.", "Elton John is a famous pop singer.", "Tom Jones is a famous musician."], "decomposition": ["Has Elton John been knighted?", "Has Tom Jones been knighted?", "Are #1 and #2 musicians?", "Are #1, #2 and #3 positive?"], "evidence": [[[["Elton John-3"]], [["Tom Jones (singer)-3"]], [["Elton John-1", "Tom Jones (singer)-1"]], ["operation"]], [[["Elton John-1"]], [["Tom Jones (singer)-1"]], [["Elton John-3", "Tom Jones (singer)-2"]], ["operation"]], [[["Elton John-3"]], [["Tom Jones (singer)-3"]], ["operation"], ["operation"]]], "golden_sentence": [["He was knighted by Elizabeth II for \"services to music and charitable services\" in 1998."], ["Jones was appointed Officer of the Order of the British Empire (OBE) in 1998 and knighted by Queen Elizabeth II for services to music in 2005."], ["", ""]]}, {"qid": "8263cb0ffc934640a600", "term": "Lemon", "description": "citrus fruit", "question": "Can a lemon aggravate dyspepsia?", "answer": true, "facts": ["Dyspepsia is a condition where the stomach is irritated.", "Lemons are highly acidic fruits.", "Common stomach irritants include alcohol, coffee, and acidic foods."], "decomposition": ["Which condition is referred to as dyspepsia?", "What are some common irritants that could aggravate #1?", "Is lemon an example of #2?"], "evidence": [[[["Indigestion-1"]], [["Indigestion-9"], "no_evidence"], ["no_evidence", "operation"]], [[["Indigestion-1"]], [["Indigestion-15"], "no_evidence"], [["Indigestion-25"], "no_evidence", "operation"]], [[["Indigestion-11"]], [["Indigestion-12"]], [["Lemon-17"]]]], "golden_sentence": [["Indigestion, also known as dyspepsia, is a condition of impaired digestion."], ["Less common causes include peptic ulcer, gastric cancer, esophageal cancer, coeliac disease, food allergy, inflammatory bowel disease, chronic intestinal ischemia and gastroparesis."]]}, {"qid": "4df1e1a501ba2d2e169f", "term": "Oscar Wilde", "description": "19th-century Irish poet, playwright and aesthete", "question": "Was Oscar Wilde's treatment under the law be considered fair in the US now?", "answer": false, "facts": ["Oscar Wilde was imprisoned for sexual indecency that amounted to having sexual relations with another man.", "In the United States, being gay is not a punishable offense. "], "decomposition": ["Why was Oscar Wilde imprisioned?", "is #1 considered a punishable offense in the US today?"], "evidence": [[[["Oscar Wilde-4"]], ["no_evidence"]], [[["Oscar Wilde-68"]], [["Same-sex marriage in the United States-1"], "operation"]], [[["Oscar Wilde-75"]], [["Sodomy laws in the United States-2"], "operation"]]], "golden_sentence": [[""]]}, {"qid": "3c270e3702fc7403b026", "term": "Jackie Chan", "description": "Hong Kong actor and martial artist", "question": "Would Jackie Chan have trouble communicating with a deaf person?", "answer": false, "facts": ["Jackie Chan speaks Cantonese, Mandarin, English, and American Sign Language.", "American Sign Language (ASL) is a natural language that serves as the predominant sign language of Deaf communities in the United States and most of Anglophone Canada."], "decomposition": ["What languages can Jackie Chan speak?", "What language do deaf people communicate with?", "Is #2 not included in #1?"], "evidence": [[[["Jackie Chan-38"]], [["Sign language-1"]], ["operation"]], [[["Jackie Chan-38"]], [["Sign language-3"]], ["operation"]], [[["Jackie Chan-38"]], [["American Sign Language-1"]], ["operation"]]], "golden_sentence": [["Chan speaks Cantonese, Mandarin, English, and American Sign Language and also speaks some German, Korean, Japanese, Spanish, and Thai."], ["Sign languages are not universal and they are not mutually intelligible with each other, although there are also striking similarities among sign languages."]]}, {"qid": "11805d148c0a6191382c", "term": "John Key", "description": "38th Prime Minister of New Zealand", "question": "Could a fan of the Botany Swarm vote for John Key?", "answer": true, "facts": ["The Botany Swarm is a hockey team based in Auckland, New Zealand", "John Key is the Prime Minister of New Zealand"], "decomposition": ["What city is the Botany Swarm based in?", "In what country is #1?", "What country was John Key the Prime Minister of?", "Is #2 the same as #3?"], "evidence": [[[["Botany Swarm-1"]], [["East Auckland-1"]], [["John Key-1"]], ["operation"]], [[["Botany Swarm-1"]], [["Botany Swarm-1"]], [["John Key-1"]], ["operation"]], [[["Botany Swarm-1"]], [["Botany Swarm-1"]], [["John Key-1"]], ["operation"]]], "golden_sentence": [["The Botany Swarm is a semi-professional ice hockey team based in East Auckland, New Zealand."], ["The name \"East Auckland\" is not an official placename, but is in popular use by New Zealanders."], ["Sir John Phillip Key GNZM AC (born 9 August 1961) is a New Zealand former politician who served as the 38th Prime Minister of New Zealand from 2008 to 2016 and as Leader of the New Zealand National Party from 2006 to 2016."]]}, {"qid": "d92820a1d6f45a7f4eee", "term": "Call of Duty", "description": "First-person shooter video game franchise", "question": "Will Conan the Barbarian hypothetically last a short time inside of Call of Duty?", "answer": true, "facts": ["Conan the Barbarian is a comic book character.", "Conan the Barbarian is equipped with a sword and does not typically wear armor.", "Call of Duty is a modern warfare video game.", "Soldiers in Call of Duty are equipped with weapons like sniper rifles, shotguns, and machine guns."], "decomposition": ["What equipment for fighting does Conan the Barbarian use?", "What equipment for fighting does Call of Duty use?", "Are the items listed in #2 deadlier than those in #1?"], "evidence": [[[["Conan the Barbarian-1"]], [["Call of Duty-1"]], ["no_evidence", "operation"]], [[["Conan the Barbarian-20"]], [["Call of Duty-46"]], [["Sword-58"], "operation"]], [[["Conan the Barbarian-16"]], [["Call of Duty-4"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "b8247aad3d722a712d7b", "term": "Vitamin C", "description": "nutrient found in citrus fruits and other foods", "question": "Do pirates care about vitamin C?", "answer": true, "facts": ["Scurvy is caused by a prolonged period of time without Vitamin C", "People spending long periods of time at sea without to vitamin C are at high risk for scurvy", "Pirates spend long periods of time at sea"], "decomposition": ["What diseases are caused by a lack of vitamin C?", "What behaviors increase people risk of getting #1?", "Do pirate engage in #2?"], "evidence": [[[["Scurvy-1"]], [["Scurvy-4"]], [["Piracy-1"], "operation"]], [[["Vitamin C-12", "Vitamin C-2"]], [["Scurvy-4"]], ["operation"]], [[["Vitamin C-9"]], [["Scurvy-2"]], ["operation"]]], "golden_sentence": [["Scurvy is a disease resulting from a lack of vitamin C (ascorbic acid)."], [""], ["Those who engage in acts of piracy are called pirates, while dedicated ships that are used by them are called pirate ships."]]}, {"qid": "9b639685bc77fa9741f4", "term": "Surveillance", "description": "monitoring of behavior, activities, or other changing information", "question": "Is video surveillance of a room possible without an obvious camera or new item?", "answer": true, "facts": ["Surveillance cameras can be built into light socket covers that look no different from a normal one.", "Surveillance cameras can be installed in special light bulbs to document activity in a room."], "decomposition": ["What are the various types of surveillance cameras based on installation?", "Are some of installed so as to be #1 hidden from view?"], "evidence": [[[["Closed-circuit television-2", "Closed-circuit television-3", "Closed-circuit television-4"]], [["Hidden camera-2"]]], [[["Hidden camera-1"], "no_evidence"], ["operation"]], [[["Hidden camera-1"]], [["Hidden camera-2"]]]], "golden_sentence": [["", "", ""], [""]]}, {"qid": "9cf430612928bbef0ae1", "term": "1800", "description": "Year", "question": "Did England win any Olympic gold medals in 1800?", "answer": false, "facts": ["Olympic medals can only be won during the Olympics.", "The Olympics were first held in 1896."], "decomposition": ["Which sporting event would England have to participate in to win an Olympic gold medal?", "When was the first modern edition of #1 held?", "Is #2 before or the same as 1800?"], "evidence": [[[["Gold medal-11"]], [["Olympic Games-2"]], [["Olympic Games-2"]]], [[["Olympic Games-4"]], [["Olympic Games-2"]], ["operation"]], [[["Olympic medal-1"]], [["Olympic Games-2"]], ["operation"]]], "golden_sentence": [[""], [""], [""]]}, {"qid": "bff33ccaef461b257bdb", "term": "Snake", "description": "limbless, scaly, elongate reptile", "question": "In the world of Harry Potter, would a snake and skull tattoo be good luck?", "answer": false, "facts": ["In Harry Potter, a tattoo of a snake and a skull is a symbol of being a \"Death Eater.\"", "Death Eaters are people who follow the word of the dark lord Voldemort, who is considered wicked and cruel.", "Death Eaters are not embraced in the wizarding communities of Harry Potter."], "decomposition": ["In Harry Potter, what does a tattoo of snake and a skull a symbol of?", "Who are #1's?", "Are #2's embraced in the wizarding communities of Harry Potter?"], "evidence": [[[["Magic in Harry Potter-85"]], [["Death Eater-1"]], [["Death Eater-1", "Order of the Phoenix (fictional organisation)-1"]]], [[["Magic in Harry Potter-85"]], [["Death Eater-1"]], ["no_evidence", "operation"]], [[["Magic in Harry Potter-85"]], [["Death Eater-1"]], [["Lord Voldemort-2"], "operation"]]], "golden_sentence": [["The Dark Mark is the symbol of Voldemort and the Death Eaters and takes the form of a skull with a snake coming out of the mouth in place of a tongue."], ["They are a terrorist group of wizards and witches, led by the dark wizard Lord Voldemort, who seek to purify the wizarding community by eliminating wizards and witches born to non-magical parents."], ["They are a terrorist group of wizards and witches, led by the dark wizard Lord Voldemort, who seek to purify the wizarding community by eliminating wizards and witches born to non-magical parents.", ""]]}, {"qid": "4f9ad0fc0f6ea7c65545", "term": "Comma", "description": "Punctuation mark", "question": "Would a Fakir be surprised if they saw a comma in their religious book?", "answer": true, "facts": ["A Fakir is a Muslim Sufi holy man or woman that lives a simple life.", "The holy book for Muslims is the Quran.", "The comma is a punctuation mark in modern language.", "The Quran does not use any forms of modern punctuation."], "decomposition": ["What religion is a Fakir from?", "What is the name of #1's Holy Book?", "What kind of punctuation mark is the comma?", "Is #2 written without using #3?"], "evidence": [[[["Fakir-1"]], [["Quran-1"]], [["Comma-1"]], ["no_evidence"]], [[["Fakir-1"]], [["Quran-1"]], [["Comma-1"]], ["no_evidence", "operation"]], [[["Fakir-1"]], [["Quran-1"]], [["Comma-2"]], [["Classical Arabic-1"], "no_evidence", "operation"]]], "golden_sentence": [["A fakir, faqeer or faqir (/f\u0259\u02c8k\u026a\u0259r/; Arabic: \u0641\u0642\u06cc\u0631\u200e (noun of faqr)), derived from faqr (Arabic: \u0641\u0642\u0631\u200e, \"poverty\") is a Sufi Muslim ascetic who has taken vows of poverty and worship, renouncing all relations and possessions."], ["The Quran (/k\u0254\u02d0r\u02c8\u0251\u02d0n/ kor-AHN; Arabic: \u0627\u0644\u0642\u0631\u0622\u0646\u200e, romanized:\u00a0al-Qur\u02bc\u0101n Arabic pronunciation:\u00a0[alqur'\u0294a\u02d0n], literally meaning \"the recitation\"), also romanized Qur'an or Koran, is the central religious text of Islam, which Muslims believe to be a revelation from God (Allah)."], ["The comma ( , ) is a punctuation mark that appears in several variants in different languages."]]}, {"qid": "bf6d92e3510ea6085fe6", "term": "Sesame Street", "description": "American children's television program", "question": "Was Elmo an original muppet character on Sesame Street?", "answer": false, "facts": ["Sesame Street started in 1969.", "Elmo first appeared on the show in 1980."], "decomposition": ["When did Sesame Street make its debut?", "When did Elmo first appear on Sesame Street?", "Is #2 the same as #1?"], "evidence": [[[["Sesame Street-1"]], [["Elmo-3"]], ["operation"]], [[["Sesame Street-1"]], [["Elmo-3"]], ["operation"]], [[["Sesame Street-21"], "no_evidence"], [["Sesame Street-7"], "operation"], ["operation"]]], "golden_sentence": [["The series premiered on November 10, 1969, to positive reviews, some controversy, and high viewership; it has aired on the U.S.'s national public television provider PBS since its debut, with its first run moving to premium channel HBO on January 16, 2016, then its sister streaming service HBO Max in 2020."], ["The puppet, originally known as \"Short Red\", was performed by Jerry Nelson in the background of episodes from the early 1980s, Brian Muehl from 1980 to 1984, and Richard Hunt from 1984 to 1985."]]}, {"qid": "40329614affdff4f37c7", "term": "Bob Marley", "description": "Jamaican singer-songwriter", "question": "Is sunscreen unhelpful for the condition that killed Bob Marley?", "answer": true, "facts": ["Bob Marley died of acral lentiginous melanoma ", "Acral lentiginous melanoma occurs on skin that may not have any sun exposure "], "decomposition": ["What disease killed Bob Marley?", "What is the cause of #1?", "Would sunscreen help with preventing #2?"], "evidence": [[[["Bob Marley-4"]], [["Acral lentiginous melanoma-4"]], ["no_evidence", "operation"]], [[["Bob Marley-4"]], [["Acral lentiginous melanoma-1"]], ["operation"]], [[["Bob Marley-26"]], [["Acral lentiginous melanoma-4"]], [["Acral lentiginous melanoma-1"]]]], "golden_sentence": [["In 1977, Marley was diagnosed with acral lentiginous melanoma; he died as a result of the illness in 1981."], [""]]}, {"qid": "c8a4b8372f8a756a85d6", "term": "Stanford University", "description": "Private research university in Stanford, California", "question": "Was John Gall from same city as Stanford University?", "answer": true, "facts": ["John Gall is a former major league baseball player born in Stanford, California.", "Stanford University was founded by Leland and Jane Stanford  in Stanford, California."], "decomposition": ["Where was John Gall (baseball player) born?", "Where is Stanford University located?", "Is #1 the same as #2?"], "evidence": [[[["John Gall (baseball)-2"]], [["Stanford University-1"]], ["operation"]], [[["John Gall (baseball)-2"]], [["Stanford University-1"]], ["operation"]], [[["John Gall (baseball)-2"]], [["Stanford University-1"]], ["operation"]]], "golden_sentence": [["Gall was born in Stanford, California, and was a successful collegiate player for Stanford University, making his NCAA debut as a freshman in 1997."], ["Stanford University, officially Leland Stanford Junior University, is a private research university in Stanford, California."]]}, {"qid": "6c27c5885b3dfcd0bd58", "term": "Snowboarding", "description": "winter sport", "question": "Snowboarding is a rarity in Hilo?", "answer": true, "facts": ["Snowboarding is a sport that involves descending snow-covered slopes.", "Hilo, a town in Hawaii, has not had snow in almost 200 years."], "decomposition": ["What kind of surface is needed for snowboarding?", "Is #1 likely to be nonexistent in Hilo, going by the prevailing climatic conditions?"], "evidence": [[[["Snowboarding-1"]], [["Hilo, Hawaii-12"], "operation"]], [[["Snowboarding-1"]], [["Hilo, Hawaii-13"], "operation"]], [[["Snowboarding-1"]], [["Hilo, Hawaii-12"], "operation"]]], "golden_sentence": [["Snowboarding is a recreational activity and Winter Olympic and Paralympic sport that involves descending a snow-covered slope while standing on a snowboard attached to a rider's feet."], [""]]}, {"qid": "2cb89f49a995d88b4fe2", "term": "Portuguese people", "description": "ethnic group", "question": "Did King of Portuguese people in 1515 have familial ties to the Tudors?", "answer": true, "facts": ["Manuel I was King of Portugal from 1495 to 1521.", "Manuel I of Portugal married Maria of Aragon.", "Maria of Aragon was the sister of Catherine of Aragon.", "Catherine of Aragon was the first wife of Henry VIII and was one of a handful that he did not behead."], "decomposition": ["Who was the King of Portugal in 1515?", "Who were in #1's immediate family?", "Were any of #2 related to the Tudors?"], "evidence": [[[["Manuel I of Portugal-1"]], [["Manuel I of Portugal-5"], "no_evidence"], ["no_evidence", "operation"]], [[["Manuel I of Portugal-1"]], [["Isabella of Portugal-4"]], [["Mary I of England-1", "Philip II of Spain-1", "Philip II of Spain-2"], "operation"]], [[["Manuel I of Portugal-1"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Manuel I (European Portuguese:\u00a0[m\u0250nu\u02c8\u025b\u026b]; 31 May 1469\u00a0\u2013 13 December 1521), known as the Fortunate (Portuguese: O Venturoso), was King of Portugal from 1495 to 1521."], [""]]}, {"qid": "c3f20031eeea02ee386a", "term": "Darth Vader", "description": "fictional character in the Star Wars franchise", "question": "Could Darth Vader hypothetically catch the Coronavirus?", "answer": false, "facts": ["The Coronavirus is transferred through infected droplets that can get into eyes, nose, or mouth.", "Darth Vader permanently wears an iron weave helmet that he needs to breathe."], "decomposition": ["How is the Coronavirus transferred?", "What does Darth Vader wear on his head?", "Can #1's get through #2?"], "evidence": [[[["Coronavirus disease 2019-13"]], [["Darth Vader-16"]], [["Coronavirus disease 2019-4"]]], [[["Coronavirus-21"]], [["Darth Vader-15"]], ["operation"]], [[["Coronavirus-21"]], [["Darth Vader-15"]], [["Coronavirus disease 2019-13"], "no_evidence"]]], "golden_sentence": [[""], ["Darth Vader designers Working from McQuarrie's designs, the costume designer John Mollo devised a costume that could be worn by an actor on-screen using a combination of clerical robes, a motorcycle suit, a German military helmet and a gas mask."], [""]]}, {"qid": "1239d3324de47bf0a89f", "term": "Funeral", "description": "ceremony for a person who has died", "question": "Is it normal to blow out candles during a funeral?", "answer": false, "facts": ["Blowing out candles is typically done during a birthday celebration, prior to eating the birthday cake.", "Funerals are typically very somber events in which cake is not served."], "decomposition": ["Blowing out candles is a typical part of which celebration?", "What kind of aura is naturally associated with #1?", "Is the atmosphere in a funeral typically similar to #2?"], "evidence": [[[["Party-5"]], [["Party-1"]], [["Funeral-88"]]], [[["Birthday cake-7"]], [["Joy-1"]], [["Funeral-1"], "operation"]], [[["Birthday cake-9"]], [["Happy, Happy Birthday Baby-1"]], [["Funeral-1"]]]], "golden_sentence": [["A birthday cake is usually served with lit candles that are to be blown out after a \"birthday wish\" has been made."], [""], ["Some events are portrayed as joyous parties, instead of a traditional somber funeral."]]}, {"qid": "717040911fa4b7e80817", "term": "Artillery", "description": "Heavy ranged guns or weapons", "question": "Would a slingshot be improperly classified as artillery?", "answer": true, "facts": ["Artillery refers to ranged weaponry that is predominantly used in breaching fortifications.", "Examples of artillery include: howitzers, mortars, and rockets.", "Mortars can have a range up to 4,680m.", "A slingshot is a string weapon that propels a rock or other small projectile.", "Some slingshots can fire projectiles up to 9m."], "decomposition": ["What are the basic characteristics of a weapon considered artillery?", "Does a slingshot fail to possess all of #1?"], "evidence": [[[["Artillery-9"]], [["Slingshot-8"]]], [[["Artillery-1"]], [["Slingshot-1"], "operation"]], [[["Artillery-1"]], ["operation"]]], "golden_sentence": [["In some armies, the weapon of artillery is the projectile, not the equipment that fires it."], [""]]}, {"qid": "f8eb83a9e7f63551ff70", "term": "Gladiator", "description": "combatant who entertained audiences in the Roman Republic and Roman Empire", "question": "Did Gladiator's weapon of choice require less hands than Soul Calibur's Faust?", "answer": true, "facts": ["Faust is a zweihander sword in the Soul Calibur video game series.", "A zweihander is a giant sword that requires two hands to wield.", "Gladiators used the Gladius which was a short one handed sword."], "decomposition": ["Faust in the Soul Calibur video game series is what kind of sword?", "How many hands would be needed to lift #1?", "How many hands would be needed to lift a typical Gladiator's sword?", "Is #3 less than #2?"], "evidence": [[[["Siegfried and Nightmare-17"]], [["Siegfried and Nightmare-17"]], [["Gladius-1", "Gladius-2"]], ["operation"]], [[["Siegfried and Nightmare-17"]], ["operation"], [["Gladius-20"], "no_evidence"], ["operation"]], [[["Soulcalibur (video game)-1"], "no_evidence"], ["no_evidence"], [["Gladius-1", "Gladius-2"]], ["no_evidence", "operation"]]], "golden_sentence": [["His weapon in Soul Edge was originally Faust, a simple zweihander (two-handed sword) with a basic design, but swapped it for Requiem, a large blade with a flat end and a black lining in the center."], ["His weapon in Soul Edge was originally Faust, a simple zweihander (two-handed sword) with a basic design, but swapped it for Requiem, a large blade with a flat end and a black lining in the center."], ["", ""]]}, {"qid": "4bcfd8b6ed8e54e24cd5", "term": "Saddam Hussein", "description": "Iraqi politician and President", "question": "Did Saddam Hussein witness the inauguration of Donald Trump?", "answer": false, "facts": ["Saddam Hussein died on December 30th, 2006.", "Donald Trump was inaugurated as the President of the United States on January 20, 2017."], "decomposition": ["When did Saddam Hussein die?", "When was Donald Trump inaugurated as President?", "Is #2 before #1?"], "evidence": [[[["Saddam Hussein-101"]], [["Timeline of the Donald Trump presidency-1"]], ["operation"]], [[["Saddam Hussein-4"]], [["Inauguration of Donald Trump-1"]], ["operation"]], [[["Saddam Hussein-1"]], [["Inauguration of Donald Trump-1"]], ["operation"]]], "golden_sentence": [[""], ["Donald Trump was elected President of the United States on November 8, 2016 and was inaugurated on January 20, 2017 as the nation\u2019s 45th and current president."]]}, {"qid": "549d5c7e3cc4b04d53e3", "term": "Batman (1989 film)", "description": "1989 film directed by Tim Burton", "question": "Is Batman (1989 film) likely to be shown on flight from NY to Kansas City?", "answer": true, "facts": ["A flight from NY to Kansas City is four and a half hours.", "The run time of Batman (1989 film) is two hours and six minutes.", "Batman (1989 film) is rated PG-13", "The average age group of passengers is 18-34.", "Airlines have relaxed their rules for in-flight movies in last few years and even R rated movies have been shown."], "decomposition": ["How long is a flight from NY to Kansas City?", "How long is the 1989 Batman film? ", "Is #2 less than #1?"], "evidence": [[["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Kansas City metropolitan area-1", "New York City-1"], "no_evidence"], [["Batman (1989 film)-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Flight length-5"], "no_evidence"], [["Batman (1989 film)-23"], "no_evidence"], ["operation"]]], "golden_sentence": []}, {"qid": "5ad32c275b05c53685f2", "term": "Eric Clapton", "description": "English musician, singer, songwriter, and guitarist", "question": "Could Eric Clapton's children play a regulation game of basketball among themselves?", "answer": false, "facts": ["NBA regulations require two teams of five players each for a game, for a total of 10 players.", "Eric Clapton has 5 children."], "decomposition": ["How many players are required for a regulation game of basketball?", "How many children does Eric Clapton have?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Basketball-1"]], [["Eric Clapton-78", "Eric Clapton-79", "Eric Clapton-80"]], ["operation"]], [[["Basketball-1"]], [["Eric Clapton-78", "Eric Clapton-79", "Eric Clapton-80"]], ["operation"]], [[["Basketball-1"]], [["Eric Clapton-78", "Eric Clapton-79", "Eric Clapton-80"]], ["operation"]]], "golden_sentence": [["Basketball is a team sport in which two teams, most commonly of five players each, opposing one another on a rectangular court, compete with the primary objective of shooting a basketball (approximately 9.4 inches (24\u00a0cm) in diameter) through the defender's hoop (a basket 18 inches (46\u00a0cm) in diameter mounted 10 feet (3.048\u00a0m) high to a backboard at each end of the court) while preventing the opposing team from shooting through their own hoop."], ["", "", "They have three daughters, Julie Rose (born 13 June 2001), Ella May (born 14 January 2003), and Sophie Belle (born 1 February 2005) His grandson Isaac Eric Owen Bartlett was born in June 2013 to his oldest daughter Ruth and her husband Dean Bartlett."]]}, {"qid": "63aad64acd6b789e4613", "term": "Supreme Court of Canada", "description": "highest court of Canada", "question": "Can the Supreme Court of Canada fight a Lucha trios match?", "answer": true, "facts": ["A Lucha trios match requires at least two teams of three wrestlers each", "The Supreme Court of Canada has nine justices"], "decomposition": ["How many Justices are in the Supreme Court of Canada?", "What is the total number of people needed to fight in a Lucha trios match?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Supreme Court of Canada-5"]], [["Lucha libre-16"]], ["operation"]], [[["Supreme Court of Canada-5"]], [["Lucha libre-1"]], ["operation"]], [[["Supreme Court of Canada-30"]], [["Lucha libre-1"]], ["operation"]]], "golden_sentence": [["In 1949, the bench reached its current composition of nine justices."], ["These three man teams participate in what are called trios matches, for tag team championship belts."]]}, {"qid": "a0efec06ea5c2ed3750e", "term": "Sudoku", "description": "Logic-based number-placement puzzle", "question": "Do you need different colored pens for sudoku?", "answer": false, "facts": ["Sudoku is played both online and offline.", "Sudoku has no color component in the game."], "decomposition": ["How is Sudoku played?", "Is color necessary to do #1?"], "evidence": [[[["Sudoku-1"]], [["Sudoku-1"]]], [[["Sudoku-1"]], ["operation"]], [[["Sudoku-1"]], ["operation"]]], "golden_sentence": [["Sudoku (\u6570\u72ec, s\u016bdoku, digit-single) (/su\u02d0\u02c8do\u028aku\u02d0/, /-\u02c8d\u0252k-/, /s\u0259-/, originally called Number Place) is a logic-based, combinatorial number-placement puzzle."], [""]]}, {"qid": "ca4b760c05e62db30b82", "term": "Dessert", "description": "A course that concludes a meal; usually sweet", "question": "Would an ancient visitor to Persia probably consume crocus threads?", "answer": true, "facts": ["Ancient Persians would have several desserts after a simple meal", "Saffron is made from crocus styles or threads", "Saffron is a common ingredient in Persian desserts"], "decomposition": ["What would Ancient Persians typically have after a simple meal?", "What was a common ingredient in #1?", "Is #2 made from crocus threads?"], "evidence": [[[["History of saffron-16"]], [["History of saffron-16"]], [["Saffron-1"]]], [[["Tahchin-1"], "no_evidence"], [["Saffron (color)-1"]], ["operation"]], [["no_evidence"], ["no_evidence"], [["Crocus sativus-6"], "no_evidence", "operation"]]], "golden_sentence": [[""], ["They mixed saffron into teas and dined on saffron rice."], [""]]}, {"qid": "753075ac9abc88559715", "term": "The Who", "description": "English rock band", "question": "Would the Who concert in international space station be audible?", "answer": true, "facts": ["Sound travels through the vibration of atoms and molecules in a medium (such as air or water). ", "There is air in the international space station. "], "decomposition": ["What is the most common medium of transmission of sound?", "Is #1 present in the international space station?"], "evidence": [[[["Speech science-10"], "no_evidence"], [["International Space Station-90"], "no_evidence", "operation"]], [[["Transmission medium-2"], "no_evidence"], [["Space station-20"], "no_evidence"]], [[["Sound-1", "Sound-5"]], [["International Space Station-89"], "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "c566fdc2aff9ce4d73c0", "term": "King Kong (2005 film)", "description": "2005 film directed by Peter Jackson", "question": "Was King Kong (2005 film) solvent?", "answer": true, "facts": ["Solvent refers to the assets of a project being greater than the liabilities.", "The assets of a movie film are the box office receipts, and the liabilities is the budget.", "King Kong (2005) had box office receipts of 562 million.", "King Kong (2005) had a budget of 207 million."], "decomposition": ["What does it mean to be solvent in business/finance?", "What was the budget (liabilities) of the 2005 movie King Kong?", "How much did 2005 movie King Kong gross (assets) worldwide?", "Does #3 compare favorably with #2 as defined in #1?"], "evidence": [[[["Solvency-1"]], [["King Kong (2005 film)-2"]], [["King Kong (2005 film)-2"]], ["operation"]], [[["Solvency-1"]], [["King Kong (2005 film)-2"]], [["King Kong (2005 film)-2"]], ["operation"]], [[["Solvency-1"]], [["King Kong (2005 film)-2"]], [["King Kong (2005 film)-2"]], ["operation"]]], "golden_sentence": [["Solvency, in finance or business, is the degree to which the current assets of an individual or entity exceed the current liabilities of that individual or entity."], ["The project's budget climbed from an initial $150 million to a then-record-breaking $207 million."], ["While it performed lower than expected, King Kong made domestic and worldwide grosses that eventually added up to $562 million, becoming the fourth-highest-grossing film in Universal Pictures history at the time and the fifth-highest-grossing film of 2005."]]}, {"qid": "4ceaf90d71d2ed923991", "term": "Don't ask, don't tell", "description": "Former policy on gay people serving in the United States military", "question": "During the time immediately after 9/11, was don't ask don't tell still in place?", "answer": true, "facts": ["Don't ask don't tell was the official military policy for LGBT service members until 2011.", "9/11 Occured on September 11th, 2001."], "decomposition": ["Until what year was \"Don't ask; Don't tell.\" in place?", "In what year did 9/11 occur?", "Is #1 more recent than #2?"], "evidence": [[[["Don't ask, don't tell-23"]], [["Post-9/11-2"]], ["operation"]], [[["Don't ask, don't tell-1"]], [["September 11 attacks-1"]], ["operation"]], [[["Don't ask, don't tell-1"]], [["September 11 attacks-1"]], ["operation"]]], "golden_sentence": [["On October 12, 2010, she granted an immediate worldwide injunction prohibiting the Department of Defense from enforcing the \"Don't Ask Don't Tell\" policy and ordered the military to suspend and discontinue any investigation or discharge, separation, or other proceedings based on it."], ["Since 9/11 and as of 2011, there have been 119,044 anti-terror arrests and 35,117 convictions in 66 countries."]]}, {"qid": "eabe1ad2597667ebedf1", "term": "Doctor Who", "description": "British science fiction TV series", "question": "Would the 10th doctor enjoy a dish of stuffed pears?", "answer": false, "facts": ["The 10th Doctor in David Who is played by David Tennant.", "In multiple episodes of the series, the 10th doctor mentions that he hates pears."], "decomposition": ["Who is the 10th Doctor?", "Does #1 like pears?"], "evidence": [[[["Tenth Doctor-1"]], ["no_evidence", "operation"]], [[["Tenth Doctor-1"]], ["no_evidence"]], [[["Tenth Doctor-14"]], [["Pear-5"], "operation"]]], "golden_sentence": [["The Tenth Doctor is an incarnation of the Doctor, the protagonist of the BBC science fiction television programme Doctor Who, who is played by David Tennant in three series as well as nine specials."]]}, {"qid": "c2365db739490e2c3bbf", "term": "The Great Gatsby", "description": "1925 novel by F. Scott Fitzgerald", "question": "Will speed reader devour The Great Gatsby before the Raven?", "answer": false, "facts": ["F. Scott Fitzgerald's The Great Gatsby is 218 pages.", "Edgar Allan Poe's The Raven is 42 pages."], "decomposition": ["How many pages does The Great Gatsby have?", "How many pages does The Raven have?", "Is #2 greater than #1?"], "evidence": [[[["The Great Gatsby-1"], "no_evidence"], [["The Raven-19"], "no_evidence"], ["operation"]], [[["The Great Gatsby-1", "The Great Gatsby-24"], "no_evidence"], [["The Raven-15"]], ["no_evidence", "operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["The small volume, his first book of poetry in 14 years, was 100 pages and sold for 31 cents."]]}, {"qid": "5704fd3a50e79bf14466", "term": "Cook (profession)", "description": "occupation involving cooking food", "question": "Can Michael Jordan become a professional cook in America? ", "answer": true, "facts": ["Based on the American Culinary Federation, the minimum requirements for entering culinary apprenticeships include being 17 years old and having a high school diploma or equivalent.", "Michael Jordan graduated from Laney High School in 1981.", "Michael Jordan was born on February 17, 1963, which makes him 57 years old in 2020."], "decomposition": ["What are the minimum requirements to become a professional cook in America?", "Does Michael Jordan satisfy all of #1?"], "evidence": [[[["Cook (profession)-16"]], [["Michael Jordan-2"], "operation"]], [[["Chef-2"]], ["no_evidence", "operation"]], [[["Chef-17"]], [["Michael Jordan-1"], "no_evidence"]]], "golden_sentence": [["Based on the American Culinary Federation, the minimum requirements for entering such programs include being 17 years old and having a high school diploma or equivalent."], [""]]}, {"qid": "86c286e7ad883d6fe049", "term": "Toyota Prius", "description": "Hybrid electric automobile", "question": "Can a microwave melt a Toyota Prius battery?", "answer": false, "facts": ["A Toyota Prius uses a 202 V nickel-metal hydride battery.", "Nickel has a melting point of 2651 F.", "Microwaves rarely warm food more than 212 F."], "decomposition": ["What kind of battery does a Toyota Prius use?", "What type of material is #1 made out of?", "What is the melting point of #2?", "Can a microwave's temperature reach at least #3?"], "evidence": [[[["Toyota Prius-53"]], ["operation"], [["Lanthanum-5"]], [["Microwave oven-50"]]], [[["Toyota Prius-53"]], [["Nickel\u2013metal hydride battery-1"]], [["Nickel\u2013cadmium battery-7"], "no_evidence"], [["Microwave oven-45"], "no_evidence", "operation"]], [[["Toyota Prius (XW20)-4"]], [["Toyota Prius (XW20)-4"]], [["Nickel-1"], "no_evidence"], [["Microwave oven-3"], "no_evidence"]]], "golden_sentence": [["Each battery pack uses 10\u201315\u00a0kg (22\u201333\u00a0lb) of lanthanum, and each Prius electric motor contains 1\u00a0kg (2\u00a0lb) of neodymium; production of the car is described as \"the biggest user of rare earths of any object in the world.\""], ["Furthermore, since the melting points of the trivalent lanthanides are related to the extent of hybridisation of the 6s, 5d, and 4f electrons, lanthanum has the second-lowest (after cerium) melting point among all the lanthanides: 920\u00a0\u00b0C."], [""]]}, {"qid": "df2985a15b9612c07f2f", "term": "Irish mythology", "description": "Pre-Christian Mythology of Ireland", "question": "Did Irish mythology inspire Washington Irving?", "answer": true, "facts": ["Washington Irving's most famous novel was The Legend of Sleepy Hollow.", "The antagonist of the Legend of Sleepy Hollow, The Headless Horseman, was a man on horseback that was missing his head.", "The Dullahan is a mythological creature in Irish mythology.", "The Dullahan comes from the Irish word Gan Ceann, meaning \"without a head\".", "The Dullahan is depicted as a headless rider, usually on a black horse, who carries their own head in their arm."], "decomposition": ["Which Washington Irving novel was the most popular?", "What are the features of the antagonist of #1?", "What are the features of Dullahan from Irish mythology?", "Does #2 considerably match #3?"], "evidence": [[[["The Legend of Sleepy Hollow-1"]], [["The Legend of Sleepy Hollow-1"]], [["Dullahan-2"]], ["operation"]], [[["The Legend of Sleepy Hollow-1"], "no_evidence"], [["The Legend of Sleepy Hollow-2"]], [["Dullahan-2"]], ["operation"]], [[["Washington Irving-1"]], [["Headless Horseman-1"]], [["Dullahan-2"]], ["operation"]]], "golden_sentence": [["\"The Legend of Sleepy Hollow\" is a gothic story by American author Washington Irving, contained in his collection of 34 essays and short stories entitled The Sketch Book of Geoffrey Crayon, Gent.."], [""], ["The Dullahan (also Dullaghan) is depicted as a headless rider, usually on a black horse, who carries their own head in their arm."]]}, {"qid": "2dde8ff78940b702bec3", "term": "Times Square", "description": "Neighborhood in Manhattan in New York City, New York", "question": "At midnight in Times Square on New Years Eve, are you likely to meet people in diapers?", "answer": true, "facts": ["Times Square holds an enormous gathering on New Year's Eve.", "There are no portable restrooms in Times Square.", "People begin waiting for the midnight event in Times Square before 10 a.m."], "decomposition": ["How early do people arrive to wait for the ball to drop on New Years Eve?", "When does the ball drop?", "How much time is between #1 and #2?", "Within #3 hours, would someone need to use the bathroom?"], "evidence": [[[["New Year's Eve-37"], "no_evidence"], [["New Year's Eve-37"]], ["no_evidence", "operation"], ["no_evidence", "operation"]], [[["New Year's Eve-39"], "no_evidence"], [["Time ball-8"]], ["operation"], [["Urinary system-8"]]], [[["Times Square-39"], "no_evidence"], [["Times Square Ball-8"]], ["operation"], ["operation"]]], "golden_sentence": [["ET, an 11,875-pound (5,386\u00a0kg), 12-foot-diameter (3.7\u00a0m) ball is lowered down a 70-foot-high (21\u00a0m) pole on the roof of One Times Square, reaching the roof of the building sixty seconds later at midnight."], ["ET, an 11,875-pound (5,386\u00a0kg), 12-foot-diameter (3.7\u00a0m) ball is lowered down a 70-foot-high (21\u00a0m) pole on the roof of One Times Square, reaching the roof of the building sixty seconds later at midnight."]]}, {"qid": "22140880d841ecf18262", "term": "United States Air Force", "description": "Air and space warfare branch of the United States Armed Forces", "question": "Would a member of the United States Air Force get a discount at Dunkin Donuts?", "answer": true, "facts": ["The United States Air Force is part of the military.", "Dunkin Donuts offers a military discount. "], "decomposition": ["What is the The United States Air Force a branch of?", "What groups of people get a discount at Dunkin Donuts?", "Is there any overlap between #1 and #2?"], "evidence": [[[["United States Air Force-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["United States Air Force-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["United States Air Force-1"]], [["Discounts and allowances-30"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["The United States Air Force (USAF) is the aerial warfare service branch of the United States Armed Forces."]]}, {"qid": "6701c806cb12244cbd6c", "term": "Bitcoin", "description": "decentralized cryptocurrency", "question": "Can you put bitcoin in your pocket?", "answer": true, "facts": ["Bitcoin is a digital crypto currency.", "Bitcoin can be stored in tangible wallets, called hard wallets.", "Cryptocurrency hard wallets are the size of a thumb drive.", "Thumb drives can fit in your pocket."], "decomposition": ["What kind of currency is bitcoin?", "What are some common ways of storing #1?", "Which of #2 is a physical object?", "Is #3 small enough to fit in a pocket?"], "evidence": [[[["Bitcoin-1"]], [["Bitcoin-44"]], [["Bitcoin-47"]], ["no_evidence"]], [[["Cryptocurrency-2"]], [["Bitcoin-47"]], ["operation"], ["operation"]], [[["Bitcoin-1"]], [["Bitcoin-46", "Bitcoin-49"]], ["operation"], ["operation"]]], "golden_sentence": [["It is a decentralized digital currency without a central bank or single administrator that can be sent from user to user on the peer-to-peer bitcoin network without the need for intermediaries."], [""], ["The paper wallet can then be stored in a safe physical location for later retrieval."]]}, {"qid": "50abd6d8d4c0137bb180", "term": "Glutamic acid", "description": "amino acid", "question": "Does Masaharu Morimoto rely on glutamic acid?", "answer": true, "facts": ["Masaharu Morimoto is a Japanese chef", "Japanese cuisine relies on several forms of seaweed as ingredients and flavorings for broth like kombu dashi", "Glutamic acid has been identified as the flavoring component in kombu seaweed"], "decomposition": ["What is Masaharu Morimoto's profession?", "What cuisine does #1 make?", "What is a main ingredient in #2?", "Is glutamic acid a flavoring component in #3?"], "evidence": [[[["Masaharu Morimoto-1"]], [["Masaharu Morimoto-2"]], [["Monosodium glutamate-2"]], [["Glutamic acid-3"]]], [[["Masaharu Morimoto-1"]], [["Masaharu Morimoto-1"]], [["Rice-8"]], ["no_evidence"]], [[["Masaharu Morimoto-1"]], [["Masaharu Morimoto-1"]], [["Japanese cuisine-2", "Soy sauce-6"], "no_evidence"], [["Glutamic acid-22"], "operation"]]], "golden_sentence": [["Masaharu Morimoto (\u68ee\u672c \u6b63\u6cbb, Morimoto Masaharu, born May 26, 1955, in Hiroshima, Japan) is a Japanese chef, best known as an Iron Chef on the Japanese TV cooking show Iron Chef and its spinoff Iron Chef America."], ["Morimoto received practical training in sushi and traditional Kaiseki cuisine in Hiroshima, and opened his own restaurant in that city in 1980."], [""], [""]]}, {"qid": "c4bc677ca9102779f7c4", "term": "Koala", "description": "An arboreal herbivorous marsupial native to Australia.", "question": "Would a nickel fit inside a koala pouch?", "answer": true, "facts": ["Koala joeys (babies) enter their mother's pouch when they are about 2 to 3 centimeters long.", "An American nickel is 2.12 centimeters in diameter."], "decomposition": ["Who usually sits in a koala's pouch?", "What is the size of #1?", "How big is a nickel?", "Is #2 more than #3?"], "evidence": [[[["Koala-2"]], [["Koala-25", "Marsupial-26"]], [["Nickel (United States coin)-1"]], ["operation"]], [[["Koala-2"]], [["Koala-1"], "no_evidence"], [["Nickel (United States coin)-1"]], ["operation"]], [[["Koala-2"]], [["Koala-23", "Koala-24"]], [["Nickel (United States coin)-1"]], ["operation"]]], "golden_sentence": [[""], ["Gradually, it spends more time away from its mother, and at 12 months it is fully weaned, weighing around 2.5\u00a0kg (5.5\u00a0lb).", "The blind, furless, miniature newborn, the size of a jelly bean, crawls across its mother's fur to make its way into the pouch, where it latches onto a teat for food."], ["Composed of 75%\u00a0copper and 25% nickel, the piece has been issued since 1866."]]}, {"qid": "1e11758f63f0295db162", "term": "Christmas carol", "description": "Song or hymn or carol on the theme of Christmas", "question": "When the shuttle Columbia 11 landed, was it the season for Christmas carols?", "answer": true, "facts": ["The Columbia 11 shuttle landed on December 10th 1990.", "Christmas is celebrated during the month of December every year."], "decomposition": ["What month did the space shuttle Columbia 11 land?", "In what month are Christmas carols typically sung?", "Are #1 and #2 the same answer?"], "evidence": [[[["STS-40-1"], "no_evidence"], [["Christmas-1"]], ["operation"]], [[["STS-40-7"]], [["Christmas and holiday season-1", "Christmas carol-1"]], ["operation"]], [[["Space Shuttle Columbia-1"], "no_evidence"], [["Christmas and holiday season-1", "Christmas carol-1"]], ["no_evidence", "operation"]]], "golden_sentence": [["STS-40, the eleventh launch of Space Shuttle Columbia, was a nine-day mission in June, 1991."], ["Christmas (or Feast of the Nativity) is an annual festival commemorating the birth of Jesus Christ, observed primarily on December 25 as a religious and cultural celebration among billions of people around the world."]]}, {"qid": "d9bc782730b16fa7f9dd", "term": "Kurt Cobain", "description": "American singer, composer, and musician", "question": "Did Kurt Cobain's music genre survive after his death?", "answer": true, "facts": ["Kurt Cobain was the lead singer of Nirvana.", "Nirvana's music is classified as Grunge rock.", "Kurt Cobain died on April 5, 1994.", "Some of the major Grunge rock bands included Alice in Chains, Pearl Jam, and Soundgarden.", "Alice in Chains and Pearl Jam released their latest albums in 2018 and 2020 respectively."], "decomposition": ["What is the musiucal genre associated with both Kurt Cobain and Pearl Jam?", "What year did Kurt Cobain die? ", "Did Pearl Jam release a #1 genre album after #2?", "Is #3 yes?"], "evidence": [[[["Grunge-2"]], [["Kurt Cobain-1"]], [["No Code-1"]], [["No Code-1"]]], [[["Kurt Cobain-2", "Pearl Jam-2"]], [["Kurt Cobain-55"]], [["Vitalogy-9"], "no_evidence"], ["operation"]], [[["Grunge-2"]], [["Kurt Cobain-1"]], [["Lightning Bolt (Pearl Jam album)-11"]], ["operation"]]], "golden_sentence": [["Grunge was commercially successful in the early to mid-1990s, due to releases such as Nirvana's Nevermind, Pearl Jam's Ten, Soundgarden's Badmotorfinger, Alice in Chains' Dirt and Stone Temple Pilots' Core."], ["Kurt Donald Cobain (February 20, 1967\u00a0\u2013 April 5, 1994) was an American singer, songwriter, and musician, best known as the guitarist and frontman of the rock band Nirvana."], [""], [""]]}, {"qid": "de13a5b9f491fce8bedf", "term": "Forbidden City", "description": "Art museum, Imperial Palace, Historic site in Beijing, China", "question": "Are people banned from entering the Forbidden City?", "answer": false, "facts": ["The Forbidden City is a tourist attraction.", "Tourist attractions allow people to enter."], "decomposition": ["Is the Forbidden City a tourist attraction?", "Are tourist attractions open to the public?", "Are the answers to #1 and #2 the same?"], "evidence": [[[["History of the Forbidden City-12"]], [["Tourist attraction-1"]], ["operation"]], [[["Forbidden City-3"]], [["Tourist attraction-1"]], ["operation"]], [[["Forbidden City-3"]], [["Tourist attraction-1"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "cb51d4838aea52fbe884", "term": "Noah's Ark", "description": "the vessel in the Genesis flood narrative", "question": "Were there eight humans on Noah's Ark?", "answer": true, "facts": ["Noah only took his family aboard the Ark.", "Noah brought his wife, three sons, and his sons' wives.", "Four couples lived on the Ark, eight total people."], "decomposition": ["How many people entered Noah's Ark?", "Is #1 greater than or equal to eight?"], "evidence": [[[["Wives aboard Noah's Ark-6"]], [["Wives aboard Noah's Ark-6"]]], [[["Wives aboard Noah's Ark-6"]], ["operation"]], [[["Wives aboard Noah's Ark-1"]], ["operation"]]], "golden_sentence": [["1 Peter 3:20 (written in the late 1st century AD) states that there were eight people on the Ark."], [""]]}, {"qid": "adf9591f64fd7c083c75", "term": "Nikola Tesla", "description": "Serbian American inventor", "question": "Was Nikola Tesla's home country involved in the American Civil War?", "answer": false, "facts": ["Nikola Tesla was born in the Austrian Empire", "The American Civil War was a domestic American conflict"], "decomposition": ["What country was Nikola Tesla born in?", "What countries were involved in the American Civil War?", "Is #1 listed in #2?"], "evidence": [[[["Nikola Tesla-5"]], [["American Civil War-1"]], ["operation"]], [[["Nikola Tesla-2"]], [["American Civil War-1", "American Civil War-7"]], ["operation"]], [[["Nikola Tesla-5"]], [["American Civil War-1"]], ["operation"]]], "golden_sentence": [["Nikola Tesla was born an ethnic Serb in the village Smiljan, Lika county, in the Austrian Empire (present day Croatia), on 10 July [O.S."], ["The American Civil War (also known by other names) was a civil war in the United States from 1861 to 1865, fought between the northern United States (loyal to the Union) and the southern United States (that had seceded from the Union and formed the Confederacy)."]]}, {"qid": "938275cbc3e6bf189acb", "term": "Queen Elizabeth The Queen Mother", "description": "Queen consort of King George VI, mother of Queen Elizabeth II", "question": "Did Queen Elizabeth The Queen Mother and her daughter share name with Tudor queen?", "answer": true, "facts": ["Queen Elizabeth the Queen Mother gave birth to Queen Elizabeth II in 1926.", "The Tudor dynasty had a number of Queens including: Mary I of England, Elizabeth I of England, and Margaret Tudor, Queen of Scots."], "decomposition": ["Which name did the Queen Mother and Queen Elizabeth have in common?", "What are the names of some queens from the Tudor dynasty?", "Is #1 included in any of #2?"], "evidence": [[[["Elizabeth II-1", "Queen Elizabeth The Queen Mother-1"]], [["House of Tudor-1"]], ["operation"]], [[["Queen Elizabeth The Queen Mother-1"]], [["Elizabeth I of England-1", "Mary I of England-1"]], ["operation"]], [[["Queen Elizabeth The Queen Mother-1"]], [["House of Tudor-1"]], ["operation"]]], "golden_sentence": [["", "Lady Elizabeth Angela Marguerite Bowes-Lyon (4 August 1900 \u2013 30 March 2002) was the wife of King George VI, and the mother of Queen Elizabeth II and Princess Margaret, Countess of Snowdon."], ["Tudor monarchs ruled the Kingdom of England and its realms, including their ancestral Wales and the Lordship of Ireland (later the Kingdom of Ireland) from 1485 until 1603, with five monarchs in that period: Henry VII, Henry VIII, Edward VI, Mary I and Elizabeth I."]]}, {"qid": "dde20a113a29e2443ab0", "term": "Compact disc", "description": "Optical disc for storage and playback of digital audio", "question": "Did John Lennon listen to Compact discs?", "answer": false, "facts": ["The Compact disc was released in 1982 by Philips and Sony.", "John Lennon was killed on December 8, 1980."], "decomposition": ["When were Compact Discs first available for use?", "When did John Lennon die?", "Is #1 before #2?"], "evidence": [[[["Compact disc-1"]], [["John Lennon-1"]], ["operation"]], [[["Compact disc-1"]], [["John Lennon-1"]], ["operation"]], [[["Compact disc-1"]], [["John Lennon-36"]], ["operation"]]], "golden_sentence": [["Compact disc (CD) is a digital optical disc data storage format that was co-developed by Philips and Sony and released in 1982."], ["John Winston Ono Lennon MBE (born John Winston Lennon, 9 October 1940\u00a0\u2013\u00a08 December 1980) was an English singer, songwriter and peace activist who gained worldwide fame as the founder, co-lead vocalist, and rhythm guitarist of the Beatles."]]}, {"qid": "fbefcad9d8bc97b69f65", "term": "Tumulus", "description": "Mound of earth and stones raised over graves", "question": "Has a tumulus been discovered on Mars?", "answer": false, "facts": ["A tumulus is a mound of earth and stones raised over a grave or graves.", "A grave is a location where a dead body (typically that of a human, although sometimes that of an animal) is buried.", "Mars has only been explored by unmanned spacecraft.", "Evidence suggests that the planet was once significantly more habitable than it is today, but whether living organisms ever existed there remains unknown. "], "decomposition": ["What do tumulus cover?", "What do #1 contain?", "What is a previous condition for something to be considered now #2?", "Have things with the characteristic of #3 been to Mars?"], "evidence": [[[["Tumulus-1"]], [["Grave-1"]], [["Death-1"]], [["Mars-58"], "operation"]], [[["Tumulus-1"]], [["Tumulus-6"], "no_evidence"], [["Funeral-1"], "no_evidence"], [["Life on Mars-1"], "operation"]], [[["Tumulus-1"]], [["Tumulus-1"]], [["Grave-1"]], [["Mars-20", "Mars-85"]]]], "golden_sentence": [["A tumulus (plural tumuli) is a mound of earth and stones raised over a grave or graves."], [""], [""], [""]]}, {"qid": "67bb78609c60f9f793eb", "term": "League of Legends", "description": "Multiplayer online battle arena video game", "question": "Could Cosmic Girls play League of Legends alone?", "answer": true, "facts": ["Cosmic Girls is a 13 member kpop group", "League of Legends is a video game requiring two teams of five players each"], "decomposition": ["How many players are needed for a League of Legends match?", "How many people are in the group \"Cosmic Girls\"?", "Is #2 greater than or equal to #1?"], "evidence": [[[["League of Legends: Wild Rift-2"], "no_evidence"], [["Cosmic Girls-1"]], ["operation"]], [[["League of Legends: Wild Rift-4"]], [["Cosmic Girls-1"]], ["operation"]], [[["League of Legends-10"]], [["Cosmic Girls-1"]], ["operation"]]], "golden_sentence": [[""], ["and with twelve members: Seola, Xuanyi, Bona, Exy, Soobin, Luda, Dawon, Eunseo, Cheng Xiao, Meiqi, Yeoreum and Dayoung."]]}, {"qid": "924ea9c1e37fdb4872c2", "term": "Torso", "description": "the central part of the living body", "question": "is the brain located in the torso?", "answer": false, "facts": ["The brain is located inside the head.", "The head is located on top of the torso. ", "The torso contains the heart, lungs, and stomach."], "decomposition": ["What part of the body stores the brain?", "Is #1 part of the torso?"], "evidence": [[[["Brain-1"]], [["Torso-1"], "operation"]], [[["Skull-1"]], [["Torso-1"]]], [[["Brain-1"]], [["Head-3"]]]], "golden_sentence": [["These neurons communicate with one another by means of long protoplasmic fibers called axons, which carry trains of signal pulses called action potentials to distant parts of the brain or body targeting specific recipient cells."], ["The torso includes: the thoracic segment of the trunk, the abdominal segment of the trunk, and the perineum."]]}, {"qid": "7e62251c48224c7b30e2", "term": "Augustus", "description": "First emperor of the Roman Empire", "question": "Was Augustus his real name?", "answer": false, "facts": ["Augustus was given the name Gaius Octavius at birth.", "After he was adopted by his uncle Julius Caesar, he took the name Gaius Iulius Caesar.", "He took the name Augustus upon the breaking of the ruling Triumvirate and becoming Emperor."], "decomposition": ["What name did Augustus have when he was born?", "Is #1 identical to Augustus?"], "evidence": [[[["Augustus-2"]], ["operation"]], [[["Augustus-2"]], ["operation"]], [[["Augustus-2"]], ["operation"]]], "golden_sentence": [["Augustus was born Gaius Octavius into an old and wealthy equestrian branch of the plebeian gens Octavia."]]}, {"qid": "e94225ac255761e2974d", "term": "Spice Girls", "description": "British girl group", "question": "Were the Spice Girls inspired by Little Mix?", "answer": false, "facts": ["The Spice Girls were formed in 1994 and mainly active during the late 1990s", "Little Mix was formed in 2011"], "decomposition": ["When was the English pop group Spice Girls formed?", "When was the British girl group Little Mix formed?", "Is #2 before #1?"], "evidence": [[[["Spice Girls-1"]], [["Little Mix-1"]], ["operation"]], [[["Spice Girls-1"]], [["Little Mix-1"]], ["operation"]], [[["Spice Girls-1"]], [["Little Mix-1"]], ["operation"]]], "golden_sentence": [["The Spice Girls are an English pop girl group formed in 1994."], ["Little Mix are a British girl group formed in 2011 during the eighth series of the British version of The X Factor."]]}, {"qid": "4ec5f5249bcc2ff39d2a", "term": "Elizabeth II", "description": "Queen of the United Kingdom and the other Commonwealth realms", "question": "Does the actress who played Elizabeth II speak fluent Arabic?", "answer": false, "facts": ["Elizabeth II was portrayed by Helen Mirren. ", "Helen Mirren doesn't speak fluent Arabic. "], "decomposition": ["Which movie has portrayed Queen Elizabeth II of the United Kingdom?", "Who acted as Queen Elizabeth II in #1?", "What is #2's nationality?", "Do they speak fluent Arabic in #3?"], "evidence": [[[["The Queen (2006 film)-2"]], [["The Queen (2006 film)-2"]], [["Helen Mirren-1", "Helen Mirren-5"]], [["English language in England-1"]]], [[["The Queen (2006 film)-4"]], [["The Queen (2006 film)-11"]], [["Helen Mirren-1"]], [["England-2"], "operation"]], [[["Elizabeth (film)-1"]], [["Elizabeth (film)-1"]], [["Cate Blanchett-5"]], [["Arab world-1"], "no_evidence"]]], "golden_sentence": [[""], ["The film was directed by Stephen Frears, written by Peter Morgan, and starred Helen Mirren in the title role of Queen Elizabeth II."], ["", "Kathleen was a working-class Englishwoman from West Ham, East London, the 13th of 14 children born to a butcher whose own father had been the butcher to Queen Victoria."], [""]]}, {"qid": "580994a06dc56bba67aa", "term": "Super Mario", "description": "platform video game series from Nintendo's Mario franchise", "question": "Does Super Mario require electricity to play?", "answer": true, "facts": ["Super Mario is a video game.", "Video games are played on electronic devices.", "Electronic devices require electricity to function."], "decomposition": ["What is Super Mario?", "Where are #1 played?", "Do #2 require electricity?"], "evidence": [[[["Super Mario-1"]], [["Nintendo video game consoles-1"]], [["Nintendo video game consoles-1"]]], [[["Super Mario-1"]], [["Nintendo Entertainment System-2"]], [["Nintendo Entertainment System-12"], "operation"]], [[["Super Mario-1"]], [["Super Mario-1", "Video game console-3"]], [["Video game console-3"]]]], "golden_sentence": [["Super Mario is a Japanese platform video game series and media franchise created by Nintendo and featuring their mascot, Mario."], [""], [""]]}, {"qid": "8f89cea57e7e3865110f", "term": "Stroke", "description": "Medical condition where poor blood flow to the brain causes cell death", "question": "Is it impossible to tell if someone is having a stroke?", "answer": false, "facts": ["Strokes have numerous physical symptoms including facial unevenness and trouble walking.", "Strokes have behavioral symptoms including slurred speech, disorientation, and trouble understanding speech."], "decomposition": ["What are the symptoms of a stroke?", "Are all of #1 hidden from physical observation?"], "evidence": [[[["Stroke-1"]], ["operation"]], [[["FAST (stroke)-2"]], ["no_evidence"]], [[["Stroke-15"]], [["Stroke-15"], "no_evidence"]]], "golden_sentence": [["Signs and symptoms of a stroke may include an inability to move or feel on one side of the body, problems understanding or speaking, dizziness, or loss of vision to one side."]]}, {"qid": "964257116124413fa3ba", "term": "Christopher Reeve", "description": "20th-century American actor, director, producer and screenwriter", "question": "If he were poor, would Christopher Reeve have lived?", "answer": false, "facts": ["Christopher Reeve suffered a serious spinal cord injury that left him a quadriplegic. ", "Christopher Reeve required a portable ventilator after his injury.", "At one point, Christopher Reeve's treatment was costing $400,000 yearly."], "decomposition": ["What injury did Christopher Reeve suffer from?", "What equipment is required for someone with #1 in order to live?", "What would be the cost of #2?", "Would a poor person be able to afford #3?"], "evidence": [[[["Christopher Reeve-3"]], [["Mobility aid-1"]], [["Disability-68"]], [["Poverty-27"], "operation"]], [[["Christopher Reeve-55"]], [["Christopher Reeve-3"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Christopher Reeve-57"], "operation"], [["Christopher Reeve-58"], "no_evidence"], ["no_evidence"], ["no_evidence"]]], "golden_sentence": [["On May 27, 1995, Reeve was left quadriplegic after being thrown from a horse during an equestrian competition in Culpeper, Virginia."], [""], [""], [""]]}, {"qid": "f94c793a86dec92a26f4", "term": "2008 Summer Olympics", "description": "Games of the XXIX Olympiad, held in Beijing in 2008", "question": "Could all of the 2008 Summer Olympics women find a hookup athlete partner?", "answer": true, "facts": ["The 2008 Summer Olympics had 4,637 women compete.", "The 2008 Summer Olympics had 6,305 men compete."], "decomposition": ["How many women participated in the 2008 Summer Olympics?", "How many men participated in the 2008 Summer Olympics?", "Is #2 at least equal to #1?"], "evidence": [[[["2008 Summer Olympics-1", "2008 Summer Olympics-2"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Athletics at the 2008 Summer Olympics-2"], "no_evidence"], [["Athletics at the 2008 Summer Olympics-2"], "no_evidence"], ["operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["", ""]]}, {"qid": "d2b32f0ee7b9f5857c53", "term": "Chinese New Year", "description": "traditional Chinese holiday", "question": "Are any mollusks on Chinese New Year calendar?", "answer": false, "facts": ["A mollusk is an invertebrate animal such as snails, slugs, mussles, and octopuses.", "The animals on Chinese New Year calendar are: rat, ox, tiger, rabbit, dragon, snake, horse, goat, monkey, rooster, dog, and pig."], "decomposition": ["What are the animals on the Chinese New Year calendar?", "Is a mollusk part of #1?"], "evidence": [[[["Chinese zodiac-4", "Chinese zodiac-5"]], [["Marine invertebrates-26"], "operation"]], [[["Chinese New Year-6", "Chinese zodiac-17", "Chinese zodiac-18", "Chinese zodiac-19", "Chinese zodiac-20"]], ["operation"]], [[["Chinese zodiac-5"]], ["operation"]]], "golden_sentence": [["", "Rat \u2013 \u9f20, sh\u01d4 (\u5b50) (Yang, 1st Trine, Fixed Element Water) Ox \u2013 \u725b, ni\u00fa (\u4e11) (Yin, 2nd Trine, Fixed Element Earth) Tiger \u2013 \u864e, h\u01d4 (\u5bc5) (Yang, 3rd Trine, Fixed Element Wood) Rabbit \u2013 \u5154, t\u00f9 (\u536f) (Yin, 4th Trine, Fixed Element Wood) Dragon \u2013 \u9f99/\u9f8d, l\u00f3ng (\u8fb0) (Yang, 1st Trine, Fixed Element Earth) Snake \u2013 \u86c7, sh\u00e9 (\u5df3) (Yin, 2nd Trine, Fixed Element Fire) Horse \u2013 \u9a6c/\u99ac, m\u01ce (\u5348) (Yang, 3rd Trine, Fixed Element Fire) Goat \u2013 \u7f8a, y\u00e1ng (\u672a) (Yin, 4th Trine, Fixed Element Earth) Monkey \u2013 \u7334, h\u00f3u (\u7533) (Yang, 1st Trine, Fixed Element Metal) Rooster \u2013 \u9e21/\u96de, j\u012b (\u9149) (Yin, 2nd Trine, Fixed Element Metal) Dog \u2013 \u72d7, g\u01d2u (\u620c) (Yang, 3rd Trine, Fixed Element Earth) Pig \u2013 \u732a/\u8c6c, zh\u016b (\u4ea5) (Yin, 4th Trine, Fixed Element Water) In Chinese astrology the animal signs assigned by year represent how others perceive you or how you present yourself."], [""]]}, {"qid": "abf991140f72f7fef906", "term": "Skull", "description": "bony structure that forms the skeleton of head in most vertebrates", "question": "Can an adult human skull hypothetically pass through the birth canal?", "answer": true, "facts": ["The largest baby ever born was 22 pounds. ", "The average human skull weighs between 10 and 11 pounds."], "decomposition": ["How big is the average baby ever delivered vaginally?", "How big is the average adult skull?", "Is #1 greater than #2?"], "evidence": [[[["Childbirth-29"], "no_evidence"], [["Human head-18"]], [["Obstetrical dilemma-14"], "operation"]], [[["Infant-9"]], [["Human head-18"]], [["Human head-18", "Infant-9"]]], [[["Infant-5", "Infant-7"]], [["Skull-1"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["[dubious \u2013 discuss] A British study by Newcastle University showed an average size of 57.2\u00a0cm for males and 55.2\u00a0cm for females with average size varying proportionally with height"], [""]]}, {"qid": "568b5f95014189a24119", "term": "Elijah Cummings", "description": "U.S. Representative from Maryland", "question": "Will Elijah Cummings cast a vote in the 2020 presidential election?", "answer": false, "facts": ["Elijah Cummings died on October 17th, 2019.", "It is not possible, or legal, for a dead person to cast a vote in a presidential election."], "decomposition": ["Are deceased people able and allowed to vote in elections?", "Is Elijah Cummings deceased?", "Are the answers to #1 and #2 the same?"], "evidence": [[[["Voting rights in the United States-2"], "no_evidence"], [["Elijah Cummings-1"]], ["operation"]], [["no_evidence", "operation"], [["Elijah Cummings-1"]], ["no_evidence", "operation"]], [[["Voter impersonation (United States)-1", "Voter impersonation (United States)-9"]], [["Elijah Cummings-24"]], ["operation"]]], "golden_sentence": [[""], ["Elijah Eugene Cummings (January 18, 1951 \u2013 October 17, 2019) was an American politician and civil rights advocate who served in the United States House of Representatives for Maryland's 7th congressional district from 1996 until his death in October of 2019."]]}, {"qid": "5f07a0730a85bdac05c4", "term": "Super Mario", "description": "platform video game series from Nintendo's Mario franchise", "question": "Does Super Mario mainly focus on a man in green?", "answer": false, "facts": ["Super Mario follows the adventures of a plumber named Mario.", "Mario wears a red shirt and plumber's overalls."], "decomposition": ["Who is the main character of the game Super Mario?", "Does #1 wear green?"], "evidence": [[[["Super Mario-1"]], [["Mario-29"]]], [[["Super Mario-2"]], [["Mario-6"], "operation"]], [[["Super Mario-1"]], [["Mario-6"]]]], "golden_sentence": [[""], [""]]}, {"qid": "fcf5dbab2721c934f751", "term": "Julia Roberts", "description": "American actress and producer", "question": "Does Julia Roberts lose the prolific acting contest in her family?", "answer": true, "facts": ["As of May 2020, Julia Roberts has acted in 64 projects.", "Julia Roberts has a brother in acting, Eric Roberts, and a niece in acting, Emma Roberts.", "As of May 2020, Eric Roberts has acted in 577 projects."], "decomposition": ["Who is Julia Roberts brother?", "Is #1 an actor?", "How many projects has #2 appeared in?", "How many projects has Julia Roberts acted in?", "Is #3 larger than #4?"], "evidence": [[[["Julia Roberts-4"]], [["Eric Roberts-1"]], [["Eric Roberts-2"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Eric Roberts-1"]], [["Eric Roberts-1"]], [["Eric Roberts filmography-4"], "no_evidence"], [["Julia Roberts-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Eric Roberts-3"]], [["Eric Roberts-1"]], [["Eric Roberts-2"]], [["Julia Roberts filmography-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["Her older brother Eric Roberts (b."], ["Eric Anthony Roberts (born April 18, 1956) is an American actor."], [""]]}, {"qid": "a32a8b1536a27accaad7", "term": "Olive oil", "description": "liquid fat extracted by pressing olives", "question": "Can olive oil kill rabies?", "answer": false, "facts": ["Olive oil is a fat used in cooking.", "Olive oil is made up of palmitic acid which is a weak acid found in plants and animals.", "Rabies is a disease from an infected animal bite.", "Rabies is treated by a shot containing immunoglobuin, a protein that is found in plasma cells.", "Plasma cells are found in the bone marrow of humans."], "decomposition": ["What is used to treat rabies?", "What is olive oil made of?", "Are any of #2 present in #1?"], "evidence": [[[["Rabies-31"]], [["Olive oil-1"]], ["operation"]], [[["Rabies-30", "Rabies-33"]], [["Olive oil-1"]], ["operation"]], [[["Rabies-30"]], [["Olive oil-3"]], ["operation"]]], "golden_sentence": [[""], ["Olive oil is a liquid fat obtained from olives (the fruit of Olea europaea; family Oleaceae), a traditional tree crop of the Mediterranean Basin."]]}, {"qid": "777d7f2b690dbb36ed00", "term": "Ludacris", "description": "American rapper and actor", "question": "Does Ludacris perform classical music?", "answer": false, "facts": ["Ludacris is a rap artist.", "Rap and hip hop music are not related to classical music."], "decomposition": ["Which kind of music does Ludacris perform?", "Is #1 the same as classical music?"], "evidence": [[[["Ludacris-6"]], ["operation"]], [[["Ludacris-6"]], ["operation"]], [[["Ludacris-1"]], ["operation"]]], "golden_sentence": [["This album was the defining example of Ludacris' fast, wild, and comedic flow, a unique style for southern rappers."]]}, {"qid": "e5af10ca14ddc5612519", "term": "U2", "description": "Four-member Irish rock band, from Dublin", "question": "Did U2 play a concert at the Polo Grounds?", "answer": false, "facts": ["U2 is an Irish rock band that formed in 1976.", "The Polo Grounds was a sports stadium that was demolished in 1964."], "decomposition": ["When was U2 (Irish rock band) formed?", "When was the Polo Grounds demolished?", "Is #1 before #2?"], "evidence": [[[["U2-1"]], [["Polo Grounds-4"]], ["operation"]], [[["U2-1"]], [["Polo Grounds-1"]], ["operation"]], [[["U2-1"]], [["Polo Grounds-32"]], ["operation"]]], "golden_sentence": [["U2 are an Irish rock band from Dublin, formed in 1976."], [""]]}, {"qid": "69c894404a159ae56e80", "term": "Carl Friedrich Gauss", "description": "German mathematician and physicist", "question": "Did Gauss have a normal brain structure?", "answer": false, "facts": ["When Gauss died in 1855, his brain was preserved for study.", "Dr. Rudolf Wagner, who studied the brain, found the mass to be slightly above average, and found highly developed convolutions on the brain."], "decomposition": ["What was the outcome of studies carried out on Gauss' brain after his death?", "Did #1 indicate that his brain was the same as the average human brain?"], "evidence": [[[["Carl Friedrich Gauss-13"]], ["operation"]], [[["Carl Friedrich Gauss-13"]], [["Carl Friedrich Gauss-13"]]], [[["Carl Friedrich Gauss-13"]], ["operation"]]], "golden_sentence": [["Gauss's brain was preserved and was studied by Rudolf Wagner, who found its mass to be slightly above average, at 1,492\u00a0grams, and the cerebral area equal to 219,588 square millimeters (340.362 square inches)."]]}, {"qid": "32886c6c0ce99e4d43ea", "term": "Sudoku", "description": "Logic-based number-placement puzzle", "question": "Could an infant solve a sudoku puzzle?", "answer": false, "facts": ["Solving a sudoku puzzle requires the use of logic and a basic understanding of numbers.", "Infants are too young to understand the numerical system involved in sudoku."], "decomposition": ["What is the skill set of an infant?", "What skills are required for sudoku?", "Is #2 included in #1?"], "evidence": [[[["Infant-2"], "no_evidence"], [["Sudoku-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Sudoku-11"]], [["Logic puzzle-1"]], [["Sudoku-1"]]], [[["Infant cognitive development-12"]], [["Sudoku code-6"]], [["Infant cognitive development-12", "Sudoku code-6"], "operation"]]], "golden_sentence": [["A newborn is, in colloquial use, an infant who is only hours, days, or up to one month old."], [""]]}, {"qid": "ca494b22bc6c332e7967", "term": "Eid al-Fitr", "description": "Islamic holiday that marks the end of Ramadan", "question": "Is Eid al-Fitr holiday inappropriate to watch entire US Office?", "answer": true, "facts": ["Eid al-Fitr is an Islamic holiday dedicated to prayer.", "Eid al_fitr lasts from 1 to 3 days depending on the country.", "The entire US Office tv show would take 4 days, three hours, and 30 minutes to watch."], "decomposition": ["How long does Eid al-Fitr last?", "What is the run time of the Office?", "Is #2 longer than #1?"], "evidence": [[[["Eid al-Fitr-4"]], [["The Office (American TV series)-2"], "no_evidence"], ["operation"]], [[["Eid al-Fitr-4"]], [["Finale (The Office)-1", "The Office (American TV series)-8"]], ["operation"]], [[["Eid al-Fitr-1"]], [["The Office-1"]], ["operation"]]], "golden_sentence": [["Eid al-Fitr is celebrated for one to three days, depending on the country."], ["The series debuted on NBC as a mid-season replacement and aired 201 episodes over the course of its run."]]}, {"qid": "ac2b2f72717081fbc08c", "term": "Japan Airlines", "description": "airline headquartered in Tokyo, Japan", "question": "Are any of the destinations of Japan Airlines former Axis Powers?", "answer": true, "facts": ["Japan Airlines flies all over the world to places such as Germany, Ireland, and Australia.", "The Axis Powers were the countries that fought against the Allies during World War II.", "Axis Powers included countries such as Germany, Italy, and Japan."], "decomposition": ["Which countries does Japan Airlines fly to?", "Which counties were part of the Axis powers?", "Are there any similarities or overlap between #1 and #2?"], "evidence": [[[["Japan Airlines-2"], "no_evidence"], [["Axis powers-24"]], ["no_evidence"]], [[["Japan Airlines-2", "Japan Airlines-84"], "no_evidence"], [["Axis powers-2"]], ["operation"]], [[["Japan Airlines-63", "Japan Airlines-84"], "no_evidence"], [["Axis powers-1", "Axis powers-225"]], ["operation"]]], "golden_sentence": [["JAL group operations include scheduled and unscheduled international and domestic passenger and cargo services to 220 destinations in 35 countries worldwide, including codeshares."], [""]]}, {"qid": "f8292c32e9a30cbfcd28", "term": "WWE Raw", "description": "WWE television program", "question": "Would a viewer of Monday Night Football be able to catch WWE Raw during commercial breaks?", "answer": true, "facts": ["Monday Night Football begins at 8pm EST on Monday nights during the NFL season", "WWE Raw airs on Monday nights between 8pm and 11pm EST"], "decomposition": ["When does Monday Night Football air?", "When does WWE Raw air?", "Is there and overlap between #1 and #2?"], "evidence": [[[["Monday Night Football-68"]], [["WWE Raw-1"]], ["operation"]], [[["Monday Night Football-7"], "operation"], [["WWE Raw-59"], "operation"], ["operation"]], [[["Monday Night Football-68"]], [["WWE Raw-1"]], ["operation"]]], "golden_sentence": [[""], ["WWE Raw, also known as Monday Night Raw or simply Raw, is an American professional wrestling television program that currently airs live on Monday evenings on the USA Network from 8:00\u201311:00 PM EST in the United States."]]}, {"qid": "c34be372a25c78f34449", "term": "Voyager 2", "description": "Space probe and the second-farthest man-made object from Earth", "question": "Would Jon Brower Minnoch break a chair before Voyager 2 launch mass?", "answer": false, "facts": ["Jon Brower Minnoch was the heaviest human being ever recorded.", "At his peak weight, Jon Brower Minnoch weighed almost 1.400 lb.", "The launch mass of Voyager 2 was 1,820 lb."], "decomposition": ["What was Jon Brower Minnoch's heaviest weight?", "What was the Voyager 2 launch mass?", "Is #1 greater than #2?"], "evidence": [[[["Jon Brower Minnoch-1"]], [["Voyager 2-9"]], ["operation"]], [[["Jon Brower Minnoch-1"]], [["Voyager 2-9"]], ["operation"]], [[["Jon Brower Minnoch-5"]], ["no_evidence"], ["operation"]]], "golden_sentence": [["Jon Brower Minnoch (September 29, 1941 \u2013 September 10, 1983) was an American man who, at his peak weight, was the heaviest human being ever recorded, weighing 1,400\u00a0lb (635 kilograms; 100 stone)."], [""]]}, {"qid": "e24fe8e9b363f27c7e72", "term": "Learning disability", "description": "Range of neurodevelopmental conditions", "question": "Do placozoa get learning disabilities?", "answer": false, "facts": ["Learning disabilities are neurodevelopmental conditions afflicting a portion of the human population", "Neurodevelopmental conditions affect the nervous system", "Placozoa are multicellular microscopic organisms which do not have a nervous system"], "decomposition": ["What bodily system do learning disabilities affect?", "Do placozoa possess #1?"], "evidence": [[[["Learning disability-1"]], [["Placozoa-7"], "operation"]], [[["Learning disability-1", "Learning disability-3"], "no_evidence"], [["Placozoa-1"], "operation"]], [[["Learning disability-5"]], [["Placozoa-7"], "operation"]]], "golden_sentence": [[""], ["they possess no tissues or organs."]]}, {"qid": "83a929514b2260a51ad9", "term": "Christmas carol", "description": "Song or hymn or carol on the theme of Christmas", "question": "Are multiple Christmas Carol's named after Saints?", "answer": true, "facts": ["The Christmas Carol Good King Wenceslas is based on the Bohemian king Wenceslaus I.", "Wenceslaus I was named a Saint in the Catholic Church and has a feast day of September 28.", "Jolly Old Saint Nicholas is a Christmas Carol named after an early Christian bishop who became a Saint."], "decomposition": ["Who was Christmas carol 'Good King Wenceslas' about?", "Who was  Christmas carol 'Jolly Old Saint Nicholas' named for?", "Are #1 and #2 Saints?"], "evidence": [[[["Good King Wenceslas-3"]], [["Jolly Old Saint Nicholas-1"]], [["Good King Wenceslas-3", "Jolly Old Saint Nicholas-1"]]], [[["Good King Wenceslas-1"]], [["Saint Nicholas-1"]], [["Saint Nicholas-1", "Wenceslaus I, Duke of Bohemia-2"]]], [[["Good King Wenceslas-1"]], [["Saint Nicholas-1"]], ["operation"]]], "golden_sentence": [[""], ["\"Jolly Old Saint Nicholas\" is a Christmas song that originated with a poem by Emily Huntington Miller (1833-1913), published as \"Lilly's Secret\" in The Little Corporal Magazine in December of 1865."], ["", ""]]}, {"qid": "053135b4442b773d0a44", "term": "New Testament", "description": "Second division of the Christian biblical canon", "question": "Are thetan levels found in the New Testament?", "answer": false, "facts": ["The New Testament is a collection of texts related to Christianity.", "Thetan levels are a term used in the body of religious beliefs and practices known as Scientology."], "decomposition": ["Which religion are thetan levels related to?", "The New Testament is a part of which Holy Book?", "Which religion is #2 associated with?", "Is #1 the same as #3?"], "evidence": [[[["Thetan-1"]], [["Bible-2"]], [["Christian biblical canons-1"]], [["Scientology beliefs and practices-35"], "operation"]], [[["Thetan-6"]], [["Historicity of the Bible-37"]], [["New Testament-137"]], [["New Testament-137", "Scientology-40"], "operation"]], [[["Thetan-1"]], [["New Testament-1"]], [["New Testament-1"]], ["operation"]]], "golden_sentence": [["In Scientology, the concept of the thetan (/\u02c8\u03b8e\u026at\u0259n/) is similar to the concept of self, or the spirit or soul as found in several belief systems."], ["The Hebrew Bible overlaps with the Greek Septuagint and the Christian Old Testament."], ["A Christian biblical canon is the set of books that a particular Christian denomination or denominational family regards as being divinely inspired and thus constituting an authorised Christian Bible."], [""]]}, {"qid": "cea8ecc2225bc660724f", "term": "Honey badger", "description": "species of mammal", "question": "Would a honey badger's dentures be different from a wolverine's?", "answer": true, "facts": ["Dentures are false teeth that resemble the wearer's natural teeth", "Honey badgers and wolverines are physically very similar, but they can be differentiated by their dentition."], "decomposition": ["What subfamily does the honey badger belong to?", "What subfamily does the wolverine belong to?", "What helps distinguish #1 from #2?", "Does #3 include dental shape?"], "evidence": [[[["Honey badger-2"]], [["Wolverine-1"]], [["Honey badger-15"], "no_evidence"], ["operation"]], [[["Honey badger-2"]], [["Wolverine-1"]], [["Honey badger-15"]], [["Wolverine-9"], "operation"]], [[["Honey badger-6"]], [["Wolverine-1"], "no_evidence"], [["Honey badger-15", "Wolverine-14"], "no_evidence"], [["Honey badger-15"], "no_evidence", "operation"]]], "golden_sentence": [["It is the only species in the genus Mellivora and in the mustelid subfamily Mellivorinae."], [""], [""]]}, {"qid": "d4303473b164300b01d5", "term": "United States Secretary of State", "description": "U.S. cabinet member and head of the U.S. State Department", "question": "Does the United States Secretary of State answer the phones for the White House?", "answer": false, "facts": ["The role of United States Secretary of State carries out the President's foreign policy.", "The White House has multiple phone lines managed by multiple people."], "decomposition": ["What are the duties of the US Secretary of State?", "Are answering phones part of #1?"], "evidence": [[[["United States Secretary of State-4"]], [["United States Secretary of State-4"]]], [[["United States Secretary of State-4"]], ["operation"]], [[["United States Secretary of State-4"]], [["United States Secretary of State-4"]]]], "golden_sentence": [["The stated duties of the secretary of state are to supervise the United States foreign service, immigration policy, and administer the Department of State."], [""]]}, {"qid": "00a803841bec177f40cf", "term": "Green", "description": "Additive primary color visible between blue and yellow", "question": "Did the color green help Theodor Geisel become famous?", "answer": true, "facts": ["Green is a color made by mixing blue and yellow", "Theodor Geisel is the American writer known as Dr. Seuss", "One of Dr. Seuss's most famous books and lines is Green Eggs and Ham"], "decomposition": ["What was Theodor Geisel's pen name?", "Which books authored by #1 made him famous?", "Are any of #2 particularly related to the color green?"], "evidence": [[[["Geisel Award-2"]], [["Green Eggs and Ham-5"]], [["Green Eggs and Ham-5"]]], [[["Dr. Seuss-1"]], [["Dr. Seuss-3"]], [["Green Eggs and Ham-3"], "operation"]], [[["Dr. Seuss-1"]], [["Dr. Seuss-3"]], ["operation"]]], "golden_sentence": [["The award is named for Theodor Geisel, also known as Dr. Seuss, who once said, \"Children want the same things we want: to laugh, to be challenged, to be entertained and delighted.\""], ["Green Eggs and Ham is one of Seuss's \"Beginner Books\", written with very simple vocabulary for beginning readers."], ["The 50 words are: a, am, and, anywhere, are, be, boat, box, car, could, dark, do, eat, eggs, fox, goat, good, green, ham, here, house, I, if, in, let, like, may, me, mouse, not, on, or, rain, Sam, say, see, so, thank, that, the, them, there, they, train, tree, try, will, with, would, you."]]}, {"qid": "7a938cd1f5ac5a6c7651", "term": "Ariana Grande", "description": "American singer, songwriter, and actress", "question": "Does Ariana Grande's signature style combine comfort items and high fashion?", "answer": true, "facts": ["Ariana Grande's signature style is a long, over-sized pullover sweater with thigh high heels.", "Oversized pullovers are considered lounge wear, for relaxing at home in. ", "High heels are associated with high style. "], "decomposition": ["What is Ariana Grande's signature top?", "What is Ariana Grande's signature shoewear?", "What type of clothing is #1 considered?", "Is #3 considered a comfort and item and is #2 considered a high style item?"], "evidence": [[[["Ariana Grande-34"], "no_evidence"], [["Ariana Grande-34"], "no_evidence"], ["no_evidence"], ["no_evidence"]], [[["Ariana Grande-34"]], ["no_evidence"], ["no_evidence"], ["operation"]], [[["Ariana Grande-34"]], [["Ariana Grande-34"]], [["Crop top-3"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], ["That year, she began to wear short skirts and crop tops with knee-high boots in performances and on red carpets."]]}, {"qid": "55535d1714c83e995e2d", "term": "Richard III of England", "description": "15th-century King of England", "question": "Did Richard III's father have greater longevity than him?", "answer": true, "facts": ["Richard III lived until the age of 32.", "Richard, Duke of York, Richard III's father, lived until the age of 49."], "decomposition": ["How many years did Richard III live to be?", "Who was Richard III's father?", "How old did #2 live to be?", "Is #3 greater than #1?"], "evidence": [[[["Richard III of England-1"]], [["Richard of York, 3rd Duke of York-1", "Richard of York, 3rd Duke of York-2"]], [["Richard of York, 3rd Duke of York-1"]], ["operation"]], [[["Richard III of England-1"]], [["Richard III of England-5"]], [["Richard of York, 3rd Duke of York-1"]], ["operation"]], [[["Richard III (disambiguation)-1"]], [["Richard III of England-5"]], [["Richard of York, 3rd Duke of York-3", "Sandal Castle-9"]], ["operation"]]], "golden_sentence": [["Richard III (2 October 1452\u00a0\u2013 22 August 1485) was King of England and Lord of Ireland from 1483 until his death in 1485."], ["Richard of York, 3rd Duke of York (22 September 1411\u00a0\u2013 30 December 1460), also named Richard Plantagenet, was a leading English magnate, a great-grandson of King Edward III through his father, and a great-great-grandson of the same king through his mother.", ""], [""]]}, {"qid": "a755f3d1de1a3915b1e2", "term": "Bluetooth", "description": "Short distance wireless technology standard", "question": "Does a dentist treat Bluetooth problems?", "answer": false, "facts": ["A dentist is a surgeon who specializes in dentistry, the diagnosis, prevention, and treatment of diseases and conditions of the oral cavity.", "Technological problems are typically handled by IT professionals.", "Bluetooth is not a physical entity."], "decomposition": ["What type of professional would handle bluetooth problems?", "Are dentists trained in #1?"], "evidence": [[[["Bluetooth-1", "Computer repair technician-1"], "no_evidence"], [["Dentist-1"], "operation"]], [[["Technical support-3"]], [["Dentist-1"]]], [[["Bluetooth-1"], "no_evidence"], [["Dentist-1"]]]], "golden_sentence": [["", ""], ["The dental team includes dental assistants, dental hygienists, dental technicians, and sometimes dental therapists."]]}, {"qid": "f0a99ebe0f848f6513d5", "term": "Walt Disney", "description": "American entrepreneur, animator, voice actor and film producer", "question": "Was Walt Disney ever interviewed by Anderson Cooper?", "answer": false, "facts": ["Walt Disney died on Dec 15, 1966", "Anderson Cooper was born on Jun 03, 1967"], "decomposition": ["When did Walt Disney pass away?", "When was Anderson Cooper born?", "Is #2 before #1?"], "evidence": [[[["Walt Disney-1"]], [["Anderson Cooper-1"]], ["operation"]], [[["Walt Disney-36"]], [["Anderson Cooper-1"]], ["operation"]], [[["Walt Disney-1"]], [["Anderson Cooper-1"]], ["operation"]]], "golden_sentence": [["Walter Elias Disney (/\u02c8d\u026azni/; December 5, 1901\u00a0\u2013\u00a0December 15, 1966) was an American entrepreneur, animator, writer, voice actor and film producer."], ["Anderson Hays Cooper (born June 3, 1967) is an American broadcast journalist."]]}, {"qid": "567cfdcd1729556f1f97", "term": "Frost", "description": "coating or deposit of ice that may form in humid air in cold conditions, usually overnight", "question": "Is it common to see frost during some college commencements?", "answer": true, "facts": ["College commencement ceremonies often happen during the months of December, May, and sometimes June. ", "Frost isn't uncommon to see during the month of December, as it is the winter."], "decomposition": ["What seasons can you expect see frost?", "What months do college commencements occur?", "Do any of #2 occur during #1?"], "evidence": [[[["Frost-1"]], [["Graduation-11"], "no_evidence"], ["operation"]], [[["Frost-30"], "no_evidence"], [["Commencement speech-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Frost-25"], "no_evidence"], [["Commencement at Central Connecticut State University-18"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "d8da647dfc1b9858130a", "term": "Eddie Murphy", "description": "American stand-up comedian and actor", "question": "Could Eddie Murphy's children hypothetically fill a basketball court by themselves?", "answer": true, "facts": ["Eddie Murphy has ten children.", "Basketball is played with two teams, each having five players on the court at one time."], "decomposition": ["How many children does Eddie Murphy have?", "How many players are on a basketball team?", "How many teams are on the basketball court at the same time?", "How much is #2 multiplied by #3?", "Is #1 greater than or equal to #4?"], "evidence": [[[["Eddie Murphy-40", "Eddie Murphy-41", "Eddie Murphy-43"]], [["Basketball-1"]], [["Basketball-1"]], ["operation"], ["operation"]], [[["Eddie Murphy-40"], "no_evidence"], [["Basketball-3"]], [["Basketball-1"]], ["operation"], ["operation"]], [[["Eddie Murphy-41"], "no_evidence"], [["Basketball-3"]], [["Basketball-1"]], ["operation"], ["operation"]]], "golden_sentence": [["Murphy and Mitchell had five children together: Bria, Myles, Shayne, Zola, and Bella.", "", ""], ["Basketball is a team sport in which two teams, most commonly of five players each, opposing one another on a rectangular court, compete with the primary objective of shooting a basketball (approximately 9.4 inches (24\u00a0cm) in diameter) through the defender's hoop (a basket 18 inches (46\u00a0cm) in diameter mounted 10 feet (3.048\u00a0m) high to a backboard at each end of the court) while preventing the opposing team from shooting through their own hoop."], ["Basketball is a team sport in which two teams, most commonly of five players each, opposing one another on a rectangular court, compete with the primary objective of shooting a basketball (approximately 9.4 inches (24\u00a0cm) in diameter) through the defender's hoop (a basket 18 inches (46\u00a0cm) in diameter mounted 10 feet (3.048\u00a0m) high to a backboard at each end of the court) while preventing the opposing team from shooting through their own hoop."]]}, {"qid": "a6918adcd67bada83446", "term": "Snow leopard", "description": "species of mammal", "question": "Can you find a snow leopard in the Yucatan?", "answer": false, "facts": ["Snow leopards are native to mountain ranges in Central and South Asia", "The Yucatan is a peninsula in Mexico", "Mexico is located in North America"], "decomposition": ["On what continent is the Yucatan peninsula?", "On what continent are snow leopards found?", "Is #1 the same as #2?"], "evidence": [[[["Mexico-1", "Yucat\u00e1n Peninsula-1"]], [["Snow leopard-1"]], ["operation"]], [[["Yucat\u00e1n Peninsula-1"]], [["Snow leopard-1"]], ["operation"]], [[["Yucatan woodpecker-5"], "operation"], ["no_evidence"], ["no_evidence"]]], "golden_sentence": [["Mexico (Spanish: M\u00e9xico [\u02c8mexiko] (listen); Nahuatl languages: M\u0113xihco), officially the United Mexican States (Spanish: Estados Unidos Mexicanos, EUM [es\u02c8ta\u00f0os u\u02c8ni\u00f0oz mexi\u02c8kanos] (listen)), is a country in the southern portion of North America.", "The Yucat\u00e1n Peninsula (/\u02ccju\u02d0k\u0259\u02c8t\u0251\u02d0n/, also UK: /\u02ccj\u028ak-/, US: /-\u02c8t\u00e6n, \u02ccju\u02d0k\u0251\u02d0\u02c8t\u0251\u02d0n/; Spanish: Pen\u00ednsula de Yucat\u00e1n), in southeastern Mexico, separates the Caribbean Sea from the Gulf of Mexico, with the northern coastline on the Yucat\u00e1n Channel."], ["The snow leopard (Panthera uncia), also known as the ounce, is a large cat native to the mountain ranges of Central and South Asia."]]}, {"qid": "397a43e3306c77d01a51", "term": "United Nations Conference on Trade and Development", "description": "organization", "question": "Could Edward Snowden have visited the headquarters of United Nations Conference on Trade and Development?", "answer": true, "facts": ["The headquarters of the United Nations Conference on Trade and Development is in Geneva, Switzerland.", "Edward Snowden was stationed in Geneva in 2007 with the task of representing the US at the UN."], "decomposition": ["What city and country is the United Nations Conference on Trade and Development located in?", "In 2007, what was Edward Snowden's tasked with?", "Was Edward Snowden stationed in #1 in 2007 to accomplish #2? "], "evidence": [[[["United Nations Conference on Trade and Development-7"]], [["Edward Snowden-13"]], ["operation"]], [[["Palace of Nations-1"]], [["Edward Snowden-13"]], ["operation"]], [[["United Nations Conference on Trade and Development-3"]], [["Edward Snowden-13"]], ["operation"]]], "golden_sentence": [["Currently, UNCTAD has 195 member states and is headquartered in Geneva, Switzerland."], ["In March 2007, the CIA stationed Snowden with diplomatic cover in Geneva, Switzerland, where he was responsible for maintaining computer-network security."]]}, {"qid": "53bec02e18b4a9e3dd41", "term": "Sony", "description": "Japanese multinational conglomerate corporation", "question": "Did Sony definitively win the video game war against Sega?", "answer": true, "facts": ["Sony is the maker of the Playstation which has sold over 108 million PS4 units by March 2020.", "Sega's last console, the Sega Dreamcast, was discontinued in 2001.", "Sony Playstation competed with Sega's Dreamcast and Saturn systems in the 1990s.", "Sega now makes games for its former competitor, Sony, including Team Sonic Racing in 2019.", "At the height of the console wars, Sega Saturn sold 9.5 million units while Sony Playstation sold 102 million units."], "decomposition": ["How many console did Sega Saturn sell?", "How many console did Sony Playstation?", "Is #2 greater than #1?"], "evidence": [[[["Sega Saturn-25"], "no_evidence"], [["PlayStation-2"]], ["operation"]], [[["Sega Saturn-3"]], [["PlayStation-81"]], [["PlayStation-81"]]], [[["Sega Saturn-3"]], [["PlayStation (console)-2"]], ["operation"]]], "golden_sentence": [["Reflecting decreased demand for the system, worldwide Saturn shipments during March to September 1997 declined from 2.35\u00a0million to 600,000 versus the same period in 1996; shipments in North America declined from 800,000 to 50,000."], ["Sony's next console, the PlayStation 3, was released in 2006, selling over 87.4 million units by March 2017."]]}, {"qid": "faaec0e4250600495b5e", "term": "Winter", "description": "one of the Earth's four temperate seasons, occurring between autumn and spring", "question": "Is winter associated with hot temperatures?", "answer": false, "facts": ["Winter is the season that occurs when a hemisphere is tilted away from the sun during Earth's orbit.", "During this season, that hemisphere gets less sunshine and is further from the sun than the other hemisphere.", "As a result, temperatures in that hemisphere are much colder during that season."], "decomposition": ["What is earth's primary source of heat energy?", "Which parts of the earth experience winters?", "What is the relative orientation of #2 with respect to #1 during winters?", "Will #3 result in hot temperatures in #2?"], "evidence": [[[["Earth-15"], "no_evidence"], [["Winter-1"]], [["Winter-4"]], ["operation"]], [[["Sun-1"]], [["Winter-1"]], [["Winter-1"]], ["operation"]], [[["The Sun-1"]], [["Winter-8"]], [["Winter-8"]], ["operation"]]], "golden_sentence": [[""], ["Winter is the coldest season of the year in polar and temperate zones (winter does not occur in most of the tropical zone)."], [""]]}, {"qid": "ebc71fd8707cb2e117de", "term": "Radioactive waste", "description": "wastes that contain nuclear material", "question": "Does the United States Navy create radioactive waste?", "answer": true, "facts": ["Radioactive waste is created by nuclear material processing", "The United States Navy uses many nuclear submarines"], "decomposition": ["Radioactive waste is a byproduct of what process?", "Does the US Navy engage in any of the activities in #1?"], "evidence": [[[["Radioactive waste-1"]], [["Nuclear submarine-4"]]], [[["Radioactive waste-1"]], [["United States Navy Nuclear Propulsion-1"], "operation"]], [[["Radioactive waste-1"]], [["United States Navy-5"], "no_evidence", "operation"]]], "golden_sentence": [["Radioactive waste is a by-product of various nuclear technology processes."], [""]]}, {"qid": "ffc52bee061eb9a2fea8", "term": "Black fly", "description": "family of insects", "question": "Was Black fly upstaged by another insect in Jeff Goldblum's 1986 film?", "answer": true, "facts": ["Jeff Goldnlum starred in the 1986 movie The Fly.", "The fly used in the movie The Fly was a common Housefly.", "The Black fly is most closely related to Chironomidae since they both feed on mammals."], "decomposition": ["Which fly was used in the 1986 movie The Fly?", "is #1 a black fly?"], "evidence": [[["no_evidence"], ["operation"]], [[["The Fly (1986 film)-4"]], [["Black fly-1", "Housefly-1"]]], [[["The Fly (1986 film)-4"]], ["operation"]]], "golden_sentence": []}, {"qid": "d787c9e665058c2e4893", "term": "Alan Rickman", "description": "British actor", "question": "Did Alan Rickman have an improperly functioning organ?", "answer": true, "facts": ["Alan Rickman died of pancreatic cancer on 14 January 2016 at age 69.", "Pancreatic cancer arises when cells in the pancreas, a glandular organ behind the stomach, begin to multiply out of control and form a mass."], "decomposition": ["What medical conditions did Alan Rickman have?", "Does any of the conditions in #1 involve an organ?"], "evidence": [[[["Alan Rickman-25"]], [["Pancreatic cancer-1"]]], [[["Alan Rickman-25"]], [["Pancreatic cancer-1"]]], [[["Alan Rickman-25"]], ["operation"]]], "golden_sentence": [["He revealed the fact that he had terminal cancer to only his closest confidants."], [""]]}, {"qid": "09f8567b988353bc084c", "term": "4", "description": "Natural number", "question": "Would four shoes be insufficient for a set of octuplets?", "answer": true, "facts": ["There are eight children in a set of octuplets.", "The typical child is born with two feet.", "Normally each foot needs to be covered with a single shoe."], "decomposition": ["How many children are in a set of octuplets?", "How many shoes does a person wear?", "What is #1 multiplied by #2?", "Is #3 greater than 4?"], "evidence": [[[["Multiple birth-6"]], [["Shoe-1"]], ["operation"], ["operation"]], [[["Multiple birth-19"]], [["Shoe-1"]], ["operation"], ["operation"]], [[["Suleman octuplets-1"]], [["Shoe-1"]], ["operation"], ["operation"]]], "golden_sentence": [["two offspring \u2013 twins three offspring \u2013 triplets four offspring \u2013 quadruplets five offspring \u2013 quintuplets six offspring \u2013 sextuplets seven offspring \u2013 septuplets eight offspring \u2013 octuplets nine offspring \u2013 nonuplets ten offspring \u2013 decuplets Terms used for multiple births or the genetic relationships of their offspring:"], [""]]}, {"qid": "1ff496a10041ced0d430", "term": "Judge", "description": "official who presides over court proceedings", "question": "Are banana trees used by judges for maintaining order?", "answer": false, "facts": ["A banana tree has seeds that only have one embryonic leaf and is called a monocot.", "Judges use gavels to maintain order in court.", "Gavels are made of hardwood.", "Hardwood comes from dicot trees.", "Oak, maple, and sycamore are dicot trees."], "decomposition": ["Which instrument do judges use to maintain order in courts?", "What kind of material are #1 made from?", "Which group of trees is #2 obtained from?", "Do banana trees belong to the same group as #3?"], "evidence": [[[["Gavel-8"]], [["Gavel-1"]], [["Hardwood-1"]], [["Banana-13"]]], [[["Gavel-2"]], [["Gavel-1"]], [["Hardwood-1"]], [["Monocotyledon-2"], "operation"]], [[["Gavel-1"]], [["Gavel-1"]], [["Banana-5"]], ["operation"]]], "golden_sentence": [["The gavel is used in courts of law in the United States and, by metonymy, is used there to represent the entire judiciary system, especially of judgeship."], ["It is often struck against a sound block, a striking surface typically also made of hardwood, to enhance its sounding qualities."], ["Hardwood contrasts with softwood (which is from gymnosperm trees)."], [""]]}, {"qid": "f4205a8e99db7b502528", "term": "Bohai Sea", "description": "The innermost gulf of the Yellow Sea and Korea Bay on the coast of Northeastern and North China", "question": "Could Rhode Island sink into the Bohai Sea?", "answer": true, "facts": ["The Bohai Sea is 30,000 square miles", "Rhode Island is 1,214 square miles"], "decomposition": ["How many square miles is the Bohai Sea?", "How many square miles is Rhode Island?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Bohai Sea-1"]], [["Rhode Island-29"]], [["Rhode Island-29"], "operation"]], [[["Bohai Sea-1"]], [["Rhode Island-29"]], ["operation"]], [[["Bohai Sea-1"]], [["Rhode Island-29"]], ["operation"]]], "golden_sentence": [["The Bohai Sea or Bo Sea, also known as Bohai Gulf, Bo Gulf or Pohai Bay (Chinese: \u6e24\u6d77; literally: 'Bo Sea'), is a marginal sea approximately 78,000\u00a0km2 (30,000\u00a0sq\u00a0mi) in area in the east coast of mainland China."], ["Rhode Island covers an area of 1,214 square miles (3,144\u00a0km2) located within the New England region and is bordered on the north and east by Massachusetts, on the west by Connecticut, and on the south by Rhode Island Sound and the Atlantic Ocean."], [""]]}, {"qid": "d506284c22573bab85b3", "term": "Sloth", "description": "tree dwelling animal noted for slowness", "question": "Will a sloth explode if it's not upside down?", "answer": false, "facts": ["sloth can climb trees in various positions.", "sloth can crawl along the ground on their stomachs. "], "decomposition": ["What are some common positions that a sloth can stay in?", "Is all of #1 upside down in orientation?"], "evidence": [[[["Sloth-4"], "no_evidence"], ["no_evidence", "operation"]], [[["Sloth-1"]], [["Sloth-1"]]], [[["Sloth-1", "Sloth-2"]], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "1a26d024caecbc8df929", "term": "Newcastle, New South Wales", "description": "City in New South Wales, Australia", "question": "Was the MLB World Series held in Newcastle, New South Wales?", "answer": false, "facts": ["The MLB World Series is held annually in a stadium belonging to one of its teams", "MLB teams are located in the United States and Canada", "New South Wales is a state in Australia"], "decomposition": ["In which countries are MLB World Series held?", "Is Australia one of #1?"], "evidence": [[[["MLB International-1"]], ["operation"]], [[["World Series-1"]], ["operation"]], [[["World Series-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["Prominently, in partnership with DirecTV and MLB Network, it produces and syndicates the All-Star Game, NLCS, ALCS, and the World Series, as well as the Caribbean Series, the Australian Baseball League Championship Series and the World Baseball Classic to broadcasters in over 200 countries, and the American Forces Network for U.S. military troops abroad."]]}, {"qid": "6ea2596edc1ff5d2631e", "term": "Nancy Pelosi", "description": "52nd speaker of the United States House of Representatives", "question": "Would Nancy Pelosi have hypothetically been on same side as Gerald Ford?", "answer": false, "facts": ["Gerald Ford was a Republican and was president from 1974-1977.", "Nancy Pelosi is the Democratic Speaker of the House.", "Domestically, Ford was consistently conservative, and led the fight against Johnson's Great Society. ", "Programs of the Great Society included Medicare, and Medicaid.", "Nancy Pelosi has consistently voted for Medicare and Medicaid."], "decomposition": ["What was Gerald Ford's political affiliation?", "What is Nancy Pelosi's political affiliation?", "Is #1 the same as #2?"], "evidence": [[[["Gerald Ford-119"]], [["Nancy Pelosi-12"]], ["operation"]], [[["Gerald Ford-4"]], [["Nancy Pelosi-12"]], ["operation"]], [[["Gerald Ford-3"]], [["Nancy Pelosi-2"]], ["operation"]]], "golden_sentence": [["He also was a member of the Republican Unity Coalition, which The New York Times described as \"a group of prominent Republicans, including former President Gerald R. Ford, dedicated to making sexual orientation a non-issue in the Republican Party\"."], [""]]}, {"qid": "7f15e59dacdc8269d000", "term": "1980 United States presidential election", "description": "49th quadrennial presidential election in the United States", "question": "Was the 1980 presidential election won by a member of the Grand Old Party?", "answer": true, "facts": ["The Republican party is nicknamed the Grand Old Party.", "The 1980 election was won by Ronald Reagan.", "Reagan was a Republican."], "decomposition": ["Which political party is also known as the Grand Old Party?", "Who won the 1980 presidential election?", "What political party did #2 belong to?", "Is #3 the same as #1?"], "evidence": [[[["Republican Party (United States)-1"]], [["1980 United States presidential election-1"]], [["Ronald Reagan-3"]], ["operation"]], [[["Republican Party (United States)-1"]], [["1980 United States presidential election-1"]], [["Ronald Reagan-3"]], ["operation"]], [[["Republican Party (United States)-1"]], [["1980 United States presidential election-1"]], [["Ronald Reagan-3"]], ["operation"]]], "golden_sentence": [["The Republican Party, also referred to as the GOP (Grand Old Party), is one of the two major contemporary political parties in the United States, along with its main rival, the Democratic Party."], ["Republican nominee Ronald Reagan defeated incumbent Democrat Jimmy Carter."], [""]]}, {"qid": "fae0692f80f19fcf3b4e", "term": "Glenn Beck", "description": "American talk radio and television host", "question": "Is Glenn Beck known for his mild temper?", "answer": false, "facts": ["Glenn Beck has gone viral for screaming at callers on his program.", "Glenn Beck has walked off of professional interviews when he doesn't like the questions."], "decomposition": ["What kind of temperament has Glenn Beck shown in public on notable ocassions?", "Did all of #1 indicate mild temper?"], "evidence": [[[["Glenn Beck-22"]], ["operation"]], [[["Glenn Beck Program-19", "Glenn Beck-89"], "no_evidence"], ["operation"]], [[["Glenn Beck-22"]], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "4f49742f126ed645a513", "term": "Kaffir lime", "description": "A citrus fruit native to tropical Southeast Asia and southern China", "question": "Would a kaffir lime be a good ingredient for making a candle?", "answer": true, "facts": ["Kaffir limes are citrus fruits originating in tropical climates.", "The Kaffir lime leaves and rind emit an intense citrus fragrance when crushed up.", "Yankee Candle, one of the largest candle companies, sells several popular varieties of citrus candles.", "Sage and Citrus is one of the highest rated scents that Yankee Candle sells."], "decomposition": ["Which fragrance do Kaffir lime leaves emit when crushed?", "What are the scents of some popular varieties of candles that Yankee Candle sells?", "Is #2 included in #1?"], "evidence": [[[["Kaffir lime-2"]], [["Yankee Candle-14"], "no_evidence"], ["operation"]], [[["Kaffir lime-8"], "no_evidence"], [["Yankee Candle-14"], "no_evidence"], ["no_evidence", "operation"]], [[["Kaffir lime-2"]], [["Yankee Candle-14"], "no_evidence"], ["no_evidence"]]], "golden_sentence": [["Its rind and crushed leaves emit an intense citrus fragrance."], ["citation needed] Yankee Candle Company markets an array of products, including candles of various scents and sizes, scented wax tarts, candle accessories, votive candles (samplers), votive candle holders, tart warmers, jar toppers (for use with the Housewarmer line of candles), reed diffusers, Electric Home Fragrance units (scented wall plug-ins), car scents, room sprays, Good Air products, and more."]]}, {"qid": "4c852c3b38d60e216e8a", "term": "Moon Jae-in", "description": "President of South Korea", "question": "Did Moon Jae-in's residence exist when the World Trade Center was completed?", "answer": false, "facts": ["The President of South Korea lives in the Blue House", "The Blue House finished construction in early 1991", "The World Trade Center complex was completed in 1987"], "decomposition": ["Where does Moon Jae-in live?", "When was the construction of #1 finished?", "When was the construction of the World Trade Center completed?", "Is #2 before #3?"], "evidence": [[[["Moon Jae-in-28"]], ["no_evidence"], [["Construction of the World Trade Center-4"]], ["operation"]], [[["Blue House-1"]], [["Blue House-8"]], [["World Trade Center (1973\u20132001)-1"]], ["operation"]], [[["Blue House-1", "Moon Jae-in-1"]], [["Blue House-8"]], [["World Trade Center (1973\u20132001)-1"]], ["operation"]]], "golden_sentence": [["Moon also promised transparency in his presidency, moving the presidential residence from the palatial and isolated Blue House to an existing government complex in downtown Seoul."], ["Four other low-level buildings were constructed as part of the World Trade Center in the early 1970s, and the complex was mostly complete by 1973."]]}, {"qid": "f85dfdc06ed27638bf04", "term": "Pulitzer Prize", "description": "U.S. award for achievements in newspaper and online journalism, literature, and musical composition", "question": "Is it impossible for Cheb Mami to win a Pulitzer Prize for musical composition?", "answer": true, "facts": ["The history Pulitzer Prize can be won by any citizen, all other Pulitzer Prize winners must be a US Citizen.", "Cheb Mami is an Algerian singer.", "Cheb Mami is a citizen of Algeria."], "decomposition": ["The Pulitzer Prize for musical composition is exclusive to the citizens of which country?", "Which country is Cheb Mami from?", "Is #1 different from #2?"], "evidence": [[[["Pulitzer Prize for Music-1"]], [["Cheb Mami-1"]], ["operation"]], [[["Pulitzer Prize for Music-1"]], [["Cheb Mami-2"]], ["operation"]], [[["Pulitzer Prize for Music-2"]], [["Cheb Mami-2"]], ["operation"]]], "golden_sentence": [["Joseph Pulitzer arranged for a music scholarship to be awarded each year, and this was eventually converted into a prize: \"For a distinguished musical composition of significant dimension by an American that has had its first performance in the United States during the year.\""], ["Mohamed Khelifati (Arabic: \u0645\u062d\u0645\u062f \u062e\u0644\u064a\u0641\u0627\u062a\u064a\u200e, mu\u1e25ammad khal\u012bf\u0101t\u012b), better known by his stage name Cheb Mami (Arabic: \u0634\u0627\u0628 \u0645\u0627\u0645\u064a\u200e, sh\u0101bb m\u0101m\u012b, born 11 July 1966), is an Algerian musician and singer-songwriter."]]}, {"qid": "38a9f1e101a3b7df1f95", "term": "DARPA", "description": "Agency of the U.S. Department of Defense responsible for the development of new technologies", "question": "Did DARPA influence Albert Einstein? ", "answer": false, "facts": ["DARPA is an agency in the US focused on defense and new technologies.", "DARPA was founded in 1958 under Dwight D Eisenhower.", "Albert Einstein was a famous physicist who died in 1955."], "decomposition": ["When was DARPA formed?", "When did Albert Einstein die?", "Is #1 before #2?"], "evidence": [[[["DARPA-2"]], [["Albert Einstein-1"]], ["operation"]], [[["DARPA-2"]], [["Albert Einstein-1"]], ["operation"]], [[["DARPA-2"]], [["Albert Einstein-1"]], ["operation"]]], "golden_sentence": [["Originally known as the Advanced Research Projects Agency (ARPA), the agency was created in February 1958 by President Dwight D. Eisenhower in response to the Soviet launching of Sputnik 1 in 1957."], ["Albert Einstein (/\u02c8a\u026ansta\u026an/ EYEN-styne; German: [\u02c8alb\u025b\u0281t \u02c8\u0294a\u026an\u0283ta\u026an] (listen); 14 March 1879\u00a0\u2013 18 April 1955) was a German-born theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics (alongside quantum mechanics)."]]}, {"qid": "331b1899ce5ff1742e91", "term": "Family Guy", "description": "American animated sitcom", "question": "Does the art from Family Guy look a lot like the art in American Dad?", "answer": true, "facts": ["Family Guy and American Dad are both Fox Animated Sitcoms animated by Seth MacFarlane.", "Family Guy and American Dad characters all share common facial features and movement styles."], "decomposition": ["Who is the animator for Family Guy?", "Who is the animator for American Dad?", "Is #1 the same as #2?"], "evidence": [[[["Seth MacFarlane-14"]], [["Seth MacFarlane-21"]], ["operation"]], [[["Family Guy-1"]], [["American Dad!-1"]], ["operation"]], [[["Family Guy-2"]], [["American Dad!-14"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "fde534f973789ae5486e", "term": "Stephen King", "description": "American author", "question": "Could Stephen King join the NASA Astronaut Corps?", "answer": false, "facts": ["NASA Astronaut Corps candidates must have a master's degree from an accredited institution in engineering, biological science, physical science or mathematics.", "Stephen King studied at the University of Maine, graduating in 1970 with a Bachelor of Arts in English."], "decomposition": ["What degrees are acceptable to meet the minimum requirement for admittance to the NASA Astronaut Corps?", "What degrees does Stephen King hold?", "Is #2 also in #1?"], "evidence": [[[["NASA Astronaut Corps-10"]], [["Stephen King-6"]], ["operation"]], [[["NASA Astronaut Corps-10"]], [["Stephen King-6"]], ["operation"]], [[["NASA Astronaut Corps-10"]], [["Stephen King-6"]], ["operation"]]], "golden_sentence": [["Candidates must have a master's degree from an accredited institution in engineering, biological science, physical science or mathematics."], ["From 1966, King studied at the University of Maine, graduating in 1970 with a Bachelor of Arts in English."]]}, {"qid": "ad2b3484b7ba13692d3f", "term": "Alexander Graham Bell", "description": "scientist and inventor known for his work on the telephone", "question": "Would Alexander Graham Bell hypothetically support Nazi eugenics?", "answer": true, "facts": ["Eugenics was the idea of selective breeding or sterilization to rid the human populace of certain traits.", "Nazis used eugenics to justify mass sterilization and mass murder.", "Alexander Graham Bell Alexander Graham Bell advocated against the use of sign language and hoped to eradicate deafness through selective breeding."], "decomposition": ["What did the Nazi's use to justify mass sterilization and mass murder?", "What is the definition of #1?", "What did Alexander Graham Bell advocate against the use of?", "Did Alexander Graham Bell use #2 to get rid of #3?"], "evidence": [[[["Nazi eugenics-3"]], [["Eugenics-1"]], [["History of eugenics-21"], "no_evidence"], ["no_evidence"]], [[["Eugenics-4"]], [["Eugenics-1"]], [["History of eugenics-21"]], ["operation"]], [[["Nazism and race-3", "Nazism-59", "Nazism-60"]], [["Racial hierarchy-11"]], [["History of eugenics-21"]], ["no_evidence", "operation"]]], "golden_sentence": [["Those humans targeted for destruction under Nazi eugenics policies were largely living in private and state-operated institutions, identified as \"life unworthy of life\" (German: Lebensunwertes Leben), including prisoners, \"degenerates\", dissidents, people with congenital cognitive and physical disabilities (including people who were \"feeble-minded\", epileptic, schizophrenic, manic-depressive, cerebral palsy, muscular dystrophy, deaf, blind) (German: erbkranken), homosexual, idle, insane, and the weak, for elimination from the chain of heredity."], ["Eugenics (/ju\u02d0\u02c8d\u0292\u025bn\u026aks/; from Greek \u03b5\u1f50- \"good\" and \u03b3\u03b5\u03bd\u03ae\u03c2 \"come into being, growing\") is a set of beliefs and practices that aim to improve the genetic quality of a human population, typically by excluding people and groups judged to be inferior and promoting those judged to be superior."], ["One of the earliest modern advocates of eugenics (before it was labeled as such) was Alexander Graham Bell."]]}, {"qid": "7fe48d9fd517c4ab2ad0", "term": "Black swan", "description": "species of bird", "question": "Can black swan's formation type help spell longest word in Dictionary?", "answer": true, "facts": ["Black swan's fly in a \"V\" formation.", "The longest word in the dictionary is pneumonoultramicroscopicsilicovolcanoconiosis."], "decomposition": ["What letter does the formation of black swans in flight resemble?", "What is the longest word in English language?", "Can #1 be found in #2?"], "evidence": [[[["Black swan-6"]], [["Longest word in English-4"]], ["operation"]], [[["Black swan-6"]], [["Pneumonoultramicroscopicsilicovolcanoconiosis-1"]], ["operation"]], [[["Swan-6"], "no_evidence"], [["Pneumonoultramicroscopicsilicovolcanoconiosis-1"]], ["operation"]]], "golden_sentence": [["In flight, a wedge of black swans will form as a line or a V, with the individual birds flying strongly with undulating long necks, making whistling sounds with their wings and baying, bugling or trumpeting calls."], ["The longest word in any of the major English language dictionaries is pneumonoultramicroscopicsilicovolcanoconiosis, a word that refers to a lung disease contracted from the inhalation of very fine silica particles, specifically from a volcano; medically, it is the same as silicosis."]]}, {"qid": "ba88d843b3572d9171c7", "term": "Glutamic acid", "description": "amino acid", "question": "Do you find glutamic acid in a severed finger?", "answer": true, "facts": ["Glutamic acid is an amino acid and neurotransmitter", "As a neurotransmitter, glutamic acid is the most abundant in the vertebrate nervous system", "A severed finger contains parts of a vertebrate's nervous system"], "decomposition": ["What kind of transmitter is glutamic acid?", "In which bodily system is #1 the most abundant?", "Does a severed finger contain #2?"], "evidence": [[[["Glutamic acid-1"]], [["Glutamic acid-1"]], ["no_evidence", "operation"]], [[["Glutamic acid-1"]], [["Glutamic acid-1"]], ["no_evidence", "operation"]], [[["Glutamic acid-1"]], [["Neurotransmitter-16"], "no_evidence"], [["Dendrite-3"], "no_evidence", "operation"]]], "golden_sentence": [[""], ["It is also an excitatory neurotransmitter, in fact the most abundant one, in the vertebrate nervous system."]]}, {"qid": "1f1827108d7d62c5e686", "term": "German Shepherd", "description": "Dog breed", "question": "Do German Shepherds worry about the Abitur?", "answer": false, "facts": ["The Abitur is a qualification granted by university-preparatory schools in Germany, Lithuania, and Estonia.", "The Abitur is conferred on students who pass their final exams at the end of their secondary education.", "Students that attend university-preparatory schools are humans.", "German Shepherds are not humans."], "decomposition": ["The Abitur qualification is conferred after which achievement?", "What kind of animal is a German Shepherd?", "Are #2 capable of completing #1 which was meant for humans?"], "evidence": [[[["Abitur-1"]], [["German Shepherd-1"]], ["operation"]], [[["Abitur-28"]], [["German Shepherd-25"]], ["operation"]], [[["Abitur-4"]], [["German Shepherd-11"]], ["operation"]]], "golden_sentence": [["It is conferred on students who pass their final exams at the end of their secondary education, usually after twelve or thirteen years of schooling (see also for Germany Abitur after twelve years)."], ["The German Shepherd (German: Deutscher Sch\u00e4ferhund, German pronunciation: [\u02c8d\u0254\u028ft\u0283\u0250 \u02c8\u0283\u025b\u02d0f\u0250\u02cch\u028ant]) is a breed of medium to large-sized working dog that originated in Germany."]]}, {"qid": "95e663100706dd7124e0", "term": "PlayStation 4", "description": "Sony's eighth-generation home video game console", "question": "Did Tom Bosley enjoy video games on the PlayStation 4?", "answer": false, "facts": ["The PlayStation 4 was launched in 2013.", "Tom Bosley died in 2010."], "decomposition": ["What year did Tom Bosley die?", "What year was the PlayStation 4 Launched?", "Is #2 before #1?"], "evidence": [[[["Tom Bosley-13"]], [["PlayStation 4-1"]], ["operation"]], [[["Tom Bosley-13"]], [["PlayStation 4-1"]], ["operation"]], [[["Tom Bosley-1"]], [["PlayStation 4-1"]], ["operation"]]], "golden_sentence": [["Bosley died from complications of a staph infection on October 19, 2010, at a hospital in Rancho Mirage, California, near his home in Palm Springs, California."], ["Announced as the successor to the PlayStation 3 in February 2013, it was launched on November 15 in North America, November 29 in Europe, South America and Australia, and on February 22, 2014 in Japan."]]}, {"qid": "94739ddb5df61a0a91d9", "term": "Toyota Hilux", "description": "Series of light commercial vehicles produced by the Japanese car-manufacturer Toyota.", "question": "Can a 2019 Toyota Hilux hypothetically support weight of thirty Big John Studd clones?", "answer": false, "facts": ["The 2019 Toyota Hilux has a maximum carry load of 3500kg or, around 7,700 pounds.", "Big John Studd was a professional wrestler that weighed 364 pounds."], "decomposition": ["What is the maximum carry load weight of a Toyota Hilux?", "How much did Big John Studd weigh?", "What is #2 multiplied by 30?", "Is #1 greater than or equal to #3?"], "evidence": [[["no_evidence"], ["no_evidence"], ["no_evidence", "operation"], ["no_evidence", "operation"]], [[["Toyota Hilux-1"], "no_evidence"], [["Big John Studd-14"], "no_evidence"], ["operation"], ["operation"]], [[["Toyota Hilux-1"], "no_evidence"], [["Big John Studd-1", "NWA Mid-Atlantic Heavyweight Championship-4"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": []}, {"qid": "c840b98652fee0f41161", "term": "Potato", "description": "plant species producing the tuber used as a staple food", "question": "Are potatoes native to the European continent?", "answer": false, "facts": ["Potatoes originated in South America and spread throughout the Americas by indigenous tribes.", "European explorers discovered potatoes and brought them back to share at home in Europe."], "decomposition": ["Where did potatoes originate?", "Is #1 located in Europe?"], "evidence": [[[["Potato-13"]], [["Potato-13"]]], [[["Potato-2"]], ["operation"]], [[["Potato-2"]], [["Peru-1"]]]], "golden_sentence": [["Most modern potatoes grown in North America arrived through European settlement and not independently from the South American sources, although at least one wild potato species, Solanum fendleri, naturally ranges from Peru into Texas, where it is used in breeding for resistance to a nematode species that attacks cultivated potatoes."], [""]]}, {"qid": "4c082880cbfd0699ccba", "term": "B\u00f6rek", "description": "Stuffed phyllo pastry", "question": "Would \u015eerafeddin Sabuncuo\u011flu have eaten B\u00f6rek?", "answer": true, "facts": ["B\u00f6rek originated in Ottoman cuisine", "\u015eerafeddin Sabuncuo\u011flu was an Ottoman scientist"], "decomposition": ["Where did Borek originate from?", "Was Serafeddin Sabuncuoglu from #1?"], "evidence": [[[["B\u00f6rek-1"]], [["Ottoman Empire-1", "Sabuncuo\u011flu \u015eerafeddin-1"], "operation"]], [[["B\u00f6rek-3"]], [["Sabuncuo\u011flu \u015eerafeddin-2"]]], [[["B\u00f6rek-1"]], [["Amasya-1", "Sabuncuo\u011flu \u015eerafeddin-2"]]]], "golden_sentence": [["B\u00f6rek (Turkish pronunciation:\u00a0[b\u0153\u02c8\u027eec]; also burek and other variants) is a family of baked filled pastries made of a thin flaky dough such as phyllo or yufka, of Turkish origins and also found in the cuisines of the Balkans, the South Caucasus, Levant, Mediterranean, and other countries in Eastern Europe and Western Asia."], ["", "\u015eerafeddin Sabuncuo\u011flu (1385\u20131468) (Ottoman Turkish: \u0634\u0631\u0641 \u0627\u0644\u062f\u0651\u06cc\u0646 \u0635\u0627\u0628\u0648\u0646\u062c\u06cc \u0627\u0648\u063a\u0644\u06cc) was a medieval Ottoman surgeon and physician."]]}, {"qid": "2ba557faf2971f52924d", "term": "Zucchini", "description": "Edible summer squash, typically green in color", "question": "Can the original name of the zucchini be typed on the top row of a QWERTY keyboard?", "answer": false, "facts": ["The original name for the zucchini in Mexican language or Nahuatl is ayokonetl.", "The top row of a QWERTY keyboard contains the keys q, w, e, r, t, y, u, i , o, and p."], "decomposition": ["What is the original name of the zucchini?", "What keys are on the top row of a QWERTY keyboard?", "Is every letter in #1 present in #2?"], "evidence": [[[["Zucchini-7"]], [["QWERTY-9"]], [["QWERTY-9", "Zucchini-7"], "no_evidence"]], [[["Zucchini-4"]], [["QWERTY-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Zucchini-1"]], [["QWERTY-1"]], ["operation"]]], "golden_sentence": [[""], [""], ["", ""]]}, {"qid": "8d6a63cb473cbbf075f9", "term": "Rice pudding", "description": "Dish made from rice mixed with water or milk", "question": "Would Cyndi Lauper use milk substitute in her rice pudding?", "answer": true, "facts": ["Cyndi Lauper wrote a song about lactose intolerance.", "Lactose intolerance leads to gastrointestinal discomfort upon eating dairy."], "decomposition": ["What conditions lead people to using milk substitutes?", "Does Cyndi Lauper suffer from any conditions listed in #1?"], "evidence": [[[["Lactose intolerance-1", "Veganism-1"]], [["Cyndi Lauper-1"], "no_evidence", "operation"]], [[["Milk allergy-9"]], [["Cyndi Lauper-76"], "no_evidence"]], [[["Milk substitute-17"]], ["no_evidence"]]], "golden_sentence": [["", ""], [""]]}, {"qid": "507ae9a0c0cbbdebe95f", "term": "Eggplant", "description": "plant species Solanum melongena", "question": "Is eggplant deadly to most atopic individuals? ", "answer": false, "facts": ["Atopic individuals have a genetic tendency to develop allergic reactions", "Eggplant allergies are usually not life-threatening "], "decomposition": ["What kind of reactions do atopic people have a tendency of getting?", "Are #1 caused by eggplant usually deadly in nature?"], "evidence": [[[["Atopy-4"]], [["Eggplant-53"], "operation"]], [[["Atopy-1"]], ["no_evidence", "operation"]], [[["Atopy-1", "Atopy-4", "Atopy-5"]], [["Atopy-6"], "no_evidence", "operation"]]], "golden_sentence": [["The likelihood of having asthma, rhinitis and atopic dermatitis together is 10 times higher than could be expected by chance."], [""]]}, {"qid": "8da1656000b915916385", "term": "Bitcoin", "description": "decentralized cryptocurrency", "question": "Could a single bitcoin ever cover cost of a Volkswagen Jetta?", "answer": true, "facts": ["The all time high price of bitcoin was $19,783 in 2017.", "The suggested retail price of a 2020 Volkswagen Jetta is $18,895."], "decomposition": ["What is the highest price for a bitcoin?", "What is the cheapest price of a Jetta?", "Is #1 greater than #2?"], "evidence": [[[["Bitcoin-22"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Bitcoin-22"]], ["no_evidence"], ["operation"]], [[["Economics of bitcoin-16"]], [["Volkswagen Jetta-1"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Prices started at $998 in 2017 and rose to $13,412.44 on 1 January 2018, after reaching its all-time high of $19,783.06 on 17 December 2017."]]}, {"qid": "f6af196cc1c5e4399e77", "term": "Drew Carey", "description": "American actor, comedian, game show host, libertarian and photographer", "question": "Has Drew Carey outshined Doug Davidson's tenure?", "answer": true, "facts": ["Drew Carey has been the host of the Price is Right for over 13 years.", "Doug Davidson hosted the Price is Right from 1994-1995."], "decomposition": ["How long has Drew Carey hosted the Price is Right?", "How long did Doug Davidson host the Price is Right?", "Is #1 longer than #2?"], "evidence": [[[["Drew Carey-14"]], [["Doug Davidson-2"]], ["operation"]], [[["The Price Is Right-1"]], [["The New Price Is Right (1994 game show)-1"]], ["operation"]], [[["Drew Carey-1"]], [["Doug Davidson-2"]], ["operation"]]], "golden_sentence": [["In 2017, Carey celebrated his 10 years as host of the show, joining Barker as the only two to have hosted for at least a decade."], ["Through connections he made during those appearances, Davidson eventually got a position hosting a five-night-a-week, syndicated, half-hour version of the game show The New Price Is Right, beginning on September 12, 1994."]]}, {"qid": "1d2afb76ba58f9f8703f", "term": "Taco Bell", "description": "American fast-food chain", "question": "Can you purchase a dish with injera at Taco Bell?", "answer": false, "facts": ["Taco Bell serves a variety of Mexican and Tex-Mex foods that include tacos, burritos, quesadillas, and nachos.", "Injera is a sour fermented flatbread with a slightly spongy texture, traditionally made out of teff flour.", "Injera is part of Ethiopian cuisine."], "decomposition": ["What kind of food is Taco Bell known to serve?", "Which country is #1 most associated with?", "Which country is Injera native to?", "Is #2 the same as #3?"], "evidence": [[[["Taco Bell-1"]], [["Taco Bell-1"]], [["Injera-1"]], ["operation"]], [[["Taco Bell-1"]], [["Taco Bell-1"]], [["Injera-1"]], ["operation"]], [[["Taco Bell-1"]], [["Mexican cuisine-6", "Tex-Mex-1"]], [["Pancake-7"]], ["operation"]]], "golden_sentence": [["Taco Bell is an American chain of fast food restaurants based in Irvine, California and a subsidiary of Yum!"], [""], ["It is the national dish of Ethiopia, Eritrea."]]}, {"qid": "b857bb92d780b91adcec", "term": "Gettysburg Battlefield", "description": "site of the Battle of Gettysburg during the American Civil War", "question": "Would a Superbowl Football Game be crowded on the Gettysburg Battlefield?", "answer": false, "facts": ["Football fields used in the Super Bowl are 100 yards long. ", "The Gettysburg Battlefield is over 5 miles long.", "There are 1760 yards in a mile."], "decomposition": ["How long is the football field superbowl?", "How long is the Gettysburg Battlefield?", "Is #1 the same as #2?"], "evidence": [[[["Comparison of American football and rugby league-6"]], [["Gettysburg Battlefield-2"]], ["operation"]], [[["Football pitch-4"]], [["Gettysburg Battlefield-2"]], ["operation"]], [[["American football-11"]], [["Gettysburg Battlefield-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["American football is played on a rectangular field 120 yards (110\u00a0m) long by 53 1\u20443 yards (48.8 m) wide."], ["Within 10 miles (16\u00a0km) of the Maryland/Pennsylvania state line, the Gettysburg battlefield is situated in the Gettysburg-Newark Basin of the Pennsylvania Piedmont entirely within the Potomac River Watershed near the Marsh and Rock creeks' triple point with the Susquehanna River Watershed (near Oak Hill) occupying an area 3.33 by 5.33 miles (5.4\u00a0km \u00d7\u00a08.6\u00a0km)."]]}, {"qid": "d0b9ac6bb918f10925e2", "term": "High Speed 1", "description": "high-speed railway between London and the Channel Tunnel", "question": "Would the tunnels at CERN fit onto the High Speed 1 rails?", "answer": true, "facts": ["High Speed 1 (HS1), legally the Channel Tunnel Rail Link (CTRL), is a 67-mile (108 km) high-speed railway.", "The CERN collider is contained in a circular tunnel, with a circumference of 26.7 kilometres (16.6 mi).", "The circumference of a circle is the length of the enclosing boundary."], "decomposition": ["How long are the tunnels at the CERN collider?", "How long is the High Speed 1 railway?", "Is #1 less than or equal to #2?"], "evidence": [[[["Large Electron\u2013Positron Collider-2"]], [["High Speed 1-1"]], ["operation"]], [[["CERN-18"]], [["High Speed 1-1"]], ["operation"]], [[["CERN-18"]], [["High Speed 1-1"]], ["operation"]]], "golden_sentence": [["It was a circular collider with a circumference of 27 kilometres built in a tunnel roughly 100\u00a0m (300\u00a0ft) underground and passing through Switzerland and France."], ["High Speed 1 (HS1), legally the Channel Tunnel Rail Link (CTRL), is a 67-mile (108\u00a0km) high-speed railway linking London with the Channel Tunnel."]]}, {"qid": "3e04198abf965cd16073", "term": "2000", "description": "Year", "question": "Would 1996 leap year baby technically be 1 year old in 2000?", "answer": true, "facts": ["A leap year happens once every 4 years and has 29 days in February.", "The years 1996, 2000, 2004, 2008, 2012, 2016, and 2020 are the last 7 leap years that have happened.", "1996 to 2000 is one leap year."], "decomposition": ["How many years apart are consecutive leap years?", "What is 2000 minus 1996?", "Is #2 divided by #1 equal to one?"], "evidence": [[[["Leap year-2"]], ["operation"], ["operation"]], [[["Leap year-2"]], ["operation"], ["operation"]], [[["Leap year-16"]], ["operation"], ["operation"]]], "golden_sentence": [[""]]}, {"qid": "8addd31ce8e0f4938f68", "term": "Snowdon", "description": "highest mountain in Wales", "question": "Would Snowdon mountain be a piece of cake for Tenzing Norgay?", "answer": true, "facts": ["Tenzing Norgay was a mountaineer that climbed Mount Everest in 1953.", "Snowdon Mountain has a peak of 3,560 feet.", "Mount Everest has a peak of over 29,000 feet."], "decomposition": ["How high is Snowdon Mountain?", "What was the highest peak ever climbed by Tenzing Norgay", "How high is #2?", "Is #3 greater than #1?"], "evidence": [[[["Snowdon-1"]], [["Tenzing Norgay-1"]], [["Mount Everest-2"]], ["operation"]], [[["Snowdon-1"]], [["Mount Everest-1", "Tenzing Norgay-1"]], [["Mount Everest-2"]], ["operation"]], [[["Snowdon-1"], "no_evidence"], [["Tenzing Norgay-1"]], [["Mount Everest-2"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Snowdon (/\u02c8sno\u028ad\u0259n/; Welsh: Yr Wyddfa, pronounced\u00a0[\u0259r \u02c8w\u026a\u00f0va]) is the highest mountain in Wales, at an elevation of 1,085 metres (3,560\u00a0ft) above sea level, and the highest point in the British Isles outside the Scottish Highlands."], ["He was one of the first two individuals known to reach the summit of Mount Everest, which he accomplished with Edmund Hillary on 29 May 1953."], [""]]}, {"qid": "64856a0db9e5478c7b7c", "term": "Reconstruction era", "description": "Era of military occupation in the Southern United States after the American Civil War (1865\u20131877)", "question": "Can a Reconstruction era coin buy DJI Mavic Pro Drone?", "answer": true, "facts": ["The DJI Mavic Pro Drone retails for around $1,000 dollars.", "THE Reconstruction Era took place from 1865-1877.", "Mint condition 1870 Seated Liberty Silver Dollar's can sell for between $2,283 to $4,933."], "decomposition": ["How much does a DJI Mavic Pro Drone retail for?", "During what years did the Reconstruction era occur?", "Of the US coins minted during the years in #2, are any of them now worth at least as much as #1?"], "evidence": [[[["DJI-26"], "no_evidence"], [["Reconstruction era-2"]], [["Three-cent silver-28", "Two-cent piece (United States)-21"], "operation"]], [[["DJI-26"], "no_evidence"], [["Reconstruction era-2"]], [["Three-cent piece-4"], "no_evidence", "operation"]], [[["Mavic (UAV)-2"], "no_evidence"], [["Reconstruction era-2"]], [["Economic history of the United States-201"], "no_evidence"]], [[["Mavic (UAV)-17"], "no_evidence"], [["Reconstruction era-1"]], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""], ["", "Yeoman's 2018 edition of A Guide Book of United States Coins lists the 1864 large motto and the 1865 as the least expensive two-cent pieces, in good (G-4) condition at $15, though every issue by year through 1871 lists for $40 or less in that condition."]]}, {"qid": "27e8a0bea641c0181f18", "term": "Sun bear", "description": "bear found in tropical forest habitats of Southeast Asia", "question": "Can an American black bear swallow a sun bear whole?", "answer": false, "facts": ["Sun bears grow to only about half the size of an American black bear.", "The total length of adult bear skulls was found to average 262 to 317 mm (10.3 to 12.5 in).", "Black bears cannot open their mouths to half their body length."], "decomposition": ["How big is the skull of an adult american black bear?", "What is the size of an adult sun bear?", "Is #2 smaller than #1?"], "evidence": [[[["American black bear-19"]], [["Sun bear-1"]], ["operation"]], [[["American black bear-19"]], [["Sun bear-1"]], ["operation"]], [[["American black bear-22"], "no_evidence"], [["Sun bear-1"]], ["no_evidence", "operation"]]], "golden_sentence": [["The skulls of American black bears are broad, with narrow muzzles and large jaw hinges."], ["It is the smallest bear, standing nearly 70\u00a0cm (28\u00a0in) at the shoulder and weighing 25\u201365\u00a0kg (55\u2013143\u00a0lb)."]]}, {"qid": "ff09f7daafaabf35562a", "term": "Scrabble", "description": "board game with words", "question": "Does monster name in West African Folklore that witches send into villages set Scrabble record?", "answer": false, "facts": ["An obia is a monster in West African folklore described as being a massive animal that witches send into villages to kidnap young girls and wear their skin for a coat.", "Obia generates 6 points in Scrabble.", "Oxyphenbutazone is said to be the highest scoring scrabble word worth 1,458 points."], "decomposition": ["What is the name of the monster in West African Folklore that witches send into villages?", "What is the highest scoring word in Scrabble?", "Is #1 the same as #2?"], "evidence": [[[["Obia (folklore)-2"]], [["Scrabble-76"]], ["operation"]], [[["Obia (folklore)-2"]], [["Scrabble-76"]], ["operation"]], [[["Obia (folklore)-2"]], [["Scrabble-76"]], ["operation"]]], "golden_sentence": [[""], ["When only adding the word sesquioxidizing to these official lists, one could theoretically score 2015 (OSPD) and 2044 (SOWPODS) points in a single move."]]}, {"qid": "7ffaec96dd20a5a3a7bc", "term": "September", "description": "ninth month in the Julian and Gregorian calendars", "question": "Does Home Depot sell item in late September zodiac sign symbol?", "answer": true, "facts": ["The zodiac sign for late September is Libra.", "The Libra zodiac sign is represented by scales. ", "Home Depot sells a number of scales including Pelouze and Ozeri brands."], "decomposition": ["What is the zodiac sign that represents late September?", "What is symbol of #1?", "What kinds of goods (and services) does The Home Depot deal in?", "Is #2 likely to be included in #3?"], "evidence": [[[["Libra (astrology)-1"]], [["Weighing scale-1"]], [["The Home Depot-1"]], ["operation"]], [[["Libra (astrology)-1"]], [["Libra (astrology)-2"]], [["The Home Depot-1"]], ["operation"]], [[["Libra-1"]], [["Libra (astrology)-2"]], [["The Home Depot-1"]], ["operation"]]], "golden_sentence": [[""], ["These are also known as mass scales, weight scales, mass balances, weight balances, or simply scales, balances, or balance scales."], ["The Home Depot, Inc. is the largest home improvement retailer in the United States, supplying tools, construction products, and services."]]}, {"qid": "207e0bd0df15d42a2820", "term": "Watergate scandal", "description": "Political scandal that occurred in the United States in the 1970s", "question": "Did the Watergate scandal help the Republican party?", "answer": false, "facts": ["Watergate resulted in President Nixon's resignation.", "President Nixon was a Republican.", "Nixon's resignation resulted in major Democratic gains in Congress during the next election."], "decomposition": ["Which central figure resigned as a result of the Watergate scandal?", "Was #1 a member of the Republican party?", "Did the scandal affect the Republican party negatively in following elections?", "Is #2 or #3 negative?"], "evidence": [[[["Richard Nixon-4"]], [["Richard Nixon-2"]], [["Watergate scandal-79"]], ["operation"]], [[["Watergate scandal-66"]], [["Richard Nixon-122"]], ["no_evidence"], ["operation"]], [[["Watergate scandal-2"]], [["Richard Nixon-1"]], [["Watergate scandal-79"]], ["operation"]]], "golden_sentence": [[""], [""], ["Disgust with the revelations about Watergate, the Republican Party, and Nixon strongly affected results of the November 1974 Senate and House elections, which took place three months after Nixon's resignation."]]}, {"qid": "539b90acd0b939ad8f85", "term": "Sesame", "description": "species of plant", "question": "Can a sesame seed grow in the human body?", "answer": false, "facts": ["Seeds need water, oxygen, and light to grow.", "The human digestive system releases powerful acid that dissolves food."], "decomposition": ["What does a seed need in order to germinate?", "Can all of #1 be found inside the human body?"], "evidence": [[[["Sesame-12"], "no_evidence"], [["Gastric acid-2"], "no_evidence", "operation"]], [[["Germination-4"]], [["Human body-6"]]], [[["Seed-52"]], [["Body water-3"], "no_evidence"]]], "golden_sentence": [[""], [""]]}, {"qid": "fdb32733ed876c646be7", "term": "Elizabeth II", "description": "Queen of the United Kingdom and the other Commonwealth realms", "question": "Did Elizabeth II frequently visit Queen Victoria?", "answer": false, "facts": ["Queen Victoria died in 1901.", "Elizabeth II was born in 1926."], "decomposition": ["When did Queen Victoria die?", "When was Queen Elizabeth II born?", "Is #2 before #1?"], "evidence": [[[["Queen Victoria-1"]], [["Elizabeth II-1"]], ["operation"]], [[["Queen Victoria-1"]], [["Elizabeth II-1"]], ["operation"]], [[["Queen Victoria-53"]], [["Elizabeth II-5"]], ["operation"]]], "golden_sentence": [["Victoria (Alexandrina Victoria; 24 May 1819\u00a0\u2013 22 January 1901) was Queen of the United Kingdom of Great Britain and Ireland from 20 June 1837 until her death."], ["Elizabeth II (Elizabeth Alexandra Mary; born 21 April 1926) is the Queen of the United Kingdom and the other Commonwealth realms."]]}, {"qid": "8923a360e456cd04567b", "term": "Rick and Morty", "description": "Animated sitcom", "question": "Can you watch Rick and Morty in Mariana Trench?", "answer": true, "facts": ["Rick and Morty is available in blu-ray format.", "You can play blu-ray on a laptop computer ", "It is possible to go to Mariana Trench inside a deep-diving submersible vehicle with a laptop."], "decomposition": ["What portable media format is Rick and Morty available in?", "What electronics do deep-diving submersibles have?", "Can any of #1 be played on any of #2?"], "evidence": [[[["Rick and Morty-28"]], [["Deep-submergence vehicle-1"], "no_evidence"], ["operation"]], [[["Rick and Morty-27"]], [["DVD player-1", "Submersible-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Rick and Morty-28"]], [["Deep diving-11"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Uncensored versions of the show are also available to purchase on various digital platforms, including iTunes and Amazon, with the digital releases of each season containing bonus material."], [""]]}, {"qid": "ede61d328ef1d2d97f7f", "term": "Oprah Winfrey", "description": "American businesswoman, talk show host, actress, producer, and philanthropist", "question": "Does Oprah Winfrey have a degree from an Ivy League university?", "answer": true, "facts": ["Oprah Winfrey has received honorary doctorates from Duke and Harvard Universities", "Harvard University is a member of the Ivy League"], "decomposition": ["What schools does Oprah Winfrey have honorary degrees from?", "Is #1 Ivy league?"], "evidence": [[[["Oprah Winfrey-4"]], [["Harvard University-1"], "operation"]], [[["Oprah Winfrey-4"]], [["Ivy League-1"], "operation"]], [[["Oprah Winfrey-4"]], [["Outline of Harvard University-2"]]]], "golden_sentence": [["In 2013, Winfrey was awarded the Presidential Medal of Freedom by President Obama and honorary doctorate degrees from Duke and Harvard."], [""]]}, {"qid": "4ed0a0e8b6849979b9f7", "term": "Darth Vader", "description": "fictional character in the Star Wars franchise", "question": "Can Darth Vader hypothetically outdunk Bill Walton without using The Force?", "answer": false, "facts": ["The Force allows a Jedi to move objects with their mind.", "Darth Vader is 6'2\" tall.", "Former basketball player Bill Walton is a towering 6'11\" tall.", "The NBA basketball rim is 10 feet high."], "decomposition": ["What characteristic determines someone's ability to dunk?", "What is Darth Vader's measurement of #1?", "What is Bill Walton's measurement of #1?", "Is #2 greater than #3?"], "evidence": [[[["Human height-1"]], [["Darth Vader-8"], "no_evidence"], [["Bill Walton-5"]], ["operation"]], [[["Slam dunk-5"]], [["Darth Vader-3", "David Prowse-2"], "no_evidence"], [["Bill Walton-5"]], ["operation"]], [[["Slam dunk-5"]], [["Darth Vader-8"], "no_evidence"], [["Bill Walton-5"]], ["operation"]]], "golden_sentence": [[""], [""], ["His listed adult playing height was 6\u00a0ft 11\u00a0in (2.11\u00a0m); it has been reported that Walton is actually taller (7\u00a0ft 2\u00a0in (2.18\u00a0m) or more), but does not like being categorized as a seven-footer."]]}, {"qid": "c007a8ac74679d0aa533", "term": "Led Zeppelin", "description": "English rock band", "question": "Did the lead singer of Led Zepplin ever perform with Ernest Chataway?", "answer": true, "facts": ["Robert Plant is the lead singer of Led Zepplin", "Robert Plant was in the band The Honeydrippers", "Ernest Chataway was in the band The Honeydrippers"], "decomposition": ["Who was the lead singer of Led Zepplin?", "Who are the members of the Honeydrippers?", "Is Ernest Chataway also part of #2?", "Is #1 in #2?", "Is #3 and #4 both yes?"], "evidence": [[[["Led Zeppelin-1"]], [["The Honeydrippers-1"]], ["operation"], ["operation"], ["operation"]], [[["Robert Plant-1"]], [["The Honeydrippers-1"]], [["The Honeydrippers-1"]], ["operation"], ["operation"]], [[["The Honeydrippers-1"]], [["The Honeydrippers-1"]], ["operation"], ["operation"], ["operation"]]], "golden_sentence": [["The group consisted of vocalist Robert Plant, guitarist Jimmy Page, bassist/keyboardist John Paul Jones, and drummer John Bonham."], ["Formed originally in Worcestershire, the band was also composed of fellow former Led Zeppelin member Jimmy Page; Jeff Beck (a former Yardbirds member like Page); and other friends and well-known studio musicians including original Judas Priest guitarist Ernest Chataway."]]}, {"qid": "f6d2caecfad076f59112", "term": "Coca", "description": "group of plant varieties cultivated for coca production", "question": "Would someone with a nosebleed benefit from Coca?", "answer": true, "facts": ["Coca constricts blood vessels.", "As a result, it serves to stop bleeding. ", "Someone with a nosebleed would want the bleeding to stop."], "decomposition": ["What does Coca do to blood vessels?", "What happens to blood when #1 occurs?", "Would someone with a nose want #2 to occur?"], "evidence": [[[["Coca-30"]], [["Blood vessel-16"]], ["operation"]], [[["Coca-30"]], ["no_evidence"], ["no_evidence"]], [[["Coca-30"]], ["no_evidence"], ["operation"]]], "golden_sentence": [["Because coca constricts blood vessels, it also serves to oppose bleeding, and coca seeds were used for nosebleeds."], [""]]}, {"qid": "5a76e43f185d7139637a", "term": "Wednesday", "description": "Day of the week", "question": "Will Communion be denied to Wednesday name origin followers?", "answer": true, "facts": ["Communion is the body and blood of Christ given out during mass.", "Communion is only given to believers baptized in the Christian Church.", "Wednesday comes from Old English Wodnesdaeg referring to Woden, also called Odin.", "Odin was the pagan god of Norse mythology.", "Vikings, believers in Norse mythology, clashed with Christians in Wessex and Northumbria for hundreds of years."], "decomposition": ["Which deity is related to the origin of the name 'Wednesday'?", "Who are the worshipers of #1?", "Which group of people are allowed to take the Communion?", "Are #2 included in #3?"], "evidence": [[[["Odin-2"]], [["Odin-2"], "no_evidence"], [["Eucharist-1"]], ["operation"]], [[["Wednesday-1"]], [["Odin-2"]], [["First Communion-1"]], ["operation"]], [[["Wednesday-1"]], [["Anglo-Saxon paganism-1"]], [["Eucharist-1"]], ["operation"]]], "golden_sentence": [["In the modern period the rural folklore of Germanic Europe continued to acknowledge Odin."], [""], ["According to the New Testament, the rite was instituted by Jesus Christ during the Last Supper; giving his disciples bread and wine during a Passover meal, Jesus commanded his disciples to \"do this in memory of me\" while referring to the bread as \"my body\" and the cup of wine as \"the new covenant in my blood\"."]]}, {"qid": "7488296ab7adf12f6790", "term": "Kingdom of Hungary", "description": "former Central European monarchy (1000\u20131946)", "question": "Were Walkman's used in the Kingdom of Hungary?", "answer": false, "facts": ["The Kingdom of Hungary ended in 1946. ", "The Walkman was invented in 1979."], "decomposition": ["When did the Kingdom of Hungary come to an end?", "When was Walkman invented?", "Is #2 before #1?"], "evidence": [[[["Kingdom of Hungary-1"]], [["Walkman-1"]], ["operation"]], [[["Kingdom of Hungary-1"]], [["Walkman-5"]], ["operation"]], [[["Kingdom of Hungary-1"]], [["Walkman-2"]], ["operation"]]], "golden_sentence": [["The Kingdom of Hungary was a monarchy in Central Europe that existed from the Middle Ages into the 20th century (1000\u20131946 with the exception of 1918\u20131920)."], ["The original Walkman, released in 1979, was a portable cassette player that changed listening habits by allowing people to listen to music of their choice on the move."]]}, {"qid": "8e051427c42b43e70090", "term": "Philippine\u2013American War", "description": "Armed conflict between the First Philippine Republic and the United States", "question": "Would a veteran of the Phillippine-American War come home craving SPAM?", "answer": false, "facts": ["War veterans are often used to the rations they eat during war and crave similar items at home.", "The Philippine-American war took place before World War II, in 1899.", "Soldiers in World War II were given SPAM in their rations. ", "SPAM was released in 1937."], "decomposition": ["The Philippine-American war took place in what year?", "What year was SPAM invented in?", "Is #1 after #2?"], "evidence": [[[["Philippine\u2013American War-1"]], [["Spam (food)-3"]], ["operation"]], [[["Philippine\u2013American War-1"]], [["Spam (food)-1"]], ["operation"]], [[["Philippine\u2013American War-1"], "operation"], ["no_evidence"], ["no_evidence"]]], "golden_sentence": [["The Philippine\u2013American War, also referred to as the Filipino\u2013American War, the Philippine War, the Philippine Insurrection or the Tagalog Insurgency (Filipino: Digmaang Pilipino\u2013Amerikano; Spanish: Guerra Filipino\u2013Estadounidense), was an armed conflict between the First Philippine Republic and the United States that lasted from February 4, 1899, to July 2, 1902."], ["Spam was introduced by Hormel on July 5, 1937."]]}, {"qid": "1a9d7c5248b49e8d74a5", "term": "Moulin Rouge", "description": "cabaret in Paris, France", "question": "Could Moulin Rouge have been hypothetically used as Spain's Spanish American War triage center?", "answer": true, "facts": ["The Moulin Rouge cabaret in France had a capacity of 850 people.", "Spain had 700-800 injured during Spanish American War."], "decomposition": ["How many people can be seated in Moulin Rouge?", "How many Spaniards were injured during the Spanish American War?", "Ia #1 greater than #2?"], "evidence": [[[["Moulin Rouge-2"], "no_evidence"], [["Spanish\u2013American War-55"]], ["operation"]], [[["Moulin Rouge-2"], "no_evidence"], [["Spanish\u2013American War-57"], "no_evidence"], ["no_evidence", "operation"]], [[["Moulin Rouge! (musical)-3"], "no_evidence"], [["Spanish\u2013American War-55"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], ["More than 200 U.S. soldiers were killed and close to 1,200 wounded in the fighting, thanks to the high rate of fire the Spanish put down range at the Americans."]]}, {"qid": "a92f7caaa10abb28492f", "term": "Metallica", "description": "American heavy metal band", "question": "Did Metallica band members cutting their hair hurt their sales?", "answer": true, "facts": ["Metallica famously cut their hair in 1996 which caused a huge divide in their fanbase.", "Metallica's best selling album, The Black Album, was released in 1991 and has sold over 20 million copies.", "Since 1996, Metallica have released 5 studio albums.", "Metalica's 5 studio albums since 1996 have sold around a combined 14 million copies"], "decomposition": ["When did Metallica band members cut their hair?", "How many copies of their best selling album has been sold?", "How many copies of their last five albums have been sold altogether?", "Is #1 after the release date of #2 and before those of #3, and #2 greater than #3?"], "evidence": [[[["Metallica-25"]], [["Metallica (album)-22"]], [["Death Magnetic-44", "Death Magnetic-45", "Hardwired... to Self-Destruct-15", "Load (album)-2", "Reload (Metallica album)-6", "St. Anger-3"], "no_evidence"], ["operation"]], [[["Metallica-25"]], [["Metallica discography-1"], "no_evidence"], ["no_evidence"], ["operation"]], [[["Load (album)-1", "Load (album)-13"], "no_evidence"], [["Metallica (album)-3"]], [["Death Magnetic-45", "Hardwired... to Self-Destruct-2", "Load (album)-2", "Reload (Metallica album)-6", "St. Anger-3"], "no_evidence"], ["no_evidence", "operation"]], [[["Metallica-25"]], [["Metallica (album)-22"]], [["Death Magnetic-47", "Hardwired... to Self-Destruct-15", "Lulu (Lou Reed and Metallica album)-16", "Reload (Metallica album)-1", "St. Anger-3"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], ["It became the first album in the SoundScan era to pass 16\u00a0million in sales, and with 16.4 million copies sold by 2016, Metallica is the best-selling album in the United States since Nielsen SoundScan tracking began in 1991."], ["It remained at #1 for two weeks and has sold over 150,000 copies to date.", "Within the first three days of the album's release, Death Magnetic sold over 100,000 copies and has been certified platinum.", "Since July 7, 2017 the album officially sold more than 1,004,000 copies in the United States.", "", "The album remained on the chart for 75 weeks, and sold just over four million copies in the United States by December 2009.", "St. Anger was certified double platinum by the Recording Industry Association of America (RIAA) for shipping two million copies in the US; it has sold nearly six million copies worldwide."]]}, {"qid": "c9f0a5fd35737e79df3e", "term": "B", "description": "letter in the Latin alphabet", "question": "Does the letter B's place in alphabet exceed number of 2008 total lunar eclipses?", "answer": true, "facts": ["The letter B is the second letter in the Latin Alphabet.", "There was one total lunar eclipse in 2008."], "decomposition": ["What number represents the position of letter B in the English alphabet?", "How many total lunar eclipses occurred in 2008?", "Is #1 greater than #2?"], "evidence": [[[["B-1"]], [["August 2008 lunar eclipse-1"]], ["operation"]], [[["B-1"]], [["February 2008 lunar eclipse-9"], "no_evidence"], ["operation"]], [[["B-1"]], [["August 2008 lunar eclipse-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["B or b is the second letter of the Latin-script alphabet."], ["A partial lunar eclipse took place on August 16, 2008, the second of two lunar eclipses in 2008, with the first being a total eclipse on February 20, 2008."]]}, {"qid": "074ff780e16343240a73", "term": "Cinco de Mayo", "description": "Annual celebration held on May 5", "question": "Would Emmanuel Macron celebrate Cinco de Mayo?", "answer": false, "facts": ["Cinco de Mayo is observed to commemorate the Mexican Army's victory over the French Empire at the Battle of Puebla, on May 5, 1862.", "Emmanuel Macron is the current president of France.", "Emmanuel Macron was born in France and his ancestry traces back to France.", "People do not typically celebrate events in which their country was defeated."], "decomposition": ["Which countries usually celebrate the Cinco de Mayo?", "Which country is Emmanuel Macron from?", "Is #2 included in any of #1?"], "evidence": [[[["Cinco de Mayo-17"]], [["Amiens-1", "Emmanuel Macron-5"]], ["operation"]], [[["Battle of Puebla-11"]], [["Emmanuel Macron-1"]], ["operation"]], [[["Cinco de Mayo-12"], "no_evidence"], [["Emmanuel Macron-63"], "operation"], ["no_evidence"]]], "golden_sentence": [["The city of Brisbane, Queensland, Australia, holds an annual Mexican Festival to honor the day, and celebrations are held in London and New Zealand."], ["Amiens Cathedral is the tallest of the large, classic, Gothic churches of the 13th century and the largest in France of its kind.", "Born in Amiens, Emmanuel Jean-Michel Fr\u00e9d\u00e9ric Macron is the son of Fran\u00e7oise Macron (n\u00e9e Nogu\u00e8s), a physician, and Jean-Michel Macron, professor of neurology at the University of Picardy."]]}, {"qid": "0930e6cfafe2fdcf5fd1", "term": "Gray whale", "description": "species of mammal", "question": "Would a baby gray whale fit in a tractor-trailer?", "answer": true, "facts": ["Gray whales measure 4.9 m (16 ft) in length for newborns.", "A semi-trailer is 48 feet long."], "decomposition": ["How large is a baby gray whale?", "How large is a tractor-trailer?", "Is #2 greater than #1?"], "evidence": [[[["Gray whale-8"]], [["Trailer (vehicle)-4"]], [["Trailer (vehicle)-4"], "operation"]], [[["Gray whale-21"]], [["Semi-trailer truck-25"]], ["operation"]], [[["Gray whale-21"]], [["Semi-trailer truck-18"]], ["operation"]]], "golden_sentence": [[""], ["In the United States trailers ranging in size from single-axle dollies to 6-axle, 13-foot-6-inch (4.11\u00a0m) high, 53-foot (16.15\u00a0m) long semi-trailers are commonplace."], [""]]}, {"qid": "f1b295d41c0f9db29468", "term": "Twenty-third Amendment to the United States Constitution", "description": "Grants residents of Washington, D.C. the right to vote in U.S. presidential elections", "question": "Was Harry Truman's presidency unaffected by the twenty-third Amendment to the US Constitution?", "answer": true, "facts": ["The 23rd Amendment to the US Constitution was passed in 1961.", "Harry Truman was the President from 1945-1953."], "decomposition": ["When was Harry Truman the president of the United States?", "When was the 23rd Amendment passed?", "Is #2 after #1?"], "evidence": [[[["Harry S. Truman-1"]], [["Twenty-third Amendment to the United States Constitution-1"]], ["operation"]], [[["Harry S. Truman-99"], "no_evidence"], [["Twenty-third Amendment to the United States Constitution-11"], "no_evidence"], ["operation"]], [[["Harry S. Truman-1"]], [["Twenty-third Amendment to the United States Constitution-1"]], ["operation"]]], "golden_sentence": [["Harry S. Truman (May 8, 1884 \u2013 December 26, 1972) was the 33rd president of the United States from 1945 to 1953, succeeding upon the death of Franklin D. Roosevelt after serving as vice president."], [""]]}, {"qid": "1231566dc500931353d9", "term": "Joker (character)", "description": "Fictional character in the DC Universe", "question": "Could Bart Simpson have owned comics with The Joker?", "answer": true, "facts": ["The first appearance of the Joker was in 1940.", "Bart Simpson first appeared as a child in \"The Simpsons\" in 1987."], "decomposition": ["When was the first appearance of the Joker?", "When did Bart Simpson first appear?", "Did #2 come after #1?"], "evidence": [[[["Joker (character)-1"]], [["Bart Simpson-1"]], ["operation"]], [[["Joker (character)-1"]], [["Bart Simpson-1"]], ["operation"]], [[["Joker (character)-59"]], [["Bart Simpson-13"], "no_evidence"], ["operation"]]], "golden_sentence": [["The Joker is a supervillain created by Bill Finger, Bob Kane, and Jerry Robinson who first appeared in the debut issue of the comic book Batman (April 25, 1940), published by DC Comics."], ["He is voiced by Nancy Cartwright and first appeared on television in The Tracey Ullman Show short \"Good Night\" on April 19, 1987."]]}, {"qid": "18053cb499b65d9eb68e", "term": "Do it yourself", "description": "building, modifying, or repairing something without the aid of experts or professionals", "question": "Do Do It Yourself channels online always show realistic projects?", "answer": false, "facts": ["The Youtube channel '5 Minute Crafts' specializes in DIY projects for all ages.", "\"5 Minute Crafts\" has come under fire for posting videos that were fraudulent or dangerous in nature. "], "decomposition": ["What are some popular Do It Yourself media?", "Of #1, which are YouTube channels?", "Are all of #2  regarded as realistic projects?"], "evidence": [[[["Do it yourself-13"]], ["no_evidence"], ["operation"]], [[["Do it yourself-13", "Do it yourself-6"]], ["no_evidence"], ["no_evidence"]], [[["Do it yourself-10", "Do it yourself-11", "Do it yourself-12", "Do it yourself-6"], "no_evidence"], ["operation"], ["no_evidence", "operation"]]], "golden_sentence": [["Beyond magazines and television, the scope of home improvement DIY continues to grow online where most mainstream media outlets now have extensive DIY-focused informational websites such as This Old House, Martha Stewart, Hometalk, and the DIY Network."]]}, {"qid": "7a33f19cd95dd0225095", "term": "Mercury (planet)", "description": "Smallest and closest planet to the Sun in the Solar System", "question": "Would only warm weather attire be a good idea on Mercury?", "answer": false, "facts": ["Warm weather attire would not protect your body in cold temperatures.", "Mercury can reach temperatures of \u2212280 \u00b0F at night."], "decomposition": ["What are the best temperatures to wear warm weather attire?", "What is the average temperature of Mercury at night?", "Is there any overlap between #1 and #2?"], "evidence": [[[["Clothing-2"]], [["Mercury (planet)-4"]], [["Clothing-2"]]], [[["Winter-18"], "no_evidence"], [["Mercury (planet)-4"]], ["operation"]], [[["Highest temperature recorded on Earth-4"], "no_evidence"], [["Mercury (planet)-4"]], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["Having almost no atmosphere to retain heat, it has surface temperatures that vary diurnally more than on any other planet in the Solar System, ranging from 100\u00a0K (\u2212173\u00a0\u00b0C; \u2212280\u00a0\u00b0F) at night to 700\u00a0K (427\u00a0\u00b0C; 800\u00a0\u00b0F) during the day across the equatorial regions."], [""]]}, {"qid": "87c5a14ea8e229b8b921", "term": "Caracal", "description": "Small wild cat", "question": "Can you measure a Caracal with a protractor?", "answer": false, "facts": ["A caracal is a small wild cat", "Cats and other animals are measured in terms of properties like length, width, and weight", "Protractors measure angles"], "decomposition": ["What are protractors used to measure?", "What is a caracal?", "Are #1 and #2 similar (or the same)?"], "evidence": [[[["Protractor-1"]], [["Caracal-1"]], ["operation"]], [[["Protractor-1"]], [["Caracal-1"]], ["operation"]], [[["Protractor-4"]], [["Caracal-1"]], ["operation"]]], "golden_sentence": [["A protractor is a measuring instrument, typically made of transparent plastic or glass, for measuring angles."], ["The caracal /\u02c8k\u00e6r\u0259k\u00e6l/ (Caracal caracal) is a medium-sized wild cat native to Africa, the Middle East, Central Asia and India."]]}, {"qid": "dd5eb746064348daed10", "term": "French toast", "description": "bread soaked in beaten eggs and then fried", "question": "Can a goat be used for one of the ingredients in French toast?", "answer": true, "facts": ["French toast is made from bread, eggs, milk, and cinnamon.", "Goats are able to produce milk, similar to cows.", "Goats milk is used in a variety of cheeses and milks sold in super markets."], "decomposition": ["What common dairy product can be obtained from goats?", "What are the typical ingredients of French toast?", "Is #1 included in #2?"], "evidence": [[[["Goat-46"]], [["French toast-1"]], ["operation"]], [[["Goat-1"]], [["French toast-2"]], ["operation"]], [[["Goat-46"]], [["French toast-1"]], ["operation"]]], "golden_sentence": [["Some goats are bred specifically for milk."], ["French toast is a dish made of sliced bread soaked in eggs and typically milk, then fried."]]}, {"qid": "6c6f57506c5ea443aa08", "term": "Dr. Seuss", "description": "American children's writer and illustrator", "question": "Did Dr. Seuss make himself famous?", "answer": false, "facts": ["Dr. Seuss's wife was Helen Palmer.", "Helen Palmer suggested that Dr. Seuss become an artist rather than a professor.", "Helen Palmer inspired much of Dr. Seuss's work."], "decomposition": ["Who was Dr. Seuss' wife?", "Did #1 not serve as inspiration and give key suggestions to Dr. Seuss?"], "evidence": [[[["Dr. Seuss-7"]], [["Dr. Seuss-7"], "no_evidence", "operation"]], [[["Helen Palmer (author)-1"]], [["Helen Palmer (author)-5"]]], [[["Helen Palmer (author)-1"]], [["Helen Palmer (author)-3"], "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "e32e6d1a988cc6c18c12", "term": "Saga", "description": "stories mostly ancient Nordic mythology and history of Germanic tribes. Written in the Old Norse language, mainly in Iceland", "question": "Was song of Roland protagonist friendly with group that had sagas?", "answer": false, "facts": ["The sagas were Nordic mythological stories that were celebrated by the Vikings.", "The Song of Roland was an epic poem about the nephew of Charlemagne.", "Charlemagne was King of the Franks.", "The Franks fought many battles against Vikings including the Siege of Paris in 845 AD."], "decomposition": ["Sagas are found in which culture?", "What culture is the hero of Song of Roland from?", "Did #1 and #2 get along well?"], "evidence": [[[["Saga-2"]], [["The Song of Roland-1"]], [["Viking raids in the Rhineland-7"], "operation"]], [[["Chivalric sagas-9"], "no_evidence"], [["The Song of Roland-27"], "no_evidence"], ["operation"]], [[["Saga-2"]], [["The Song of Roland-1"]], [["Vikings-1"], "operation"]]], "golden_sentence": [["But sagas' subject matter is diverse, including pre-Christian Scandinavian legends; saints and bishops both from Scandinavia and elsewhere; Scandinavian kings and contemporary Icelandic politics; and chivalric romances either translated from Continental European languages or composed locally."], [""], [""]]}, {"qid": "51197d763d62a85470d2", "term": "Johns Hopkins University", "description": "Private research university in Baltimore, Maryland", "question": "Could the endowment of Johns Hopkins University pay off the MBTA debt?", "answer": false, "facts": ["Johns Hopkins University had an endowment of $6.28 billion in 2019.", "The MBTA is in debt for approximately $9 billion."], "decomposition": ["How much was Johns Hopkins University endowment in 2019?", "How much is the MBTA debt?", "Is #1 greater than #2?"], "evidence": [[["no_evidence"], [["Massachusetts Bay Transportation Authority-91"]], ["operation"]], [[["Johns Hopkins University-11"], "no_evidence"], [["Massachusetts Bay Transportation Authority-90", "Massachusetts Bay Transportation Authority-91"], "no_evidence"], ["operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["The FY2014 budget includes $1.422 billion for operating expenses and $443.8M in debt and lease payments."]]}, {"qid": "2a3f899bea4f5adc5cfa", "term": "Winter", "description": "one of the Earth's four temperate seasons, occurring between autumn and spring", "question": "Is Christmas celebrated during winter?", "answer": true, "facts": ["Winter begins around December 20.", "Christmas is celebrated on December 25, a few days later."], "decomposition": ["When is Christmas celebrated?", "When does winter begin in the US?", "When does winter end in the US?", "Is #1 between #2 and #3?"], "evidence": [[[["Christmas-1"]], [["Winter-8"]], [["Winter-8"]], ["operation"]], [[["Christmas-1"]], [["Winter-9"]], [["Winter-9"]], ["operation"]], [[["Christmas-28"]], [["Winter-9"], "no_evidence"], [["Winter-9"], "no_evidence"], ["operation"]]], "golden_sentence": [["Christmas (or Feast of the Nativity) is an annual festival commemorating the birth of Jesus Christ, observed primarily on December 25 as a religious and cultural celebration among billions of people around the world."], ["In one version of this definition, winter begins at the winter solstice and ends at the vernal equinox."], ["In one version of this definition, winter begins at the winter solstice and ends at the vernal equinox."]]}, {"qid": "6d11e77434ae0cd418d8", "term": "Sun bear", "description": "bear found in tropical forest habitats of Southeast Asia", "question": "Do sun bears stay active during winter?", "answer": true, "facts": ["The sun bear is a species in the family Ursidae occurring in the tropical forests of Southeast Asia.", " Sun bears do not seem to hibernate.", "Hibernation is a seasonal heterothermy characterized by low body-temperature, slow breathing and heart-rate, and low metabolic rate. It most commonly occurs during winter months."], "decomposition": ["What characterizes the state of hibernation that some animals go into during winter?", "Are sun bears known to not exhibit the behavior described by #1?"], "evidence": [[[["Hibernation-3"]], [["Sun bear-2"], "operation"]], [[["Hibernation-1"]], [["Sun bear-2"], "operation"]], [[["Hibernation-1"]], [["Sun bear-11"]]]], "golden_sentence": [[""], ["They do not seem to hibernate, possibly because food resources are available the whole year throughout the range."]]}, {"qid": "296b25c6874724f980b0", "term": "Caracal", "description": "Small wild cat", "question": "Could a student at the University of Houston see a caracal on campus?", "answer": false, "facts": ["The caracal is native to Africa, the Middle East, Central Asia, and India.", "The University of Houston is located in the United States.", "The United States is located in North America."], "decomposition": ["What areas is the caracal native to?", "In what area is the University of Houston located?", "Is there an area present in both #1 and #2?"], "evidence": [[[["Caracal-1"]], [["University of Houston-1"]], ["operation"]], [[["Caracal-20"]], [["University of Houston-11"]], [["Houston-2", "Kyzylkum Desert-1"], "operation"]], [[["Caracal-1"]], [["University of Houston-1"]], ["operation"]]], "golden_sentence": [["The caracal /\u02c8k\u00e6r\u0259k\u00e6l/ (Caracal caracal) is a medium-sized wild cat native to Africa, the Middle East, Central Asia and India."], ["Its campus spans 667 acres in southeast Houston, and was known as University of Houston\u2013University Park from 1983 to 1991."]]}, {"qid": "420dd11c98d4e89efe62", "term": "Euro", "description": "European currency", "question": "Would someone pay for a coffee in NYC with Euros?", "answer": false, "facts": ["New York City is located within the United States.", "The currency used in the United States is the United States dollar, not the Euro."], "decomposition": ["In what country is New York City?", "What is the currency for #1?", "Is #2 the Euro?"], "evidence": [[[["New York City-1"]], [["United States dollar-1"]], ["operation"]], [[["New York City-1"]], [["United States dollar-1"]], ["operation"]], [[["New York City-1"]], [["United States dollar-1"]], ["operation"]]], "golden_sentence": [[""], ["The United States dollar (sign: $; code: USD; also abbreviated US$ and referred to as the dollar, U.S. dollar, or American dollar) is the official currency of the United States and its territories per the Coinage Act of 1792."]]}, {"qid": "75a0344574fea4024644", "term": "Honey bee", "description": "Eusocial flying insect of genus Apis, producing surplus honey", "question": "Can a single honey bee sting multiple humans?", "answer": false, "facts": ["When a honey bee stings a human, the stinger becomes stuck in the skin and detaches from the bee.", "This usually results in the bee's death.", "Even if it survives, it no longer has a stinger to attack another person with."], "decomposition": ["What happens to a bee's stinger when it stings a human?", "What happens to a bee when #1 occurs?", "Can #2 sting another person?"], "evidence": [[[["Bee sting-6"]], [["Bee sting-6"]], [["Bee sting-6"], "operation"]], [[["Honey bee-61"]], [["Honey bee-61"]], ["operation"]], [[["Bee sting-8"]], [["Bee sting-8"]], [["Bee sting-8"]]]], "golden_sentence": [["Although it is widely believed that a worker honey bee can sting only once, this is a partial misconception: although the stinger is in fact barbed so that it lodges in the victim's skin, tearing loose from the bee's abdomen and leading to its death in minutes, this only happens if the skin of the victim is sufficiently thick, such as a mammal's."], ["Although it is widely believed that a worker honey bee can sting only once, this is a partial misconception: although the stinger is in fact barbed so that it lodges in the victim's skin, tearing loose from the bee's abdomen and leading to its death in minutes, this only happens if the skin of the victim is sufficiently thick, such as a mammal's."], ["Although it is widely believed that a worker honey bee can sting only once, this is a partial misconception: although the stinger is in fact barbed so that it lodges in the victim's skin, tearing loose from the bee's abdomen and leading to its death in minutes, this only happens if the skin of the victim is sufficiently thick, such as a mammal's."]]}, {"qid": "b1e38436184ecfb3a2d6", "term": "666 (number)", "description": "Natural number", "question": "Would the number 666 appear in a church?", "answer": false, "facts": ["A church is a place of worship in Christianity.", "Jesus Christ is worshiped by adherents of Christianity.", "666 is a symbolic representation of the Antichrist.", "An Antichrist is someone that opposes Jesus Christ."], "decomposition": ["What does the number 666 represent to Christians?", "Would Christians want to be associated with #1?"], "evidence": [[[["Number of the Beast-1"]], ["operation"]], [[["Number of the Beast-1"]], ["operation"]], [[["666 (number)-8"]], [["Satan-1"]]]], "golden_sentence": [[""]]}, {"qid": "6324a164ddee48a210cc", "term": "Cape Town", "description": "Legislative capital of South Africa", "question": "Is Cape Town south of the Equator?", "answer": true, "facts": ["Cape Town is an important city in South Africa.", "South Africa is located entirely south of the Equator."], "decomposition": ["What country is Cape Town located in?", "Is #1 located south of the equator?"], "evidence": [[[["Cape Town-74"], "operation"], ["no_evidence"]], [[["Cape Town-3"]], [["Equator-4"]]], [[["Cape Town-1"]], [["South Africa-2"]]]], "golden_sentence": [[""]]}, {"qid": "8e99204f270ccb8efea7", "term": "Silicon", "description": "Chemical element with atomic number 14", "question": "Will silicon wedding rings outsell bromine wedding rings?", "answer": true, "facts": ["Wedding rings are typically made of precious shiny stones such as diamonds.", "Silicon is a solid rock like element at room temperature that has a natural lustre.", "Bromine is a liquid at room temperature that is toxic to the touch."], "decomposition": ["What state of matter is necessary for something to be worn as a ring at room temperature?", "What state of matter is bromine at room temperature?", "Is #2 the same as #1?", "If #3 is no, then a bromide wedding ring does not exist.", "What state of matter is silicon at room temperature? (see comment for #4)"], "evidence": [[[["Ring (jewellery)-1", "Solid-1"]], [["Bromine-1"]], ["operation"], ["operation"], [["Silicon-1"]]], [["no_evidence"], [["Bromine-1"]], ["operation"], ["no_evidence"], [["Silicon-1"]]], [[["Solid-1"]], [["Bromine-1"]], ["operation"], ["operation"], [["Silicon-1"]]]], "golden_sentence": [["", "Solid is one of the Four fundamental states of matter (the others being liquid, gas and plasma)."], ["It is the third-lightest halogen, and is a fuming red-brown liquid at room temperature that evaporates readily to form a similarly coloured gas."], [""]]}, {"qid": "fbbe22e81a3deae25567", "term": "Ontology", "description": "study of the nature of being, becoming, existence or reality, as well as the basic categories of being and their relations", "question": "Does ontology require a scalpel?", "answer": false, "facts": ["A scalpel is used during surgery.", "Ontology is a philosophical domain, not a medical one."], "decomposition": ["What are the areas of focus of ontology?", "Where does a scalpel find application?", "Is #2 included in #1?"], "evidence": [[[["Ontology-1"]], [["Scalpel-1"]], ["operation"]], [[["Ontology-1"]], [["Scalpel-1"]], ["operation"]], [[["Ontology-1"]], [["Scalpel-1"]], ["operation"]]], "golden_sentence": [["More broadly, it studies concepts that directly relate to being, in particular becoming, existence, reality, as well as the basic categories of being and their relations."], ["A scalpel, or lancet, or bistoury, is a small and extremely sharp bladed instrument used for surgery, anatomical dissection, podiatry and various arts and crafts (called a hobby knife)."]]}, {"qid": "3817a79e633225013dcf", "term": "Godzilla", "description": "Giant monster or kaiju", "question": "Is Godzilla's image likely grounds for a lawsuit in 2050?", "answer": false, "facts": ["The copyright for Godzilla is owned by Toho Company Limited.", "The first Godzilla film was released by Toho in 1954.", "Works that are significantly old enter the public domain and can be used without copyright permission.", "Godzilla will enter the public domain in the year 2049."], "decomposition": ["When can a copyrighted item be used without permission?", "In what year will Godzilla as a creative piece of work attain #1 status?", "Is #2 after 2050?"], "evidence": [[[["Copyright term-2"]], ["operation"], ["operation"]], [[["Copyright-4"]], [["Godzilla-1", "Tomoyuki Tanaka-1"]], ["operation"]], [[["Public domain-16"]], [["Godzilla-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["musical composition or novel), whether the work has been published or not, and whether the work was created by an individual or a corporation."]]}, {"qid": "85dca87239a656b8f10f", "term": "Hyphen", "description": "Punctuation mark used to join words", "question": "Is Olivia Newton-John hyphenated celebrity name with most letters?", "answer": false, "facts": ["Olivia Newton-John has sixteen letters in her name.", "Actress Catherine Zeta-Jones has 18 letters in her name.", "Actor Joseph Gordon-Levitt has 18 letters in his name."], "decomposition": ["How many letters are in the name Olivia Newton-John?", "How many letters are in the name Catherine Zeta-Jones?", "How many letters are in the name Joseph Gordon-Levitt?", "Is #1 greater than both #2 and #3?"], "evidence": [[["operation"], ["operation"], ["operation"], [["Letter (alphabet)-3"], "operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Olivia Newton-John-46"], "no_evidence"], [["Catherine Zeta-Jones-8"], "no_evidence"], [["Joseph Gordon-Levitt-5"], "no_evidence"], ["no_evidence"]]], "golden_sentence": [[""]]}, {"qid": "8514577552459151ef4d", "term": "Daytona 500", "description": "Auto race held in Daytona, Florida, United States", "question": "Will electric car struggle to finish Daytona 500?", "answer": true, "facts": ["The Daytona 500 is a 2.5 mile long race.", "The Daytona 500 requires 200 laps to complete.", "The best electric car engines last around 390 miles."], "decomposition": ["How long (in miles) is the Daytona 500 race?", "What is the maximum electric range (in miles) of the world's best selling electric car?", "Is #2 less than #1?"], "evidence": [[[["Daytona 500-1"]], [["Tesla Model 3-1"]], ["operation"]], [[["Daytona 500-7"]], [["Electric car-20"]], ["operation"]], [[["Daytona 500-7"]], [["Electric car-3"]], ["operation"]]], "golden_sentence": [["The Daytona 500 is a 500-mile-long (805\u00a0km) NASCAR Cup Series motor race held annually at Daytona International Speedway in Daytona Beach, Florida."], ["The Model 3 Standard Range Plus version delivers an EPA-rated all-electric range of 250 miles (402\u00a0km) and the Long Range versions deliver 322 miles (518\u00a0km)."]]}, {"qid": "569ff94818a5fcf3a643", "term": "Swastika", "description": "a geometrical figure and an ancient religious icon in the cultures of Eurasia and 20th-century symbol of Nazism", "question": "Are swastikas used in the most common religion in India?", "answer": true, "facts": ["The swastika is a religious symbol that is used in Hinduism, Buddhism, and Jainism.", "Almost 80% of people in India practice Hinduism."], "decomposition": ["Which religions use the swastika as a symbol?", "What is the most common religion in India?", "Is #2 included in #1?"], "evidence": [[[["Swastika-1"]], [["Hinduism in India-1"]], ["operation"]], [[["Swastika-1"]], [["Hinduism-1"]], ["operation"]], [[["Swastika-1"]], [["Religion in India-1"]], ["operation"]]], "golden_sentence": [["It is used as a symbol of divinity and spirituality in Indian religions, including Hinduism, Buddhism and Jainism."], ["Hinduism is the largest religion in India, with 79.8% of the population identifying themselves as Hindus, that accounts for 966 million Hindus as of National Census of India in 2011 making it as the world's largest Hindu populated country, while 14.2% of the population follow Islam and the remaining 6% adhere to other religions (such as Christianity, Sikhism, Buddhism, Jainism, various indigenous ethnically-bound faiths, Atheism and Irreligion)."]]}, {"qid": "c117eaa9b8e18eb65ada", "term": "President of India", "description": "Ceremonial head of state of India", "question": "Is it more expensive to run for President of India than to buy a new iPhone 11?", "answer": false, "facts": ["Candidates for the presidency of India must pay a deposit of Rs 15,000", "A brand new iPhone 11 costs Rs 67,300"], "decomposition": ["How much must a candidate pay to run for president in India?", "How much does a new iPhone 11 cost?", "Is #1 more than #2?"], "evidence": [[[["President of India-63"], "no_evidence"], [["IPhone-10"], "no_evidence"], ["operation"]], [[["President of India-2"], "no_evidence"], [["IPhone 11-1"], "no_evidence"], ["no_evidence", "operation"]], [[["President of India-57"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["\u2014\u2009Article 60, Constitution of India The president of India used to receive \u20b910,000 (US$100) per month per the Second Schedule of the constitution."], ["The two initial models, a 4\u00a0GB model priced at US$499 and an 8\u00a0GB model at US$599 (both requiring a two-year contract), went on sale in the United States on June 29, 2007, at 6:00\u00a0pm local time, while hundreds of customers lined up outside the stores nationwide."]]}, {"qid": "5c881213b3c0c64e57c1", "term": "Rahul Dravid", "description": "Indian cricketer", "question": "Is it hard for Rahul Dravid to order food at a restaurant in Aurangabad?", "answer": false, "facts": ["Aurangabad is located in Maharashtra.", "Marathi is an Indo-Aryan language spoken predominantly by around 100 million Marathi people of Maharashtra, India.", "Rahul Dravid is fluent in Marathi."], "decomposition": ["What languages can Rahul Dravid speak fluently?", "In which state is Aurangabad located?", "What is the official language of #2?", "Is #1 exclusive of #3?"], "evidence": [[[["Rahul Dravid-9"]], [["Aurangabad-1"]], [["Maharashtra-2"]], ["operation"]], [[["Rahul Dravid-9"]], [["Aurangabad-1"]], [["Marathi language-1"]], ["operation"]], [[["Rahul Dravid-9"]], [["Aurangabad district, Maharashtra-1"]], [["Maharashtra-2"]], ["operation"]]], "golden_sentence": [["He is fluent in several languages, Marathi, Kannada, English and Hindi."], ["Aurangabad (pronunciation\u00a0(help\u00b7info)) is a city in the Indian state of Maharashtra."], ["Maharashtra was formed on 1 May 1960 by splitting the bilingual Bombay State, which had existed since 1956, into majority Marathi speaking Maharashtra, and Gujarati speaking Gujarat respectively."]]}, {"qid": "f054715965a77c3e2654", "term": "Quantum mechanics", "description": "Branch of physics that acts as an abstract framework formulating all the laws of nature", "question": "Did Terry Pratchett write about quantum mechanics?", "answer": true, "facts": ["\u201cWhat're quantum mechanics?\"  \"I don't know. People who repair quantums, I suppose.\u201d", "\"Granny Weatherwax wouldn\u2019t know what a pattern of quantum inevitability was if she found it eating her dinner. If you mentioned the words \u2018paradigm of space-time\u2019 to her she\u2019d just say \u2018What?\u2019 But that didn\u2019t mean she was ignorant. It just meant that she didn\u2019t have truck with words, especially gibberish.\""], "decomposition": ["What was Terry Pratchett's occupation?", "Which subject deals with the study of quantum mechanics?", "Is #2 a part of #1's job?"], "evidence": [[[["Terry Pratchett-1"]], [["Quantum mechanics-1"]], ["no_evidence"]], [[["Terry Pratchett-1"]], [["Quantum mechanics-1"]], ["operation"]], [[["The Science of Discworld-1"]], [["Quantum mechanics-1"]], ["operation"]]], "golden_sentence": [["Sir Terence David John Pratchett OBE (28 April 1948 \u2013 12 March 2015) was an English humorist, satirist, and author of fantasy novels, especially comical works."], [""]]}, {"qid": "ef1be70be91463681734", "term": "Reza Shah", "description": "Shah of Iran, Founder of the Imperial state of iran", "question": "Did number of Imams Reza Shah believed in exceed number of Jesus's disciples?", "answer": false, "facts": ["Reza Shah, the founder of the Imperial state of Iran, was a Twelver Shia Muslim.", "Twelver Shia Muslims believe that there are 12 Imams.", "Jesus is typically represented as having 12 disciples."], "decomposition": ["What was the religion signature of Reza Shah?", "How many Imams do adherents of #1 believe in?", "How many disciples did Jesus have?", "Is #2 greater than #3?"], "evidence": [[[["Reza Shah-19"]], [["Imam-2"]], [["Disciple whom Jesus loved-4"]], ["operation"]], [[["Reza Shah-26"], "no_evidence"], [["Twelver-1"]], [["Apostles-1"]], ["operation"]], [[["Reza Shah-26"], "no_evidence"], [["Twelver-1"]], [["Apostles-1"]], ["operation"]]], "golden_sentence": [["Reza Shah was the first Iranian Monarch in 1400 years who paid respect to the Jews by praying in the synagogue when visiting the Jewish community of Isfahan; an act that boosted the self-esteem of the Iranian Jews and made Reza Shah their second most respected Iranian leader after Cyrus the Great."], [""], ["Matthew, Mark, and Luke do not mention any one of the 12 disciples having witnessed the crucifixion."]]}, {"qid": "bae569aa298111ef6d74", "term": "Final Fantasy VI", "description": "1994 video game", "question": "Does Final Fantasy VI require electricity to play?", "answer": true, "facts": ["Final Fantasy VI is a video game.", "Video games are played using a video game console and television.", "Video game consoles and televisions require electricity in order to function."], "decomposition": ["Which device(s) would be needed to play the video game Final Fantasy VI?", "Do any of #1 run on electricity?"], "evidence": [[[["Final Fantasy VI-2"]], [["Super Nintendo Entertainment System-22"]]], [[["Final Fantasy VI-1", "Super Nintendo Entertainment System-1"]], ["operation"]], [[["Final Fantasy VI-1"]], [["Video game console-1"], "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "9bc7ecf13042d8cafb42", "term": "Newbie", "description": "slang term for a novice or newcomer", "question": "Would Dale Earnhardt Jr. be considered a newbie?", "answer": false, "facts": ["Dale Earnhardt Jr. is a Nascar racer with 19 years of experience.", "The average Nascar career length is 3.7 years."], "decomposition": ["What is a newbie?", "How many years has Dale Earnhardt Jr been a racer?", "Are #1 and #2 similar?"], "evidence": [[[["Newbie-1"]], [["Dale Earnhardt Jr.-3", "Dale Earnhardt Jr.-5"]], ["operation"]], [[["Newbie-1"]], [["Dale Earnhardt Jr.-1", "Dale Earnhardt Jr.-5"]], ["operation"]], [[["Newbie-1"]], [["Dale Earnhardt-11"]], ["operation"]]], "golden_sentence": [["Newbie, newb, noob, nub, or n00b is a slang term for a novice or newcomer, or somebody inexperienced in a profession or activity."], ["", ""]]}, {"qid": "71a0009133e3221f47bb", "term": "Pelvis", "description": "lower part of the trunk of the human body between the abdomen and the thighs (sometimes also called pelvic region of the trunk", "question": "Is cycling a high-risk activity for pelvis fractures?", "answer": false, "facts": ["Cycling is a low-impact activity ", "Stress fractures in a pelvic bone often develop as a result of repetitive, high-impact activity that puts stress on the pelvis, such as long-distance running or ballet"], "decomposition": ["What type of activity can result in stress fractures?", "Would cycling be considered #1?"], "evidence": [[[["Stress fracture-6"]], [["Stationary bicycle-7"], "no_evidence", "operation"]], [[["Stress fracture-1"]], ["operation"]], [[["Pelvic fracture-20"]], ["operation"]]], "golden_sentence": [["Stress fractures commonly occur in sedentary people who suddenly undertake a burst of exercise (whose bones are not used to the task) They may also occur in athletes completing high volume, high impact training, such as running or jumping sports."], [""]]}, {"qid": "0f481bfa2fab6e93f64e", "term": "Anorexia nervosa", "description": "Eating disorder characterized by refusal to maintain a healthy body weight, and fear of gaining weight due to a distorted self image", "question": "Would a person with Anorexia nervosa be more likely to break a bone than a regular person?", "answer": true, "facts": ["People with Anorexia Nervosa restrict food and as a result lack essential nutrients.", "Many people with Anorexia Nervosa, are at high risk for osteoporosis(and to a lesser extent bulimia nervosa) will have low bone density and consequently reduced bone strength.", "People with Anorexia Nervosa, are at high risk for osteoporosis. "], "decomposition": ["What kind of eating behavior do people with anorexia nervosa exhibit?", "Does #1 lead to reduced bone strength?"], "evidence": [[[["Anorexia nervosa-1"]], [["Osteoporosis-1"], "operation"]], [[["Anorexia nervosa-1"]], [["Anorexia nervosa-54"]]], [[["Anorexia nervosa-4"]], [["Malnutrition-3"]]]], "golden_sentence": [["Many people with anorexia see themselves as overweight even though they are, in fact, underweight."], [""]]}, {"qid": "37a998b561a5cf1a1849", "term": "Jennifer Lawrence", "description": "American actress", "question": "Is Jennifer Lawrence's middle name similar to the name of a Scorsese collaborator?", "answer": true, "facts": ["Jennifer Lawrence's middle name is Shrader.", "Paul Schrader is a screenwriter and director.", "Paul Schrader wrote the screenplay for Taxi Driver, Raging Bull, The Last Temptation of Christ, and Bringing Out the Dead.", "Martin Scorsese directed Taxi Driver, Raging Bull, The Last Temptation of Christ, and Bringing Out the Dead."], "decomposition": ["What is Jennifer Lawrence's middle name?", "Who has collaborated with Scorsese?", "Does #2 include someone with #1 in their name?"], "evidence": [[[["Jennifer Lawrence-1"]], [["Paul Schrader-1"]], ["operation"]], [[["Jennifer Lawrence-1"]], [["Paul Schrader-1"], "no_evidence"], ["operation"]], [[["Jennifer Lawrence-1"]], [["Paul Schrader-1"]], ["operation"]]], "golden_sentence": [["Jennifer Shrader Lawrence (born August 15, 1990) is an American actress."], [""]]}, {"qid": "e8ca1f6e49f423a04c26", "term": "Ten-pin bowling", "description": "sport", "question": "Can a ten-pin bowling pin be a deadly weapon?", "answer": true, "facts": ["A regulation ten-pin bowling pin weighs around four pounds.", "The average rolling pin used in cooking weighs slighty over four pounds.", "A 2015 case covered in the BBC involved a rolling pin as the murder weapon."], "decomposition": ["How much does a ten-pin bowling pin weigh?", "What kind of pin has been used as a murder weapon?", "How much does #2 weigh?", "Is #3 roughly the same as #1?"], "evidence": [[[["Bowling pin-2"]], [["Sammy White's Brighton Bowl-3"]], [["Candlepin bowling-11"]], ["operation"]], [[["Bowling pin-2"]], [["Firing pin-1"]], [["Lock time-4"], "no_evidence"], [["Bowling pin-2"]]], [[["Bowling pin-2"]], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Bowling pin-2"]], [["Rolling pin-3"], "no_evidence"], [["Rolling pin-2"], "no_evidence"], ["operation"]]], "golden_sentence": [["They weigh 3\u00a0pounds 6\u00a0ounces (1.5 kilograms), pins weighing up to 3\u00a0lb 10\u00a0oz (1.6\u00a0kg) are approved."], ["Found at the scene was a bloody bowling pin determined to be the weapon used to bludgeon the victims."], ["The candlepins themselves are 15\u00a03\u20444 inches (40\u00a0cm) tall, have a cylindrical shape which tapers equally towards each end (and therefore having no distinct \"top\" or \"bottom\" end, unlike a tenpin), giving them an overall appearance somewhat like that of a candle, and have a maximum weight of 2\u00a0lb 8\u00a0oz (1.1\u00a0kg) apiece."]]}, {"qid": "14715e763a4c2810e1d9", "term": "Parachuting", "description": "action sport of exiting an aircraft and returning to Earth using a parachute", "question": "Can parachuting amateurs ignore hurricane force winds bulletins?", "answer": false, "facts": ["A hurricane force wind warning is issued by the National Weather Service for winds above 74 mph ", "Solo student parachuters are prohibited from jumping in winds exceeding 14 mph"], "decomposition": ["What's the minimum wind speed above which the National Weather Service issues hurricane force wind warnings?", "What's the maximum wind speed in which a solo student parachuter can jump?", "Is #2 greater than #1?"], "evidence": [[[["Hurricane force wind warning-1"]], [["Parachuting-21"]], ["operation"]], [[["Saffir\u2013Simpson scale-2"]], [["Parachuting-21"]], ["operation"]], [[["Hurricane force wind warning-1"]], [["Parachute-36"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["In the United States, the USPA's Basic Safety Requirements prohibit solo student skydivers from jumping in winds exceeding 14\u00a0mph while using ram-air equipment."]]}, {"qid": "57615ede43064507c9ae", "term": "Moli\u00e8re", "description": "17th-century French playwright and actor", "question": "Was Moliere Queen Margot's ill fated lover?", "answer": false, "facts": ["Queen Margot is a character in Alexande Dumas's La Reine Margot.", "Queen Margot keeps the head of her executed lover.", "Joseph Boniface de La M\u00f4le, nicknamed La Mole, was executed as a conspirator against Queen Margot's kingdom.", "Queen Margot is set during the St. Bartholomew's Day Massacre which occurred in 1572.", "Moliere was born in 1622."], "decomposition": ["In what work by Alexande Dumas does Queen Margot appear?", "When was #1 written?", "In what year was Moliere born?", "Is #3 before #2?"], "evidence": [[[["La Reine Margot (novel)-1"]], [["La Reine Margot (novel)-1"]], [["Moli\u00e8re-1"]], ["operation"]], [[["La Reine Margot (novel)-1", "La Reine Margot (novel)-3"]], [["La Reine Margot (novel)-1", "La Reine Margot (novel)-3"]], [["Moli\u00e8re-1"]], ["operation"]], [[["La Reine Margot (novel)-1", "La Reine Margot (novel)-5"]], [["La Reine Margot (novel)-1"]], [["Moli\u00e8re-1"]], ["operation"]]], "golden_sentence": [["La Reine Margot (English:Queen Margot) is a historical novel written in 1845 by Alexandre Dumas, p\u00e8re."], [""], ["Jean-Baptiste Poquelin (baptised 15 January 1622; died 17 February 1673), known by his stage name Moli\u00e8re (UK: /\u02c8m\u0252li\u025b\u0259r, \u02c8mo\u028al-/, US: /mo\u028al\u02c8j\u025b\u0259r, \u02ccmo\u028ali\u02c8\u025b\u0259r/, French:\u00a0[m\u0254lj\u025b\u0281]), was a French playwright, actor and poet, widely regarded as one of the greatest writers in the French language and universal literature."]]}, {"qid": "9289b9af1dc13c110687", "term": "Hypothermia", "description": "A human body core temperature below 35.0\u00b0C", "question": "Would you be more likely to die of hypothermia in New York than Florida?", "answer": true, "facts": ["Central New York Winters are between 12-30 degrees Fahrenheit.", "Florida winters are between 65 and 77 degrees Fahrenheit."], "decomposition": ["What is the typical temperature range of the coldest time of the year in New York?", "What is the typical temperature range of the coldest time of the year in Florida?", "Is #1 lower than #2?"], "evidence": [[[["New York City-62"], "no_evidence"], [["Climate of Florida-7", "Climate of Florida-8"], "no_evidence"], ["operation"]], [[["New York (state)-43"]], [["Florida-45"]], ["operation"]], [[["New York (state)-43"]], [["Geography of Florida-5"]], ["operation"]]], "golden_sentence": [["Extreme temperatures have ranged from \u221215\u00a0\u00b0F (\u221226\u00a0\u00b0C), recorded on February 9, 1934, up to 106\u00a0\u00b0F (41\u00a0\u00b0C) on July 9, 1936; the coldest recorded wind chill was \u221237\u00a0\u00b0F (\u221238\u00a0\u00b0C) on the same day as the all-time record low."], ["Average lows range from 65\u00a0\u00b0F (18\u00a0\u00b0C) in Key West to near 41\u00a0\u00b0F (5\u00a0\u00b0C) degrees Fahrenheit at Tallahassee, while daytime highs range from 64\u00a0\u00b0F (18\u00a0\u00b0C) at Tallahassee to 77\u00a0\u00b0F (25\u00a0\u00b0C) at Miami.", ""]]}, {"qid": "e8abe74aac798ef0aff7", "term": "Television", "description": "Telecommunication medium for transmitting and receiving moving images", "question": "Did the Democratic Party's nominee for President of the U.S. in 1908 watch TV?", "answer": false, "facts": ["William Jennings Bryan was the Democratic Party's nominee for President of the U.S. in 1908", "William Jennings Bryan died Jul 26, 1925", "Television was invented in 1927"], "decomposition": ["Who was the Democratic Party's nominee for President of the U.S. in 1908?", "When did #1 die?", "When was the television invented?", "Is #3 before #2?"], "evidence": [[[["William Jennings Bryan 1908 presidential campaign-1"]], [["William Jennings Bryan-56"]], [["History of television-14"]], ["operation"]], [[["1908 United States presidential election-1"]], [["William Jennings Bryan-1"]], [["Television-10"]], ["operation"]], [[["William Jennings Bryan-30"], "operation"], ["no_evidence"], [["Television Electronic Disc-2"], "operation"], ["no_evidence"]]], "golden_sentence": [["In this election, Roosevelt's chosen successor, Republican William Howard Taft, ran in large part on Roosevelt's Progressive legacy and decisively defeated former Congressman and three-time Democratic U.S. Presidential candidate William Jennings Bryan (who also advocated progressive ideas in his campaign)."], ["On Sunday, July 26, 1925, Bryan died in his sleep after attending a church service in Dayton."], ["By 1935, Takayanagi had invented the first all-electronic television."]]}, {"qid": "f9acb3da747676f36faf", "term": "Cuisine of Hawaii", "description": "Cuisine of Hawaii", "question": "Does the cuisine of Hawaii embrace foods considered gross in the continental US?", "answer": true, "facts": ["SPAM has a reputation for being an unpleasant dish in the continental US.", "SPAM is so popular in Hawaii that it has made it onto McDonalds menus. "], "decomposition": ["Does SPAM have a reputation as an unpleasant dish in the continental US?", "Does SPAM have a reputation as a pleasant dish in Hawaii?", "Are #1 and #2 positive?"], "evidence": [[[["Spam (food)-34"]], [["Spam (food)-43"]], [["Spam (food)-34", "Spam (food)-43"]]], [[["Spam (food)-34", "Spam (food)-8"]], [["Spam (food)-10"]], ["operation"]], [[["Spam (food)-8"]], [["Spam (food)-10"]], [["Spam (food)-10"]]]], "golden_sentence": [["Spam has long had a somewhat dubious reputation in the United States and (to a lesser degree) United Kingdom as a poverty food."], ["The museum tells the history of the Hormel company, the origin of Spam, and its place in world culture, including Hawaii where Spam is eaten daily by locals."], ["", ""]]}, {"qid": "84fcf7489c546fdad97e", "term": "2010 United Kingdom general election", "description": "election of members to the House of Commons in 2010", "question": "Did John Kerry run in the 2010 United Kingdom general election?", "answer": false, "facts": ["John Kerry is an American citizen and politician", "Only citizens of the UK, Ireland or a Commonwealth nation are eligible to run in the United Kingdom general elections"], "decomposition": ["In order to run in the UK general election, a person must be a citizen of one of which countries? ", "John Kerry is a citizen of what country?", "Is #2 listed in #1?"], "evidence": [[[["Elections in the United Kingdom-7"]], [["John Kerry-1"]], ["operation"]], [[["Member of parliament-34"]], [["John Kerry-2"]], ["operation"]], [[["Citizenship-38"], "no_evidence"], [["John Kerry-5"], "no_evidence"], ["operation"]]], "golden_sentence": [["In addition, to qualify to appear on the Electoral Register, applicants who are Commonwealth citizens must either possess leave to enter or remain in the UK or not require such leave on the date of their application and no applicant may be a convicted person detained in prison or a mental hospital (or unlawfully at large if they would otherwise have been detained) or a person found guilty of certain corrupt or illegal practices."], [""]]}, {"qid": "c343c5bd3917145d2ffa", "term": "One Thousand and One Nights", "description": "Collection of Middle Eastern stories and folk tales compiled in Arabic during the Islamic Golden Age", "question": "Was The Canterbury Tales written before One Thousand and One Nights?", "answer": false, "facts": ["One Thousand and One Nights was compiled during the Islamic Golden Age.", "The Islamic Golden Age lasted from 800 AD to 1258 AD.", "The Canterbury Tales was written in 1392."], "decomposition": ["When were the The Canterbury Tales written?", "When was One Thousand and One Nights written?", "Which years are included in #2?", "Is #1 before #3?"], "evidence": [[[["The Canterbury Tales-7"]], [["One Thousand and One Nights-1"]], [["One Thousand and One Nights-1"]], [["The Canterbury Tales-7"]]], [[["The Canterbury Tales-1"]], [["One Thousand and One Nights-13"], "no_evidence"], [["One Thousand and One Nights-33"], "no_evidence"], ["operation"]], [[["The Canterbury Tales-1"]], [["One Thousand and One Nights-1"]], [["Islamic Golden Age-1"]], ["operation"]]], "golden_sentence": [["The first version of The Canterbury Tales to be published in print was William Caxton's 1476 edition."], ["One Thousand and One Nights (Arabic: \u0623\u064e\u0644\u0652\u0641\u064f \u0644\u064e\u064a\u0652\u0644\u064e\u0629\u064d \u0648\u064e\u0644\u064e\u064a\u0652\u0644\u064e\u0629\u064c\u200e, \u02beAlf Laylah wa-Laylah) is a collection of Middle Eastern folk tales compiled in Arabic during the Islamic Golden Age."], [""], [""]]}, {"qid": "6fa33ef36ba9b4cf2178", "term": "Lionel Richie", "description": "American singer-songwriter, musician, record producer and actor", "question": "Does  Lionel Richie believe in holistic medicine?", "answer": true, "facts": ["Lionel Richie suffered prolonged throat problems and had surgery four times in four years before being told by conventional doctors that he could lose his singing career. ", "Lionel Richie finally turned to a holistic doctor who said that the problem was simply acid reflux caused by foods he was eating before going to bed."], "decomposition": ["Which doctor diagnosed Lionel Richie satisfactorily after he had surgeries for a prolonged throat problem?", "Is #1 a holistic doctor?"], "evidence": [[["no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], ["operation"]], [["no_evidence"], ["operation"]]], "golden_sentence": []}, {"qid": "90ab0ba7235fc437e644", "term": "Superman", "description": "Fictional superhero", "question": "Did villain that killed Superman murder Robin?", "answer": false, "facts": ["Superman was killed by the villain Doomsday in Superman #75.", "Robin is killed by The Joker in the Batman comics."], "decomposition": ["Who was Superman killed by?", "Who was Robin killed by?", "Is #1 the same as #2?"], "evidence": [[[["Doomsday (DC Comics)-2"]], [["Joker (character)-28"]], ["operation"]], [[["The Death of Superman-3"]], [["Jason Todd-28"]], ["operation"]], [[["The Death of Superman-3"]], [["Jason Todd-2"]], ["operation"]]], "golden_sentence": [[""], ["Believing that Robin (Tim Drake) has been killed in the chaos, Dick Grayson beats the Joker to death (although Batman revives his foe to keep Grayson from being a murderer), and the villain succeeds in making a member of the Bat-family break their rule against killing."]]}, {"qid": "99920137c1f9e46af420", "term": "Ocelot", "description": "Small wild cat", "question": "Is an ocelot a good present for a kindergartener?", "answer": false, "facts": ["An ocelot is a small wild cat native to South America, Mexico, and the southern US.", "Ocelots are carnivores that hunt other animals and are aggressive, and strong for their size.", "Kindergarteners are usually 5 years old and weigh around 39 pounds."], "decomposition": ["What is the behavior and social nature of an ocelot?", "Is #1 suitable for kindergartners considering their age?"], "evidence": [[[["Ocelot-2"]], [["Kindergarten-1"], "operation"]], [[["Exotic felids as pets-9"]], [["Kindergarten-1"], "no_evidence", "operation"]], [[["Ocelot-25"], "operation"], ["no_evidence"]]], "golden_sentence": [["Typically active during twilight and at night, the ocelot tends to be solitary and territorial."], [""]]}, {"qid": "715881a92b795f94c739", "term": "Missionary", "description": "member of a religious group sent into an area to do evangelism", "question": "Were the first missionaries required to attend mass on Sundays?", "answer": true, "facts": ["The word \"mission\" originates from 1598 when the Jesuits sent members abroad.", "Jesuits are a Roman Catholic order of religious men", "The Roman Catholic religion requires members to attend mass on Sundays"], "decomposition": ["What religion were the first missionaries?", "Does #1 require mass attendance?"], "evidence": [[[["Missionary-4"]], [["Mass (liturgy)-1", "Mass (liturgy)-13"], "operation"]], [[["Missionary-4"]], [["Eucharist in the Catholic Church-76"], "no_evidence"]], [[["Guadalupe Missionaries-1"]], [["Mass (liturgy)-13"], "operation"]]], "golden_sentence": [["During the Middle Ages, the Christian monasteries and missionaries such as Saint Patrick (5th century), and Adalbert of Prague (ca 956-997) propagated learning and religion beyond the European boundaries of the old Roman Empire."], ["", ""]]}, {"qid": "55b85e1497ef214098bb", "term": "Donald Duck", "description": "Disney cartoon character", "question": "Will Donald Duck hypothetically purchase bell-bottoms for himself?", "answer": false, "facts": ["Bell-bottoms were a style of pants that were popular in the 60s and 70s.", "Donald Duck is an animated Disney character that never wears pants."], "decomposition": ["What article of clothing are bell-bottoms?", "What articles of clothing does Donald Duck wear?", "Is #1 listed in #2?"], "evidence": [[[["Bell-bottoms-1"]], [["Donald Duck-1", "Donald Duck-4"]], ["operation"]], [[["Bell-bottoms-1"]], [["Donald Duck-1"]], ["operation"]], [[["Bell-bottoms-1"]], [["Donald Duck-1"]], ["operation"]]], "golden_sentence": [["Bell-bottoms (or flares) are a style of trousers that become wider from the knees downward, forming a bell-like shape of the trouser leg."], ["He typically wears a sailor shirt and cap with a bow tie.", "The character featured in the cover is much different from the modern Donald Duck, being drawn more like a normal duck and sporting a green hat and pants."]]}, {"qid": "9a650f864b08e8ddfe30", "term": "Christina Aguilera", "description": "American singer, songwriter, actress, and television personality", "question": "Was Christina Aguilera born in the forgotten borough?", "answer": true, "facts": ["Christina Maria Aguilera was born on December 18, 1980, in Staten Island, New York.", "Staten Island has sometimes been called \"the forgotten borough\" by inhabitants who feel neglected by the city government."], "decomposition": ["Where was Christina Aguilera born?", "What place is known as the forgotten borough?", "Is #1 the same as #2?"], "evidence": [[[["Christina Aguilera-4"]], [["Staten Island-2"]], ["operation"]], [[["Christina Aguilera-4"]], [["Staten Island-2"]], ["operation"]], [[["Christina Aguilera-2"]], [["Staten Island-2"]], ["operation"]]], "golden_sentence": [["Christina Mar\u00eda Aguilera was born in Staten Island, New York City, on December 18, 1980, to musician Shelly Loraine Kearns (n\u00e9e Fidler) and United States Army soldier Fausto Xavier Aguilera."], ["Staten Island has sometimes been called \"the forgotten borough\" by inhabitants who feel neglected by the city government."]]}, {"qid": "ce29733a24e4ab36cb7a", "term": "Heart failure", "description": "condition in which the heart is unable to provide sufficient pump action", "question": "Would ramen be bad for someone with heart failure?", "answer": true, "facts": ["People with heart failure have to limit their sodium intake.", "Ramen is notorious for having incredibly high sodium levels. "], "decomposition": ["What is the recommended maximum daily sodium allowance for someone with heart failure?", "How much sodium is in a bowl of ramen?", "Is #2 greater than #1 divided by three?"], "evidence": [[[["Reference Daily Intake-15"], "no_evidence"], [["Ramen-11"], "no_evidence"], ["operation"]], [[["Reference Daily Intake-14"], "no_evidence"], [["Instant noodle-23"], "no_evidence"], ["no_evidence", "operation"]], [[["Sodium-35"], "no_evidence"], [["Instant noodle-12"]], ["no_evidence", "operation"]]], "golden_sentence": [["The recommended maximum daily intake of sodium \u2013 the amount above which health problems appear \u2013 is 2,300 milligrams per day for adults, about 1 teaspoon of salt (5.9\u00a0g)."], [""]]}, {"qid": "362952a6b869c23f2ab1", "term": "Ohio University", "description": "public university in Athens, Ohio, United States", "question": "Would the current president of Ohio University hypothetically wear a jockstrap?", "answer": true, "facts": ["The current president of Ohio University is Duane Nellis.", "Duane Nellis is a man.", "A jockstrap is an undergarment for protecting the testes and penis during cycling, contact sports or other vigorous physical activity.", "The testes and penis are the sexual organs of men."], "decomposition": ["Which gender wears jockstrap?", "Who is the current President of Ohio University?", "Does #2 identify with the gender #1?"], "evidence": [[[["Jockstrap-1"]], [["Duane Nellis-1"]], ["operation"]], [[["Jockstrap-1"]], [["Kristina M. Johnson-1"], "no_evidence"], ["operation"]], [[["Thong-27"]], [["Duane Nellis-1"]], ["operation"]]], "golden_sentence": [[""], ["Marvin Duane Nellis is an American educator and university administrator, currently the president of Ohio University in Athens."]]}, {"qid": "d9a1c7e23fd0e5f4dc7a", "term": "Mile", "description": "Unit of length", "question": "Would an Olympic athlete be tired out after running a mile?", "answer": false, "facts": ["The Olympic standard for men in running one mile is 4 minutes, 19 seconds. ", "The Olympic standard for women in running one mile is at least 4 minutes and 40 seconds. "], "decomposition": ["What is the Olympic standard time in running one mile for men?", "What is the Olympic standard time in running one mile for women?", "Is #1 or #2 a very long period of time?"], "evidence": [[[["Mile run-4"], "no_evidence"], [["Mile run-4"], "no_evidence"], ["operation"]], [[["Mile run world record progression-1"], "no_evidence"], [["Mile run world record progression-1"], "no_evidence"], ["operation"]], [[["Mile run-4"], "no_evidence"], [["Mile run-4"], "no_evidence"], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "8306eda46b8b6288953f", "term": "Swallow", "description": "family of birds", "question": "Can an ostrich fit into the nest of a swallow?", "answer": false, "facts": ["Swallows weigh less than an ounce.", "An ostrich can weigh over 200 pounds."], "decomposition": ["How much does a swallow weigh?", "How much does an ostrich weigh?", "Is #2 within 20% of #1?"], "evidence": [[[["Swallow-7"]], [["Common ostrich-5"]], ["operation"]], [[["Swallow-7"]], [["Common ostrich-5"]], ["operation"]], [[["Swallow-7"]], [["Common ostrich-5"]], ["operation"]]], "golden_sentence": [["Their body length ranges from about 10\u201324\u00a0cm (3.9\u20139.4\u00a0in) and their weight from about 10\u201360\u00a0g (0.35\u20132.12\u00a0oz)."], ["At one year of age, common ostriches weigh approximately 45 kilograms (99\u00a0lb)."]]}, {"qid": "bb1b48ac196cd4b7e591", "term": "Daytona 500", "description": "Auto race held in Daytona, Florida, United States", "question": "Did Dale Jr hug his dad after their last Daytona 500 together?", "answer": false, "facts": ["Dale Jr. and his father Dale Sr. last raced together at the Daytona 500 in 2001.", "During the 2001 Daytona 500 Dale Sr. suffered a basilar skull fracture and died. "], "decomposition": ["Which race did Dale Jr and his father participate in last together?", "Which notable incident took place during #1?", "Was Dale Jr.'s father well enough to hug his son after #2?"], "evidence": [[[["Dale Earnhardt Jr.-6"]], [["Dale Earnhardt Jr.-6"]], ["no_evidence", "operation"]], [[["Dale Earnhardt-23"]], [["Dale Earnhardt-23"]], ["operation"]], [[["Dale Earnhardt Jr.-10"]], [["Dale Earnhardt-23"]], ["operation"]]], "golden_sentence": [["Earnhardt Jr. ran nine Busch Series races between 1996 and 1997 for Dale Earnhardt, Inc., and Ed Whitaker, respectively, before driving for his father's team in the Busch Series full-time in 1998, in which he started the season with an amazing blow over after contact with Dick Trickle and Buckshot Jones at Daytona, on the same weekend that his father had his first and only Daytona 500 win."], [""]]}, {"qid": "79f2fafbfc1a0fe8858a", "term": "Wednesday", "description": "Day of the week", "question": "Did Wednesday have something to do with Thor?", "answer": true, "facts": ["Wednesday is the middle of the modern work week and comes from the name Wodan.", "The Germanic god Woden is also known as Wodanaz or Odin.", "Odin, in Norse mythology, was the father of Thor."], "decomposition": ["Which Germanic god is the name 'Wednesday' etymologically related to?", "Is #1 related to Thor?"], "evidence": [[[["Wednesday-5"]], [["Odin-4"]]], [[["Wednesday-1"]], [["Thor-3"], "operation"]], [[["Wednesday-5"]], [["Odin-4"]]]], "golden_sentence": [["The name is a calque of the Latin dies Mercurii \"day of Mercury\", reflecting the fact that the Germanic god Woden (Wodanaz or Odin) during the Roman era was interpreted as \"Germanic Mercury\"."], ["Odin is the son of Bestla and Borr and has two brothers, Vili and V\u00e9."]]}, {"qid": "1d23043c594c9c9fde13", "term": "Lil Wayne", "description": "American rapper, record executive and actor from Louisiana", "question": "Will AC/DC album sales buy more B-52 bombers than Lil Wayne's?", "answer": true, "facts": ["The B-52 bomber plane cost 60 million dollars in 2018.", "AC/DC has sold over 200 million albums.", "Lil Wayne has sold 120 million records worldwide."], "decomposition": ["How much does one B-52 bomber cost?", "How much is AC/DC worth due to the sales of their albums?", "Lil Wayne has made how much from his album sales?", "Is #2 more than both #1 and #3?"], "evidence": [[[["Boeing B-52 Stratofortress-6"], "no_evidence"], [["AC/DC-5"], "no_evidence"], [["Lil Wayne-4"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], [["AC/DC-5"], "no_evidence"], ["no_evidence"], ["operation"]], [[["Bomber B-5"], "no_evidence"], [["AC/DC-2"], "no_evidence"], [["Lil Wayne-3"], "operation"], ["no_evidence"]]], "golden_sentence": [[""], ["AC/DC have sold more than 200 million records worldwide, including 71.5 million albums in the United States, making them the tenth highest-selling artist in the United States and the 14th best selling artist worldwide."], ["Lil Wayne has sold over 120 million records worldwide, including more than 15 million albums and 37 million digital tracks in the United States, making him one of the world's best-selling music artists."]]}, {"qid": "45fe9a6f11884a3795df", "term": "Mount Sharp", "description": "mountain on Mars", "question": "Do mountain goats inhabit the summit of Mount Sharp?", "answer": false, "facts": ["Mountain goats are animals", "Animals require oxygen in order to live", "Mount Sharp is located on Mars", "The atmosphere of Mars contains only trace amounts of oxygen"], "decomposition": ["Where is Mount Sharp located?", "Does #1 have enough atmospheric oxygen to support the life of animals, such as mountain goats?"], "evidence": [[[["Mount Sharp-1"]], [["Life on Mars-9"]]], [[["Mount Sharp-1"]], [["Mars-59"], "operation"]], [[["Mount Sharp-1"]], [["Life on Mars-1"]]]], "golden_sentence": [["Mount Sharp, officially Aeolis Mons (IPA:\u00a0[\u02c8i\u02d0\u0259l\u0268s \u02c8m\u0252nz]), is a mountain on Mars."], [""]]}, {"qid": "a473ecf3711c63185440", "term": "Larry King", "description": "American television and radio host", "question": "Could Larry King's marriages be counted on two feet?", "answer": true, "facts": ["The typical person has 10 toes spread across their two feet.", "Larry King has been married 8 times.", "You can count each marriage on each toe."], "decomposition": ["How many times has Larry King been married?", "How many toes do most people have?", "Is #2 at least as much as #1?"], "evidence": [[[["Larry King-37"]], [["Toe-2"]], ["operation"]], [[["Larry King-43"]], [["Toe-2"]], ["operation"]], [[["Larry King-37"]], [["Toe-2"]], ["operation"]]], "golden_sentence": [["King has been married eight times, to seven women."], ["Each toe is surrounded by skin, and present on all five toes is a toenail."]]}, {"qid": "15c1843d2e79da78e717", "term": "Gladiator", "description": "combatant who entertained audiences in the Roman Republic and Roman Empire", "question": "Were gladiators associated with the Coloseum?", "answer": true, "facts": ["The Colosseum was a stadium in the ancient city of Rome, large by even today's standards.", "During the Roman era, it was primarily used to host large spectacles including gladiatorial combat, mock battles, and executions."], "decomposition": ["What kind of building was the Colosseum and which city did it exist in?", "In the era of #1, which kind of events were held in such buildings?", "Did any of #2 involve gladiators?"], "evidence": [[[["Colosseum-1"]], [["Colosseum-2"]], [["Colosseum-2"], "operation"]], [[["Colosseum-1"]], [["Colosseum-1"]], [["Colosseum-2"], "operation"]], [[["Colosseum-1"]], [["Colosseum-2"]], [["Inaugural games of the Flavian Amphitheatre-2"], "operation"]]], "golden_sentence": [["The Colosseum or Coliseum (/\u02cck\u0252l\u0259\u02c8si\u02d0\u0259m/ KOL-\u0259-SEE-\u0259m), also known as the Flavian Amphitheatre (Latin: Amphitheatrum Flavium; Italian: Anfiteatro Flavio [a\u0271fite\u02c8a\u02d0tro \u02c8fla\u02d0vjo] or Colosseo [kolos\u02c8s\u025b\u02d0o]), is an oval amphitheatre in the centre of the city of Rome, Italy."], ["The Colosseum could hold an estimated 50,000 to 80,000 spectators at various points of its history over the centuries, having an average audience of some 65,000; it was used for gladiatorial contests and public spectacles such as mock sea battles (for only a short time as the hypogeum was soon filled in with mechanisms to support the other activities), animal hunts, executions, re-enactments of famous battles, and dramas based on Classical mythology."], ["The Colosseum could hold an estimated 50,000 to 80,000 spectators at various points of its history over the centuries, having an average audience of some 65,000; it was used for gladiatorial contests and public spectacles such as mock sea battles (for only a short time as the hypogeum was soon filled in with mechanisms to support the other activities), animal hunts, executions, re-enactments of famous battles, and dramas based on Classical mythology."]]}, {"qid": "44b3d0cbf09bf48e5002", "term": "Rupert Murdoch", "description": "Australian-born American media mogul", "question": "Does Rupert Murdoch's alma mater have more history than the USA?", "answer": true, "facts": ["Rupert Murdoch's alma mater is Worcester College.", "Worcester College was founded in 1714.", "The first documented use of the term the United States of America was in a January 2, 1776 letter."], "decomposition": ["What is Rupert Murdoch's alma mater?", "When was #1 founded?", "When was the United States founded?", "Is #2 prior to #3?"], "evidence": [[[["Rupert Murdoch-8"]], [["Worcester College, Oxford-1"]], [["United States Declaration of Independence-1"]], ["operation"]], [[["Rupert Murdoch-8"]], [["Worcester College, Oxford-1"]], [["United States Declaration of Independence-1"]], ["operation"]], [[["Rupert Murdoch-8"]], [["Worcester College, Oxford-1"]], [["United States-27"]], ["operation"]]], "golden_sentence": [["Murdoch studied Philosophy, Politics and Economics at Worcester College, Oxford in England, where he kept a bust of Lenin in his rooms and came to be known as \"Red Rupert\"."], ["The college was founded in 1714 by the benefaction of Sir Thomas Cookes, 2nd Baronet (1648-1701) of Norgrove, Worcestershire, whose coat of arms was adopted by the College."], ["The United States Declaration of Independence is the pronouncement adopted by the Second Continental Congress meeting at the Pennsylvania State House (now known as Independence Hall) in Philadelphia, Pennsylvania, on July 4, 1776."]]}, {"qid": "ae90fb2299abfe301098", "term": "Ubuntu", "description": "Linux distribution based on Debian", "question": "Do the Ubuntu people speak Ubuntu?", "answer": false, "facts": ["Ubuntu is a of free and open-source software used on computers.", "The Ubuntu people are a tribe that lives in Africa.", "The Ubuntu people derive their language from Nguni Bantu.", "As of 2019 only 10% of households in Africa have a computer."], "decomposition": ["What is Ubuntu?", "Can people speak #1?"], "evidence": [[[["Ubuntu-1"]], [["Linux distribution-1", "Operating system-1", "Spoken language-1"]]], [[["Ubuntu-1"]], [["Programming language-1"]]], [[["Ubuntu-1"]], [["Ubuntu-1"]]]], "golden_sentence": [["Ubuntu (/\u028a\u02c8b\u028antu\u02d0/ (listen) uu-BUUN-too) is a free and open-source Linux distribution based on Debian."], ["", "", ""]]}, {"qid": "42bdb30b3cc135143bbc", "term": "Miami", "description": "City in Florida, United States", "question": "Can native wolverines be found in Miami?", "answer": false, "facts": ["Wolverines are native to northern boreal forests", "Miami is not a northern boreal habitat"], "decomposition": ["What is the native range of wolverines?", "What state is Miami located in?", "Is #2 included in #1?"], "evidence": [[[["Wolverine-16"]], [["Miami-1"]], ["operation"]], [[["Wolverine-2"]], [["Miami-1"]], ["operation"]], [[["Wolverine-2"]], [["Miami-1"]], ["operation"]]], "golden_sentence": [["Fish and Wildlife Service publication, as of 2014 \"wolverines are found in the North Cascades in Washington and the Northern Rocky Mountains in Idaho, Montana, Oregon (Wallowa Range), and Wyoming."], ["Miami (/ma\u026a\u02c8\u00e6mi/), officially the City of Miami, is the seat of Miami-Dade County, and the cultural, economic and financial center of South Florida in the United States."]]}, {"qid": "f4c16ce94151618833d4", "term": "Paparazzi", "description": "profession", "question": "Were paparazzi involved in the death of a member of the royal family?", "answer": true, "facts": ["Diana Spencer was being pursued by paparazzi when her vehicle was involved in a fatal accident.", "Diana Spencer was known as 'Princess Diana' and was the Princess of Wales."], "decomposition": ["What were the circumstances surrounding the death of Diana Spencer?", "Is Diana Spencer a member of the royal family?", "Was paparazzi involved in #1?", "Are #2 and #3 positive?"], "evidence": [[[["Diana, Princess of Wales-4"]], [["Diana, Princess of Wales-3"]], [["Death of Diana, Princess of Wales-2"]], ["operation"]], [[["Death of Diana, Princess of Wales-1"]], [["Diana, Princess of Wales-1", "Diana, Princess of Wales-26"]], [["Death of Diana, Princess of Wales-2"]], ["operation"]], [[["Diana, Princess of Wales-53"]], [["Diana, Princess of Wales-1"]], [["Diana, Princess of Wales-53"]], ["operation"]]], "golden_sentence": [["Media attention and public mourning were extensive after her death in a car crash in a Paris tunnel in 1997 and subsequent televised funeral."], [""], ["No evidence was found that paparazzi were near the car when it crashed."]]}, {"qid": "97696a5af1d7243fdcc9", "term": "Moon Jae-in", "description": "President of South Korea", "question": "Was Moon Jae-in born outside of Khanbaliq?", "answer": true, "facts": ["Khanbaliq was the winter capital of the Mongol Empire. ", "Khanbaliq was located at the center of what is now modern day Beijing, China.", "Moon Jae-In was born in Geoje, South Korea."], "decomposition": ["Where was Moon Jae-in born?", "What is the modern day location of Khanbaliq?", "Is #1 different from #2?"], "evidence": [[[["Moon Jae-in-5"]], [["Khanbaliq-1"]], ["operation"]], [[["Moon Jae-in-2"]], [["Khanbaliq-1"]], ["operation"]], [[["Moon Jae-in-5"]], [["Khanbaliq-1"]], ["operation"]]], "golden_sentence": [["Upon the end of the Korean War, Moon Jae-in was born in Geoje, South Korea as the second child and oldest son among five children of father Moon Yong-hyung and mother Kang Han-ok. His parents were refugees from South Hamgyeong Province, North Korea who fled their native city of Hungnam during the Hungnam evacuation during the Korean War."], ["Khanbaliq or Dadu was the winter capital of the Yuan dynasty, the main center of the Mongol Empire founded by Kublai Khan in what is now Beijing, also the capital of China today."]]}, {"qid": "c5897a315257436cda10", "term": "Greyhound", "description": "Dog breed used in dog racing", "question": "Can a greyhound walk on two legs?", "answer": false, "facts": ["Greyhounds are dogs.", "Dogs walk on four legs. "], "decomposition": ["What type of animal is a greyhound?", "Does #1 walk on two legs?"], "evidence": [[[["Greyhound-1"]], ["no_evidence", "operation"]], [[["Greyhound-1"]], ["no_evidence"]], [[["Greyhound-1"]], [["Bipedalism-22", "Quadrupedalism-1"]]]], "golden_sentence": [["The Greyhound is a breed of dog, a sighthound which has been bred for coursing game and greyhound racing."]]}, {"qid": "21bc3519012bb3c89e0c", "term": "Sloth", "description": "tree dwelling animal noted for slowness", "question": "Do moths that live on sloths have family dinners?", "answer": false, "facts": ["Algae grows on sloths", "Sloth moths feed on algae that grows on sloths", "Sloth moth caterpillars feed on sloth dung ", "Sloths defecate far from their ususl abode"], "decomposition": ["What do sloth moths enjoy eating from the body of sloths?", "Where is #1 found on the sloth?", "What do baby or caterpillar sloth moths enjoy eating from the body of sloths?", "Where is #3 found relative to the sloth?", "Is #2 found in the same location as #4?"], "evidence": [[[["Sloth moth-2"], "no_evidence"], [["Sloth moth-3"], "no_evidence"], ["no_evidence"], ["no_evidence"], ["operation"]], [[["Sloth moth-4"]], [["Sloth moth-1"]], [["Sloth moth-2"]], [["Arthropods associated with sloths-12"]], ["operation"]], [[["Sloth moth-1", "Sloth moth-4"]], [["Sloth-2"]], [["Sloth moth-2"], "no_evidence"], ["no_evidence", "operation"], ["operation"]]], "golden_sentence": [[""], ["Chrysaugine moths, such as Cryptoses spp., spend their lives as adults in the fur of sloths, particularly the three-toed species, except when the sloths descend to defecate and females fly to the sloth dung to oviposit."]]}, {"qid": "142a8dc4a5e3efd96cea", "term": "New Year's Day", "description": "Holiday", "question": "Do Jehovah's Witnesses celebrate day before New Year's Day?", "answer": false, "facts": ["The Day before New Year's Day is New Year's Eve.", "Jehovah's Witnesses do not celebrate holidays, citing in many cases that they have pagan origins.", "New Year's has origins in pagan Babylonia."], "decomposition": ["Which holidays do Jehovah's Witnesses refrain from celebrating or participating in?", "What is the day before New Year's Day known as?", "Is #2 included in #1?"], "evidence": [[[["Jehovah's Witnesses practices-42", "Jehovah's Witnesses-3"]], [["New Year's Eve-1"]], ["operation"]], [[["Jehovah's Witnesses-3"]], [["New Year's Eve-1"], "no_evidence"], ["operation"]], [[["Jehovah's Witnesses-41"]], [["New Year's Eve-41"]], [["Jehovah's Witnesses-41", "New Year's Eve-41"], "operation"]]], "golden_sentence": [["Other common celebrations and religious or national holidays such as birthdays, Halloween, Easter and Christmas are not celebrated because they believe that these continue to involve \"false religious beliefs or activities.\"", "They do not observe Christmas, Easter, birthdays or other holidays and customs they consider to have pagan origins incompatible with Christianity."], ["The celebrations generally go on past midnight into New Year's Day, 1 January."]]}, {"qid": "6635985eb55ff5d956af", "term": "Caracal", "description": "Small wild cat", "question": "Would a caracal be defeated by Javier Sotomayor in a high jump competition?", "answer": false, "facts": ["The caracal can leap higher than 12 feet in the air.", "Javier Sotomayor is the current men's high jump record holder with a jump of 2.45 m (8 ft 1\u20444 in)."], "decomposition": ["How high was Javier Sotomayor's highest jump?", "How high are caracals known to jump?", "Is #1 greater than #2?"], "evidence": [[[["Javier Sotomayor-11"]], [["Caracal-24"]], ["operation"]], [[["Javier Sotomayor-1"]], [["Caracal-2"]], ["operation"]], [[["Javier Sotomayor-1"]], [["Caracal-2"]], ["operation"]]], "golden_sentence": [[""], ["The powerful hind legs allow it to leap more than 3\u00a0m (10\u00a0ft) in the air to catch birds on the wing."]]}, {"qid": "d5503bbb8a0e5def8915", "term": "Karaoke", "description": "form of entertainment involving singing to recorded music", "question": "Were karaoke and the turtle power tiller patented in the same country?", "answer": true, "facts": ["Roberto L. del Rosario holds the patent for the karaoke system", "del Rosario is Filipino", "Magdalena Smith Villaruz patented the turtle power tiller", "Villaruz is Filipino "], "decomposition": ["Who is the patent holder of the karaoke system?", "Which country is #1 from?", "Who patented the turtle power tiller?", "Which country is #3 from?", "Is #2 the same as #4?"], "evidence": [[[["Karaoke-8"]], [["Roberto del Rosario-1"]], [["Magdalena Villaruz-2"]], [["Magdalena Villaruz-1"]], ["operation"]], [[["Karaoke-5"], "no_evidence"], [["Daisuke Inoue-3"]], [["Magdalena Villaruz-1"]], [["Magdalena Villaruz-1"]], ["operation"]], [[["Roberto del Rosario-2"]], [["Philippines-1", "Roberto del Rosario-1"]], [["Magdalena Villaruz-1"]], [["Magdalena Villaruz-1"]], ["operation"]]], "golden_sentence": [["The patent holder of the karaoke machine is Roberto del Rosario."], [""], [""], ["Magdalena Smith Villaruz (born 1934) is an entrepreneur and inventor from the Philippines."]]}, {"qid": "be92e0dfbf4cf73a67a5", "term": "Guru", "description": "A \"teacher, guide, expert, or master\" in Sanskrit", "question": "Is Kim Kardashian a guru?", "answer": false, "facts": ["A guru is a teacher or guide, particularly a spiritual one.", "Kim Kardashian is a socialite and a model."], "decomposition": ["What makes someone a guru?", "Does Kim Kardashian satisfy all the conditions of #1?"], "evidence": [[[["Guru-1"]], [["Kim Kardashian-1", "Kim Kardashian-14", "Kim Kardashian-4"]]], [[["Guru-1"]], ["operation"]], [[["Guru-1"]], [["Kim Kardashian-1"], "no_evidence", "operation"]]], "golden_sentence": [["In pan-Indian traditions, guru is more than a teacher, in Sanskrit guru means the one who dispels the darkness and takes towards light, traditionally a reverential figure to the student, with the guru serving as a \"counselor, who helps mold values, shares experiential knowledge as much as literal knowledge, an exemplar in life, an inspirational source and who helps in the spiritual evolution of a student\"."], ["", "", ""]]}, {"qid": "d4379850fde69fbe236b", "term": "Conducting", "description": "Directing a musical performance by way of visible gestures", "question": "Do solo pianists require a conductor?", "answer": false, "facts": ["Conductors direct a group of musicians by ensuring they all keep the same beat and place in the music.", "A solo pianist performing alone can keep their own pace."], "decomposition": ["What is the minimum number of music performers that would need a conductor?", "Is #1 less than or equal to the number in a solo performance?"], "evidence": [[[["Conducting-32"]], [["Conducting-32"]]], [[["Conducting-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Conducting-1"], "no_evidence"], [["Solo performance-1"], "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "e0ed48935f66cc33a2e2", "term": "Shaggy (musician)", "description": "Reggae singer and former U.S. Marine", "question": "Would Shaggy and Redenbacher popcorn founder both raise hand during first name roll call?", "answer": true, "facts": ["Roll call is when teachers call the names of students and they raise their hand to show they are present.", "The founder of Redenbacher popcorn was Orville Redenbacher.", "Reggae musician Shaggy was born Orville Richard Burrell."], "decomposition": ["What is the first name of the person who founded Redenbacher popcorn?", "What is the first name of Reggae musician Shaggy?", "Is #1 the same as #2?"], "evidence": [[[["Orville Redenbacher-1"]], [["Shaggy (musician)-1"]], ["operation"]], [[["Orville Redenbacher-1"]], [["Shaggy (musician)-1"]], ["operation"]], [[["Orville Redenbacher-1"]], [["Shaggy (musician)-1"]], ["operation"]]], "golden_sentence": [["Orville Clarence Redenbacher (July 16, 1907 \u2013 September 19, 1995) was an American businessman most often associated with the brand of popcorn that bears his name that is now owned by ConAgra."], ["Orville Richard Burrell CD (born October 22, 1968), better known by his stage name Shaggy, is a Jamaican reggae musician, singer, DJ, and actor who scored hits with the songs \"It Wasn't Me\", \"Boombastic\", \"In The Summertime\", \"Oh Carolina\", and \"Angel\"."]]}, {"qid": "abd6a5ea7b9252ff08b7", "term": "Moustache", "description": "Facial hair grown on the upper lip", "question": "Has a baby ever had a moustache?", "answer": true, "facts": ["When babies are in the womb, many have hair known as lanugo.", "Lanugo is unpigmented, downy hair that is sometimes found on the body of fetuses and babies.", "Lanugo can grow anywhere on a baby's body but is usually shed before the baby is born and is reabsorbed by the mother."], "decomposition": ["What kind of hair does a baby have when it is in the womb?", "Can #1 grow anywhere on the body?"], "evidence": [[[["Lanugo-1"]], [["Lanugo-1"]]], [[["Lanugo-1"]], ["no_evidence", "operation"]], [[["Lanugo-1"]], [["Lanugo-1"], "no_evidence"]]], "golden_sentence": [["Lanugo is very thin, soft, usually unpigmented, downy hair that is sometimes found on the body of a fetal or new-born human."], [""]]}, {"qid": "4bd675cad2b0e9499fb4", "term": "Metallica", "description": "American heavy metal band", "question": "Did the original lead guitarist of Metallica fail after parting from the band?", "answer": false, "facts": ["Metallica's original lead guitarist was Dave Mustaine.", "Dave Mustaine was fired from Metallica in 1983.", "Dave Mustaine formed the band Megadeth in 1983 and is the lead vocalist.", "Megadeth has sold over 38 million records worldwide."], "decomposition": ["Who was the original lead guitarist of Metallica?", "What band did #1 start after leaving Metallica?", "Is #2 an unsuccessful band?"], "evidence": [[[["Metallica-6"]], [["Dave Mustaine-1"]], [["Megadeth-4"], "operation"]], [[["Metallica-1"]], [["Megadeth-1"]], [["Megadeth-88"], "operation"]], [[["Dave Mustaine-1"]], [["Dave Mustaine-1"]], [["Megadeth-4"]]]], "golden_sentence": [["The song generated word of mouth and the band played its first live performance on March 14, 1982, at Radio City in Anaheim, California, with newly recruited bassist Ron McGovney."], ["He is best known as the co-founder, lead vocalist, guitarist and primary songwriter of the American heavy metal band Megadeth, as well as the original lead guitarist of the heavy metal band Metallica."], [""]]}, {"qid": "56abb7d184a142ba643e", "term": "John Muir", "description": "Scottish-born American naturalist and author", "question": "Would John Muir not likely have a vitamin D deficiency?", "answer": true, "facts": ["John Muir frequently spent time exploring various places in nature.", "Spending time in nature increases your exposure to sunlight.", "Skin exposure to sunlight increases vitamin D levels in the body."], "decomposition": ["What is the most common cause of vitamin D deficiency?", "What was the nature of John Muir's life's work?", "Does #2 ensure that he does not experience #1?"], "evidence": [[[["Vitamin D-17"]], [["John Muir-2"]], [["John Muir-2"], "operation"]], [[["Vitamin D-13"]], [["John Muir-52"]], ["operation"]], [[["Vitamin D deficiency-1"]], [["John Muir-1"]], ["operation"]]], "golden_sentence": [["Vitamin D deficiency remains the main cause of rickets among young infants in most countries because breast milk is low in vitamin D and social customs and climatic conditions can prevent adequate sun exposure."], ["In his later life, Muir devoted most of his time to the preservation of the Western forests."], [""]]}, {"qid": "a948e21a67647b8314ea", "term": "Monogamy", "description": "Relationship form where each individual has only one partner during their lifetime or at any one time", "question": "Did Thomas Greenhill's parents violate the concept of monogamy?", "answer": false, "facts": ["Thomas Greenhill was a surgeon born to William and Elizabeth Greenhill.", "William and Elizabeth Greenhill had 39 children.", "Monogamy is a committed relationship between two people where usually they remain together for life.", "Thomas Greenhill was the last of his parents 39 children and was born shortly after his father died."], "decomposition": ["Who was Thomas Greenhill's father?", "How many wives did #1 marry in his lifetime?", "Is #2 greater than one?"], "evidence": [[[["Thomas Greenhill (surgeon)-4"]], [["Thomas Greenhill (surgeon)-5"]], ["operation"]], [[["Thomas Greenhill (surgeon)-4"]], [["Thomas Greenhill (surgeon)-7"]], ["operation"]], [[["Thomas Greenhill (surgeon)-1"]], [["William Greenhill-5"], "no_evidence"], ["operation"]]], "golden_sentence": [["Thomas Greenhill was the last of 39 children by his parents Elizabeth and William Greenhill."], [""]]}, {"qid": "fdd46ef7dd69af219e62", "term": "Bull shark", "description": "Species of fish", "question": "Is the bull shark more bull than shark?", "answer": false, "facts": ["The bull shark is a fish species that lives in warm shallow waters along coasts and rivers.", "Bull sharks feed on bony fish and other smaller sharks.", "A bull is an adult male mammal that lives on land.", "Bulls feed on plants located on land."], "decomposition": ["What is the main diet of bulls and where do they find their food?", "What is the main diet of sharks and where do they find their food?", "What is the main diet of bull sharks and where do they find their food?", "Is #3 more similar to #1 than to #2?"], "evidence": [[[["Cattle feeding-1"], "no_evidence"], [["Fish jaw-31"], "no_evidence"], [["Bull shark-23"]], ["operation"]], [[["Cattle-43"]], [["Shark-59"]], [["Bull shark-21"]], ["operation"]], [[["Bull-1", "Cattle-19"]], [["Shark-59", "Shark-60", "Shark-62"]], [["Bull shark-21"]], ["operation"]]], "golden_sentence": [["Cattle raised on a primarily foraged diet are termed grass-fed or pasture-raised; for example meat or milk may be called grass-fed beef or pasture-raised dairy."], ["Tooth shape depends on the shark's diet: those that feed on mollusks and crustaceans have dense and flattened teeth used for crushing, those that feed on fish have needle-like teeth for gripping, and those that feed on larger prey such as mammals have pointed lower teeth for gripping and triangular upper teeth with serrated edges for cutting."], ["As part of their survival mechanism, bull sharks will regurgitate the food in their stomachs in order to escape from a predator."]]}, {"qid": "85fe96a60e2cd3e1db2d", "term": "Liberty Bell", "description": "bell that serves as a symbol of American independence and liberty", "question": "Will a Holstein cow and the Liberty Bell balance out a giant scale?", "answer": false, "facts": ["The Liberty Bell weighs 2,080 pounds.", "A mature Holstein cow weighs around 1,500 pounds."], "decomposition": ["What is the average weight of a mature Holstein cow?", "What is the weight of the Liberty Bell?", "Is #1 closely the same as #2?"], "evidence": [[[["Holstein Friesian cattle-7"]], [["Liberty Bell-27"]], ["operation"]], [[["Holstein Friesian cattle-7"]], [["Liberty Bell-27"], "no_evidence"], ["operation"]], [[["Holstein Friesian cattle-7"]], [["Liberty Bell-36"]], ["operation"]]], "golden_sentence": [["A mature Holstein cow typically weighs 680\u2013770\u00a0kg (1500\u20131700\u00a0lb), and stands 145\u2013165\u00a0cm (58\u201365\u00a0in) tall at the shoulder."], ["(Its weight was reported as 2,080\u00a0lb (940\u00a0kg) in 1904.)"]]}, {"qid": "59b6401abbca792d2f5c", "term": "Citrus", "description": "genus of fruit-bearing plants (source of fruit such as lemons and oranges)", "question": "Can citrus grow in Ulaanbaatar?", "answer": false, "facts": ["Citrus can withstand short periods down to as cold as \u221210 \u00b0C (14 \u00b0F), but realistically temperatures not falling below \u22122 \u00b0C (28 \u00b0F) are required for successful cultivation.", "Ulaanbaatar has an average annual temperature of \u22120.4 \u00b0C or 31.3 \u00b0F."], "decomposition": ["What climates are suitable for growing citrus?", "What is the climate of Ulaanbaatar?", "Is #2 similar to #1?"], "evidence": [[[["Citrus-34"]], [["Ulaanbaatar-39"]], [["Citrus-34"]]], [[["Citrus-26", "Citrus-31"]], [["Ulaanbaatar-39"]], ["operation"]], [[["Citrus-31"]], [["Ulaanbaatar-40"]], ["operation"]]], "golden_sentence": [["Consistent climate, sufficient sunlight, and proper watering are crucial if the trees are to thrive and produce fruit."], ["citation needed] Owing to its high elevation, its relatively high latitude, its location hundreds of kilometres from any coast, and the effects of the Siberian anticyclone, Ulaanbaatar is the coldest national capital in the world, with a monsoon-influenced, cold semi-arid climate (K\u00f6ppen BSk, USDA Plant Hardiness Zone 3b) that closely borders a subarctic climate (Dwc) and a warm-summer humid continental climate (Dwb)."], [""]]}, {"qid": "ae8b7dd28ebef8c07779", "term": "Snow leopard", "description": "species of mammal", "question": "Can a snow leopard swim?", "answer": true, "facts": ["except for giraffes and apes, all four legged mammals can swim", "a snow leopard is a mammal", "snow leopards have four legs"], "decomposition": ["Is a snow leopard a four legged mammal?"], "evidence": [[[["Cat-1", "Snow leopard-1"], "no_evidence"]], [["no_evidence", "operation"]], [[["Felidae-1", "Quadrupedalism-1", "Snow leopard-1"]]]], "golden_sentence": [["", "The snow leopard (Panthera uncia), also known as the ounce, is a large cat native to the mountain ranges of Central and South Asia."]]}, {"qid": "9b5249e314f785a34749", "term": "LinkedIn", "description": "Social networking website for people in professional occupations", "question": "Are LinkedIn and LeafedIn related companies?", "answer": false, "facts": ["LinkedIn successfully sued LeafedIn for their choice of name.", "LeafedIn changed their company name to LeafedOut"], "decomposition": ["Who owns LinkedIn?", "Who owns LeafedIn?", "IS #1 the same as #2?"], "evidence": [[[["LinkedIn-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["LinkedIn-10"]], ["no_evidence"], ["operation"]], [[["LinkedIn-1"]], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Since December 2016 it has been a wholly owned subsidiary of Microsoft."]]}, {"qid": "6f256544d1737dffa6b1", "term": "Dance", "description": "A performing art consisting of movement of the body", "question": "Is waltz less injurious than slam dance?", "answer": true, "facts": ["The waltz is a rhythmic dance performed in triple time by a couple.", "A slam dance is a type of dance in which leaping dancers collide against each other."], "decomposition": ["What kinds of body movements are involved in waltz?", "What kinds of body movements are involved in slam dance?", "Is #1 less likely to cause injuries than #2?"], "evidence": [[[["Waltz-2"], "no_evidence"], [["Moshing-1"]], [["Moshing-4"], "operation"]], [[["Waltz-5"]], [["Moshing-1"]], [["Moshing-4"], "operation"]], [[["Ballroom dance-28", "Waltz-1"]], [["Moshing-1"]], [["Moshing-4"], "operation"]]], "golden_sentence": [["Play media There are many references to a sliding or gliding dance that would evolve into the waltz that date from 16th century Europe, including the representations of the printmaker Hans Sebald Beham."], ["It is intended to be energetic and full of body contact."], [""]]}, {"qid": "2316123e895ad4624abb", "term": "Squid", "description": "order of molluscs", "question": "Is one blast from double-barreled shotgun likely to kill all squid brains?", "answer": false, "facts": ["A double-barreled shotgun fires two rounds in one single blast.", "Squids have three brains."], "decomposition": ["How many rounds are fired in one blast from a double-barreled shotgun?", "How many brains do squid have?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Double-barreled shotgun-1"]], [["Squid-20"], "no_evidence"], ["no_evidence", "operation"]], [[["Double-barreled shotgun-7"], "no_evidence"], [["Brain-15"]], ["operation"]], [[["Double-barreled shotgun-1"]], [["Squid-20"], "no_evidence"], ["operation"]]], "golden_sentence": [["A double-barreled shotgun is a shotgun with two parallel barrels, allowing two shots to be fired in quick succession."], [""]]}, {"qid": "5ac6dbceb527798c0920", "term": "Viscosity", "description": "Resistance of a fluid to shear deformation", "question": "Is viscosity unimportant in making jello shots?", "answer": false, "facts": ["Jello shots are a combination of alcohol and jello to create an edible intoxicant. ", "If the liquid for the Jello shots has too low a viscosity, it will not become a semi-solid. "], "decomposition": ["What are the ingredients used in making jello shots?", "Which properties of liquids among #1 are important for good results?", "Is viscosity not included in #2?"], "evidence": [[[["Jell-O-30"]], [["Mixed drink-1"], "no_evidence"], [["Viscosity-1"], "operation"]], [[["Jell-O-30"]], [["Jell-O-30"], "no_evidence"], ["no_evidence"]], [[["Jell-O-30"]], [["Jell-O-29"]], ["operation"]]], "golden_sentence": [["It is important to adjust the proportions of alcohol and cold water to ensure that the mixture sets when experimenting with various liquors."], [""], [""]]}, {"qid": "1a176ed1873a25172249", "term": "Earth Day", "description": "Annual event on 22 April", "question": "Is Earth Day celebrated in summer?", "answer": false, "facts": ["Earth Day is celebrated on April 22.", "Summer runs from about June 20 to September 20."], "decomposition": ["What is summer?", "What is the date of Earth day?", "Is #2 in #1?"], "evidence": [[[["Summer-2"]], [["Earth Day-30"]], ["operation"]], [[["Summer-5"]], [["Earth Day-1"]], ["operation"]], [[["Summer-2"]], [["Earth Day-1"]], ["operation"]]], "golden_sentence": [["From an astronomical view, the equinoxes and solstices would be the middle of the respective seasons, but sometimes astronomical summer is defined as starting at the solstice, the time of maximal insolation, often identified with the 21st day of June or December."], ["The first Earth Day proclamation was issued by San Francisco Mayor Joseph Alioto on March 21, 1970."]]}, {"qid": "31bcc3a0bf8e952ccd43", "term": "Smooth jazz", "description": "category of music", "question": "Are you likely to hear Rammstein playing in smooth jazz clubs?", "answer": false, "facts": ["Smooth jazz is a combination of jazz with easy-listening pop music and lightweight R&B.", "Smooth jazz began in the United States in the 1970s.", "Rammstein is a German band that plays heavy metal music."], "decomposition": ["What kinds of music is played at a smooth jazz club?", "What kinds of music does Rammstein play?", "Is there an overlap between #1 and #2?"], "evidence": [[[["Jazz club-2"]], [["Rammstein-55"]], ["operation"]], [[["Smooth jazz-2"]], [["Rammstein-45"]], ["operation"]], [[["Jazz club-2"], "operation"], ["no_evidence"], ["no_evidence"]]], "golden_sentence": [["They can be rather small compared to other music venues, such as rock music clubs, reflecting the intimate atmosphere of jazz shows and long-term decline in popular interest in jazz."], ["The New York Times described Rammstein's music as a \"powerful strain of brutally intense rock... bringing gale-force music and spectacular theatrics together\"."]]}, {"qid": "a09aecb1ce6a5dfca3ba", "term": "Moon Jae-in", "description": "President of South Korea", "question": "Did Moon Jae-in earn the Abitur as a teenager?", "answer": false, "facts": ["Moon Jae-in attended high school in South Korea.", "The Abitur is a qualification granted by university-preparatory schools in Germany, Lithuania, and Estonia."], "decomposition": ["Which countries' schools award Abitur to their students?", "Which country did Moon Jae-in school in as a teenager?", "Is #2 included in #1?"], "evidence": [[[["Abitur-1"]], [["Moon Jae-in-7"]], ["operation"]], [[["Abitur-1"]], [["Moon Jae-in-7"]], ["operation"]], [[["Abitur-1"]], [["Kyungnam High School-2", "Moon Jae-in-7"]], ["operation"]]], "golden_sentence": [["Abitur (German: [abi\u02c8tu\u02d0\u0250\u032f]) is a qualification granted by university-preparatory schools in Germany, Lithuania, and Estonia."], [""]]}, {"qid": "a0b4a5534afb0e356c7e", "term": "French toast", "description": "bread soaked in beaten eggs and then fried", "question": "Can French Toast hypothetically kill a Lannister?", "answer": true, "facts": ["The Lannister's are a wealthy family in the Song of Ice and Fire book series.", "French Toast is made from bread dipped in an egg batter.", "Salmonella is a deadly bacteria that can be carried by spoiled eggs."], "decomposition": ["What are the ingredients in French toast?", "Which things in #1 can spoil?", "What diseases can be carried by spoiled #2?", "What species are susceptible to #3?", "Are the Lannisters members of a species listed in #4?"], "evidence": [[[["French toast-1"]], [["Egg as food-35"]], [["Salmonella enterica-3"]], [["Salmonella-18"]], [["World of A Song of Ice and Fire-44"], "operation"]], [[["French toast-1"]], [["French toast-1"]], [["Dairy-58"], "no_evidence"], [["Raw milk-14"], "no_evidence"], [["World of A Song of Ice and Fire-44"], "no_evidence", "operation"]], [[["French toast-7"]], [["Milk-118"]], [["Foodborne illness-6"]], [["Foodborne illness-31"]], [["Game of Thrones-10"], "operation"]]], "golden_sentence": [["French toast is a dish made of sliced bread soaked in eggs and typically milk, then fried."], ["Refrigeration also preserves the taste and texture, however, intact eggs (unwashed and unbroken) may be left unrefrigerated for several months without spoiling."], [""], [""], [""]]}, {"qid": "6fade5ad4988de35da0f", "term": "Agnosticism", "description": "view that the existence of any deity is unknown or unknowable", "question": "Can a believer in agnosticism become pope?", "answer": false, "facts": ["The pope is the head of the Catholic Church.", "The pope is required to be a devout follower of Christ.", "Popes preach about the teachings of Christ and the belief in one god.", "Agnostics do not acknowledge the existence of god and instead state that no one knows if there is a god or not."], "decomposition": ["What do agnostics believe about the existence of God?", "Which religious group does a pope head?", "What are the beliefs held by #2 concerning God's existence?", "Is #1 in agreement with #3?"], "evidence": [[[["Agnosticism-1"]], [["Pontifex maximus-41"]], [["Catholic Church-3"]], ["operation"]], [[["Agnosticism-1"]], [["Pope-1"]], [["Catholic Church-38"]], ["operation"]], [[["Agnosticism-1"]], [["Pope-37"]], [["Pope-74"]], ["operation"]]], "golden_sentence": [["Agnosticism is the view that the existence of God, of the divine or the supernatural is unknown or unknowable."], ["The word pontifex, Latin for \"pontiff\", was used in ancient Rome to designate a member of the College of Pontiffs."], [""]]}, {"qid": "6b1665613cbb7185bd1a", "term": "Ronda Rousey", "description": "American professional wrestler, actress, author, mixed martial artist and judoka", "question": "Will Ronda Rousey hypothetically defeat X-Men's Colossus in a fight?", "answer": false, "facts": ["Ronda Rousey is a mixed martial artist and wrestler.", "Ronda Rousey relies on striking moves and submission tactics to dominate her opponents.", "X-Men's Colossus has the ability to change his appearance.", "Colossus's mutation allows him to create an organic steel layer, that acts as an impenetrable external shell."], "decomposition": ["What type of profession is Ronda Rousey in?", "What moves do #1 use to beat their opponents?", "What special ability does X-men have?", "Can someone with #2 easily beat someone with #3?"], "evidence": [[[["Ronda Rousey-1"]], [["Ronda Rousey-43"]], [["X-Men-2"]], ["no_evidence"]], [[["Ronda Rousey-1"]], [["Professional wrestling-1"]], [["Colossus (comics)-55"]], ["operation"]], [[["Ronda Rousey-1"]], [["Grappling position-5"], "no_evidence"], [["Colossus (comics)-2"]], ["operation"]]], "golden_sentence": [["Ronda Jean Rousey (/\u02c8ra\u028azi/; born February 1, 1987) is an American professional wrestler, actress, author, and former mixed martial artist and judoka who is signed to WWE as a wrestler since 2018."], ["A decorated judoka, Rousey typically grounds an opponent with hip throws and sweeps, then seeks to finish with strikes or submissions."], ["Most of the X-Men are mutants, a subspecies of humans who are born with superhuman abilities activated by the \"X-Gene\"."]]}, {"qid": "734420dd8277fb5d4da5", "term": "Brewing", "description": "production of beer", "question": "Can brewing occur in a prison environment?", "answer": true, "facts": ["Pruno is a product made almost exclusively in prisons.", "Pruno is a fermented beverage that is made of fruit to produce alcohol."], "decomposition": ["What is Pruno?", "Where is #1 made?", "Is #2 same as prison?"], "evidence": [[[["Pruno-1"]], [["Pruno-1"]], ["operation"]], [[["Pruno-1"]], [["Pruno-1"]], [["Pruno-1"]]], [[["Pruno-1"]], [["Pruno-1"]], ["operation"]]], "golden_sentence": [["Pruno, or prison wine, is an alcoholic beverage variously made from apples, oranges, fruit cocktail, fruit juices, hard candy, sugar, high fructose syrup, and possibly other ingredients, including crumbled bread."], [""]]}, {"qid": "895f44aaa137fdfea797", "term": "Wheelchair", "description": "chair with wheels, used by people for whom walking is difficult or impossible due to illness, injury, or disability", "question": "Do American wheelchair users know what the ADA is?", "answer": true, "facts": ["The ADA is the Americans with Disabilities Act.", "Non-ADA compliant businesses include those without wheelchair access points."], "decomposition": ["Which areas of interest are affected by the ADA?", "Is any of #1 of particular interest to wheelchair users in America?"], "evidence": [[[["Americans with Disabilities Act of 1990-1"]], [["Disability-4"], "operation"]], [[["American Association of People with Disabilities-1"]], [["American Association of People with Disabilities-1"]]], [[["Americans with Disabilities Act of 1990-6"]], [["Americans with Disabilities Act of 1990-6"]]]], "golden_sentence": [[""], ["For the purposes of the Americans with Disabilities Act of 1990, the US Equal Employment Opportunity Commission regulations provide a list of conditions that should easily be concluded to be disabilities: deafness, blindness, an intellectual disability (formerly termed mental retardation), partially or completely missing limbs or mobility impairments requiring the use of a wheelchair, autism, cancer, cerebral palsy, diabetes, epilepsy, HIV/AIDS, multiple sclerosis, muscular dystrophy, major depressive disorder, bipolar disorder, post-traumatic stress disorder, obsessive compulsive disorder, and schizophrenia."]]}, {"qid": "02fda8270cb121d40ffb", "term": "Family of Barack Obama", "description": "List of members of the family of Barack Obama", "question": "Does Lupita Nyongo have citizenship in paternal Family of Barack Obama's origin country?", "answer": true, "facts": ["Actress Lupita Nyongo has dual citizenship in Kenya and Mexico.", "Barack Obama's father was born in Nyang\u2019oma Kogelo, Rachuonyo District, Kenya Colony.", "Barack Obama's father was a Kenyan of the Luo tribe."], "decomposition": ["In what nations does Lupita Nyongo have citizenship?", "Who is Barack Obama's father?", "In what nations does #2 have citizenship?", "Is at least one country in #1 also found in #3?"], "evidence": [[[["Lupita Nyong'o-7"]], [["Barack Obama-6"]], [["Barack Obama-6"]], ["operation"]], [[["Lupita Nyong'o-7"]], [["Barack Obama Sr.-1"]], [["Barack Obama-6"]], ["operation"]], [[["Lupita Nyong'o-7"]], [["Barack Obama Sr.-1"]], [["Barack Obama Sr.-3"]], ["operation"]]], "golden_sentence": [[""], ["His father, Barack Obama Sr. (1936\u20131982), was a married Luo Kenyan from Nyang'oma Kogelo."], [""]]}, {"qid": "4daad94a18a8550a3000", "term": "Equator", "description": "Intersection of a sphere's surface with the plane perpendicular to the sphere's axis of rotation and midway between the poles", "question": "Is most coffee produced South of the Equator?", "answer": true, "facts": ["The countries with the highest coffee production are in South America.", "Almost all of South America is in the Southern Hemisphere."], "decomposition": ["Which countries produce the most coffee?", "Which hemisphere are most of #1 located?", "Is #2 south of the equator?"], "evidence": [[[["Coffee-45"], "no_evidence"], ["no_evidence"], [["Brazil-43"], "no_evidence", "operation"]], [[["Coffee-5"], "no_evidence"], [["Southern Hemisphere-9"]], [["Southern Hemisphere-1"], "operation"]], [[["Coffee-5"]], [["Southern Hemisphere-9"]], [["Southern Hemisphere-9"]]]], "golden_sentence": [[""], [""]]}, {"qid": "37633b531ceb891076f2", "term": "New Mexico", "description": "U.S. state in the United States", "question": "Is the largest city in New Mexico also known as Yoot\u00f3?", "answer": false, "facts": ["Yoot\u00f3 stands for Bead Water Place.", "The area Santa Fe occupied was known by the Navajo people as Yoot\u00f3.", "The largest city in New Mexico is Albuquerque."], "decomposition": ["What is the largest city in New Mexico?", "Is #1 known as Yoot\u00f3?"], "evidence": [[[["Albuquerque, New Mexico-1"]], ["operation"]], [[["Albuquerque, New Mexico-1"]], ["operation"]], [["no_evidence"], [["Santa Fe, New Mexico-1"]]]], "golden_sentence": [["Since the city's founding it has continued to be included on travel and trade routes including Santa Fe Railway (ATSF), Route 66, Interstate 25, Interstate 40, and the Albuquerque International Sunport."]]}, {"qid": "737fe54d5fb2879062ad", "term": "Tongue", "description": "mouth organ that tastes and facilitates speech", "question": "Is the tongue part of a creature's head?", "answer": true, "facts": ["A creature's tongue is inside its mouth.", "A creature's mouth is part of its head."], "decomposition": ["In what body part is the tongue located?", "Is #1 located in the head?"], "evidence": [[[["Tongue-1"]], ["no_evidence", "operation"]], [[["Tongue-5"]], [["Tongue-5"]]], [[["Tongue-1"]], [["Head-1"], "operation"]]], "golden_sentence": [["The tongue is a muscular organ in the mouth of most vertebrates that manipulates food for mastication and is used in the act of swallowing."]]}, {"qid": "5dc95e4ae024b6f6c2e2", "term": "Rush Limbaugh", "description": "American radio talk show host, commentator, author, and television personality", "question": "Does Coast to Coast AM have more longevity than the Rush Limbaugh show?", "answer": true, "facts": ["As of 2020, The Rush Limbaugh Show has been on the airwaves since 1988.", "As of 2020, Coast to Coast AM has been on the airwaves since 1984."], "decomposition": ["When did the Rush Limbaugh show first air?", "When did Coast to Coast AM first air?", "Is #2 before #1?"], "evidence": [[[["The Rush Limbaugh Show-1"]], [["Coast to Coast AM-2"]], ["operation"]], [[["The Rush Limbaugh Show-1"]], [["Coast to Coast AM-2"]], ["operation"]], [[["Rush Limbaugh-1"]], [["Coast to Coast AM-2"]], ["operation"]]], "golden_sentence": [["Since its nationally syndicated premiere in 1988, The Rush Limbaugh Show has become the highest-rated talk radio show in the United States."], ["In 1988, Bell and Alan Corberth renamed the show Coast to Coast AM and moved its studios from the Plaza Hotel in Las Vegas to Bell's home in Pahrump."]]}, {"qid": "96994da649cac75133bd", "term": "E.T. the Extra-Terrestrial", "description": "1982 American science fiction film directed by Steven Spielberg", "question": "Is the E.T. the Extra-Terrestrial Atari Landfill story an urban legend?", "answer": true, "facts": ["An urban legend is a humorous or horrifying story based on hearsay that is circulated as true.", "E.T. the Extra Terrestrial was panned as one of the worst video games ever made.", "A widespread story stated that thousands of copies of E.T. the Extra Terrestrial video game were buried in a landfill", "A former Atari manager stated that 728,000 Atari games were in fact buried in a landfill.", "The Atari landfill was dug up and nearly 900 games were recovered, but there was only one copy of E.T. included."], "decomposition": ["Was what the widespread landfill rumor concerning copies of E.T. the Extra Terrestial video game?", "When the landfill was dug up, were the claims in #1 found to be false?", "Considering #2, does the rumor fit the description of an urban legend?"], "evidence": [[[["Atari video game burial-2"]], [["Atari video game burial-17"]], [["Legend-15"]]], [[["Atari video game burial-1"]], [["Atari video game burial-18"]], ["operation"]], [[["E.T. the Extra-Terrestrial (video game)-3"]], ["operation"], [["Urban legend-1"], "operation"]]], "golden_sentence": [[""], ["Contrary to the urban legend that claims millions of cartridges were buried there, Heller stated that only 728,000 cartridges were buried."], [""]]}, {"qid": "ada4cb39311da00e60a1", "term": "Argon", "description": "Chemical element with atomic number 18", "question": "Can you chew argon?", "answer": false, "facts": ["Chewing is the act of breaking down solid objects with your teeth", "Under normal conditions, argon exists as a gas"], "decomposition": ["What kind of substance is argon?", "Do humans usually chew #1?"], "evidence": [[[["Argon-1"]], [["Chewing-1"], "operation"]], [[["Argon-1"]], [["Chewing-1"]]], [[["Argon-1"]], ["operation"]]], "golden_sentence": [["It is in group 18 of the periodic table and is a noble gas."], [""]]}, {"qid": "5d3465567dd1a4d369a7", "term": "Alice's Adventures in Wonderland", "description": "book by Lewis Carroll", "question": "Would a Jehovah's witness approve of Alice's Adventures in Wonderland?", "answer": false, "facts": ["Jehovah's Witness is a religious group that strictly forbids tobacco and smoking.", "A prominent character in Alice's Adventures in Wonderland, the caterpillar, blows rings of smoke from a large pipe."], "decomposition": ["What are Jehovah's Witnesses?", "What items do #1's forbid?", "In Alice's Adventures in Wonderland, what is the caterpillar seen doing with a pipe?", "Is #2 different from #3?"], "evidence": [[[["Jehovah's Witnesses-1"]], [["Religious views on smoking-6"]], [["Caterpillar (Alice's Adventures in Wonderland)-6"]], ["operation"]], [[["Jehovah's Witnesses-1"]], [["Jehovah's Witnesses-36"]], [["Alice's Adventures in Wonderland-13"]], ["operation"]], [[["Jehovah's Witnesses-1"]], [["Jehovah's Witnesses practices-27"], "no_evidence"], [["Alice's Adventures in Wonderland-13"]], ["no_evidence", "operation"]]], "golden_sentence": [["Jehovah's Witnesses is a millenarian restorationist Christian denomination with nontrinitarian beliefs distinct from mainstream Christianity."], ["The virtue of temperance disposes us to avoid every kind of excess: the abuse of food, alcohol, tobacco, or medicine."], ["He blows smoke in Alice's face and when she needs assistance he ignores her."]]}, {"qid": "958f97d8dc79807a9f03", "term": "Santa Claus", "description": "Folkloric figure, said to deliver gifts to children on Christmas Eve", "question": "Does Santa Claus hypothetically give Joffrey Baratheon presents?", "answer": false, "facts": ["Santa Claus is a figure in folklore that gives good children presents and bad children coal.", "Joffrey Baratheon is a character in the Game of Thrones TV series.", "Joffrey is a young royal that has a man's tongue cut out.", "Joffrey shoots his crossbow into the flesh of innocent people for his amusement."], "decomposition": ["What kind of children would Santa Claus give presents to?", "What is Joffrey Baratheon's character like?", "Do #1 usually exhibit #2?"], "evidence": [[[["Santa Claus-1"]], [["Joffrey Baratheon-2"]], ["operation"]], [[["Santa Claus-1"]], [["Joffrey Baratheon-2"]], ["operation"]], [[["Santa Claus-3"]], [["Joffrey Baratheon-6"]], ["no_evidence"]]], "golden_sentence": [["Santa Claus, also known as Father Christmas, Saint Nicholas, Saint Nick, Kris Kringle, or simply Santa, is an imaginary figure originating in Western Christian culture who is said to bring gifts to the homes of well-behaved children on the night of Christmas Eve (24 December) or during the early morning hours of Christmas Day (25 December)."], ["He is characterized as a spoiled, sadistic bully and is a frequent abuser of Sansa Stark, to whom he becomes engaged in the first novel, and his uncle Tyrion, whom he enjoys ridiculing."]]}, {"qid": "e943dad3584917aff976", "term": "Lapidary", "description": "gemstone cutter", "question": "Does a lapidary work with items that are studied by geologists?", "answer": true, "facts": ["Some of the things geologists study include gemstones, minerals, and stone", "Lapidarists work with stone, minerals and gemstones"], "decomposition": ["What are the materials a lapidary works with?", "What do geologists study?", "Is any of #1 derived from #2?"], "evidence": [[[["Lapidary-1"]], [["Geologist-9"]], ["operation"]], [[["Lapidary-1"]], [["Geology-1"]], ["operation"]], [[["Lapidary-1"]], [["Geology-1"]], ["operation"]]], "golden_sentence": [["A lapidary (lapidarist, Latin: lapidarius) is an artist or artisan who forms stone, minerals, or gemstones into decorative items such as cabochons, engraved gems (including cameos), and faceted designs."], ["Planetary geology: the study of geology as it relates to other celestial bodies, namely planets and their moons."]]}, {"qid": "242ce6bf10e284f6e799", "term": "The Great Gatsby", "description": "1925 novel by F. Scott Fitzgerald", "question": "Was The Great Gatsby inspired by the novel 1984?", "answer": false, "facts": ["The Great Gatsby was published in 1925.", "The novel 1984 was published in 1949."], "decomposition": ["When was the Great Gatsby published?", "When was 1984 written?", "Is #2 before #1?"], "evidence": [[[["The Great Gatsby-3"]], [["Nineteen Eighty-Four-1"]], ["operation"]], [[["The Great Gatsby-1"]], [["Nineteen Eighty-Four-1"]], ["operation"]], [[["The Great Gatsby-1"]], [["Nineteen Eighty-Four-1"]], ["operation"]]], "golden_sentence": [["First published by Scribner's in April 1925, The Great Gatsby received mixed reviews and sold poorly."], [""]]}, {"qid": "ea9e3716211c457f8dc8", "term": "Tibia", "description": "larger of the two bones of the leg below the knee for vertebrates", "question": "Is the tibia required for floor exercises?", "answer": true, "facts": ["The tibia is a bone in the lower leg", "Floor exercises are a program in gymnastics competitions", "Gymnastics requires use of arms and legs, as well as other parts of the body"], "decomposition": ["What sport are floor exercises part of?  ", "What body parts does #1 require?", "What part of the body part is the tibia?", "Is #3 in #2?"], "evidence": [[[["Floor (gymnastics)-17"]], [["Leg-6"]], [["Tibia-1"]], ["operation"]], [[["Floor (gymnastics)-1"]], [["Gymnastics-1"]], [["Tibia-1"]], ["operation"]], [[["Floor (gymnastics)-1"]], [["Floor (gymnastics)-2"]], [["Tibia-1"]], ["operation"]]], "golden_sentence": [["Floor exercises is a category also in the rhythmic gymnastics, but it considers only the youngest gymnasts, up to 10 years old, who perform their routines freehand, which means without any apparatus (in contrary to the remaining five - rope, hoop, ball, clubs and ribbon)."], [""], ["The tibia /\u02c8t\u026abi\u0259/ (plural tibiae /\u02c8t\u026abii/ or tibias), also known as the shinbone or shankbone, is the larger, stronger, and anterior (frontal) of the two bones in the leg below the knee in vertebrates (the other being the fibula, behind and to the outside of the tibia), and it connects the knee with the ankle bones."]]}, {"qid": "21976c8512788f57cddf", "term": "Europa (moon)", "description": "The smallest of the four Galilean moons of Jupiter", "question": "Is Europa linked to Viennese waltzes?", "answer": true, "facts": ["Europa is a moon of Jupiter", "Europa played an important role in Stanley Kubrick's film 2001: A Space Odyssey", "The soundtrack to 2001: A Space Odyssey prominently featured The Blue Danube", "The Blue Danube is a famous Viennese waltz composed by Johan Strauss II"], "decomposition": ["Which moon of Jupiter played an important role in the film '2001: A Space Odyssey'?", "Is #1 Europa?", "Which soundtrack was prominently featured in the movie?", "Is #3 a Viennese waltz?", "Are #2 and #4 positive?"], "evidence": [[["no_evidence"], ["operation"], [["2001: A Space Odyssey (film)-2", "The Blue Danube-14"]], [["The Blue Danube-1"]], ["operation"]], [[["2001: A Space Odyssey (film)-7"], "no_evidence"], [["Europa (moon)-1"], "operation"], [["2001: A Space Odyssey (film)-2"]], [["The Blue Danube-1"]], ["operation"]], [[["2001: A Space Odyssey (film)-7"], "no_evidence"], ["no_evidence", "operation"], [["2001: A Space Odyssey (film)-2"]], [["Johann Strauss II-1", "The Blue Danube-1"]], ["no_evidence", "operation"]]], "golden_sentence": [["The soundtrack incorporates numerous works of classical music, among them Also sprach Zarathustra by Richard Strauss, \"The Blue Danube\" by Johann Strauss II, and works by Aram Khachaturian and Gy\u00f6rgy Ligeti.", ""], [""]]}, {"qid": "805860202fa8ca5b241e", "term": "Cholera", "description": "Bacterial infection of the small intestine", "question": "Is a platypus immune from cholera?", "answer": true, "facts": ["Cholera is a bacteria that damages the small intestines in humans.", "The intestines are part of the stomach of humans.", "A platypus does not have a stomach."], "decomposition": ["What parts of the body does Cholera damage?", "Does a platypus not have #1?"], "evidence": [[[["2016\u20132020 Yemen cholera outbreak-6"]], [["Platypus-4"], "no_evidence"]], [[["Vibrio cholerae-6"]], ["no_evidence"]], [[["Diseases and epidemics of the 19th century-10"]], ["no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "6a3a40414c29a117c781", "term": "Jerry Seinfeld", "description": "American comedian and actor", "question": "Did Jerry Seinfeld have reason to cheer in 1986?", "answer": true, "facts": ["Jerry Seinfeld is a fan of the New York Mets baseball team", "The New York Mets won a World Series title in 1986"], "decomposition": ["Do fans cheer if their team wins?", "Is Jerry Seinfeld a NY Mets fan?", "Did the NY Mets win the World Series in 1986?", "Is #1, #2 and #3 \"yes\"?"], "evidence": [[[["Cheering-20"]], [["Jerry Seinfeld-28"]], [["1986 World Series-4"]], ["operation"]], [[["Cheering-18"], "no_evidence"], [["Jerry Seinfeld-28"]], [["1986 World Series-1"]], ["operation"]], [[["Cheering-1"]], [["The Boyfriend (Seinfeld)-2"]], [["1986 World Series-1"]], ["operation"]]], "golden_sentence": [["If the losing team makes a play, and that teams fans chant for that, fans of the winning team will start chanting \"Scoreboard,\" indicating that even after the one play, the other team is losing."], ["A fan of the New York Mets, Seinfeld periodically calls Steve Somers' show on WFAN-AM, a sports talk radio station, as \"Jerry from Queens\"."], [""]]}, {"qid": "64c47a60fb9420b41e61", "term": "John the Baptist", "description": "1st-century Jewish preacher and later Christian saint", "question": "Would John the Baptist be invited to a hypothetical cephalophore reunion in heaven?", "answer": false, "facts": ["John the Baptist was a preacher that became a Catholic Saint.", "John the Baptist was beheaded by king Herod.", "A cephalophore is a Saint martyred by beheading, and is depicted in art as carrying their own head.", "Saint Denis was one of several beheaded saints that is said to have carried his own head and is depicted as such in art.", "John the Baptist did not carry his head, since it was on a plate owned by King Herod's stepdaughter."], "decomposition": ["What does one carry for one to be considered a cephalophore?", "Did John the Baptist carry #1?"], "evidence": [[[["Cephalophore-1"]], [["Cephalophore-4"], "operation"]], [[["Cephalophore-1"]], [["Cephalophore-5"], "operation"]], [[["Cephalophore-1"]], [["John the Baptist-188"], "no_evidence", "operation"]]], "golden_sentence": [["A cephalophore (from the Greek for \"head-carrier\") is a saint who is generally depicted carrying their own head."], [""]]}, {"qid": "6ac7643ce547b2ad2184", "term": "Common carp", "description": "Species of fish", "question": "Are common carp sensitive to their environments?", "answer": false, "facts": ["Common carp are a type of fish.", "Common carp are considered a destructive invasive species. ", "Common carp are tolerant of most conditions. ", "Common carp are able to survive frozen over ponds and low oxygenated waters. "], "decomposition": ["What kinds of environments are common carp native to?", "What kinds of environments have common carp expanded into?", "Do #1 and #2 include similar climate conditions?"], "evidence": [[[["Common carp-1"]], [["Common carp-10"]], ["no_evidence"]], [[["Common carp-1"]], [["Common carp-1"]], ["operation"]], [[["Carp-1"]], [["Common carp-16"]], ["operation"]]], "golden_sentence": [["The native wild populations are considered vulnerable to extinction by the International Union for Conservation of Nature (IUCN), but the species has also been domesticated and introduced (see aquaculture) into environments worldwide, and is often considered a destructive invasive species, being included in the list of the world's 100 worst invasive species."], [""]]}, {"qid": "ed3cd54c54b76ec7fae4", "term": "Douglas fir", "description": "species of tree", "question": "Have Douglas fir been used to fight wars?", "answer": true, "facts": ["Douglas fir are a type of tree.", "Douglas fir are used to make ships.", "The Minesweeper is a small warship made from Douglas fir.", "The Minesweeper was made of wood to reduce it's risk magnetic signature and likely hood of detonating mines."], "decomposition": ["What are the uses of Douglas Fir?", "Does #1 include vehicles of war?"], "evidence": [[[["Douglas fir-16", "Douglas fir-23"], "no_evidence"], ["operation"]], [["no_evidence"], [["War-1"], "no_evidence"]], [["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["The timber is used for joinery, veneer, flooring and construction due to its strength, hardness and durability.", ""]]}, {"qid": "3ef1869c26d09715dd80", "term": "Ludacris", "description": "American rapper and actor", "question": "Can you watch the Borgia's World of Wonders before Ludacris's Release Therapy finishes?", "answer": true, "facts": ["World of Wonders is an episode of the Showtime TV series The Borgias, with a run time of 49 minutes.", "Ludacris's 2006 album Release Therapy has a run time of 62 minutes."], "decomposition": ["What is the run time of  the Borgia's World of Wonders?", "What is the run time of  Ludacris's Release Therapy?", "Is #1 shorter than #2?"], "evidence": [[[["The Borgias (2011 TV series)-14"], "no_evidence"], [["Release Therapy-1"], "no_evidence"], ["operation"]], [[["The Borgias (2011 TV series)-1", "The Borgias (2011 TV series)-4"], "no_evidence"], [["Release Therapy-1"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [[""], ["Release Therapy is the sixth studio album by American hip hop recording artist Ludacris."]]}, {"qid": "6a8e31c1f4be7271fdae", "term": "Winter", "description": "one of the Earth's four temperate seasons, occurring between autumn and spring", "question": "Is winter solstice in Northern Hemisphere closer to July than in Southern Hemisphere? ", "answer": false, "facts": ["The winter solstice in the Northern Hemisphere happens in December.", "The winter solstice in the Southern Hemisphere happens in June."], "decomposition": ["When does the winter solstice occur in the Northern Hemisphere?", "When does the winter solstice occur in the Southern Hemisphere?", "How many days are in between #1 and July?", "How many days are between #2 and July?", "Is #4 greater than #3?"], "evidence": [[[["Winter solstice-2"]], [["Winter solstice-2"]], ["operation"], ["operation"], ["operation"]], [[["Winter solstice-2"]], [["Winter solstice-2"]], ["no_evidence", "operation"], ["no_evidence", "operation"], ["no_evidence", "operation"]], [[["Winter solstice-2"]], [["Winter solstice-2"]], ["no_evidence"], ["no_evidence"], ["operation"]]], "golden_sentence": [["The winter solstice occurs during the hemisphere's winter."], ["The winter solstice occurs during the hemisphere's winter."]]}, {"qid": "206458e560612d17b629", "term": "Traffic collision", "description": "occurs when a vehicle collides with another vehicle, pedestrian, animal, road debris, or other stationary obstruction, such as a tree, pole or building or drives off the road", "question": "Can a traffic collision make someone a millionaire?", "answer": true, "facts": ["Traffic collisions sometimes result in extremely expensive physical damage.", "Physical damage is compensated by insurance companies in the form of monetary payment.", "Million dollar verdicts are sometimes awarded for traffic collisions that result in major damage. "], "decomposition": ["What can kind of damage can traffic collisions cause?", "If #1 occurs, how would insurance companies react?", "Can #2 sometimes occur in a million dollar verdict?"], "evidence": [[[["Personal injury-1"], "no_evidence"], [["Personal injury-26"], "no_evidence"], [["Pain and suffering-5"], "no_evidence", "operation"]], [[["Traffic collision-1"]], [["Vehicle insurance in the United States-1"]], [["Traffic collision-84"]]], [[["Traffic collision-1"]], [["Vehicle insurance-1"]], ["no_evidence"]]], "golden_sentence": [[""], ["Therefore, an insurance company will provide a legal defense to the defendant and may settle with the plaintiff (victim)."], [""]]}, {"qid": "0987b291033883757db0", "term": "Deciduous", "description": "Trees or shrubs that lose their leaves seasonally", "question": "Are Christmas trees typically deciduous?", "answer": false, "facts": ["Christmas trees are almost always pine trees.", "Christmas trees are green all year round. "], "decomposition": ["What types of trees are used as Christmas trees?", "Are any of #1 deciduous?"], "evidence": [[[["Christmas tree-1"]], [["Deciduous-1"], "operation"]], [[["Christmas tree-1"]], [["Deciduous-1"], "operation"]], [[["Christmas tree-1"]], [["Evergreen-1"]]]], "golden_sentence": [["A Christmas tree is a decorated tree, usually an evergreen conifer, such as a spruce, pine or fir, or an artificial tree of similar appearance, associated with the celebration of Christmas, hence its name, originating in Northern Europe."], ["Deciduous forest In the fields of horticulture and botany, the term deciduous (/d\u026a\u02c8s\u026adju\u02d0\u0259s/; US: /d\u026a\u02c8s\u026ad\u0292u\u0259s/) means \"falling off at maturity\" and \"tending to fall off\", in reference to trees and shrubs that seasonally shed leaves, usually in the autumn; to the shedding of petals, after flowering; and to the shedding of ripe fruit."]]}, {"qid": "3fef3e534c155b9cd237", "term": "Swan Lake", "description": "Ballet by Pyotr Ilyich Tchaikovsky", "question": "Can you drown in a Swan Lake performance?", "answer": false, "facts": ["Drowning is defined as respiratory impairment as a result of being in or under a liquid.", "Swan Lake is not a body of water, but rather an example of performance dance."], "decomposition": ["What is a necessary condition for drowning?", "Does Swan Lake possess #1?"], "evidence": [[[["Drowning-1"]], [["Swan Lake-1"], "operation"]], [[["Drowning-1"]], [["Ballet-1", "Swan Lake-1"]]], [[["Drowning-2"]], ["no_evidence"]]], "golden_sentence": [["Drowning is defined as respiratory impairment as a result of being in or under a liquid."], ["Despite its initial failure, it is now one of the most popular ballets of all time."]]}, {"qid": "a99755e6f7b5fa164303", "term": "Spaghetti", "description": "Type of pasta", "question": "Is it unusual to eat spaghetti without a fork?", "answer": true, "facts": ["Spaghetti noodles are long and thin, they are difficult to scoop and must be twirled.", "Spaghetti is never served in a restaurant without a fork."], "decomposition": ["Is Spaghetti usually eaten using a fork?"], "evidence": [[[["Spaghetti-1"], "no_evidence"]], [[["Italian cuisine-17", "Spaghetti-1"], "no_evidence", "operation"]], [[["Spaghetti-15"], "no_evidence"]]], "golden_sentence": [[""]]}, {"qid": "8d3882499e0d6668b523", "term": "Narcissism", "description": "Personality trait of self love of a fake perfect self.", "question": "Is narcissism's origin a rare place to get modern words from?", "answer": false, "facts": ["Narcissism comes from the ancient Greek story of Narcissus, who fell in love with his own reflection.", "Aphrodisiac comes from stories about the ancient Greek goddess Aphrodite.", "Europe comes from Europa, an ancient Greek princess.", "The word stygian relates to the river of Hades in Greek mythology.", "Hypnosis comes from Hypnos, the Greek god of sleep."], "decomposition": ["From what culture does the word \"narcissism\" come? ", "What percent of English words come from #1?", "Is #2 small enough to be considered \"rare\"?"], "evidence": [[[["Narcissism-5"]], [["English words of Greek origin-34"]], ["operation"]], [[["Narcissism-1"]], [["English language-105", "English language-108", "English words of Greek origin-1"], "no_evidence"], ["operation"]], [[["Narcissism-5"]], [["English words of Greek origin-4"], "no_evidence"], ["operation"]]], "golden_sentence": [["The term \"narcissism\" comes from the Greek myth about Narcissus (Greek: \u039d\u03ac\u03c1\u03ba\u03b9\u03c3\u03c3\u03bf\u03c2, Narkissos), a handsome Greek youth who, according to Ovid, rejected the desperate advances of the nymph Echo."], ["In a typical English dictionary of 80,000 words, which corresponds very roughly to the vocabulary of an educated English speaker, about 5% of the words are borrowed from Greek."]]}, {"qid": "a99c7253b359d04f7954", "term": "Kobe", "description": "Designated city in Kansai, Japan", "question": "Is Kobe's famous animal product used in a BLT?", "answer": false, "facts": ["Kobe's famous animal product is Kobe beef.", "The animal product used in a BLT is bacon.", "Beef is derived from cows.", "Bacon is derived from pigs."], "decomposition": ["What animal product is Kobe, Japan most famous for?", "What animal product comes is used in a BLT?", "What animal does #1 come from?", "What animal does #2 come from?", "Is #3 the same as #4?"], "evidence": [[[["Kobe beef-3"]], [["BLT-1"]], [["Beef-57"]], [["Bacon-39"]], ["operation"]], [[["Kobe-3"]], [["BLT-1"]], [["Bacon-7", "Pork belly-1"]], [["Beef-1", "Cattle-1"]], ["operation"]], [[["Kobe beef-1"]], [["BLT-1"]], [["Japanese Black-1"]], [["Bacon-1", "Pork-1"]], ["operation"]]], "golden_sentence": [["Cattle were brought to Japan from China at the same time as the cultivation of rice, in about the second century AD, in the Yayoi period."], ["A BLT is a type of sandwich, named for the initials of its primary ingredients, bacon, lettuce and tomato."], [""], ["Many petitions and protests have been made trying to raise awareness and change how producers treat their pigs."]]}, {"qid": "f295b7f14b9a0b57ef6a", "term": "Chinese Americans", "description": "Ethnic group", "question": "Do Chinese Americans face discrimination at a Federal level in the US?", "answer": true, "facts": ["The President of the United States frequently referred to the COVID-19 pandemic as a 'Chinese Virus' and 'Kung Flu', encouraging the use of derogatory language towards Chinese Americans.", "The President of the United States has not called for the violence and hate towards Chinese Americans in response to COVID-19 to end."], "decomposition": ["Who is the head of the US Federal Government?", "Does #1 behave in a discriminatory way toward Chinese Americans?"], "evidence": [[[["Federal government of the United States-17"]], [["Anti-Chinese sentiment in the United States-28"], "no_evidence"]], [[["Donald Trump-1"]], [["Donald Trump-128"], "no_evidence", "operation"]], [[["Donald Trump-1"]], [["Donald Trump-128", "Donald Trump-154"], "operation"]]], "golden_sentence": [["The president is both the head of state and government, as well as the military commander-in-chief and chief diplomat."], [""]]}, {"qid": "d2bf007913a7c8a688b7", "term": "Asteroid", "description": "Minor planet that is not a comet", "question": "Could largest asteroid crush a whole city?", "answer": true, "facts": ["The largest asteroids are the size of miniature planets.", "Mercury is the smallest planet and has a radius of 1,516 miles.", "New York City is 13.4 miles long and 2.3 miles wide. ", "Mercury weighs 3.285 \u00d7 10^23 kg."], "decomposition": ["What is the size of the largest asteroid?", "What is the size of New York City?", "Is #1 bigger than #2?"], "evidence": [[[["Ceres (dwarf planet)-1"]], [["New York City-1"]], ["operation"]], [[["Asteroid-38"]], [["New York City-1"]], [["New York City-1", "Tunguska event-1"], "operation"]], [[["Ceres (dwarf planet)-1"]], [["New York metropolitan area-1"]], ["operation"]]], "golden_sentence": [["With a diameter of 940\u00a0km (580\u00a0mi), Ceres is both the largest of the asteroids and the only unambiguous dwarf planet currently inside Neptune's orbit."], ["With an estimated 2018 population of 8,398,748 distributed over about 302.6 square miles (784\u00a0km2), New York is also the most densely populated major city in the United States."]]}, {"qid": "397fe9b93751c24f028c", "term": "Evander Holyfield", "description": "American boxer", "question": "Did Mike Tyson do something very different than McGruff's slogan to Evander Holyfield in 1997?", "answer": false, "facts": ["McGruff was an animated dog spokesman for the National Crime Prevention Council.", "McGruff's slogan was, \"Take a bite out of crime.\"", "Mike Tyson was disqualified in a 1997 boxing bout against Evander Holyfield for taking a bite out of his ear."], "decomposition": ["What is the slogan of McGruff?", "What did Mike Tyson do to Evander Holyfield during their match?", "Is #2 an action that occurs in #1?"], "evidence": [[[["McGruff the Crime Dog-7"]], [["Evander Holyfield-4"]], ["operation"]], [[["McGruff the Crime Dog-7"]], [["Mike Tyson-37"]], ["operation"]], [[["McGruff the Crime Dog-7"]], [["Evander Holyfield vs. Mike Tyson II-1"]], ["operation"]]], "golden_sentence": [["After coming up with the slogan\u2014\"Take a bite out of crime\"\u2014he settled upon the idea of a dog."], ["Holyfield won a 1997 rematch against Tyson, which saw the latter disqualified in round three for biting off part of Holyfield's ear."]]}, {"qid": "58edc9c92d56f7bd8656", "term": "Mona Lisa", "description": "Painting by Leonardo da Vinci", "question": "After viewing the Mona Lisa, could you get lunch nearby on foot?", "answer": true, "facts": ["The Mona Lisa is housed in The Louvre.", "There are many restaurants within walking distance of The Louvre."], "decomposition": ["Where is the Mona Lisa located?", "Is #1 a place likely to have at least a restaurant/hotel nearby?"], "evidence": [[[["Mona Lisa-54"]], [["Louvre-60"]]], [[["Mona Lisa-29"]], [["Louvre-60"]]], [[["Louvre-1", "Mona Lisa-2"]], ["operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "b173adc36f9e28f752b2", "term": "Aloe", "description": "genus of plants", "question": "Do all parts of the aloe vera plant taste good?", "answer": false, "facts": ["There is a layer of yellow latex liquid between the outside of an aloe leaf and the gel inside.", "The latex inside aloe tastes very bitter."], "decomposition": ["How do the various parts of the Aloe vera taste?", "Is all of #1 pleasant?"], "evidence": [[[["Aloe vera-1"], "no_evidence"], [["Aloe vera-16"], "no_evidence", "operation"]], [[["Aloe vera-21"], "no_evidence"], ["operation"]], [[["Aloe vera-17"], "no_evidence"], [["Aloe vera-21"], "no_evidence", "operation"]]], "golden_sentence": [[""], [""]]}, {"qid": "b8dc9c2ff607db8f9975", "term": "Fever", "description": "common medical sign characterized by elevated body temperature", "question": "Is a fever cured by listening to a cowbell?", "answer": false, "facts": ["A fever is an increase in body temperature above the normal range", "Fever can be treated with medication or will usually disappear if left alone", "A cowbell is a musical instrument"], "decomposition": ["What are some common ways of treating a fever?", "Is listening to a cowbell included in #1?"], "evidence": [[[["Fever-2"]], [["Cowbell-1"], "operation"]], [[["Fever-37"]], [["Cowbell-1"]]], [[["Fever-2"]], ["operation"]]], "golden_sentence": [["Treatment of associated pain and inflammation, however, may be useful and help a person rest."], [""]]}, {"qid": "3fece7f54293c2083a56", "term": "Saint Kitts and Nevis", "description": "country in Central America and Caribbean", "question": "Are brown rock fish found in the waters surrounding Saint Kitts and Nevis?", "answer": false, "facts": ["Saint Kitts and Nevis is located in the Caribbean Sea and Atlantic Sea", "Brown rock fish are found in the Pacific Ocean"], "decomposition": ["What waters surround Saint Kitts and Nevis?", "In what body of water are brown rock fish found?", "Is #1 the same as #2?"], "evidence": [[[["Saint Kitts and Nevis-1"]], [["Brown rockfish-3"]], ["operation"]], [[["Saint Kitts-1"]], [["Brown rockfish-3"]], ["operation"]], [[["Saint Kitts-1"]], [["Brown rockfish-2"]], ["operation"]]], "golden_sentence": [[""], ["The brown rockfish is native to the northwestern Pacific."]]}, {"qid": "7c83d4a91a78dde3d0d6", "term": "Welfare", "description": "Means-oriented social benefit", "question": "Do Republicans reject all forms of welfare?", "answer": false, "facts": ["Welfare is all of the social programs that provide benefits to citizens for little or no money.", "Republicans have traditionally voted against welfare benefits in the form of food stamps and medicaid expansion.", "Public roads are a form of welfare since people are not required to build their own road each time they need to get to work."], "decomposition": ["What welfare policies are on the Republican platform?", "Are government-funded public works absent from #1?"], "evidence": [[[["Political positions of the Republican Party-3"]], [["Political positions of the Republican Party-3"]]], [[["Political positions of the Republican Party-3"]], ["operation"]], [[["Republican Party (United States)-22"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["To this end, they advocate in favor of laissez-faire economics, fiscal conservatism, and eliminating government run welfare programs in favor of private sector nonprofits and encouraging personal responsibility."], [""]]}, {"qid": "c8c0fd7218d99d7fc17c", "term": "Old English", "description": "Early form of English; Anglo-Saxon", "question": "Did the confederate states speak Old English before the Civil War?", "answer": false, "facts": ["Old English is the earliest form of English during the middle ages.", "Modern English replaced old English during the seventeenth century.", "American English was created and spoken during the formation of the first US colonies. ", "The civil war started in 1861, and was a war between the northern states and southern states. "], "decomposition": ["In approximately which years was Old English spoken?", "In which years did the Confederate States exist? ", "Was any part of #2 within #1?"], "evidence": [[[["Old English-1"]], [["Confederate States of America-1"]], ["operation"]], [[["Old English-6"]], [["Confederate States of America-9"]], ["operation"]], [[["Old English-1"]], [["Confederate States of America-1"]], ["operation"]]], "golden_sentence": [["Old English (\u00c6nglisc, Anglisc, Englisc, pronounced\u00a0[\u02c8\u00e6\u014b\u0261li\u0283]), or Anglo-Saxon, is the earliest historical form of the English language, spoken in England and southern and eastern Scotland in the early Middle Ages."], ["), commonly referred to as the Confederacy, was an unrecognized republic in North America that existed from 1861 to 1865."]]}, {"qid": "f7e359aa5262f34b3fcc", "term": "Armenians", "description": "ethnic group native to the Armenian Highland", "question": "Do Armenians tend to dislike System of a Down?", "answer": false, "facts": ["System of a Down is an Armenian-American rock band.", "System of a Down has numerous songs bringing light to the plight of Armenian people and the Armenian Genocide."], "decomposition": ["Is System of a Down an Armenian-American rock band?", "Would members of #1 rock band dislike where they are from?"], "evidence": [[[["System of a Down-1"]], [["Armenian Genocide in culture-37"], "no_evidence"]], [[["System of a Down-1"]], ["operation"]], [[["System of a Down-6"]], [["Patriotism-1"], "no_evidence", "operation"]]], "golden_sentence": [["System of a Down is an American heavy metal band formed in Glendale, California in 1994."], [""]]}, {"qid": "eb14ae9a5791fda5c61b", "term": "Green", "description": "Additive primary color visible between blue and yellow", "question": "Is a paleo dieter unlikely to color beverages green for St. Patrick's Day?", "answer": true, "facts": ["There is no natural source for green food coloring approved by the FDA", "A paleo diet avoids artificial colors and flavors"], "decomposition": ["What are some common FDA approved sources of green color applied to beverages?", "What kind of foods would a paleo dieter avoid?", "Is #1 included in #2?"], "evidence": [[[["Food coloring-11"]], [["Paleolithic diet-3"]], ["operation"]], [[["Food coloring-15"]], [["Paleolithic diet-12"]], [["Paleolithic diet-12"], "operation"]], [[["Fast Green FCF-1"]], [["Paleolithic diet-3"]], ["operation"]]], "golden_sentence": [["Examples of exempt colors include annatto, beet extract, caramel, beta-carotene, turmeric and grape skin extract."], ["While there is wide variability in the way the paleo diet is interpreted, the diet typically includes vegetables, fruits, nuts, roots, and meat and typically excludes foods such as dairy products, grains, sugar, legumes, processed oils, salt, alcohol, and coffee."]]}, {"qid": "8a13e4e10832055e48b2", "term": "French Defence", "description": "Chess opening", "question": "Can French Defence initial move defend against four move checkmate?", "answer": false, "facts": ["The French Defence involves moving pawn in front of the queen forward two spaces.", "The four move checkmate involves moving the queen and bishop to crowd the king.", "The four move checkmate cannot be defended by pawn in front of queen."], "decomposition": ["Which move is first played in the French defense in chess?", "What are some common techniques for making a four move checkmate in chess?", "Can #1 be used to defend against any of #2?"], "evidence": [[[["French Defence-3"]], [["Scholar's mate-10", "Scholar's mate-3"], "no_evidence"], ["operation"]], [[["French Defence-2"]], [["Scholar's mate-2"]], [["Scholar's mate-8"], "operation"]], [[["French Defence-2"], "no_evidence"], [["Scholar's mate-10", "Scholar's mate-2", "Scholar's mate-9"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["Following the opening moves 1.e4 e6, the game usually continues 2.d4 d5 (see below for alternatives)."], ["In some languages, including Catalan, Dutch, Esperanto, French, German, Czech, Latvian, Portuguese, Spanish and Turkish: Shepherd's Mate In Italian: Barber's Mate In Persian, Greek and Arabic: Napoleon's Plan In Belorussian, Latvian, Russian and Ukrainian\u00a0: Children's Mate In Croatian, Danish, German, Hebrew, Hungarian, Polish (where Fool's Mate is known as Scholar's Mate), Slovakian and Slovenian: Shoemaker's Mate In Danish, Finnish, Swedish and Norwegian: School Mate Scholar's Mate has sometimes also been given other names in English, such as Schoolboy's Mate (which in modern English perhaps better connotes the sense of 'novice' intended by the word Scholar's) and Blitzkrieg (German for \"lightning war\"), meaning a quick and short engagement (Kidder 1960).", ""]]}, {"qid": "634a2e92255355ec8e80", "term": "Pound sterling", "description": "Official currency of the United Kingdom and other territories", "question": "Was Emperor Commodus paid tribute in Pound sterling?", "answer": false, "facts": ["Commodus was Roman Emperor until 192 AD.", "Coins featuring the image of Commodus were the currency during the late second century AD.", "The Pound sterling has origins with the fifth century AD Anglo Saxon pound."], "decomposition": ["When was Commodus Roman emperor?", "When did the Pound sterling originate?", "Was #1 before #2?"], "evidence": [[[["Commodus-1"]], [["Pound sterling-19"]], ["operation"]], [[["Commodus-11"]], [["Pound sterling-22"]], ["operation"]], [[["Commodus-1"]], [["Pound sterling-19"]], ["operation"]]], "golden_sentence": [["Commodus (/\u02c8k\u0252m\u0259d\u0259s/; 31 August 161 \u2013 31 December 192), born Lucius Aurelius Commodus and died Lucius Aelius Aurelius Commodus, was Roman emperor with his father Marcus Aurelius from 177 until his father's death in 180, and solely until 192."], ["The origins of sterling lie in the reign of King Offa of Mercia (757\u2013796), who introduced the silver penny."]]}, {"qid": "6908a297967d48522d1a", "term": "3D printing", "description": "Additive process used to make a three-dimensional object", "question": "Do you need a large room if you want to get into 3D printing?", "answer": false, "facts": ["Home 3D printers are sized to be able to sit on a desk or table.", "The accessories and materials needed for 3D Printers can be stored easily and efficiently in a box or tote."], "decomposition": ["What are the equipment needed for 3D printing?", "How were #1 designed to be accommodated?", "Would #2 require a larger-than-average sized room?"], "evidence": [[[["3D printing-47"], "no_evidence"], [["3D printing-47"], "no_evidence"], [["3D printing-47"], "no_evidence"]], [[["3D printing-47"]], ["no_evidence"], ["no_evidence", "operation"]], [[["3D printing processes-39"]], [["3D printing processes-39"]], ["operation"]]], "golden_sentence": [["For example, General Electric uses high-end 3D Printers to build parts for turbines."], [""], [""]]}, {"qid": "357b5283f0e960e4f4e8", "term": "Second Coming", "description": "Christian and Islamic belief regarding the future (or past) return of Jesus after his ascension", "question": "Does Woody Allen await the Second Coming?", "answer": false, "facts": ["The Second Coming refers to Jesus Christ returning to earth", "Christians and Muslims believe in Jesus Christ", "Woody Allen is Jewish"], "decomposition": ["Which religious groups believe in the second coming?", "Does Woody Allen belong to any of #1?"], "evidence": [[[["Second Coming-1"]], [["Woody Allen-4"], "operation"]], [[["Second Coming-1"]], ["no_evidence", "operation"]], [[["Second Coming-1"]], [["Woody Allen-4"]]]], "golden_sentence": [["The Second Coming (sometimes called the Second Advent or the Parousia) is a Christian, Islamic, and Baha'i belief regarding the return of Jesus after his ascension to heaven about two thousand years ago."], [""]]}, {"qid": "c2cd854868e958e78400", "term": "Lust", "description": "Human emotion", "question": "Do you have to pass through circle of lust to find Saladin in Dante's Inferno?", "answer": false, "facts": ["Dante's Inferno was a book written by Dante Alighieri that outlines 9 circles of hell.", "The circle of lust is the second circle in Dante's Inferno.", "Saladin is placed in the first circle of hell in Dante's Inferno.", "The first circle of hell is limbo which is reserved for virtuous unbaptized pagans."], "decomposition": ["In Dante's Inferno, what circle is for people guilty of lust?", "In Dante's Inferno, what circle is Saladin in?", "Would someone traversing the Inferno pass through #2 before #1?"], "evidence": [[[["Inferno (Dante)-13"]], [["Inferno (Dante)-8", "Inferno (Dante)-9", "Limbo-1"]], ["operation"]], [[["Dante's Inferno (song)-3"]], [["Dante's Inferno: An Animated Epic-5"]], ["operation"]], [[["Inferno (Dante)-13"]], [["Inferno (Dante)-8", "Inferno (Dante)-9"]], ["operation"]]], "golden_sentence": [["In the second circle of Hell are those overcome by lust."], ["Canto IV Dante wakes up to find that he has crossed the Acheron, and Virgil leads him to the first circle of the abyss, Limbo, where Virgil himself resides.", "Dante also views Saladin, a Muslim military leader known for his struggle against the Crusaders as well as his generous, chivalrous, and merciful conduct.", ""]]}, {"qid": "eb6f5d08e9c0fbb58095", "term": "Intel", "description": "American semiconductor chip manufacturer", "question": "Could Intel products be purchased at McDonald's?", "answer": false, "facts": ["Intel is a technology company that produces computer products such as processors, chipsets, and GPUs.", "McDonald's is a fast food franchise that sells food and beverage products."], "decomposition": ["What type of products does Intel produce?", "What kind of products does McDonald's sell?", "Is #1 included in #2?"], "evidence": [[[["Intel-1"]], [["McDonald's-2"]], ["operation"]], [[["Intel-44"], "no_evidence"], [["McDonald's-29"], "operation"], ["no_evidence"]], [[["Intel-1"]], [["McDonald's-2"]], ["operation"]]], "golden_sentence": [[""], ["Although McDonald's is best known for its hamburgers, cheeseburgers and french fries, they feature chicken products, breakfast items, soft drinks, milkshakes, wraps, and desserts."]]}, {"qid": "4b26b3775c5f10ac9d69", "term": "Yellow pages", "description": "Telephone directory of businesses by category", "question": "Is the Yellow Pages the fastest way to find a phone number?", "answer": false, "facts": ["The Yellow Pages is a book that contains alphabetized phone listings.", "Yellow pages involves going through many listings and remembering your alphabet.", "Google allows a person to type in a name quickly and look for a phone number.", "Household AI assistants like Echo allow people to merely speak a name and ask for number."], "decomposition": ["How are the phone numbers organized in the Yellow Pages?", "To find a phone number in #1, what does one have to do?", "To find a phone number on Google, what does one have to do?", "Is #2 faster than #3?"], "evidence": [[[["Yellow pages-1"]], [["Yellow pages-5"], "no_evidence"], [["Google Search-3"]], ["operation"]], [[["Yellow pages-1"]], [["Yellow pages-5"]], [["Google Search-16"]], ["operation"]], [[["Yellow pages-1"]], ["no_evidence"], [["Web search engine-1"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["The yellow pages are any telephone directory of businesses, organised by category rather than alphabetically by business name, and in which advertising is sold."], [""], [""]]}, {"qid": "6c683ad118b4376d89fb", "term": "Shinto", "description": "Ethnic religion of Japan", "question": "Do Shinto practitioners keep to a kosher diet?", "answer": false, "facts": ["Shinto is a religion originating from Japan.", "Shinto does not have any dietary restrictions.", "Seafood including shellfish is a staple of the Japanese diet.", "Jewish kosher laws prohibit the consumption of shellfish."], "decomposition": ["Which type of seafood does the Jewish kosher laws prohibit?", "Are Shinto practitioners restricted from eating #1?"], "evidence": [[[["Kashrut-3"]], [["Seafood-17", "Shinto-1"], "no_evidence", "operation"]], [[["Kashrut-3"]], ["no_evidence", "operation"]], [[["Seafood-33"]], [["Shinto-65"], "no_evidence", "operation"]]], "golden_sentence": [["Only certain types of mammals, birds and fish meeting specific criteria are kosher; the consumption of the flesh of any animals that do not meet these criteria, such as pork and shellfish, is forbidden Kosher mammals and birds must be slaughtered according to a process known as shechita; blood may never be consumed and must be removed from meat by a process of salting and soaking in water for the meat to be permissible for use Meat and meat derivatives may never be mixed with milk and milk derivatives: separate equipment for the storage and preparation of meat-based and dairy-based foods must be used Every food that is considered kosher is also categorized as follows:"], ["", "There is no central authority in control of the movement and much diversity exists among practitioners."]]}, {"qid": "af744c7337c0e502b326", "term": "Waiting staff", "description": "staff serving in restaurant or private homes", "question": "Are there some countries where waiting staff need no tip?", "answer": true, "facts": ["In Japan, leaving a tip for a server is considered rude.", "In Denmark, servers and wait staff are well paid and tipping is very uncommon."], "decomposition": ["In how many countries is it socially acceptable to not tip the waiting staff?", "Is #1 greater than one?"], "evidence": [[[["Gratuity-15", "Gratuity-18", "Gratuity-36", "Gratuity-48"]], ["operation"]], [[["Gratuity-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Gratuity-19"]], ["operation"]]], "golden_sentence": [["", "", "", ""]]}, {"qid": "15e09f51d8541c1d0b23", "term": "Hamster", "description": "subfamily of mammals", "question": "Could a hamster experience two leap years?", "answer": false, "facts": ["Pet hamsters typically have a maximum lifespan of three years.", "Leap years are typically separated by four years."], "decomposition": ["How long is the lifespan of a hamster?", "How many years are between two leap years?", "Is #1 longer than #2?"], "evidence": [[[["Hamster-27"]], [["Leap year-16"]], ["operation"]], [[["Hamster-27"]], [["Leap year-16"]], ["operation"]], [[["Hamster-27"]], [["Leap year-6"]], ["operation"]]], "golden_sentence": [["The smaller Roborovski hamster often lives to three years in captivity."], [""]]}, {"qid": "3254d8c3f80f874c086e", "term": "Citrus", "description": "genus of fruit-bearing plants (source of fruit such as lemons and oranges)", "question": "Is there a Marvel villain with the same name as a kind of citrus fruit?", "answer": true, "facts": ["Mandarins are a type of orange popular in Asian cuisine.", "The Mandarin is also the name of a villain associated with Iron Man in the Marvel universe."], "decomposition": ["Which popular villains has Marvel's Ironman faced off against?", "Do any of #1's name also refer to a citrus fruit?"], "evidence": [[[["Iron Man-24"]], [["Mandarin orange-1"]]], [[["Iron Man-24"]], [["Mandarin orange-1"], "operation"]], [[["Captain Citrus-1"], "no_evidence"], ["no_evidence", "operation"]]], "golden_sentence": [["To that end, Iron Man fights threats to his company (e.g., Communist opponents Black Widow, the Crimson Dynamo, and the Titanium Man), as well as independent villains like the Mandarin (who becomes his greatest enemy)."], ["The mandarin orange (Citrus reticulata), also known as the mandarin or mandarine, is a small citrus tree with fruit resembling other oranges, usually eaten plain or in fruit salads."]]}, {"qid": "2c9b69cfa497edec86fb", "term": "Selfie", "description": "Photographic self-portrait", "question": "Are selfies more dangerous than plague in modern times?", "answer": true, "facts": ["There are an average of 7 human plague cases reported each year according to the CDC.", "Selfies have caused people to fall off of cliffs while trying to get the perfect picture.", "From October 2011 and November 2017, there were 259 selfie deaths in 137 incidents."], "decomposition": ["How many cases of the plague are there yearly?", "How many people die yearly while taking selfies?", "Is #2 greater than #1?"], "evidence": [[[["Epidemiology of plague-23"]], ["no_evidence"], ["operation"]], [[["Epidemiology of plague-23"], "operation"], ["no_evidence"], ["no_evidence"]], [[["Epidemiology of plague-1"]], [["Selfie-53"], "no_evidence"], ["operation"]]], "golden_sentence": [["On 1 July 2010, eight cases of Bubonic plague were reported in humans in the District of Chicama, Peru."]]}, {"qid": "4af68d4552370d2a540a", "term": "Catfish", "description": "order of fishes", "question": "Is a cory catfish likely to eat another living fish?", "answer": false, "facts": ["The cory catfish is a fish that is described as a bottom feeder.", "The cory catfish feeds on food located at the bottom of an ocean.", "Fish cannot live too deep in oceans for very long because of the intense water pressure.", "The bottom of oceans is populated by algae, coral, and microorganisms."], "decomposition": ["What do cory catfish eat?", "Is fish part of #1?"], "evidence": [[[["Corydoras-5"]], [["Corydoras-5"], "operation"]], [[["Corydoras-5"]], ["operation"]], [[["Corydoras-4", "Corydoras-5"]], ["operation"]]], "golden_sentence": [["Although no corys are piscivorous, they will eat flesh from dead fishes."], ["Although no corys are piscivorous, they will eat flesh from dead fishes."]]}, {"qid": "ab89974b71d4e5351dd4", "term": "Achilles", "description": "Greek mythological hero", "question": "Does Thiago Moises May 13 2020 submission move hypothetically hurt Achilles?", "answer": true, "facts": ["Thiago Moises is a mixed martial arts fighter in the UFC.", "Thiago Moises beat Michael Johnson by a heel hook submission.", "Greek hero Achilles had one weakness, his heel."], "decomposition": [" What was Thiago Moises' winning move the match he played on May 13 2020?", "Which part of the opponent's body did #1 affect?", "Which part of Achilles' body is his weaknes?", "Is #2 the same as #3?"], "evidence": [[["no_evidence"], ["no_evidence"], [["Achilles-2"]], ["operation"]], [["no_evidence"], [["Heel-1"]], [["Achilles-2"]], ["operation"]], [["no_evidence"], [["Paul Sass-6"], "no_evidence"], [["Achilles-2"]], ["operation"]]], "golden_sentence": [["Later legends (beginning with Statius' unfinished epic Achilleid, written in the 1st century AD) state that Achilles was invulnerable in all of his body except for his heel because, when his mother Thetis dipped him in the river Styx as an infant, she held him by one of his heels."]]}, {"qid": "6cc901006d4a5607c324", "term": "Alec Baldwin", "description": "American actor, writer, producer, and comedian", "question": "Does Alec Baldwin have more children than Clint Eastwood?", "answer": false, "facts": ["Alec Baldwin has 5 children as of 2020.", "Actor Clint Eastwood has 7 children as of 2020."], "decomposition": ["How many children does Alec Baldwin presently have?", "How many children does Clint Eastwood presently have?", "Is #1 more than #2?"], "evidence": [[[["Alec Baldwin-42", "Alec Baldwin-47"]], [["Clint Eastwood-65", "Clint Eastwood-66", "Clint Eastwood-67"]], ["operation"]], [[["Alec Baldwin-47"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Alec Baldwin-47", "Kim Basinger-24"]], [["Clint Eastwood-66", "Clint Eastwood-67", "Clint Eastwood-68", "Personal life of Clint Eastwood-1"], "no_evidence"], ["operation"]]], "golden_sentence": [["", "They have four children together, daughter Carmen (born August 23, 2013), and sons Rafael (born June 17, 2015), Leonardo Angel Charles (born September 2016) and Romeo Alejandro David (born May 2018)."], ["", "Johnson evidently tolerated the open marriage with Eastwood, and eventually they had two children, Kyle (born 1968) and Alison (born 1972).", "In an unpublicized affair, Eastwood sired two legally fatherless children, Scott (born 1986) and Kathryn (born 1988) with Jacelyn Reeves, a flight attendant."]]}, {"qid": "17d2380f802412e9c4f8", "term": "Prime Minister of the United Kingdom", "description": "Head of UK Government", "question": "Does highest US Court have enough seats for every Prime Minister of the United Kingdom since 1952?", "answer": false, "facts": ["The highest court in the US is the Supreme Court.", "There are nine seats on the Supreme Court.", "There have been fifteen Prime Ministers of the United Kingdom since 1952."], "decomposition": ["What is the highest United States court?", "How many positions  are there in #1?", "How many United Kingdom Prime Ministers have there been since 1952?", "Is #2 equal to or greater than #3?"], "evidence": [[[["Supreme Court of the United States-1"]], [["Supreme Court of the United States-20"]], [["Anthony Eden-1", "Boris Johnson-1"], "no_evidence"], ["operation"]], [[["Supreme Court of the United States-1"]], [["Supreme Court of the United States-20"]], [["Alec Douglas-Home-1", "Anthony Eden-1", "Boris Johnson-1", "David Cameron-1", "Edward Heath-1", "Gordon Brown-1", "Harold Macmillan-1", "Harold Wilson-1", "James Callaghan-1", "John Major-1", "Margaret Thatcher-1", "Theresa May-1", "Tony Blair-1", "Winston Churchill-1"]], ["operation"]], [[["Supreme Court of the United States-1"]], [["Supreme Court of the United States-20"]], ["no_evidence"], ["no_evidence"]]], "golden_sentence": [["The Supreme Court of the United States (SCOTUS) is the highest court in the federal judiciary of the United States of America."], ["In 1866, at the behest of Chief Justice Chase and in an attempt to limit the power of Andrew Johnson, Congress passed an act providing that the next three justices to retire would not be replaced, which would thin the bench to seven justices by attrition."], ["", ""]]}]